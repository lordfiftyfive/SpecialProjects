{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "/root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info.\n",
      "  warnings.warn(\"WARNING: failed to get cudart_version from onnxruntime build info.\")\n",
      "/root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/nebulgym/training_learners/model_engines/rammer_engine.py:16: UserWarning: No valid Rammer installation found. Using the Rammer backend will result in an error.\n",
      "  warnings.warn(\n",
      "2024-01-22 15:10:16,968\tWARNING __init__.py:10 -- QMIX has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.6.0, llvm 15.0.4, commit f1c6fbbd, linux, python 3.10.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 01/22/24 15:10:18.977 515348] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "/root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "#mo_gym is for creating enviroments with multiple reward functions simultaneously \n",
    "import mo_gymnasium as mo_gym#we are going to phase out gym in favor of mo_gym\n",
    "\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE, deprecation_warning, Deprecated\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "#from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "import functools\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "if we do this carefully we can use taichi to carry out speedup\n",
    "\n",
    "The mixer and the bmodel would be ti.funcs\n",
    "\n",
    "qmixpolicy would also be a ti.func\n",
    "\n",
    "qmix would be the ti.kernel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import torch\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2, _unpack_obs\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "#from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "#from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "#from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE, make_multi_agent\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import Predictive, SVI, Trace_ELBO#TracePosterior\n",
    "import mlflow #deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dadaptation import DAdaptAdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.ode import odeint\n",
    "#\n",
    "#from jax.random import PRNGKey\n",
    "\n",
    "import numpyro\n",
    "from numpyro import *\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.examples.datasets import LYNXHARE, load_dataset\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "from numpyro.infer import Predictive, SVI, Trace_ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import chex\n",
    "import numpy as np\n",
    "from flax import struct\n",
    "from functools import partial\n",
    "\n",
    "# from gymnax.environments import environment, spaces\n",
    "from gymnax.environments.spaces import Box as BoxGymnax, Discrete as DiscreteGymnax\n",
    "from typing import Optional, List, Tuple, Union\n",
    "from jaxmarl.environments.spaces import Box, Discrete, MultiDiscrete\n",
    "from jaxmarl.environments.multi_agent_env import MultiAgentEnv, State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is an end-to-end implemntation of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxMARLWrapper(object):\n",
    "    \"\"\"Base class for all jaxmarl wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env: MultiAgentEnv):\n",
    "        self._env = env\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    # def _batchify(self, x: dict):\n",
    "    #     x = jnp.stack([x[a] for a in self._env.agents])\n",
    "    #     return x.reshape((self._env.num_agents, -1))\n",
    "\n",
    "    def _batchify_floats(self, x: dict):\n",
    "        return jnp.stack([x[a] for a in self._env.agents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTRolloutManager(JaxMARLWrapper):\n",
    "    \"\"\"\n",
    "    Rollout Manager for Centralized Training of with Parameters Sharing. Used by JaxMARL Q-Learning Baselines.\n",
    "    - Batchify multiple environments (the number of parallel envs is defined by batch_size in __init__).\n",
    "    - Adds a global state (obs[\"__all__\"]) and a global reward (rewards[\"__all__\"]) in the env.step returns.\n",
    "    - Pads the observations of the agents in order to have all the same length.\n",
    "    - Adds an agent id (one hot encoded) to the observation vectors.\n",
    "\n",
    "    By default:\n",
    "    - global_state is the concatenation of all agents' observations.\n",
    "    - global_reward is the sum of all agents' rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: MultiAgentEnv, batch_size:int, training_agents:List=None, preprocess_obs:bool=True):\n",
    "        \n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # the agents to train could differ from the total trainable agents in the env (f.i. if using pretrained agents)\n",
    "        # it's important to know it in order to compute properly the default global rewards and state\n",
    "        self.training_agents = self.agents if training_agents is None else training_agents  \n",
    "        self.preprocess_obs = preprocess_obs  \n",
    "\n",
    "        # TOREMOVE: this is because overcooked doesn't follow other envs conventions\n",
    "        if len(env.observation_spaces) == 0:\n",
    "            self.observation_spaces = {agent:self.observation_space() for agent in self.agents}\n",
    "        if len(env.action_spaces) == 0:\n",
    "            self.action_spaces = {agent:env.action_space() for agent in self.agents}\n",
    "        \n",
    "        # batched action sampling\n",
    "        self.batch_samplers = {agent: jax.jit(jax.vmap(self.action_space(agent).sample, in_axes=0)) for agent in self.agents}\n",
    "\n",
    "        # assumes the observations are flattened vectors\n",
    "        self.max_obs_length = max(list(map(lambda x: get_space_dim(x), self.observation_spaces.values())))\n",
    "        self.max_action_space = max(list(map(lambda x: get_space_dim(x), self.action_spaces.values())))\n",
    "        self.obs_size = self.max_obs_length + len(self.agents)\n",
    "\n",
    "        # agents ids\n",
    "        self.agents_one_hot = {a:oh for a, oh in zip(self.agents, jnp.eye(len(self.agents)))}\n",
    "        # valid actions\n",
    "        self.valid_actions = {a:jnp.arange(u.n) for a, u in self.action_spaces.items()}\n",
    "        self.valid_actions_oh ={a:jnp.concatenate((jnp.ones(u.n), jnp.zeros(self.max_action_space - u.n))) for a, u in self.action_spaces.items()}\n",
    "\n",
    "        # custom global state and rewards for specific envs\n",
    "        if 'smax' in env.name.lower():\n",
    "            self.global_state = lambda obs, state: obs['world_state']\n",
    "            self.global_reward = lambda rewards: rewards[self.training_agents[0]]\n",
    "        elif 'hanabi' in env.name.lower():\n",
    "            self.global_state = self.hanabi_world_state\n",
    "        elif 'overcooked' in env.name.lower():\n",
    "            self.global_state = lambda obs, state:  jnp.concatenate([obs[agent].ravel() for agent in self.agents], axis=-1)\n",
    "            self.global_reward = lambda rewards: rewards[self.training_agents[0]]\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def batch_reset(self, key):\n",
    "        keys = jax.random.split(key, self.batch_size)\n",
    "        return jax.vmap(self.wrapped_reset, in_axes=0)(keys)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def batch_step(self, key, states, actions):\n",
    "        keys = jax.random.split(key, self.batch_size)\n",
    "        return jax.vmap(self.wrapped_step, in_axes=(0, 0, 0))(keys, states, actions)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def wrapped_reset(self, key):\n",
    "        obs_, state = self._env.reset(key)\n",
    "        if self.preprocess_obs:\n",
    "            obs = jax.tree_util.tree_map(self._preprocess_obs, {agent:obs_[agent] for agent in self.agents}, self.agents_one_hot)\n",
    "        else:\n",
    "            obs = obs_\n",
    "        obs[\"__all__\"] = self.global_state(obs_, state)\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def wrapped_step(self, key, state, actions):\n",
    "        if 'hanabi' in self._env.name.lower():\n",
    "            actions = jax.tree_util.tree_map(lambda x:jnp.expand_dims(x, 0), actions)\n",
    "        obs_, state, reward, done, infos = self._env.step(key, state, actions)\n",
    "        if self.preprocess_obs:\n",
    "            obs = jax.tree_util.tree_map(self._preprocess_obs, {agent:obs_[agent] for agent in self.agents}, self.agents_one_hot)\n",
    "            obs = jax.tree_util.tree_map(lambda d, o: jnp.where(d, 0., o), {agent:done[agent] for agent in self.agents}, obs) # ensure that the obs are 0s for done agents\n",
    "        else:\n",
    "            obs = obs_\n",
    "        obs[\"__all__\"] = self.global_state(obs_, state)\n",
    "        reward[\"__all__\"] = self.global_reward(reward)\n",
    "        return obs, state, reward, done, infos\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def global_state(self, obs, state):\n",
    "        return jnp.concatenate([obs[agent] for agent in self.agents], axis=-1)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def global_reward(self, reward):\n",
    "        return jnp.stack([reward[agent] for agent in self.training_agents]).sum(axis=0) \n",
    "    \n",
    "    def batch_sample(self, key, agent):\n",
    "        return self.batch_samplers[agent](jax.random.split(key, self.batch_size)).astype(int)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _preprocess_obs(self, arr, extra_features):\n",
    "        # flatten\n",
    "        arr = arr.flatten()\n",
    "        # pad the observation vectors to the maximum length\n",
    "        pad_width = [(0, 0)] * (arr.ndim - 1) + [(0, max(0, self.max_obs_length - arr.shape[-1]))]\n",
    "        arr = jnp.pad(arr, pad_width, mode='constant', constant_values=0)\n",
    "        # concatenate the extra features\n",
    "        arr = jnp.concatenate((arr, extra_features), axis=-1)\n",
    "        return arr\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def hanabi_world_state(self, obs, state):\n",
    "        \"\"\" \n",
    "        For each agent: [agent obs, own hand]\n",
    "        \"\"\"\n",
    "        all_obs = jnp.array([obs[agent] for agent in self._env.agents])\n",
    "        hands = state.env_state.player_hands.reshape((self._env.num_agents, -1))\n",
    "        return jnp.concatenate((all_obs, hands), axis=1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "opt = optax.contrib.prodigy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem below to fix\n",
    "\n",
    "https://stackoverflow.com/questions/48796169/how-to-fix-ipykernel-launcher-py-error-unrecognized-arguments-in-jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides ...]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/root/.local/share/jupyter/runtime/kernel-v2-9569Mb8b44hMTVR.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "End-to-End JAX Implementation of QMix.\n",
    "\n",
    "Notice:\n",
    "- Agents are controlled by a single RNN architecture.\n",
    "- You can choose if sharing parameters between agents or not.\n",
    "- Works also with non-homogenous agents (different obs/action spaces)\n",
    "- Experience replay is a simple buffer with uniform sampling.\n",
    "- Uses Double Q-Learning with a target agent network (hard-updated).\n",
    "- You can use TD Loss (pymarl2) or DDQN loss (pymarl)\n",
    "- Adam optimizer is used instead of RMSPROP.\n",
    "- The environment is reset at the end of each episode.\n",
    "- Trained with a team reward (reward['__all__'])\n",
    "- At the moment, last_actions are not included in the agents' observations.\n",
    "\n",
    "The implementation closely follows the original Pymarl: https://github.com/oxwhirl/pymarl/blob/master/src/learners/q_learner.py\n",
    "\"\"\"\n",
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import NamedTuple, Dict, Union\n",
    "\n",
    "import chex\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from flax.training.train_state import TrainState\n",
    "import flashbax as fbx\n",
    "from flax.core import frozen_dict\n",
    "import wandb\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from safetensors.flax import save_file\n",
    "from flax.traverse_util import flatten_dict\n",
    "\n",
    "from jaxmarl import make\n",
    "from jaxmarl.wrappers.baselines import LogWrapper, SMAXLogWrapper#, CTRolloutManager\n",
    "from jaxmarl.environments.smax import map_name_to_scenario\n",
    "from jaxmarl.environments.overcooked import overcooked_layouts\n",
    "\n",
    "\n",
    "class ScannedRNN(nn.Module):\n",
    "\n",
    "    @partial(\n",
    "        nn.scan,\n",
    "        variable_broadcast=\"params\",\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        split_rngs={\"params\": False},\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"Applies the module.\"\"\"\n",
    "        rnn_state = carry\n",
    "        ins, resets = x\n",
    "        hidden_size = ins.shape[-1]\n",
    "        rnn_state = jnp.where(\n",
    "            resets[:, np.newaxis],\n",
    "            self.initialize_carry(hidden_size, *ins.shape[:-1]),\n",
    "            rnn_state,\n",
    "        )\n",
    "        new_rnn_state, y = nn.GRUCell(hidden_size)(rnn_state, ins)\n",
    "        return new_rnn_state, y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(hidden_size, *batch_size):\n",
    "        # Use a dummy key since the default state init fn is just zeros.\n",
    "        return nn.GRUCell(hidden_size, parent=None).initialize_carry(\n",
    "            jax.random.PRNGKey(0), (*batch_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    \n",
    "class AgentRNN(nn.Module):\n",
    "    # homogenous agent for parameters sharing, assumes all agents have same obs and action dim\n",
    "    action_dim: int\n",
    "    hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        obs, dones = x\n",
    "        embedding = nn.Dense(self.hidden_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "        \n",
    "        q_vals = nn.Dense(self.action_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(embedding)\n",
    "\n",
    "        return hidden, q_vals\n",
    "    \n",
    "\n",
    "class HyperNetwork(nn.Module):\n",
    "    \"\"\"HyperNetwork for generating weights of QMix' mixing network.\"\"\"\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.hidden_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.))(x)\n",
    "        return x\n",
    "\n",
    "class MixingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixing network for projecting individual agent Q-values into Q_tot. Follows the original QMix implementation.\n",
    "    \"\"\"\n",
    "    embedding_dim: int\n",
    "    hypernet_hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, q_vals, states):\n",
    "        \n",
    "        n_agents, time_steps, batch_size = q_vals.shape\n",
    "        q_vals = jnp.transpose(q_vals, (1, 2, 0)) # (time_steps, batch_size, n_agents)\n",
    "        \n",
    "        # hypernetwork\n",
    "        w_1 = HyperNetwork(hidden_dim=self.hypernet_hidden_dim, output_dim=self.embedding_dim*n_agents, init_scale=self.init_scale)(states)\n",
    "        b_1 = nn.Dense(self.embedding_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.))(states)\n",
    "        w_2 = HyperNetwork(hidden_dim=self.hypernet_hidden_dim, output_dim=self.embedding_dim, init_scale=self.init_scale)(states)\n",
    "        b_2 = HyperNetwork(hidden_dim=self.embedding_dim, output_dim=1, init_scale=self.init_scale)(states)\n",
    "        \n",
    "        # monotonicity and reshaping\n",
    "        w_1 = jnp.abs(w_1.reshape(time_steps, batch_size, n_agents, self.embedding_dim))\n",
    "        b_1 = b_1.reshape(time_steps, batch_size, 1, self.embedding_dim)\n",
    "        w_2 = jnp.abs(w_2.reshape(time_steps, batch_size, self.embedding_dim, 1))\n",
    "        b_2 = b_2.reshape(time_steps, batch_size, 1, 1)\n",
    "    \n",
    "        # mix\n",
    "        hidden = nn.elu(jnp.matmul(q_vals[:, :, None, :], w_1) + b_1)\n",
    "        q_tot  = jnp.matmul(hidden, w_2) + b_2\n",
    "        \n",
    "        return q_tot.squeeze() # (time_steps, batch_size)\n",
    "\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    \"\"\"Epsilon Greedy action selection\"\"\"\n",
    "\n",
    "    def __init__(self, start_e: float, end_e: float, duration: int):\n",
    "        self.start_e  = start_e\n",
    "        self.end_e    = end_e\n",
    "        self.duration = duration\n",
    "        self.slope    = (end_e - start_e) / duration\n",
    "        \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def get_epsilon(self, t: int):\n",
    "        e = self.slope*t + self.start_e\n",
    "        return jnp.clip(e, self.end_e)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def choose_actions(self, q_vals: dict, t: int, rng: chex.PRNGKey):\n",
    "        \n",
    "        def explore(q, eps, key):\n",
    "            key_a, key_e   = jax.random.split(key, 2) # a key for sampling random actions and one for picking\n",
    "            greedy_actions = jnp.argmax(q, axis=-1) # get the greedy actions \n",
    "            random_actions = jax.random.randint(key_a, shape=greedy_actions.shape, minval=0, maxval=q.shape[-1]) # sample random actions\n",
    "            pick_random    = jax.random.uniform(key_e, greedy_actions.shape)<eps # pick which actions should be random\n",
    "            chosed_actions = jnp.where(pick_random, random_actions, greedy_actions)\n",
    "            return chosed_actions\n",
    "        \n",
    "        eps = self.get_epsilon(t)\n",
    "        keys = dict(zip(q_vals.keys(), jax.random.split(rng, len(q_vals)))) # get a key for each agent\n",
    "        chosen_actions = jax.tree_map(lambda q, k: explore(q, eps, k), q_vals, keys)\n",
    "        return chosen_actions\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    obs: dict\n",
    "    actions: dict\n",
    "    rewards: dict\n",
    "    dones: dict\n",
    "    infos: dict\n",
    "\n",
    "\n",
    "def make_train(config, env):\n",
    "\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    def train(rng):\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        wrapped_env = CTRolloutManager(env, batch_size=config[\"NUM_ENVS\"])\n",
    "        test_env = CTRolloutManager(env, batch_size=config[\"NUM_TEST_EPISODES\"]) # batched env for testing (has different batch size)\n",
    "        init_obs, env_state = wrapped_env.batch_reset(_rng)\n",
    "        init_dones = {agent:jnp.zeros((config[\"NUM_ENVS\"]), dtype=bool) for agent in env.agents+['__all__']}\n",
    "\n",
    "        # INIT BUFFER\n",
    "        # to initalize the buffer is necessary to sample a trajectory to know its strucutre\n",
    "        def _env_sample_step(env_state, unused):\n",
    "            rng, key_a, key_s = jax.random.split(jax.random.PRNGKey(0), 3) # use a dummy rng here\n",
    "            key_a = jax.random.split(key_a, env.num_agents)\n",
    "            actions = {agent: wrapped_env.batch_sample(key_a[i], agent) for i, agent in enumerate(env.agents)}\n",
    "            obs, env_state, rewards, dones, infos = wrapped_env.batch_step(key_s, env_state, actions)\n",
    "            transition = Transition(obs, actions, rewards, dones, infos)\n",
    "            return env_state, transition\n",
    "        _, sample_traj = jax.lax.scan(\n",
    "            _env_sample_step, env_state, None, config[\"NUM_STEPS\"]\n",
    "        )\n",
    "        sample_traj_unbatched = jax.tree_map(lambda x: x[:, 0], sample_traj) # remove the NUM_ENV dim\n",
    "        buffer = fbx.make_trajectory_buffer(\n",
    "            max_length_time_axis=config['BUFFER_SIZE']//config['NUM_ENVS'],\n",
    "            min_length_time_axis=config['BUFFER_BATCH_SIZE'],\n",
    "            sample_batch_size=config['BUFFER_BATCH_SIZE'],\n",
    "            add_batch_size=config['NUM_ENVS'],\n",
    "            sample_sequence_length=1,\n",
    "            period=1,\n",
    "        )\n",
    "        buffer_state = buffer.init(sample_traj_unbatched) \n",
    "\n",
    "        # INIT NETWORK\n",
    "        # init agent\n",
    "        agent = AgentRNN(action_dim=wrapped_env.max_action_space, hidden_dim=config[\"AGENT_HIDDEN_DIM\"], init_scale=config['AGENT_INIT_SCALE'])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        if config[\"PARAMETERS_SHARING\"]:\n",
    "            init_x = (\n",
    "                jnp.zeros((1, 1, wrapped_env.obs_size)), # (time_step, batch_size, obs_size)\n",
    "                jnp.zeros((1, 1)) # (time_step, batch size)\n",
    "            )\n",
    "            init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], 1) # (batch_size, hidden_dim)\n",
    "            agent_params = agent.init(_rng, init_hs, init_x)\n",
    "        else:\n",
    "            init_x = (\n",
    "                jnp.zeros((len(env.agents), 1, 1, wrapped_env.obs_size)), # (time_step, batch_size, obs_size)\n",
    "                jnp.zeros((len(env.agents), 1, 1)) # (time_step, batch size)\n",
    "            )\n",
    "            init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents),  1) # (n_agents, batch_size, hidden_dim)\n",
    "            rngs = jax.random.split(_rng, len(env.agents)) # a random init for each agent\n",
    "            agent_params = jax.vmap(agent.init, in_axes=(0, 0, 0))(rngs, init_hs, init_x)\n",
    "\n",
    "        # init mixer\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros((len(env.agents), 1, 1))\n",
    "        state_size = sample_traj.obs['__all__'].shape[-1]  # get the state shape from the buffer\n",
    "        init_state = jnp.zeros((1, 1, state_size))\n",
    "        mixer = MixingNetwork(config['MIXER_EMBEDDING_DIM'], config[\"MIXER_HYPERNET_HIDDEN_DIM\"], config['MIXER_INIT_SCALE'])\n",
    "        mixer_params = mixer.init(_rng, init_x, init_state)\n",
    "\n",
    "        # init optimizer\n",
    "        network_params = frozen_dict.freeze({'agent':agent_params, 'mixer':mixer_params})\n",
    "        def linear_schedule(count):\n",
    "            frac = 1.0 - (count / config[\"NUM_UPDATES\"])\n",
    "            return config[\"LR\"] * frac\n",
    "        lr = linear_schedule if config.get('LR_LINEAR_DECAY', False) else config['LR']\n",
    "        tx = optax.contrib.prodigy()\n",
    "        \"\"\"\n",
    "        optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adamw(learning_rate=lr, eps=config['EPS_ADAM'], weight_decay=config['WEIGHT_DECAY_ADAM']),\n",
    "        )\n",
    "        \"\"\"\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=None,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "        # target network params\n",
    "        target_network_params = jax.tree_map(lambda x: jnp.copy(x), train_state.params)\n",
    "\n",
    "        # INIT EXPLORATION STRATEGY\n",
    "        explorer = EpsilonGreedy(\n",
    "            start_e=config[\"EPSILON_START\"],\n",
    "            end_e=config[\"EPSILON_FINISH\"],\n",
    "            duration=config[\"EPSILON_ANNEAL_TIME\"]\n",
    "        )\n",
    "\n",
    "        # depending if using parameters sharing or not, q-values are computed using one or multiple parameters\n",
    "        if config[\"PARAMETERS_SHARING\"]:\n",
    "            def homogeneous_pass(params, hidden_state, obs, dones):\n",
    "                # concatenate agents and parallel envs to process them in one batch\n",
    "                agents, flatten_agents_obs = zip(*obs.items())\n",
    "                original_shape = flatten_agents_obs[0].shape # assumes obs shape is the same for all agents\n",
    "                batched_input = (\n",
    "                    jnp.concatenate(flatten_agents_obs, axis=1), # (time_step, n_agents*n_envs, obs_size)\n",
    "                    jnp.concatenate([dones[agent] for agent in agents], axis=1), # ensure to not pass other keys (like __all__)\n",
    "                )\n",
    "                hidden_state, q_vals = agent.apply(params, hidden_state, batched_input)\n",
    "                q_vals = q_vals.reshape(original_shape[0], len(agents), *original_shape[1:-1], -1) # (time_steps, n_agents, n_envs, action_dim)\n",
    "                q_vals = {a:q_vals[:,i] for i,a in enumerate(agents)}\n",
    "                return hidden_state, q_vals\n",
    "        else:\n",
    "            def homogeneous_pass(params, hidden_state, obs, dones):\n",
    "                # homogeneous pass vmapped in respect to the agents parameters (i.e., no parameter sharing)\n",
    "                agents, flatten_agents_obs = zip(*obs.items())\n",
    "                batched_input = (\n",
    "                    jnp.stack(flatten_agents_obs, axis=0), # (n_agents, time_step, n_envs, obs_size)\n",
    "                    jnp.stack([dones[agent] for agent in agents], axis=0), # ensure to not pass other keys (like __all__)\n",
    "                )\n",
    "                # computes the q_vals with the params of each agent separately by vmapping\n",
    "                hidden_state, q_vals = jax.vmap(agent.apply, in_axes=0)(params, hidden_state, batched_input)\n",
    "                q_vals = {a:q_vals[i] for i,a in enumerate(agents)}\n",
    "                return hidden_state, q_vals\n",
    "\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "\n",
    "            train_state, target_network_params, env_state, buffer_state, time_state, init_obs, init_dones, test_metrics, rng = runner_state\n",
    "\n",
    "            # EPISODE STEP\n",
    "            def _env_step(step_state, unused):\n",
    "\n",
    "                params, env_state, last_obs, last_dones, hstate, rng, t = step_state\n",
    "\n",
    "                # prepare rngs for actions and step\n",
    "                rng, key_a, key_s = jax.random.split(rng, 3)\n",
    "\n",
    "                # SELECT ACTION\n",
    "                # add a dummy time_step dimension to the agent input\n",
    "                obs_   = {a:last_obs[a] for a in env.agents} # ensure to not pass the global state (obs[\"__all__\"]) to the network\n",
    "                obs_   = jax.tree_map(lambda x: x[np.newaxis, :], obs_)\n",
    "                dones_ = jax.tree_map(lambda x: x[np.newaxis, :], last_dones)\n",
    "                # get the q_values from the agent netwoek\n",
    "                hstate, q_vals = homogeneous_pass(params, hstate, obs_, dones_)\n",
    "                # remove the dummy time_step dimension and index qs by the valid actions of each agent \n",
    "                valid_q_vals = jax.tree_util.tree_map(lambda q, valid_idx: q.squeeze(0)[..., valid_idx], q_vals, wrapped_env.valid_actions)\n",
    "                # explore with epsilon greedy_exploration\n",
    "                actions = explorer.choose_actions(valid_q_vals, t, key_a)\n",
    "\n",
    "                # STEP ENV\n",
    "                obs, env_state, rewards, dones, infos = wrapped_env.batch_step(key_s, env_state, actions)\n",
    "                transition = Transition(last_obs, actions, rewards, dones, infos)\n",
    "\n",
    "                step_state = (params, env_state, obs, dones, hstate, rng, t+1)\n",
    "                return step_state, transition\n",
    "\n",
    "\n",
    "            # prepare the step state and collect the episode trajectory\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            if config[\"PARAMETERS_SHARING\"]:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents)*config[\"NUM_ENVS\"]) # (n_agents*n_envs, hs_size)\n",
    "            else:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents), config[\"NUM_ENVS\"]) # (n_agents, n_envs, hs_size)\n",
    "\n",
    "            step_state = (\n",
    "                train_state.params['agent'],\n",
    "                env_state,\n",
    "                init_obs,\n",
    "                init_dones,\n",
    "                hstate, \n",
    "                _rng,\n",
    "                time_state['timesteps'] # t is needed to compute epsilon\n",
    "            )\n",
    "\n",
    "            step_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, step_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # BUFFER UPDATE: save the collected trajectory in the buffer\n",
    "            buffer_traj_batch = jax.tree_util.tree_map(\n",
    "                lambda x:jnp.swapaxes(x, 0, 1)[:, np.newaxis], # put the batch dim first and add a dummy sequence dim\n",
    "                traj_batch\n",
    "            ) # (num_envs, 1, time_steps, ...)\n",
    "            buffer_state = buffer.add(buffer_state, buffer_traj_batch)\n",
    "\n",
    "            # LEARN PHASE\n",
    "            def q_of_action(q, u):\n",
    "                \"\"\"index the q_values with action indices\"\"\"\n",
    "                q_u = jnp.take_along_axis(q, jnp.expand_dims(u, axis=-1), axis=-1)\n",
    "                return jnp.squeeze(q_u, axis=-1)\n",
    "\n",
    "            def _loss_fn(params, target_network_params, init_hstate, learn_traj):\n",
    "\n",
    "                obs_ = {a:learn_traj.obs[a] for a in env.agents} # ensure to not pass the global state (obs[\"__all__\"]) to the network\n",
    "                _, q_vals = homogeneous_pass(params['agent'], init_hstate, obs_, learn_traj.dones)\n",
    "                _, target_q_vals = homogeneous_pass(target_network_params['agent'], init_hstate, obs_, learn_traj.dones)\n",
    "\n",
    "                # get the q_vals of the taken actions (with exploration) for each agent\n",
    "                chosen_action_qvals = jax.tree_map(\n",
    "                    lambda q, u: q_of_action(q, u)[:-1], # avoid last timestep\n",
    "                    q_vals,\n",
    "                    learn_traj.actions\n",
    "                )\n",
    "\n",
    "                # get the target q value of the greedy actions for each agent\n",
    "                valid_q_vals = jax.tree_util.tree_map(lambda q, valid_idx: q[..., valid_idx], q_vals, wrapped_env.valid_actions)\n",
    "                target_max_qvals = jax.tree_map(\n",
    "                    lambda t_q, q: q_of_action(t_q, jnp.argmax(q, axis=-1))[1:], # avoid first timestep\n",
    "                    target_q_vals,\n",
    "                    jax.lax.stop_gradient(valid_q_vals)\n",
    "                )\n",
    "\n",
    "                # compute q_tot with the mixer network\n",
    "                chosen_action_qvals_mix = mixer.apply(\n",
    "                    params['mixer'], \n",
    "                    jnp.stack(list(chosen_action_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][:-1] # avoid last timestep\n",
    "                )\n",
    "                target_max_qvals_mix = mixer.apply(\n",
    "                    target_network_params['mixer'], \n",
    "                    jnp.stack(list(target_max_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][1:] # avoid first timestep\n",
    "                )\n",
    "\n",
    "                # compute target\n",
    "                if config.get('TD_LAMBDA_LOSS', True):\n",
    "                    # time difference loss\n",
    "                    def _td_lambda_target(ret, values):\n",
    "                        reward, done, target_qs = values\n",
    "                        ret = jnp.where(\n",
    "                            done,\n",
    "                            target_qs,\n",
    "                            ret*config['TD_LAMBDA']*config['GAMMA']\n",
    "                            + reward\n",
    "                            + (1-config['TD_LAMBDA'])*config['GAMMA']*(1-done)*target_qs\n",
    "                        )\n",
    "                        return ret, ret\n",
    "\n",
    "                    ret = target_max_qvals_mix[-1] * (1-learn_traj.dones['__all__'][-1])\n",
    "                    ret, td_targets = jax.lax.scan(\n",
    "                        _td_lambda_target,\n",
    "                        ret,\n",
    "                        (learn_traj.rewards['__all__'][-2::-1], learn_traj.dones['__all__'][-2::-1], target_max_qvals_mix[-1::-1])\n",
    "                    )\n",
    "                    targets = td_targets[::-1]\n",
    "                    loss = jnp.mean(0.5*((chosen_action_qvals_mix - jax.lax.stop_gradient(targets))**2))\n",
    "                else:\n",
    "                    # standard DQN loss\n",
    "                    targets = (\n",
    "                        learn_traj.rewards['__all__'][:-1]\n",
    "                        + config['GAMMA']*(1-learn_traj.dones['__all__'][:-1])*target_max_qvals_mix\n",
    "                    )\n",
    "                    loss = jnp.mean((chosen_action_qvals_mix - jax.lax.stop_gradient(targets))**2)\n",
    "                \n",
    "                return loss\n",
    "\n",
    "\n",
    "            # sample a batched trajectory from the buffer and set the time step dim in first axis\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            learn_traj = buffer.sample(buffer_state, _rng).experience # (batch_size, 1, max_time_steps, ...)\n",
    "            learn_traj = jax.tree_map(\n",
    "                lambda x: jnp.swapaxes(x[:, 0], 0, 1), # remove the dummy sequence dim (1) and swap batch and temporal dims\n",
    "                learn_traj\n",
    "            ) # (max_time_steps, batch_size, ...)\n",
    "            if config[\"PARAMETERS_SHARING\"]:\n",
    "                init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents)*config[\"BUFFER_BATCH_SIZE\"]) # (n_agents*batch_size, hs_size)\n",
    "            else:\n",
    "                init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents), config[\"BUFFER_BATCH_SIZE\"]) # (n_agents, batch_size, hs_size)\n",
    "\n",
    "            # compute loss and optimize grad\n",
    "            grad_fn = jax.value_and_grad(_loss_fn, has_aux=False)\n",
    "            loss, grads = grad_fn(train_state.params, target_network_params, init_hs, learn_traj)\n",
    "            train_state = train_state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "            # UPDATE THE VARIABLES AND RETURN\n",
    "            # reset the environment\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            init_obs, env_state = wrapped_env.batch_reset(_rng)\n",
    "            init_dones = {agent:jnp.zeros((config[\"NUM_ENVS\"]), dtype=bool) for agent in env.agents+['__all__']}\n",
    "\n",
    "            # update the states\n",
    "            time_state['timesteps'] = step_state[-1]\n",
    "            time_state['updates']   = time_state['updates'] + 1\n",
    "\n",
    "            # update the target network if necessary\n",
    "            target_network_params = jax.lax.cond(\n",
    "                time_state['updates'] % config['TARGET_UPDATE_INTERVAL'] == 0,\n",
    "                lambda _: jax.tree_map(lambda x: jnp.copy(x), train_state.params),\n",
    "                lambda _: target_network_params,\n",
    "                operand=None\n",
    "            )\n",
    "\n",
    "            # update the greedy rewards\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            test_metrics = jax.lax.cond(\n",
    "                time_state['updates'] % (config[\"TEST_INTERVAL\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]) == 0,\n",
    "                lambda _: get_greedy_metrics(_rng, train_state.params['agent'], time_state),\n",
    "                lambda _: test_metrics,\n",
    "                operand=None\n",
    "            )\n",
    "\n",
    "            # update the returning metrics\n",
    "            metrics = {\n",
    "                'timesteps': time_state['timesteps']*config['NUM_ENVS'],\n",
    "                'updates' : time_state['updates'],\n",
    "                'loss': loss,\n",
    "                'rewards': jax.tree_util.tree_map(lambda x: jnp.sum(x, axis=0).mean(), traj_batch.rewards),\n",
    "                'eps': explorer.get_epsilon(time_state['timesteps'])\n",
    "            }\n",
    "            metrics['test_metrics'] = test_metrics # add the test metrics dictionary\n",
    "\n",
    "            if config.get('WANDB_ONLINE_REPORT', False):\n",
    "                def callback(metrics, infos):\n",
    "                    info_metrics = {\n",
    "                        k:v[...,0][infos[\"returned_episode\"][..., 0]].mean()\n",
    "                        for k,v in infos.items() if k!=\"returned_episode\"\n",
    "                    }\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"returns\": metrics['rewards']['__all__'].mean(),\n",
    "                            \"timestep\": metrics['timesteps'],\n",
    "                            \"updates\": metrics['updates'],\n",
    "                            \"loss\": metrics['loss'],\n",
    "                            'epsilon': metrics['eps'],\n",
    "                            **info_metrics,\n",
    "                            **{k:v.mean() for k, v in metrics['test_metrics'].items()}\n",
    "                        }\n",
    "                    )\n",
    "                jax.debug.callback(callback, metrics, traj_batch.infos)\n",
    "\n",
    "            runner_state = (\n",
    "                train_state,\n",
    "                target_network_params,\n",
    "                env_state,\n",
    "                buffer_state,\n",
    "                time_state,\n",
    "                init_obs,\n",
    "                init_dones,\n",
    "                test_metrics,\n",
    "                rng\n",
    "            )\n",
    "\n",
    "            return runner_state, metrics\n",
    "\n",
    "        def get_greedy_metrics(rng, params, time_state):\n",
    "            \"\"\"Help function to test greedy policy during training\"\"\"\n",
    "            def _greedy_env_step(step_state, unused):\n",
    "                params, env_state, last_obs, last_dones, hstate, rng = step_state\n",
    "                rng, key_s = jax.random.split(rng)\n",
    "                obs_   = {a:last_obs[a] for a in env.agents}\n",
    "                obs_   = jax.tree_map(lambda x: x[np.newaxis, :], obs_)\n",
    "                dones_ = jax.tree_map(lambda x: x[np.newaxis, :], last_dones)\n",
    "                hstate, q_vals = homogeneous_pass(params, hstate, obs_, dones_)\n",
    "                actions = jax.tree_util.tree_map(lambda q, valid_idx: jnp.argmax(q.squeeze(0)[..., valid_idx], axis=-1), q_vals, test_env.valid_actions)\n",
    "                obs, env_state, rewards, dones, infos = test_env.batch_step(key_s, env_state, actions)\n",
    "                step_state = (params, env_state, obs, dones, hstate, rng)\n",
    "                return step_state, (rewards, dones, infos)\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            init_obs, env_state = test_env.batch_reset(_rng)\n",
    "            init_dones = {agent:jnp.zeros((config[\"NUM_TEST_EPISODES\"]), dtype=bool) for agent in env.agents+['__all__']}\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            if config[\"PARAMETERS_SHARING\"]:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents)*config[\"NUM_TEST_EPISODES\"]) # (n_agents*n_envs, hs_size)\n",
    "            else:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents), config[\"NUM_TEST_EPISODES\"]) # (n_agents, n_envs, hs_size)\n",
    "            step_state = (\n",
    "                params,\n",
    "                env_state,\n",
    "                init_obs,\n",
    "                init_dones,\n",
    "                hstate, \n",
    "                _rng,\n",
    "            )\n",
    "            step_state, (rewards, dones, infos) = jax.lax.scan(\n",
    "                _greedy_env_step, step_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            # compute the metrics of the first episode that is done for each parallel env\n",
    "            def first_episode_returns(rewards, dones):\n",
    "                first_done = jax.lax.select(jnp.argmax(dones)==0., dones.size, jnp.argmax(dones))\n",
    "                first_episode_mask = jnp.where(jnp.arange(dones.size) <= first_done, True, False)\n",
    "                return jnp.where(first_episode_mask, rewards, 0.).sum()\n",
    "            all_dones = dones['__all__']\n",
    "            first_returns = jax.tree_map(lambda r: jax.vmap(first_episode_returns, in_axes=1)(r, all_dones), rewards)\n",
    "            first_infos   = jax.tree_map(lambda i: jax.vmap(first_episode_returns, in_axes=1)(i[..., 0], all_dones), infos)\n",
    "            metrics = {\n",
    "                'test_returns': first_returns['__all__'],# episode returns\n",
    "                **{'test_'+k:v for k,v in first_infos.items()}\n",
    "            }\n",
    "            if config.get('VERBOSE', False):\n",
    "                def callback(timestep, val):\n",
    "                    print(f\"Timestep: {timestep}, return: {val}\")\n",
    "                jax.debug.callback(callback, time_state['timesteps']*config['NUM_ENVS'], first_returns['__all__'].mean())\n",
    "            return metrics\n",
    "        \n",
    "        time_state = {\n",
    "            'timesteps':jnp.array(0),\n",
    "            'updates':  jnp.array(0)\n",
    "        }\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        test_metrics = get_greedy_metrics(_rng, train_state.params['agent'],time_state) # initial greedy metrics\n",
    "\n",
    "        # train\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (\n",
    "            train_state,\n",
    "            target_network_params,\n",
    "            env_state,\n",
    "            buffer_state,\n",
    "            time_state,\n",
    "            init_obs,\n",
    "            init_dones,\n",
    "            test_metrics,\n",
    "            _rng\n",
    "        )\n",
    "        runner_state, metrics = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {'runner_state':runner_state, 'metrics':metrics}\n",
    "    \n",
    "    return train\n",
    "\n",
    "@hydra.main(version_base=None, config_path=\"./config\", config_name=\"config\")\n",
    "def main(config):\n",
    "    config = OmegaConf.to_container(config)\n",
    "\n",
    "    print('Config:\\n', OmegaConf.to_yaml(config))\n",
    "\n",
    "    env_name = config[\"env\"][\"ENV_NAME\"]\n",
    "    alg_name = f'qmix_{\"ps\" if config[\"alg\"].get(\"PARAMETERS_SHARING\", True) else \"ns\"}'\n",
    "    \n",
    "    # smac init neeeds a scenario\n",
    "    if 'smax' in env_name.lower():\n",
    "        config['env']['ENV_KWARGS']['scenario'] = map_name_to_scenario(config['env']['MAP_NAME'])\n",
    "        env_name = f\"{config['env']['ENV_NAME']}_{config['env']['MAP_NAME']}\"\n",
    "        env = make(config[\"env\"][\"ENV_NAME\"], **config['env']['ENV_KWARGS'])\n",
    "        env = SMAXLogWrapper(env)\n",
    "   # overcooked needs a layout \n",
    "    elif 'overcooked' in env_name.lower():\n",
    "        config['env'][\"ENV_KWARGS\"][\"layout\"] = overcooked_layouts[config['env'][\"ENV_KWARGS\"][\"layout\"]]\n",
    "        env = make(config[\"env\"][\"ENV_NAME\"], **config['env']['ENV_KWARGS'])\n",
    "        env = LogWrapper(env)\n",
    "    else:\n",
    "        env = make(config[\"env\"][\"ENV_NAME\"], **config['env']['ENV_KWARGS'])\n",
    "        env = LogWrapper(env)\n",
    "\n",
    "    #config[\"alg\"][\"NUM_STEPS\"] = config[\"alg\"].get(\"NUM_STEPS\", env.max_steps) # default steps defined by the env\n",
    "    \n",
    "    wandb.init(\n",
    "        entity=config[\"ENTITY\"],\n",
    "        project=config[\"PROJECT\"],\n",
    "        tags=[\n",
    "            alg_name.upper(),\n",
    "            env_name.upper(),\n",
    "            \"RNN\",\n",
    "            \"TD_LOSS\" if config[\"alg\"].get(\"TD_LAMBDA_LOSS\", True) else \"DQN_LOSS\",\n",
    "            f\"jax_{jax.__version__}\",\n",
    "        ],\n",
    "        name=f'{alg_name}_{env_name}',\n",
    "        config=config,\n",
    "        mode=config[\"WANDB_MODE\"],\n",
    "    )\n",
    "    \n",
    "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "    rngs = jax.random.split(rng, config[\"NUM_SEEDS\"])\n",
    "    train_vjit = jax.jit(jax.vmap(make_train(config[\"alg\"], env)))\n",
    "    outs = jax.block_until_ready(train_vjit(rngs))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stuff below is our hyperparameter optimization work. We will need to determine how to make ray tune work with jax marl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_model(hyperparameters):\n",
    "    # Implement your JAX model here\n",
    "    # Use hyperparameters in the model construction\n",
    "    # Return a performance metric to be optimized\n",
    "    return performance_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'learning_rate': tune.loguniform(1e-4, 1e-1),\n",
    "    'hidden_size': tune.choice([32, 64, 128]),\n",
    "    # Add other hyperparameters as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_samples': 10,  # Number of hyperparameter configurations to try\n",
    "    'config': search_space,\n",
    "    'metric': 'performance_metric',  # Change to your actual metric name\n",
    "    'mode': 'max',  # or 'min' depending on whether you're maximizing or minimizing the metric\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()\n",
    "\n",
    "analysis = tune.run(\n",
    "    jax_model,\n",
    "    num_samples=config['num_samples'],\n",
    "    config=config['config'],\n",
    "    metric=config['metric'],\n",
    "    mode=config['mode'],\n",
    ")\n",
    "\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
