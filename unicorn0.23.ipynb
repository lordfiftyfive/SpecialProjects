{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix for matmul error that is causing last bug with memory\n",
    "\n",
    "\n",
    "#this goes inside reset function \n",
    "#self.images = np.pad(self.images, ((0, 0), (0, 0), (0, 1)), mode='constant', constant_values=0)\n",
    "\n",
    "#these go inside step function\n",
    "\n",
    "\"\"\"\n",
    "obs[agent_id] = {\n",
    "    \"image\": self.images[self.current_step].astype(np.float32),  # Shape (175, 175)\n",
    "    \"other_action\": np.array(other_actions, dtype=np.float32).reshape(1,)\n",
    "}\n",
    "\n",
    "# Convert image to 1D and replace the last element (which was 0) with other_action\n",
    "obs[agent_id] = np.concatenate((\n",
    "    obs[agent_id][\"image\"].flatten(),  # Shape (30625,)\n",
    "    obs[agent_id][\"other_action\"]      # Shape (1,)\n",
    "))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subarno/miniconda3/envs/primary/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/subarno/.cache/kagglehub/datasets/lgching/cichy-118-image-matrices/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "cichy_118_image_matrices_path = kagglehub.dataset_download('lgching/cichy-118-image-matrices')\n",
    "\n",
    "cichy_et_al_2014_path = kagglehub.dataset_download('wan2022/cichy-et-al-2014')\n",
    "\n",
    "print(cichy_118_image_matrices_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import scipy\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import cv2\n",
    "def loadmatrix(matfile):\n",
    "  \"\"\"Function to load .mat files.\n",
    "  Parameters\n",
    "  ----------\n",
    "  matfile : str\n",
    "      path to `matfile` containing fMRI data for a given trial.\n",
    "  Returns\n",
    "  -------\n",
    "  dict\n",
    "      dictionary containing data in key 'vol' for a given trial.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    f = h5py.File(matfile)\n",
    "  except (IOError, OSError):\n",
    "    return scipy.io.loadmat(matfile)\n",
    "  else:\n",
    "    return {name: np.transpose(f.get(name)) for name in f.keys()}\n",
    "\n",
    "\n",
    "x = loadmatrix(\"/home/subarno/.cache/kagglehub/datasets/wan2022/cichy-et-al-2014/versions/1/Cichy_92_Image_Set_ROI_RDMs/92_Image_Set/92images.mat\")\n",
    "\n",
    "y = loadmatrix(\"/home/subarno/.cache/kagglehub/datasets/wan2022/cichy-et-al-2014/versions/1/Cichy_92_Image_Set_ROI_RDMs/92_Image_Set/target_fmri.mat\")\n",
    "\n",
    "y1 = y['EVC_RDMs']\n",
    "y2 = y['IT_RDMs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 175, 175)\n"
     ]
    }
   ],
   "source": [
    "# Load .mat file\n",
    "mat_data = scipy.io.loadmat(\"/home/subarno/.cache/kagglehub/datasets/wan2022/cichy-et-al-2014/versions/1/Cichy_92_Image_Set_ROI_RDMs/92_Image_Set/92images.mat\")\n",
    "\n",
    "# Extract visual stimuli (images stored as tuples with filenames)\n",
    "visual_stimuli = mat_data[\"visual_stimuli\"]\n",
    "\n",
    "# Convert images to grayscale\n",
    "grayscale_images = []\n",
    "\n",
    "for i in range(visual_stimuli.shape[1]):  # Loop over images\n",
    "    filename, rgb_image = visual_stimuli[0, i]  # Extract (name, image)\n",
    "    gray_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "    grayscale_images.append((filename, gray_image))  # Store as (name, gray_image)\n",
    "\n",
    "# Convert list to NumPy array (if needed)\n",
    "grayscale_array = np.array([img[1] for img in grayscale_images])\n",
    "\n",
    "print(grayscale_array.shape)\n",
    "\n",
    "x = grayscale_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntrained_pop, fitnesses = train_multi_agent(\\n    pop=pop_agent,\\n    env=env,\\n    algo=\"MATD3\",\\n    env_name=\"cichyenv\",\\n    memory=memory,\\n    swap_channels=False,\\n    max_steps=500000,\\n    evo_steps=10000,\\n    eval_steps=None,\\n    eval_loop=1,\\n    target=200.0,\\n    tournament=tournament,\\n    wb=False,\\n)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from pettingzoo import ParallelEnv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from agilerl.utils.utils import create_population as Population\n",
    "from agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\n",
    "from agilerl.training.train_offline import train_offline\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.training.train_multi_agent import train_multi_agent\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "\n",
    "from agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\n",
    "\n",
    "import supersuit as ss\n",
    "#set device to the dual t4s. \n",
    "import pygame\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "# -------------------------------\n",
    "# Environment Definition\n",
    "# -------------------------------\n",
    "class CichyEnv(ParallelEnv):\n",
    "    metadata = {\"render_modes\": [],\"name\": \"cichyenv\"}\n",
    "\n",
    "    def __init__(self, images, y1, y2, render_mode=None):\n",
    "        self.images = images.reshape(92, 175, 175)  # Ensure correct shape\n",
    "        self.y1 = y1  # Rewards for agent IT\n",
    "        self.y2 = y2  # Rewards for agent EVC\n",
    "        self.agents = [\"IT\", \"EVC\"]\n",
    "        self.possible_agents = self.agents\n",
    "        self.agent_ids = [\"IT\", \"EVC\"]\n",
    "        self.current_step = 0  # Initialize current_step\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        # Merging observation spaces into a single space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"image\": spaces.Box(low=0, high=255, shape=(175, 175), dtype=np.uint8),  # Expecting image shape of (175, 175)\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(93,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0  # Ensure it's set to 0 at the start of each episode\n",
    "        self.actions = {agent: [] for agent in self.agent_ids}\n",
    "        \n",
    "        # Creating observation dictionary\n",
    "        obs = {}\n",
    "        for agent in self.agent_ids:\n",
    "            image = self.images[self.current_step]  # Get the image for the current step\n",
    "            if image.shape != (175, 175):\n",
    "                raise ValueError(f\"Unexpected image shape: {image.shape}, expected (175, 175)\")\n",
    "\n",
    "            # Expand the image dimensions to make it 5D for Conv3D (batch, channels, depth, height, width)\n",
    "            image_expanded = np.expand_dims(image, axis=0)  # Adds a batch dimension, (1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds a channel dimension, (1, 1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds depth dimension, (1, 1, 1, 175, 175)\n",
    "\n",
    "            obs[agent] = {\n",
    "                \"image\": image_expanded,  # Now shape (1, 1, 1, 175, 175)\n",
    "                \"other_action\": np.array([0.0]),  # Dummy value for the action of the other agent\n",
    "            }\n",
    "\n",
    "            #uncomment out soon \n",
    "\n",
    "        obs = {\n",
    "            agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "            for agent_id in self.agent_ids\n",
    "        }\n",
    "                    \n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        if self.current_step >= len(self.images):\n",
    "            self.current_step = len(self.images) - 1\n",
    "\n",
    "        obs, rewards, dones, infos = {}, {}, {}, {}\n",
    "\n",
    "        for agent_id in self.agent_ids:\n",
    "            other_agent_id = \"EVC\" if agent_id == \"IT\" else \"IT\"\n",
    "\n",
    "            full_action = action_dict.get(agent_id, np.zeros((93,)))\n",
    "            agent_actions = full_action[:92].reshape(-1, 1)#(full_action[:92]).reshape((92, 1))  \n",
    "            other_action = full_action[92:]  \n",
    "\n",
    "            self.actions[agent_id].append(agent_actions)  \n",
    "\n",
    "            other_actions = action_dict.get(other_agent_id, np.zeros((93,)))[92:] \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            # Padding other_action into the image array (assuming you want to treat the action as part of the input)\n",
    "            image_and_action = np.concatenate(\n",
    "                (self.images[self.current_step].flatten(), other_actions), axis=0\n",
    "            )\n",
    "            obs[agent_id] = torch.tensor(image_and_action, dtype=torch.float32)\n",
    "\n",
    "            \n",
    "            \"\"\" \n",
    "\n",
    "            obs[agent_id] = {\n",
    "                \"image\": torch.tensor(self.images[self.current_step], dtype=torch.float32),  # Convert to tensor\n",
    "                \"other_action\": torch.tensor(other_actions, dtype=torch.float32),  # Convert to tensor\n",
    "            }\n",
    "\n",
    "            #rewards[agent_id] = self._calculate_reward(agent_id)  \n",
    "\n",
    "            rewards[agent_id] = np.array([self._calculate_reward(agent_id)], dtype=np.float32)\n",
    "            dones[agent_id] = self.current_step >= len(self.images) - 1\n",
    "            infos[agent_id] = {}\n",
    "        dones = {agent_id: self.current_step >= len(self.images) - 1 for agent_id in self.agent_ids}\n",
    "        dones[\"__all__\"] = all(dones.values())\n",
    "        self.current_step += 1\n",
    "        obs = {\n",
    "            agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "            for agent_id in self.agent_ids\n",
    "            }  \n",
    "        \n",
    "        trunc = {agent_id: False for agent_id in self.agent_ids}\n",
    "        return obs, rewards, dones, trunc, infos\n",
    "    def _calculate_reward(self, agent_id):\n",
    "        \"\"\"Computes the reward based on similarity to the expert RDM using all 92 actions per step.\"\"\"\n",
    "        actions = np.array(self.actions[agent_id])  # Shape: (num_steps, 92, 1)\n",
    "        \n",
    "        num_steps = actions.shape[0]  \n",
    "        num_images = actions.shape[1]  \n",
    "\n",
    "        if num_steps < 2:  \n",
    "            return 0  \n",
    "\n",
    "        simulated_rdm = np.zeros((num_images, num_images))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for j in range(num_images):\n",
    "                if i != j:\n",
    "                    sim = cosine_similarity(actions[:, i].reshape(-1, 1), actions[:, j].reshape(-1, 1))[0][0]\n",
    "                    simulated_rdm[i, j] = 1 - sim\n",
    "\n",
    "        expert_rdm = self.y1 if agent_id == \"IT\" else self.y2\n",
    "\n",
    "        min_size = min(simulated_rdm.shape[0], expert_rdm.shape[0])\n",
    "        reward = -np.sum((simulated_rdm[:min_size, :min_size] - expert_rdm[:min_size, :min_size]) ** 2)\n",
    "        return reward\n",
    "    \"\"\"\n",
    "    def enable_render(self, mode=\"human\"):\n",
    "        if not self.renderOn and mode == \"human\":\n",
    "            global app\n",
    "            app = Ursina()\n",
    "            self.renderOn = True\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "        global last_message_time\n",
    "\n",
    "        now = time.time()\n",
    "\n",
    "        # Send a message every 0.5s\n",
    "        if now - last_message_time > 0.5:\n",
    "            send_message()\n",
    "            last_message_time = now\n",
    "\n",
    "        # Update messages\n",
    "        for msg in messages[:]:  # copy the list to avoid modification during iteration\n",
    "            msg.progress += time.dt * msg.speed\n",
    "            if msg.progress <= 1:\n",
    "                msg.position = lerp(msg.start, msg.end, msg.progress)\n",
    "            else:\n",
    "                destroy(msg)\n",
    "                messages.remove(msg)\n",
    "        # Step the Ursina engine (must be called every frame)\n",
    "        app.step()\n",
    "    def draw(self):\n",
    "        global node_a, node_b, line\n",
    "\n",
    "        # Nodes\n",
    "        node_a = Entity(model='sphere', color=color.red, scale=0.3, position=(-2, 0, 0))\n",
    "        node_b = Entity(model='sphere', color=color.green, scale=0.3, position=(2, 0, 0))\n",
    "\n",
    "        # Connecting line\n",
    "        line = Entity(model=Mesh(vertices=[node_a.position, node_b.position], mode='line', thickness=2), color=color.white)\n",
    "\n",
    "     def close(self):\n",
    "        global app\n",
    "        application.quit()\n",
    "    \"\"\"\n",
    "#version number 2 \n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training Setup with AgileRL\n",
    "# -------------------------------8-=\n",
    "x_train = x#np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = y1#np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = y2#np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "\n",
    "env = CichyEnv(x_train, y_train1, y_train2)\n",
    "num_envs = 8\n",
    "\n",
    "#vec_env = AsyncPettingZooVecEnv([lambda : env for _ in range(num_envs)]) #do later \n",
    "\n",
    "# Initial Hyperparameters\n",
    "INIT_HP = {\n",
    "    \"DOUBLE\": True,\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"POPULATION_SIZE\": 2,\n",
    "    \"O_U_NOISE\": 0.2,\n",
    "    \"EXPL_NOISE\": 0.1,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"LR\": 0.001,\n",
    "    \"LR_ACTOR\": 0.002,\n",
    "    \"LR_CRITIC\": 0.002,\n",
    "    \"TAU\": 0.5,\n",
    "    \"GAMMA\": 1.0,\n",
    "    \"LAMBDA\": 1.0,\n",
    "    \"REG\": 0.0625,\n",
    "    \"LEARN_STEP\": 2,\n",
    "    \"MEAN_NOISE\": 1,\n",
    "    \"THETA\": 1,\n",
    "    \"DT\": 1,\n",
    "    \"POLICY_FREQ\": 2,\n",
    "    \"AGENT_IDS\": [\"IT\",\"EVC\"],\n",
    "    \"MEMORY_SIZE\": 100000\n",
    "        \n",
    "}\n",
    "\n",
    "hp_config = HyperparameterConfig(\n",
    "    #lr=RLParameter(min=6.25e-5, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n",
    ")\n",
    "\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "\n",
    "# Create populations for each agent\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# Tournament selection\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,\n",
    "    elitism=True,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    eval_loop=1,\n",
    ")\n",
    "\n",
    "# Mutation settings\n",
    "mutations = Mutations(\n",
    "    no_mutation=0.4,\n",
    "    architecture=0.2,\n",
    "    new_layer_prob=0.2,\n",
    "    parameters=0.2,\n",
    "    activation=0,\n",
    "    rl_hp=0.2,\n",
    "    mutation_sd=0.1,\n",
    "    rand_seed=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Offline training\n",
    "trained_pop, pop_fitnesses = train_multi_agent(#train_on_policy(#train_offline(\n",
    "    pop=[pop_agent1, pop_agent2],\n",
    "    env=env,\n",
    "    algo=\"MATD3\",\n",
    "    env_name=\"cichyenv\",\n",
    "    #dataset=[x_train, y_train1, y_train2],\n",
    "    memory=memory,  # Replay buffer if needed\n",
    "    swap_channels=False,  # Ensure channel order is correct\n",
    "    max_steps=500000,\n",
    "    evo_steps=10000,\n",
    "    eval_steps=None,\n",
    "    eval_loop=1,\n",
    "    target=200.0,\n",
    "    tournament=tournament,\n",
    "    #mutation=mutations,\n",
    "    wb=False,  # Weights & Biases logging\n",
    "    #accelerator=device\n",
    ")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "trained_pop, fitnesses = train_multi_agent(\n",
    "    pop=pop_agent,\n",
    "    env=env,\n",
    "    algo=\"MATD3\",\n",
    "    env_name=\"cichyenv\",\n",
    "    memory=memory,\n",
    "    swap_channels=False,\n",
    "    max_steps=500000,\n",
    "    evo_steps=10000,\n",
    "    eval_steps=None,\n",
    "    eval_loop=1,\n",
    "    target=200.0,\n",
    "    tournament=tournament,\n",
    "    wb=False,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: the baseline core algorithm on the random data on episode 0 produces cumulative reward Fitnesses: ['-4656445.83']\n",
    "\n",
    "The tasha did an area based analysis that suggested that the number of representative agents that are needed are 5 agents for EVC and 3 agents for IT. Therefore we will use 9 agents total since 1 needs to represent the Thalamus\n",
    "\n",
    "Core development TODOs\n",
    "\n",
    "TODO: figure out visualization and running the trained simulation on new data \n",
    "\n",
    "2nd TODO: finish the 9 agent simulation by doing ID based observation masking  \n",
    "\n",
    "other TODO: switch over from RDMs to MEG \n",
    "\n",
    "other TODO: swap out classical neural networks for UODEs\n",
    "\n",
    "Optimization TODOs\n",
    "\n",
    "TODO: vectorize enviroment and figure out how to increase number of enviroments\n",
    "\n",
    "\n",
    "\n",
    "In order to do this switch over the research team needs to figure out which channels correspond to which parts of the brain so that irrelavent channels can be removed. \n",
    "\n",
    "how to do groupings? the answer is to use hierarchical learning \n",
    "\n",
    "other TODO: swap out classical neural network for UODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary code for vectorizing pettingzoo enviroment\n",
    "\n",
    "\"\"\"\n",
    "from agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\n",
    "env = CustomEnv()\n",
    "num_envs = 8\n",
    "AsyncPettingZooVecEnv([lambda : env for _ in range(num_envs)])\n",
    "observations, infos = vec_env.reset()\n",
    "for step in range(25):\n",
    "    actions = {\n",
    "        agent: [vec_env.single_action_space(agent).sample() for n in range(num_envs)]\n",
    "        for agent in vec_env.agents\n",
    "    }\n",
    "    observations, rewards, terminations, truncations, infos = vec_env.step(actions)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IT': False, 'EVC': False, '__all__': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:19<?, ?step/s]\n",
      "250000step [00:27, 11942.54step/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global steps 20000 ---\n",
      "Steps [0, 20000]\n",
      "Scores: ['0 completed episodes']\n",
      "Fitnesses: ['-4656445.83']\n",
      "5 fitness avgs: ['nan', '-4656445.83']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subarno/miniconda3/envs/primary/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/subarno/miniconda3/envs/primary/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "250000step [07:23, 563.11step/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "max_steps = 20000 \n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "\n",
    "training_steps = 100\n",
    "env = CichyEnv(x_train, y_train1, y_train2)#.parallel_env()\n",
    "action_dict = {\n",
    "    \"IT\": np.random.uniform(-1, 1, 93),  # Random action for agent IT (size 93)\n",
    "    \"EVC\": np.random.uniform(-1, 1, 93)  # Random action for agent EVC (size 93)\n",
    "}\n",
    "#env.step(action_dict)\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]},}\n",
    "obs = env.reset()  # This ensures self.actions is initialized\n",
    "obs, rewards, dones,bb, infos = env.step(action_dict)\n",
    "\n",
    "print(dones)\n",
    "#print(type(obs), obs)\n",
    "\n",
    "\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "#print(type(obs), obs)\n",
    "num_envs = 8\n",
    "learning_delay = 0  # Steps before starting learning\n",
    "evo_steps = 100  # Evolution frequency\n",
    "eval_steps = None  # Evaluation steps per episode - go until done\n",
    "eval_loop = 1  \n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "agent_ids = [\"IT\", \"EVC\"]\n",
    "#agent = pop_agent[0]\n",
    "#print(agent)\n",
    "pbar = trange(max_steps, unit=\"step\")\n",
    "while np.less([agent.steps[-1] for agent in pop_agent], max_steps).all():\n",
    "    pop_episode_scores = []\n",
    "    for agent in pop_agent:  # Loop through population\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        scores = np.zeros(num_envs)\n",
    "        completed_episode_scores = []\n",
    "        steps = 0\n",
    "\n",
    "    for idx_step in range(training_steps // num_envs):\n",
    "\n",
    "        # Ensure obs is flattened before passing to agent\n",
    "        cont_actions, discrete_action = agent.get_action(obs=obs, training=True, infos=info)\n",
    "        if agent.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "\n",
    "        # Act in environment\n",
    "        next_state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "        scores += np.sum(np.array(list(reward.values())).transpose(), axis=-1)\n",
    "        total_steps += num_envs\n",
    "        steps += num_envs\n",
    "        \n",
    "        # Save experiences to replay buffer\n",
    "\n",
    "        #memory is no longer erroring out but the elif right after is when memory.save_to_memory is uncommented out \n",
    "        \"\"\"\n",
    "        \n",
    "                \n",
    "        memory.save_to_memory(\n",
    "            state,\n",
    "            cont_actions,\n",
    "            reward,\n",
    "            next_state, \n",
    "            termination,\n",
    "            #is_vectorised=True,\n",
    "        )\n",
    "        \"\"\"    \n",
    "\n",
    "        # Learn according to learning frequency\n",
    "        # Handle learn steps > num_envs\n",
    "        if agent.learn_step > num_envs:\n",
    "            learn_step = agent.learn_step // num_envs\n",
    "            if (\n",
    "                idx_step % learn_step == 0\n",
    "                and len(memory) >= agent.batch_size\n",
    "                and memory.counter > learning_delay\n",
    "            ):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "        # Handle num_envs > learn step; learn multiple times per step in env\n",
    "        elif (\n",
    "            len(memory) >= agent.batch_size and memory.counter > learning_delay\n",
    "        ):\n",
    "            for _ in range(num_envs // agent.learn_step):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)#where bug is currently occuring.\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Calculate scores and reset noise for finished episodes\n",
    "        reset_noise_indices = []\n",
    "        term_array = np.array(list(termination.values())).transpose()\n",
    "        trunc_array = np.array(list(truncation.values())).transpose()\n",
    "        for idx, (d, t) in enumerate(zip(term_array, trunc_array)):\n",
    "            if np.any(d) or np.any(t):\n",
    "                completed_episode_scores.append(scores[idx])\n",
    "                agent.scores.append(scores[idx])\n",
    "                scores[idx] = 0\n",
    "                reset_noise_indices.append(idx)\n",
    "        agent.reset_action_noise(reset_noise_indices)\n",
    "\n",
    "    pbar.update(evo_steps // len(pop_agent))\n",
    "\n",
    "    agent.steps[-1] += steps\n",
    "    pop_episode_scores.append(completed_episode_scores)\n",
    "\n",
    "# Evaluate population\n",
    "fitnesses = [\n",
    "    agent.test( #agent.test is a potential bottleneck\n",
    "        env,\n",
    "        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "        max_steps=eval_steps,\n",
    "        loop=eval_loop,\n",
    "    )\n",
    "]\n",
    "mean_scores = [\n",
    "    (\n",
    "        np.mean(episode_scores)\n",
    "        if len(episode_scores) > 0\n",
    "        else \"0 completed episodes\"\n",
    "    )\n",
    "    for episode_scores in pop_episode_scores\n",
    "]\n",
    "\n",
    "print(f\"--- Global steps {total_steps} ---\")\n",
    "print(f\"Steps {[agent.steps[-1] for agent in pop_agent]}\")\n",
    "print(f\"Scores: {mean_scores}\")\n",
    "print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n",
    "print(\n",
    "    f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop_agent]}'\n",
    ")\n",
    "\n",
    "# Tournament selection and population mutation\n",
    "elite, pop = tournament.select(pop_agent)\n",
    "pop = mutations.mutation(pop)\n",
    "\n",
    "# Update step counter\n",
    "for agent in pop:\n",
    "    agent.steps.append(agent.steps[-1])\n",
    "\n",
    "\"\"\"\n",
    "path = \"\\\\wsl.localhost\\Debian\\home\\subarno\\SpecialProjects\\MATD3_trained_agent.pt\"\n",
    "filename = \"MADDPG_trained_agent.pt\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "save_path = os.path.join(path, filename)\n",
    "agent.save_checkpoint(save_path)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pbar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    path = \"\\\\wsl.localhost\\Debian\\home\\subarno\\SpecialProjects\\MATD3_trained_agent.pt\"\n",
    "    matd3 = MATD3.load(path, device)\n",
    "    episodes = 10\n",
    "    frames = []\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "        for _ in range(max_steps):\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = matd3.get_action(\n",
    "                state, training=False, infos=info\n",
    "            )\n",
    "            if matd3.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Save the frame for this step and append to frames list\n",
    "            #frame = env.render()\n",
    "            #frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(\n",
    "                {agent: a.squeeze() for agent, a in action.items()}\n",
    "            )\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Record agent specific episodic reward\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "n_samples, h, w = lfw_people.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "#resnet18 = models.resnet18()\n",
    "alexnet = models.alexnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IT': array([False]), 'EVC': array([False]), '__all__': array([False])}\n",
      "[False]\n",
      "<agilerl.algorithms.matd3.MATD3 object at 0x7f926c5d2bc0>\n"
     ]
    }
   ],
   "source": [
    "dones = {\n",
    "    agent: np.array([done]) for agent, done in dones.items()\n",
    "}\n",
    "print(dones)\n",
    "print(dones['IT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updated intelligence of chaeyeon and tasha indicates we need to update number of agents to 11-15\n",
    "\n",
    "5-7 for IT and 6-8 for EVC\n",
    "\n",
    "we should do all 15 agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subarno/miniconda3/envs/primary/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 01:30:57,357] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subarno/miniconda3/envs/primary/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/subarno/miniconda3/envs/primary/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from pettingzoo import ParallelEnv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from agilerl.utils.utils import create_population as Population\n",
    "from agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\n",
    "from agilerl.training.train_offline import train_offline\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.training.train_multi_agent import train_multi_agent\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "#set device to the dual t4s. \n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "# -------------------------------\n",
    "# Environment Definition\n",
    "# -------------------------------\n",
    "class CichyEnv(ParallelEnv):\n",
    "    metadata = {\"name\": \"cichyenv\"}\n",
    "\n",
    "    def __init__(self, images, y1, y2):\n",
    "        self.images = images.reshape(92, 175, 175)  # Ensure correct shape\n",
    "        self.y1 = y1  # Rewards for agent IT\n",
    "        self.y2 = y2  # Rewards for agent EVC\n",
    "        self.agents = [\"IT1\", \"IT2\", \"IT3\",\"IT4\",\"IT5\",\"IT6\",\"IT7\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\", \"EVC6\",\"EVC7\",\"EVC8\"] #add one more agent reprsenting the occipital lobe bill kennedy talked about\n",
    "        self.agent_ids = [\"IT1\", \"IT2\", \"IT3\",\"IT4\",\"IT5\",\"IT6\",\"IT7\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\", \"EVC6\",\"EVC7\",\"EVC8\"]#[\"IT\", \"EVC\"]\n",
    "        self.current_step = 0  # Initialize current_step\n",
    "        \n",
    "        # Merging observation spaces into a single space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"image\": spaces.Box(low=0, high=255, shape=(175, 175), dtype=np.uint8),  # Expecting image shape of (175, 175)\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(93,), dtype=np.float32)\n",
    "    def reset(self):\n",
    "        self.current_step = 0  # Reset the current step\n",
    "        self.actions = {agent: [] for agent in self.agent_ids}\n",
    "        \n",
    "        obs = {}\n",
    "        for agent in self.agent_ids:\n",
    "            image = self.images[self.current_step]\n",
    "            if image.shape != (175, 175):\n",
    "                raise ValueError(f\"Unexpected image shape: {image.shape}, expected (175, 175)\")\n",
    "\n",
    "            image_flat = image.flatten()  # Flatten the image to size 30625\n",
    "            other_action = np.zeros(92)  # Initialize dummy other_action (size 92)\n",
    "            \n",
    "            obs[agent] = {\n",
    "                \"image\": image_flat,  # Flattened image\n",
    "                \"other_action\": other_action  # Other action (dummy initially)\n",
    "            }\n",
    "\n",
    "        # Flatten the observations\n",
    "        obs = {\n",
    "            agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "            for agent_id in self.agent_ids\n",
    "        }\n",
    "                        \n",
    "        return obs, {}\n",
    "\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        if self.current_step >= len(self.images):\n",
    "            self.current_step = len(self.images) - 1\n",
    "\n",
    "        obs, rewards, dones, infos = {}, {}, {}, {}\n",
    "\n",
    "        for agent_id in self.agent_ids:\n",
    "            other_agent_id = \"EVC\" if agent_id.startswith(\"IT\") else \"IT\"\n",
    "\n",
    "            full_action = action_dict.get(agent_id, np.zeros(93))  # Action for the current agent\n",
    "            agent_actions = full_action[:92].reshape(-1, 1)  # Reshape to match the 92 action dimensions\n",
    "            other_actions = action_dict.get(other_agent_id, np.zeros(93))[:92]  # Right: shape (92,)#other_actions = action_dict.get(other_agent_id, np.zeros(93))[92:]  # Action of the other agent\n",
    "            \n",
    "            self.actions[agent_id].append(agent_actions)  # Record actions for reward calculation\n",
    "\n",
    "            # Replace the dummy other_action with the actual other_actions\n",
    "            obs[agent_id] = {\n",
    "                \"image\": self.images[self.current_step].flatten(),  # Flatten the image\n",
    "                \"other_action\": other_actions  # Actual action of the other agent\n",
    "            }\n",
    "\n",
    "            rewards[agent_id] = np.array([self._calculate_reward(agent_id)], dtype=np.float32)\n",
    "            dones[agent_id] = self.current_step >= len(self.images) - 1\n",
    "            infos[agent_id] = {}\n",
    "\n",
    "        dones[\"__all__\"] = all(dones.values())\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Flatten the observations\n",
    "        obs = {\n",
    "            agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "            for agent_id in self.agent_ids\n",
    "        }\n",
    "\n",
    "        trunc = {agent_id: False for agent_id in self.agent_ids}\n",
    "        return obs, rewards, dones, trunc, infos\n",
    "    def _calculate_reward(self, agent_id):\n",
    "        \"\"\"Computes the reward based on similarity to the expert RDM using all 92 actions per step.\"\"\"\n",
    "        actions = np.array(self.actions[agent_id])  # Shape: (num_steps, 92, 1)\n",
    "        \n",
    "        num_steps = actions.shape[0]  \n",
    "        num_images = actions.shape[1]  \n",
    "\n",
    "        if num_steps < 2:  \n",
    "            return 0  \n",
    "\n",
    "        simulated_rdm = np.zeros((num_images, num_images))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for j in range(num_images):\n",
    "                if i != j:\n",
    "                    sim = cosine_similarity(actions[:, i].reshape(-1, 1), actions[:, j].reshape(-1, 1))[0][0]\n",
    "                    simulated_rdm[i, j] = 1 - sim\n",
    "\n",
    "        expert_rdm = self.y1 if agent_id == \"IT\" else self.y2\n",
    "\n",
    "        min_size = min(simulated_rdm.shape[0], expert_rdm.shape[0])\n",
    "        reward = -np.sum((simulated_rdm[:min_size, :min_size] - expert_rdm[:min_size, :min_size]) ** 2)\n",
    "        return reward\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training Setup with AgileRL\n",
    "# -------------------------------8-=\n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "\n",
    "env = CichyEnv(x_train, y_train1, y_train2)\n",
    "\n",
    "# Initial Hyperparameters\n",
    "INIT_HP = {\n",
    "    \"DOUBLE\": True,\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"POPULATION_SIZE\": 15,\n",
    "    \"O_U_NOISE\": 0.2,\n",
    "    \"EXPL_NOISE\": 0.1,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"LR\": 0.001,\n",
    "    \"LR_ACTOR\": 0.002,\n",
    "    \"LR_CRITIC\": 0.002,\n",
    "    \"TAU\": 0.5,\n",
    "    \"GAMMA\": 1.0,\n",
    "    \"LAMBDA\": 1.0,\n",
    "    \"REG\": 0.0625,\n",
    "    \"LEARN_STEP\": 2,\n",
    "    \"MEAN_NOISE\": 1,\n",
    "    \"THETA\": 1,\n",
    "    \"DT\": 1,\n",
    "    \"POLICY_FREQ\": 2,\n",
    "    \"AGENT_IDS\": [\"IT1\", \"IT2\", \"IT3\",\"IT4\",\"IT5\",\"IT6\",\"IT7\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\", \"EVC6\",\"EVC7\",\"EVC8\"],\n",
    "    \"MEMORY_SIZE\": 100000\n",
    "        \n",
    "}\n",
    "\n",
    "hp_config = HyperparameterConfig(\n",
    "    #lr=RLParameter(min=6.25e-5, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n",
    ")\n",
    "\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "\n",
    "# Create populations for each agent\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space)] * 15,\n",
    "    action_space=[env.action_space]*15,#,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# Tournament selection\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,\n",
    "    elitism=True,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    eval_loop=1,\n",
    ")\n",
    "\n",
    "# Mutation settings\n",
    "mutations = Mutations(\n",
    "    no_mutation=0.4,\n",
    "    architecture=0.2,\n",
    "    new_layer_prob=0.2,\n",
    "    parameters=0.2,\n",
    "    activation=0,\n",
    "    rl_hp=0.2,\n",
    "    mutation_sd=0.1,\n",
    "    rand_seed=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: output an ID along with the other action to make it visible to only one other agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'IT1': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'IT2': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'IT3': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'IT4': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'IT5': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'IT6': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'IT7': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'EVC1': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'EVC2': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'EVC3': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'EVC4': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'EVC5': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'EVC6': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'EVC7': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32), 'EVC8': array([ 37., 244., 193., ..., 230., 136.,   0.], dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/20000 [20:18<564:09:47, 101.61s/step]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (64,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 102\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mlen\u001b[39m(memory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mand\u001b[39;00m memory\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m>\u001b[39m learning_delay\n\u001b[1;32m     99\u001b[0m ):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_envs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m agent\u001b[38;5;241m.\u001b[39mlearn_step):\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# Sample replay buffer\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m         experiences \u001b[38;5;241m=\u001b[39m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# Learn according to agent's RL algorithm\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         agent\u001b[38;5;241m.\u001b[39mlearn(experiences)\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/components/multi_agent_replay_buffer.py:169\u001b[0m, in \u001b[0;36mMultiAgentReplayBuffer.sample\u001b[0;34m(self, batch_size, *args)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03mReturns sample of experiences from memory.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m:rtype: Tuple\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m experiences \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory, k\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m--> 169\u001b[0m transition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(transition\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/components/multi_agent_replay_buffer.py:145\u001b[0m, in \u001b[0;36mMultiAgentReplayBuffer._process_transition\u001b[0;34m(self, experiences, np_array)\u001b[0m\n\u001b[1;32m    142\u001b[0m ts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mgetattr\u001b[39m(e, field)[agent_id] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences_filtered]\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Stack transitions if necessary\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m ts \u001b[38;5;241m=\u001b[39m \u001b[43mMultiAgentReplayBuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_binary_field:\n\u001b[1;32m    148\u001b[0m     ts \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/components/multi_agent_replay_buffer.py:99\u001b[0m, in \u001b[0;36mMultiAgentReplayBuffer.stack_transitions\u001b[0;34m(transitions)\u001b[0m\n\u001b[1;32m     97\u001b[0m     ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(_ts)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     ts \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ts\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    101\u001b[0m         ts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(ts, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (64,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "max_steps = 20000 \n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "training_steps = 100\n",
    "env = CichyEnv(x_train, y_train1, y_train2)#.parallel_env()\n",
    "agent_ids = [\"IT1\", \"IT2\", \"IT3\",\"IT4\",\"IT5\",\"IT6\",\"IT7\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\", \"EVC6\",\"EVC7\",\"EVC8\"]\n",
    "action_dict = {\n",
    "    agent_id: np.random.uniform(-1, 1, 93)\n",
    "    for agent_id in env.agents  # env.agents should list all 15 agent IDs\n",
    "}\n",
    "#env.step(action_dict)\n",
    "#NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "obs = env.reset()  # This ensures self.actions is initialized\n",
    "obs, rewards, dones,bb, infos = env.step(action_dict)\n",
    "print(type(obs), obs)\n",
    "\n",
    "\"\"\"\n",
    "#slated for deletion \n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space)]*15,#, spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space]*15,#,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device\n",
    ")\n",
    "\"\"\"\n",
    "#print(type(obs), obs)\n",
    "num_envs = 4\n",
    "learning_delay = 0  # Steps before starting learning\n",
    "evo_steps = 100  # Evolution frequency\n",
    "eval_steps = None  # Evaluation steps per episode - go until done\n",
    "eval_loop = 1  \n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "#[\"IT1\", \"IT2\", \"IT3\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\"]\n",
    "#agent = pop_agent[0]\n",
    "#print(agent)\n",
    "pbar = trange(max_steps, unit=\"step\")\n",
    "while np.less([agent.steps[-1] for agent in pop_agent], max_steps).all():\n",
    "    pop_episode_scores = []\n",
    "    for agent in pop_agent:  # Loop through population\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        scores = np.zeros(num_envs)\n",
    "        completed_episode_scores = []\n",
    "        steps = 0\n",
    "\n",
    "    for idx_step in range(training_steps // num_envs):\n",
    "        # Ensure obs is flattened before passing to agent\n",
    "        cont_actions, discrete_action = agent.get_action(obs=obs, training=True, infos=info)\n",
    "        if agent.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "\n",
    "        # Act in environment\n",
    "        next_state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "        scores += np.sum(np.array(list(reward.values())).transpose(), axis=-1)\n",
    "        total_steps += num_envs\n",
    "        steps += num_envs\n",
    "\n",
    "        # Save experiences to replay buffer\n",
    "\n",
    "        #erroring out must fix\n",
    "\n",
    "        memory.save_to_memory(\n",
    "            state,\n",
    "            cont_actions,\n",
    "            reward,\n",
    "            next_state,\n",
    "            termination,\n",
    "            #is_vectorised=True,\n",
    "        )\n",
    "        #\"\"\"\n",
    "\n",
    "        # Learn according to learning frequency\n",
    "        # Handle learn steps > num_envs\n",
    "        if agent.learn_step > num_envs:\n",
    "            learn_step = agent.learn_step // num_envs\n",
    "            if (\n",
    "                idx_step % learn_step == 0\n",
    "                and len(memory) >= agent.batch_size\n",
    "                and memory.counter > learning_delay\n",
    "            ):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "        # Handle num_envs > learn step; learn multiple times per step in env\n",
    "        elif (\n",
    "            len(memory) >= agent.batch_size and memory.counter > learning_delay\n",
    "        ):\n",
    "            for _ in range(num_envs // agent.learn_step):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Calculate scores and reset noise for finished episodes\n",
    "        reset_noise_indices = []\n",
    "        term_array = np.array(list(termination.values())).transpose()\n",
    "        trunc_array = np.array(list(truncation.values())).transpose()\n",
    "        for idx, (d, t) in enumerate(zip(term_array, trunc_array)):\n",
    "            if np.any(d) or np.any(t):\n",
    "                completed_episode_scores.append(scores[idx])\n",
    "                agent.scores.append(scores[idx])\n",
    "                scores[idx] = 0\n",
    "                reset_noise_indices.append(idx)\n",
    "        agent.reset_action_noise(reset_noise_indices)\n",
    "\n",
    "    pbar.update(evo_steps // len(pop_agent))\n",
    "\n",
    "    agent.steps[-1] += steps\n",
    "    pop_episode_scores.append(completed_episode_scores)\n",
    "\n",
    "# Evaluate population\n",
    "fitnesses = [\n",
    "    agent.test(\n",
    "        env,\n",
    "        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "        max_steps=eval_steps,\n",
    "        loop=eval_loop,\n",
    "    )\n",
    "    for agent in pop_agent\n",
    "]\n",
    "mean_scores = [\n",
    "    (\n",
    "        np.mean(episode_scores)\n",
    "        if len(episode_scores) > 0\n",
    "        else \"0 completed episodes\"\n",
    "    )\n",
    "    for episode_scores in pop_episode_scores\n",
    "]\n",
    "\n",
    "print(f\"--- Global steps {total_steps} ---\")\n",
    "print(f\"Steps {[agent.steps[-1] for agent in pop_agent]}\")\n",
    "print(f\"Scores: {mean_scores}\")\n",
    "print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n",
    "print(\n",
    "    f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop_agent]}'\n",
    ")\n",
    "\n",
    "# Tournament selection and population mutation\n",
    "elite, pop = tournament.select(pop_agent)\n",
    "pop = mutations.mutation(pop)\n",
    "\n",
    "# Update step counter\n",
    "for agent in pop:\n",
    "    agent.steps.append(agent.steps[-1])\n",
    "\n",
    "pbar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation for IT1: (30717,)\n",
      "Observation for IT2: (30717,)\n",
      "Observation for IT3: (30717,)\n",
      "Observation for IT4: (30717,)\n",
      "Observation for IT5: (30717,)\n",
      "Observation for IT6: (30717,)\n",
      "Observation for IT7: (30717,)\n",
      "Observation for EVC1: (30717,)\n",
      "Observation for EVC2: (30717,)\n",
      "Observation for EVC3: (30717,)\n",
      "Observation for EVC4: (30717,)\n",
      "Observation for EVC5: (30717,)\n",
      "Observation for EVC6: (30717,)\n",
      "Observation for EVC7: (30717,)\n",
      "Observation for EVC8: (30717,)\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "for agent_id in obs:\n",
    "    print(f\"Observation for {agent_id}: {obs[agent_id].shape}\")\n",
    "\n",
    "\n",
    "#note: 30626 vs 30625 was the origninal error. FOr some reason observations have a shape of 30717 which is 90 rows greater then 30627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT1: (30717,), dtype: float32\n",
      "IT2: (30717,), dtype: float32\n",
      "IT3: (30717,), dtype: float32\n",
      "IT4: (30717,), dtype: float32\n",
      "IT5: (30717,), dtype: float32\n",
      "IT6: (30717,), dtype: float32\n",
      "IT7: (30717,), dtype: float32\n",
      "EVC1: (30717,), dtype: float32\n",
      "EVC2: (30717,), dtype: float32\n",
      "EVC3: (30717,), dtype: float32\n",
      "EVC4: (30717,), dtype: float32\n",
      "EVC5: (30717,), dtype: float32\n",
      "EVC6: (30717,), dtype: float32\n",
      "EVC7: (30717,), dtype: float32\n",
      "EVC8: (30717,), dtype: float32\n",
      "2\n",
      "IT1: (30626,), dtype: float32\n",
      "IT2: (30626,), dtype: float32\n",
      "IT3: (30626,), dtype: float32\n",
      "IT4: (30626,), dtype: float32\n",
      "IT5: (30626,), dtype: float32\n",
      "IT6: (30626,), dtype: float32\n",
      "IT7: (30626,), dtype: float32\n",
      "EVC1: (30626,), dtype: float32\n",
      "EVC2: (30626,), dtype: float32\n",
      "EVC3: (30626,), dtype: float32\n",
      "EVC4: (30626,), dtype: float32\n",
      "EVC5: (30626,), dtype: float32\n",
      "EVC6: (30626,), dtype: float32\n",
      "EVC7: (30626,), dtype: float32\n",
      "EVC8: (30626,), dtype: float32\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "for k, v in obs.items():\n",
    "    print(f\"{k}: {v.shape}, dtype: {v.dtype}\")\n",
    "action_dict = {\n",
    "    agent_id: np.random.uniform(-1, 1, 93)\n",
    "    for agent_id in env.agents  # env.agents should list all 15 agent IDs\n",
    "}\n",
    "actions = action_dict\n",
    "print(\"2\")\n",
    "obs, rewards, term, trunc, info = env.step(actions)\n",
    "for k, v in obs.items():\n",
    "    print(f\"{k}: {v.shape}, dtype: {v.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prisoner': (0, 48, 24), 'guard': (0, 48, 24)}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import random\n",
    "from copy import copy\n",
    "prisoner_y = 0\n",
    "possible_agents = [\"prisoner\", \"guard\"]\n",
    "\n",
    "agents = copy(possible_agents)\n",
    "guard_x = 6\n",
    "guard_y = 6\n",
    "prisoner_x = 0\n",
    "escape_x = random.randint(2, 5)\n",
    "escape_y = random.randint(2, 5)\n",
    "pobs = {\n",
    "    a: (\n",
    "            prisoner_x + 7 * prisoner_y,\n",
    "            guard_x + 7 * guard_y,\n",
    "            escape_x + 7 * escape_y,\n",
    "            )\n",
    "            for a in agents\n",
    "}\n",
    "print(p)\"\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pop, fitnesses = train_multi_agent(\n",
    "    pop=pop_agent,\n",
    "    env=env,\n",
    "    algo=\"MATD3\",\n",
    "    env_name=\"cichyenv\",\n",
    "    memory=memory,\n",
    "    swap_channels=False,\n",
    "    max_steps=500000,\n",
    "    evo_steps=10000,\n",
    "    eval_steps=None,\n",
    "    eval_loop=1,\n",
    "    target=200.0,\n",
    "    tournament=tournament,\n",
    "    wb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CichyEnv(x_train, y_train1, y_train2)\n",
    "\n",
    "# Initial Hyperparameters\n",
    "INIT_HP = {\n",
    "    \"DOUBLE\": True,\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"POPULATION_SIZE\": 9,\n",
    "    \"O_U_NOISE\": 0.2,\n",
    "    \"EXPL_NOISE\": 0.1,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"LR\": 0.001,\n",
    "    \"LR_ACTOR\": 0.002,\n",
    "    \"LR_CRITIC\": 0.002,\n",
    "    \"TAU\": 0.5,\n",
    "    \"GAMMA\": 1.0,\n",
    "    \"LAMBDA\": 1.0,\n",
    "    \"REG\": 0.0625,\n",
    "    \"LEARN_STEP\": 2,\n",
    "    \"MEAN_NOISE\": 1,\n",
    "    \"THETA\": 1,\n",
    "    \"DT\": 1,\n",
    "    \"POLICY_FREQ\": 2,\n",
    "    \"AGENT_IDS\": [\"IT\",\"EVC\"],\n",
    "    \"MEMORY_SIZE\": 100000\n",
    "        \n",
    "}\n",
    "\n",
    "hp_config = HyperparameterConfig(\n",
    "    #lr=RLParameter(min=6.25e-5, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n",
    ")\n",
    "\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "\n",
    "# Create populations for each agent\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# Tournament selection\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,\n",
    "    elitism=True,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    eval_loop=1,\n",
    ")\n",
    "\n",
    "# Mutation settings\n",
    "mutations = Mutations(\n",
    "    no_mutation=0.4,\n",
    "    architecture=0.2,\n",
    "    new_layer_prob=0.2,\n",
    "    parameters=0.2,\n",
    "    activation=0,\n",
    "    rl_hp=0.2,\n",
    "    mutation_sd=0.1,\n",
    "    rand_seed=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code below trains skills\n",
    "\n",
    "note: skills are not full envs. They are classes that connect to an env and only consist of a single reward function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.wrappers.learning import Skill\n",
    "import os\n",
    "from agilerl.algorithms.ppo import PPO\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.wrappers.learning import Skill\n",
    "from agilerl.utils.algo_utils import obs_channels_to_first\n",
    "from agilerl.utils.utils import (\n",
    "   create_population,\n",
    "   make_skill_vect_envs,\n",
    "   make_vect_envs,\n",
    "   observation_space_channels_to_first\n",
    ")\n",
    "NET_CONFIG = {\n",
    "   \"encoder_config\": {\"hidden_size\": [64, 64]}  # Actor encoder hidden size\n",
    "}\n",
    "\n",
    "INIT_HP = {\n",
    "   \"ENV_NAME\": \"LunarLander-v2\",\n",
    "   \"ALGO\": \"PPO\",\n",
    "   \"POPULATION_SIZE\": 1,  # Population size\n",
    "   \"BATCH_SIZE\": 128,  # Batch size\n",
    "   \"LR\": 1e-3,  # Learning rate\n",
    "   \"LEARN_STEP\": 128,  # Learning frequency\n",
    "   \"GAMMA\": 0.99,  # Discount factor\n",
    "   \"GAE_LAMBDA\": 0.95,  # Lambda for general advantage estimation\n",
    "   \"ACTION_STD_INIT\": 0.6,  # Initial action standard deviation\n",
    "   \"CLIP_COEF\": 0.2,  # Surrogate clipping coefficient\n",
    "   \"ENT_COEF\": 0.01,  # Entropy coefficient\n",
    "   \"VF_COEF\": 0.5,  # Value function coefficient\n",
    "   \"MAX_GRAD_NORM\": 0.5,  # Maximum norm for gradient clipping\n",
    "   \"TARGET_KL\": None,  # Target KL divergence threshold\n",
    "   \"TARGET_SCORE\": 2000,\n",
    "   \"MAX_STEPS\": 1_000_000,\n",
    "   \"EVO_STEPS\": 10_000,\n",
    "   \"UPDATE_EPOCHS\": 4,  # Number of policy update epochs\n",
    "   # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "   \"CHANNELS_LAST\": False,\n",
    "   \"WANDB\": True,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Directory to save trained agents and skills\n",
    "save_dir = \"./models/PPO\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "skills = {\n",
    "   \"stabilize\": StabilizeSkill,\n",
    "   \"center\": CenterSkill,\n",
    "   \"landing\": LandingSkill,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for skill in skills.keys():\n",
    "   env = make_skill_vect_envs(\n",
    "         INIT_HP[\"ENV_NAME\"], skills[skill], num_envs=1\n",
    "   )  # Create environment\n",
    "\n",
    "   observation_space = env.single_observation_space\n",
    "   action_space = env.single_action_space\n",
    "   if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "         observation_space = observation_space_channels_to_first(observation_space)\n",
    "\n",
    "   pop = create_population(\n",
    "         algo=\"PPO\",  # Algorithm\n",
    "         observation_space=observation_space,  # Observation space\n",
    "         action_space=action_space,  # Action space\n",
    "         net_config=NET_CONFIG,  # Network configuration\n",
    "         INIT_HP=INIT_HP,  # Initial hyperparameters\n",
    "         population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "         device=device,\n",
    "   )\n",
    "\n",
    "   trained_pop, pop_fitnesses = train_on_policy(\n",
    "         env=env,  # Gym-style environment\n",
    "         env_name=f\"{INIT_HP['ENV_NAME']}-{skill}\",  # Environment name\n",
    "         algo=INIT_HP[\"ALGO\"],  # Algorithm\n",
    "         pop=pop,  # Population of agents\n",
    "         swap_channels=INIT_HP[\n",
    "            \"CHANNELS_LAST\"\n",
    "         ],  # Swap image channel from last to first\n",
    "         max_steps=INIT_HP[\"MAX_STEPS\"],  # Max number of training episodes\n",
    "         evo_steps=INIT_HP[\"EVO_STEPS\"],  # Evolution frequency\n",
    "         evo_loop=3,  # Number of evaluation episodes per agent\n",
    "         target=INIT_HP[\"TARGET_SCORE\"],  # Target score for early stopping\n",
    "         tournament=None,  # Tournament selection object\n",
    "         mutation=None,  # Mutations object\n",
    "         wb=INIT_HP[\"WANDB\"],  # Weights and Biases tracking\n",
    "   )\n",
    "\n",
    "   # Save the trained algorithm\n",
    "   filename = f\"PPO_trained_agent_{skill}.pt\"\n",
    "   save_path = os.path.join(save_dir, filename)\n",
    "   trained_pop[0].save_checkpoint(save_path)\n",
    "\n",
    "   env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is for the meta selector agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stabilize_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_stabilize.pt\"))\n",
    "center_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_center.pt\"))\n",
    "landing_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_landing.pt\"))\n",
    "\n",
    "trained_skills = {\n",
    "   0: {\"skill\": \"stabilize\", \"agent\": stabilize_agent, \"skill_duration\": 40},\n",
    "   1: {\"skill\": \"center\", \"agent\": center_agent, \"skill_duration\": 40},\n",
    "   2: {\"skill\": \"landing\", \"agent\": landing_agent, \"skill_duration\": 40},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vect_envs(INIT_HP[\"ENV_NAME\"], num_envs=1)  # Create environment\n",
    "\n",
    "observation_space = env.single_observation_space\n",
    "\n",
    "action_dim = len(\n",
    "   trained_skills\n",
    ")  # Selector will be trained to choose which trained skill to use\n",
    "\n",
    "action_space = spaces.Discrete(action_dim)\n",
    "\n",
    "if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "   observation_space = observation_space_channels_to_first(observation_space)\n",
    "\n",
    "pop = create_population(\n",
    "   algo=\"PPO\",  # Algorithm\n",
    "   observation_space=observation_space,  # Observation space\n",
    "   action_space=action_space,  # Action space\n",
    "   net_config=NET_CONFIG,  # Network configuration\n",
    "   INIT_HP=INIT_HP,  # Initial hyperparameters\n",
    "   population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "   device=device,\n",
    ")\n",
    "\n",
    "if INIT_HP[\"WANDB\"]:\n",
    "   wandb.init(\n",
    "         # set the wandb project where this run will be logged\n",
    "         project=\"EvoWrappers\",\n",
    "         name=\"{}-EvoHPO-{}-{}\".format(\n",
    "            INIT_HP[\"ENV_NAME\"],\n",
    "            INIT_HP[\"ALGO\"],\n",
    "            datetime.now().strftime(\"%m%d%Y%H%M%S\"),\n",
    "         ),\n",
    "         # track hyperparameters and run metadata\n",
    "         config={\n",
    "            \"algo\": f\"Evo HPO {INIT_HP['ALGO']}\",\n",
    "            \"env\": INIT_HP[\"ENV_NAME\"],\n",
    "            \"INIT_HP\": INIT_HP,\n",
    "         },\n",
    "   )\n",
    "\n",
    "bar_format = \"{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]\"\n",
    "pbar = trange(\n",
    "  INIT_HP[\"MAX_STEPS\"],\n",
    "  unit=\"step\",\n",
    "  bar_format=bar_format,\n",
    "  ascii=True)\n",
    "\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.less([agent.steps[-1] for agent in pop], INIT_HP[\"MAX_STEPS\"]).all():\n",
    "   for agent in pop:  # Loop through population\n",
    "         state = env.reset()[0]  # Reset environment at start of episode\n",
    "         score = 0\n",
    "\n",
    "         states = []\n",
    "         actions = []\n",
    "         log_probs = []\n",
    "         rewards = []\n",
    "         terminations = []\n",
    "         values = []\n",
    "\n",
    "         for idx_step in range(500):\n",
    "            # Get next action from agent\n",
    "            action, log_prob, _, value = agent.get_action(state)\n",
    "\n",
    "            # Internal loop to execute trained skill\n",
    "            skill_agent = trained_skills[action[0]][\"agent\"]\n",
    "            skill_duration = trained_skills[action[0]][\"skill_duration\"]\n",
    "            reward = 0\n",
    "            for skill_step in range(skill_duration):\n",
    "               # If landed, do nothing\n",
    "               if state[0][6] or state[0][7]:\n",
    "                     next_state, skill_reward, termination, truncation, _ = env.step(\n",
    "                        [0]\n",
    "                     )\n",
    "               else:\n",
    "                     skill_action, _, _, _ = skill_agent.get_action(state)\n",
    "                     next_state, skill_reward, termination, truncation, _ = env.step(\n",
    "                        skill_action\n",
    "                     )  # Act in environment\n",
    "               reward += skill_reward\n",
    "               if np.any(termination) or np.any(truncation):\n",
    "                     break\n",
    "               state = next_state\n",
    "            score += reward\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            terminations.append(termination)\n",
    "            values.append(value)\n",
    "\n",
    "         agent.scores.append(score)\n",
    "\n",
    "         # Learn according to agent's RL algorithm\n",
    "         agent.learn(\n",
    "            (\n",
    "               states,\n",
    "               actions,\n",
    "               log_probs,\n",
    "               rewards,\n",
    "               terminations,\n",
    "               values,\n",
    "               next_state,\n",
    "            )\n",
    "         )\n",
    "\n",
    "         agent.steps[-1] += idx_step + 1\n",
    "         total_steps += idx_step + 1\n",
    "\n",
    "   if (agent.steps[-1]) % INIT_HP[\"EVO_STEPS\"] == 0:\n",
    "      mean_scores = np.mean([agent.scores[-20:] for agent in pop], axis=1)\n",
    "      if INIT_HP[\"WANDB\"]:\n",
    "          wandb.log(\n",
    "              {\n",
    "                  \"global_step\": total_steps,\n",
    "                  \"train/mean_score\": np.mean(mean_scores),\n",
    "              }\n",
    "          )\n",
    "      print(\n",
    "          f\"\"\"\n",
    "          --- Global Steps {total_steps} ---\n",
    "          Score:\\t\\t{mean_scores}\n",
    "          \"\"\",\n",
    "          end=\"\\r\",\n",
    "      )\n",
    "\n",
    "if INIT_HP[\"WANDB\"]:\n",
    "   wandb.finish()\n",
    "env.close()\n",
    "\n",
    "# Save the trained selector\n",
    "filename = \"PPO_trained_agent_selector.pt\"\n",
    "save_path = os.path.join(save_dir, filename)\n",
    "pop[0].save_checkpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
