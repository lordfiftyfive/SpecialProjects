{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52d5ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "if we do this carefully we can use taichi to carry out speedup\n",
    "\n",
    "The mixer and the bmodel would be ti.funcs\n",
    "\n",
    "qmixpolicy would also be a ti.func\n",
    "\n",
    "qmix would be the ti.kernel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import torch\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "#from nebulgym.decorators.torch_decorators import accel\n",
    "\n",
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "#from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE, make_multi_agent\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "\n",
    "import nitime\n",
    "from deeptime.sindy import SINDy\n",
    "\n",
    "#data visualization\n",
    "import pygwalker as pyg\n",
    "\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0731a0f3",
   "metadata": {},
   "source": [
    "The expected free energy is minimized by selecting those observations that cause a large change in beliefs, in contrast to variational free energy that is minimized when observations comply wiht current beliefs\n",
    "\n",
    "This is the difference between optmizing beliefs in relation to data that have already been gathered (VFE) and selecting data that will best optimize beliefs \n",
    "\n",
    "Expected free energy furnishes our prior and is a prerequesite in minimzing variational free energy\n",
    "\n",
    "Note: the expected free energy will only be used on the level of the mixer network \n",
    "\n",
    "prior at top level agent will be: dirchlet by itself\n",
    "\n",
    "variational free energy according to friston determines the fit between generative model and past and current observations\n",
    "\n",
    "Variational free energy only depends on present and past observations. expected free energy is an extenstion that requires predicted future observations \n",
    "\n",
    "\"variational free neergy is at the core of Active inference. it measrues the fit between the generative model and current and past observations\"\n",
    "\n",
    "\"expected free energy is a way to score alternative policies for planning\"\n",
    "\n",
    "\"variational free energy minimzation is the outer loop of active inference... an active inference agent can also be endowed with a generative model of the consequences of its action that entails an evaluation of expected free energy (inside loop)\"\n",
    "\n",
    "\n",
    "First q will be produced using variational free energy while second q will be used to evaluate first q using expected free energy\n",
    "\n",
    "all of the neuronal agents will be rewarded based entirely on what happens in the inverse cartpole\n",
    "\n",
    "reward will be if the cartpole remains upright\n",
    "\n",
    "loss in qmix loss will deal with variational and expected free energy\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "676cdad1",
   "metadata": {},
   "source": [
    "In the presentation we are going to characterize the brain as a natural experimenter\n",
    "\n",
    "Expected free energy is an extension that allows for planning. it essentially allows for what if and counterfactual simulations\n",
    "\n",
    "We may need a different model for each type of q in double q learning. the main q will be produced with variational free energy and the evaluation q will be produced by expected free energy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f760fa5",
   "metadata": {},
   "source": [
    "there are two pathways that summarize basal ganglia connectivity according to friston:\n",
    "\n",
    "the direct pathway which stimulates motor action and conext sensitive priors in the indirect pathway inhibits unlikely policies. we are going to have to add two policy layers at the same level. Too little dopamine favors the indirect pathway whereas more dopamine favors the direct pathway and too much can promote impulsive behaviors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d8bed4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmc_eig' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_24132\\4097240091.py\"\u001b[1;36m, line \u001b[1;32m15\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    eig = nmc_eig(pmodel, design, observation_labels=[\"y\"], target_labels=[\"theta\"], N=2500, M=50)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'nmc_eig' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pmodel(design):\n",
    "\n",
    "    # This line allows batching of designs, treating all batch dimensions as independent\n",
    "    with pyro.plate_stack(\"plate_stack\", design.shape):\n",
    "\n",
    "        # We use a dirchlet prior for theta\n",
    "        theta = pyro.sample(\"theta\", dist.Normal(torch.tensor(0.0), torch.tensor(1.0)))\n",
    "        #theta = pyro.sample(\"theta\", dist.Dirichlet())\n",
    "        # We use a simple logistic regression model for the likelihood\n",
    "        logit_p = theta - design\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p))\n",
    "\n",
    "        return y\n",
    "\n",
    "eig = nmc_eig(pmodel, design, observation_labels=[\"y\"], target_labels=[\"theta\"], N=2500, M=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "def pmodel(polling_allocation):\n",
    "    # This allows us to run many copies of the model in parallel\n",
    "    with pyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "        # Begin by sampling alpha\n",
    "        alpha = pyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "            prior_mean, covariance_matrix=prior_covariance))\n",
    "\n",
    "        # Sample y conditional on alpha\n",
    "        poll_results = pyro.sample(\"y\", dist.Binomial(\n",
    "            polling_allocation, logits=alpha).to_event(1))\n",
    "\n",
    "        # Now compute w according to the (approximate) electoral college formula\n",
    "        dem_win = election_winner(alpha)\n",
    "        pyro.sample(\"w\", dist.Delta(dem_win))\n",
    "\n",
    "        return poll_results, dem_win, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class OutcomePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b2759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_entropy = dist.Bernoulli(prior_w_prob).entropy()\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "poll_in_florida = torch.zeros(51)\n",
    "poll_in_florida[9] = 1000\n",
    "\n",
    "poll_in_dc = torch.zeros(51)\n",
    "poll_in_dc[8] = 1000\n",
    "\n",
    "uniform_poll = (1000 // 51) * torch.ones(51)\n",
    "\n",
    "# The swing score measures how close the state is to 50/50\n",
    "swing_score = 1. / (.5 - torch.tensor(prior_prob_dem.sort_values(\"State\").values).squeeze()).abs()\n",
    "swing_poll = 1000 * swing_score / swing_score.sum()\n",
    "swing_poll = swing_poll.round()\n",
    "\n",
    "poll_strategies = OrderedDict([(\"Florida\", poll_in_florida),\n",
    "                               (\"DC\", poll_in_dc),\n",
    "                               (\"Uniform\", uniform_poll),\n",
    "                               (\"Swing\", swing_poll)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6831704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "293cb9f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "939bedff",
   "metadata": {},
   "source": [
    "For expected free energy the prior entropy will come from a dirchlet distribution\n",
    "\n",
    "The \"guide\" will be a ML substitute for marginal density p(y|d). In the example election case the ml substitute binary classifier because of the w binary variable. \n",
    "\n",
    "in the example w is the target labels y is the observation labels and allocation is the design\n",
    "\n",
    "For project pegasus we will use a multi-label classifier with a dirchlet prior plugged into a posterior_eig \n",
    "\n",
    "\n",
    "final plan for expected free energy\n",
    "\n",
    "guide calculates probability of p(y|d) where y is the outcome and d is the policy. \n",
    "\n",
    "we are going to have a guide function with 2 layers: a LSTM layer and a variational gaussian process layer. This guide function will figure out an outcome given a design (basically policy). \n",
    "\n",
    "in this case what our algorithm will be tasked with figuring out is given a particular policy what Y can we hope to see\n",
    "\n",
    "in our case w is the reward that we get from the cartpole whereas y is what we observe\n",
    "\n",
    "we \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "\n",
    "eigs = {}\n",
    "best_strategy, best_eig = None, 0\n",
    "\n",
    "for strategy, allocation in poll_strategies.items():\n",
    "    print(strategy, end=\" \")\n",
    "    guide = OutcomePredictor()\n",
    "    pyro.clear_param_store()\n",
    "    # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "    # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "    # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "    ape = posterior_eig(pmodel, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "    eigs[strategy] = prior_entropy - ape\n",
    "    print(eigs[strategy].item())\n",
    "    if eigs[strategy] > best_eig:\n",
    "        best_strategy, best_eig = strategy, eigs[strategy]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90f0b26c",
   "metadata": {},
   "source": [
    "work on in 3/22/2023\n",
    "\n",
    "develop emodels into expected free energy and work on integration of variational free energy in the qmix loss module \n",
    "\n",
    "also work on visualization module \n",
    "\n",
    "we need to also create a neural network which based on observations it sees adjusts the prior covariance (prior precision)\n",
    "\n",
    "expected free energy is (expected information gain - correlation between observation and states) - pragmatic value\n",
    "\n",
    "where pragmatic value is the probability of observation given prior\n",
    "\n",
    "\n",
    "we are going to assume distance does not matter and proceed with visualziation \n",
    "\n",
    "plan for visualization is that the neurons at each level will recieve a random placement withen a certain range\n",
    "\n",
    "we aslo need to figure out how to calculate the pragmatic value, which is the probability of observation given state \n",
    "\n",
    "we also need to start developing how the motor neurons, the higher level neurons and the cartpole will fit together \n",
    "\n",
    "\n",
    "the guide is q(y|d) where d is design or for our case is policy and y is what is observed \n",
    "\n",
    "y is the observations. theta is the hidden variable which for our purposes is the reward. \n",
    "\n",
    "in our case q will be a expected free energy submodule. In one of the example it was also a neural network classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd68fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create this like the outcome predictor. This is the expected free energy sub-module ESM\n",
    "#this will essentially calculate the  \n",
    "class ESM(nn.Module,ivy.Module):\n",
    "     def __init__(self):\n",
    "        super().__init__()\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        #model = tf.keras.Sequential([\n",
    "        u = tfkl.InputLayer(input_shape=input_shape),\n",
    "        \n",
    "        u = tf.keras.layers.LSTM(25,kernel_initializer='zeros',activation='tanh', dtype = x.dtype, use_bias=True)(u),\n",
    "        u = tfp.layers.VariationalGaussianProcess(\n",
    "                num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\n",
    "                inducing_index_points_initializer=tf.compat.v1.constant_initializer(\n",
    "                    np.linspace(0,x_range, num=1125,\n",
    "                                dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))), mean_fn=None,\n",
    "                jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)(u)\n",
    "\n",
    "  \n",
    "    #in unconstrained thing replace astype with tf.dtype thing.    #tf.initializers.constant(-10.0)\n",
    "    #])\n",
    "    def compute_dem_probability(self, y):\n",
    "        #fk.Model()\n",
    "        o = tf.nn.relu(u)\n",
    "        return o \n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        \n",
    "        #bmodel = FullLaplace(bmodel,'regression',prior_precision=2)\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c725bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.504077, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#expected free energy\n",
    "\n",
    "class emodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        guide = ESM()\n",
    "        pyro.clear_param_store()\n",
    "        # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "        # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "        # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "        \n",
    "        #ape = average posterior entropy\n",
    "        ape = posterior_eig(model, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                            Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "        eigs[strategy] = prior_entropy - ape\n",
    "        print(eigs[strategy].item())\n",
    "        if eigs[strategy] > best_eig:\n",
    "            best_strategy, best_eig = strategy, eigs[strategy]\n",
    "            \n",
    "        pragmatic_value = 0\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Next objective: determine the probaility of hidden state given observation\n",
    "        \n",
    "        we should use a neural network to calculate the probability of reward given action \n",
    "        \n",
    "        if there is 0 correlation, that is to say reward does not change given a certain action then p(s|y) = 0\n",
    "        \n",
    "        this is the pragmatic value\n",
    "        \n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        #q = self.fc2(h)\n",
    "        vae = tfk.Model(inputs=input_dict[\"obs_flat\"],\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n",
    "ivy.set_framework('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b522a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "#@ti.func\n",
    "class bmodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        \"\"\"\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(priora, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        #q = self.fc2(h)\n",
    "        vae = tfk.Model(inputs=input_dict[\"obs_flat\"],\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        kl = tf.keras.losses.KLDivergence()\n",
    "        model.add_loss(kl)\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595127bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "#@ti.func\n",
    "class bmodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        \"\"\"\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(priora, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        #q = self.fc2(h)\n",
    "        vae = tfk.Model(inputs=input_dict[\"obs_flat\"],\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        kl = tf.keras.losses.KLDivergence()\n",
    "        model.add_loss(kl)\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf07e2e3",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: We were creating a variational autoencoder that will infer hidden states from observations  in the context of a partially observed markov decision process and generate actions\n",
    "\n",
    "However as of april 6 2023 we can conclude that we do not need a variational autoencoder. The reason being that bmodel which the VAE was for and is part of the variational autoencoder optimization IS NOT our action/policy selection algorithm. it is our ACTION/policy EVALUATION algoritym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f3c954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "#guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "#elbo = elbo_(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e35a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\"\"\"\n",
    "next objective for april 8th 2023: we are going to have to modify the mixer network \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#@ti.func\n",
    "class QMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape, mixing_embed_dim):\n",
    "        super(QMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.embed_dim = mixing_embed_dim\n",
    "        self.state_dim = int(np.prod(state_shape))\n",
    "\n",
    "        self.hyper_w_1 = nn.Linear(self.state_dim, self.embed_dim * self.n_agents)\n",
    "        self.hyper_w_final = nn.Linear(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # State dependent bias for hidden layer\n",
    "        self.hyper_b_1 = nn.Linear(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # V(s) instead of a bias for the last layers\n",
    "        self.V = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embed_dim, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, agent_qs, states):\n",
    "        \"\"\"Forward pass for the mixer.\n",
    "        Args:\n",
    "            agent_qs: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            states: Tensor of shape [B, T, state_dim]\n",
    "        \"\"\"\n",
    "        bs = agent_qs.size(0)\n",
    "        states = states.reshape(-1, self.state_dim)\n",
    "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
    "        # First layer\n",
    "        w1 = torch.abs(self.hyper_w_1(states))\n",
    "        b1 = self.hyper_b_1(states)\n",
    "        w1 = w1.view(-1, self.n_agents, self.embed_dim)\n",
    "        b1 = b1.view(-1, 1, self.embed_dim)\n",
    "        hidden = nn.functional.elu(torch.bmm(agent_qs, w1) + b1)\n",
    "        # Second layer\n",
    "        w_final = torch.abs(self.hyper_w_final(states))\n",
    "        w_final = w_final.view(-1, self.embed_dim, 1)\n",
    "        # State-dependent bias\n",
    "        v = self.V(states).view(-1, 1, 1)\n",
    "        # Compute final output\n",
    "        y = torch.bmm(hidden, w_final) + v\n",
    "        # Reshape and return\n",
    "        q_tot = y.view(bs, -1, 1)\n",
    "        return q_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca58d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape, mixing_embed_dim):\n",
    "        super(QMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.embed_dim = mixing_embed_dim\n",
    "        self.state_dim = int(np.prod(state_shape))\n",
    "\n",
    "        self.hyper_w_1 = nn.Linear(self.state_dim, self.embed_dim * self.n_agents)\n",
    "        self.hyper_w_final = nn.Linear(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # State dependent bias for hidden layer\n",
    "        self.hyper_b_1 = nn.Linear(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # V(s) instead of a bias for the last layers\n",
    "        self.V = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embed_dim, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, agent_qs, states):\n",
    "        \"\"\"Forward pass for the mixer.\n",
    "        Args:\n",
    "            agent_qs: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            states: Tensor of shape [B, T, state_dim]\n",
    "        \"\"\"\n",
    "        bs = agent_qs.size(0)\n",
    "        states = states.reshape(-1, self.state_dim)\n",
    "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
    "        # First layer\n",
    "        w1 = torch.abs(self.hyper_w_1(states))\n",
    "        b1 = self.hyper_b_1(states)\n",
    "        w1 = w1.view(-1, self.n_agents, self.embed_dim)\n",
    "        b1 = b1.view(-1, 1, self.embed_dim)\n",
    "        hidden = nn.functional.elu(torch.bmm(agent_qs, w1) + b1)\n",
    "        # Second layer\n",
    "        w_final = torch.abs(self.hyper_w_final(states))\n",
    "        w_final = w_final.view(-1, self.embed_dim, 1)\n",
    "        # State-dependent bias\n",
    "        v = self.V(states).view(-1, 1, 1)\n",
    "        # Compute final output\n",
    "        y = torch.bmm(hidden, w_final) + v\n",
    "        # Reshape and return\n",
    "        q_tot = y.view(bs, -1, 1)\n",
    "        return q_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50fcf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ti.func\n",
    "class QMixLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        target_model,\n",
    "        mixer,\n",
    "        target_mixer,\n",
    "        n_agents,\n",
    "        n_actions,\n",
    "        double_q=True,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.mixer = mixer\n",
    "        self.target_mixer = target_mixer\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        self.double_q = double_q\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rewards,\n",
    "        actions,\n",
    "        terminated,\n",
    "        mask,\n",
    "        obs,\n",
    "        next_obs,\n",
    "        action_mask,\n",
    "        next_action_mask,\n",
    "        state=None,\n",
    "        next_state=None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the loss.\n",
    "        Args:\n",
    "            rewards: Tensor of shape [B, T, n_agents]\n",
    "            actions: Tensor of shape [B, T, n_agents]\n",
    "            terminated: Tensor of shape [B, T, n_agents]\n",
    "            mask: Tensor of shape [B, T, n_agents]\n",
    "            obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            next_obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            next_action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            state: Tensor of shape [B, T, state_dim] (optional)\n",
    "            next_state: Tensor of shape [B, T, state_dim] (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert either none or both of state and next_state are given\n",
    "        if state is None and next_state is None:\n",
    "            state = obs  # default to state being all agents' observations\n",
    "            next_state = next_obs\n",
    "        elif (state is None) != (next_state is None):\n",
    "            raise ValueError(\n",
    "                \"Expected either neither or both of `state` and \"\n",
    "                \"`next_state` to be given. Got: \"\n",
    "                \"\\n`state` = {}\\n`next_state` = {}\".format(state, next_state)\n",
    "            )\n",
    "\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = _unroll_mac(self.model, obs)\n",
    "\n",
    "        # Pick the Q-Values for the actions taken -> [B * n_agents, T]\n",
    "        chosen_action_qvals = torch.gather(\n",
    "            mac_out, dim=3, index=actions.unsqueeze(3)\n",
    "        ).squeeze(3)\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = _unroll_mac(self.target_model, next_obs)\n",
    "\n",
    "        # Mask out unavailable actions for the t+1 step\n",
    "        ignore_action_tp1 = (next_action_mask == 0) & (mask == 1).unsqueeze(-1)\n",
    "        target_mac_out[ignore_action_tp1] = -np.inf\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.double_q:\n",
    "            # Double Q learning computes the target Q values by selecting the\n",
    "            # t+1 timestep action according to the \"policy\" neural network and\n",
    "            # then estimating the Q-value of that action with the \"target\"\n",
    "            # neural network\n",
    "            \n",
    "            #target neural network does expected free energy while policy\n",
    "            #neural network will be variational free energy\n",
    "\n",
    "            # Compute the t+1 Q-values to be used in action selection\n",
    "            # using next_obs\n",
    "            mac_out_tp1 = _unroll_mac(self.model, next_obs)\n",
    "\n",
    "            # mask out unallowed actions\n",
    "            mac_out_tp1[ignore_action_tp1] = -np.inf\n",
    "\n",
    "            # obtain best actions at t+1 according to policy NN\n",
    "            cur_max_actions = mac_out_tp1.argmax(dim=3, keepdim=True)\n",
    "\n",
    "            # use the target network to estimate the Q-values of policy\n",
    "            # network's selected actions\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(\n",
    "                3\n",
    "            )\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "        assert (\n",
    "            target_max_qvals.min().item() != -np.inf\n",
    "        ), \"target_max_qvals contains a masked action; \\\n",
    "            there may be a state with no valid actions.\"\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state)\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, next_state)\n",
    "\n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n",
    "        \"\"\"\n",
    "        \n",
    "        guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        elbo = elbo_(model, guide)\n",
    "        \n",
    "        el = elbo(data)\n",
    "        \n",
    "        #rewards = {self.agent_1:kullbacker, self.agent_2: el}\n",
    "        #loss = lambda y, rv_y: rv_y.variational_loss(y, kl_weight=np.array(batch_size, x.dtype) / x.shape[0])\n",
    "                \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Td-error\n",
    "        #we need to replace this with a variational free energy error\n",
    "        td_error = chosen_action_qvals - targets.detach()\n",
    "        #te_error= tf.keras.losses.KLDivergence(chosen_action_qvals - targets.detach()).numpy()\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)#this is ELBO \n",
    "        \n",
    "        #we are going to ahve a kldivergence loss and a -ELBO regularizer for the mixer network \n",
    "        \n",
    "        mask = mask.expand_as(td_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = td_error * mask\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        \n",
    "        #guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        #elbo_ = pyro.infer.Trace_ELBO(num_particles=1)\n",
    "\n",
    "        # Fix the model/guide pair\n",
    "        #elbo = elbo_(model, guide)        \n",
    "        \n",
    "        #data = obs + preds \n",
    "        #loss = -ln()\n",
    "        #loss = lambda y, rv_y: -rv_y.log_prob(y)\n",
    "        loss = (masked_td_error**2).sum() / mask.sum()\n",
    "        return loss, mask, masked_td_error, chosen_action_qvals, targets\n",
    "\n",
    "    \n",
    "#this part just above is what we need to revise\n",
    "    \n",
    "#@ti.func\n",
    "class QMixTorchPolicy(TorchPolicy):\n",
    "    \"\"\"QMix impl. Assumes homogeneous agents for now.\n",
    "    You must use MultiAgentEnv.with_agent_groups() to group agents\n",
    "    together for QMix. This creates the proper Tuple obs/action spaces and\n",
    "    populates the '_group_rewards' info field.\n",
    "    Action masking: to specify an action mask for individual agents, use a\n",
    "    dict space with an action_mask key, e.g. {\"obs\": ob, \"action_mask\": mask}.\n",
    "    The mask space must be `Box(0, 1, (n_actions,))`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        # We want to error out on instantiation and not on import, because tune\n",
    "        # imports all RLlib algorithms when registering them\n",
    "        # TODO (Artur): Find a way to only import algorithms when needed\n",
    "        if not torch:\n",
    "            raise ImportError(\"Could not import PyTorch, which QMix requires.\")\n",
    "\n",
    "        _validate(obs_space, action_space)\n",
    "        config = dict(ray.rllib.algorithms.qmix.qmix.DEFAULT_CONFIG, **config)\n",
    "        self.framework = \"torch\"\n",
    "\n",
    "        self.n_agents = 3000#len(obs_space.original_space.spaces)\n",
    "        config[\"model\"][\"n_agents\"] = self.n_agents\n",
    "        self.n_actions = action_space.spaces[0].n\n",
    "        self.h_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "        self.has_env_global_state = False\n",
    "        self.has_action_mask = False\n",
    "\n",
    "        agent_obs_space = obs_space.original_space.spaces[0]\n",
    "        if isinstance(agent_obs_space, gym.spaces.Dict):\n",
    "            space_keys = set(agent_obs_space.spaces.keys())\n",
    "            if \"obs\" not in space_keys:\n",
    "                raise ValueError(\"Dict obs space must have subspace labeled `obs`\")\n",
    "            self.obs_size = _get_size(agent_obs_space.spaces[\"obs\"])\n",
    "            if \"action_mask\" in space_keys:\n",
    "                mask_shape = tuple(agent_obs_space.spaces[\"action_mask\"].shape)\n",
    "                if mask_shape != (self.n_actions,):\n",
    "                    raise ValueError(\n",
    "                        \"Action mask shape must be {}, got {}\".format(\n",
    "                            (self.n_actions,), mask_shape\n",
    "                        )\n",
    "                    )\n",
    "                self.has_action_mask = True\n",
    "            if ENV_STATE in space_keys:\n",
    "                self.env_global_state_shape = _get_size(\n",
    "                    agent_obs_space.spaces[ENV_STATE]\n",
    "                )\n",
    "                self.has_env_global_state = True\n",
    "            else:\n",
    "                self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            # The real agent obs space is nested inside the dict\n",
    "            config[\"model\"][\"full_obs_space\"] = agent_obs_space\n",
    "            agent_obs_space = agent_obs_space.spaces[\"obs\"]\n",
    "        else:\n",
    "            self.obs_size = _get_size(agent_obs_space)\n",
    "            self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "        #model = bmodel()#CModel()#CModel()\n",
    "        #bmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#\n",
    "        ivy.set_framework('torch')\n",
    "        #model = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')\n",
    "        bmodel = bmodel()\n",
    "        bmodel = FullLaplace(bmodel,'regression',prior_precision=2)#0.0000000000000000000001)\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"model\",\n",
    "            default_model=bmodel#a#RNNModel#bmodel()#RNNModel,\n",
    "        )\n",
    "\n",
    "        super().__init__(obs_space, action_space, config, model=self.model)\n",
    "\n",
    "        self.target_model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"target_model\",\n",
    "            default_model=bmodel#bmodel()#RNNModel\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.exploration = self._create_exploration()\n",
    "\n",
    "        # Setup the mixer network.\n",
    "        if config[\"mixer\"] is None:\n",
    "            self.mixer = None\n",
    "            self.target_mixer = None\n",
    "        elif config[\"mixer\"] == \"qmix\":\n",
    "            self.mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "            self.target_mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.cur_epsilon = 1.0\n",
    "        self.update_target()  # initial sync\n",
    "\n",
    "        # Setup optimizer\n",
    "        self.params = list(self.model.parameters())\n",
    "        if self.mixer:\n",
    "            self.params += list(self.mixer.parameters())\n",
    "        self.loss = QMixLoss(\n",
    "            self.model,\n",
    "            self.target_model,\n",
    "            self.mixer,\n",
    "            self.target_mixer,\n",
    "            self.n_agents,\n",
    "            self.n_actions,\n",
    "            self.config[\"double_q\"],\n",
    "            self.config[\"gamma\"],\n",
    "        )\n",
    "        from torch.optim import RMSprop\n",
    "\n",
    "        self.rmsprop_optimizer = RMSprop(\n",
    "            params=self.params,\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"optim_alpha\"],\n",
    "            eps=config[\"optim_eps\"],\n",
    "        )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions_from_input_dict(\n",
    "        self,\n",
    "        input_dict: Dict[str, TensorType],\n",
    "        explore: bool = None,\n",
    "        timestep: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:\n",
    "\n",
    "        obs_batch = input_dict[SampleBatch.OBS]\n",
    "        state_batches = []\n",
    "        i = 0\n",
    "        while f\"state_in_{i}\" in input_dict:\n",
    "            state_batches.append(input_dict[f\"state_in_{i}\"])\n",
    "            i += 1\n",
    "\n",
    "        explore = explore if explore is not None else self.config[\"explore\"]\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        # We need to ensure we do not use the env global state\n",
    "        # to compute actions\n",
    "\n",
    "        # Compute actions\n",
    "        with torch.no_grad():\n",
    "            q_values, hiddens = _mac(\n",
    "                self.model,\n",
    "                torch.as_tensor(obs_batch, dtype=torch.float, device=self.device),\n",
    "                [\n",
    "                    torch.as_tensor(np.array(s), dtype=torch.float, device=self.device)\n",
    "                    for s in state_batches\n",
    "                ],\n",
    "            )\n",
    "            avail = torch.as_tensor(action_mask, dtype=torch.float, device=self.device)\n",
    "            masked_q_values = q_values.clone()\n",
    "            masked_q_values[avail == 0.0] = -float(\"inf\")\n",
    "            masked_q_values_folded = torch.reshape(\n",
    "                masked_q_values, [-1] + list(masked_q_values.shape)[2:]\n",
    "            )\n",
    "            actions, _ = self.exploration.get_exploration_action(\n",
    "                action_distribution=TorchCategorical(masked_q_values_folded),\n",
    "                timestep=timestep,\n",
    "                explore=explore,\n",
    "            )\n",
    "            actions = (\n",
    "                torch.reshape(actions, list(masked_q_values.shape)[:-1]).cpu().numpy()\n",
    "            )\n",
    "            hiddens = [s.cpu().numpy() for s in hiddens]\n",
    "\n",
    "        return tuple(actions.transpose([1, 0])), hiddens, {}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions(self, *args, **kwargs):\n",
    "        return self.compute_actions_from_input_dict(*args, **kwargs)\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "    ):\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        return np.zeros(obs_batch.size()[0])\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        obs_batch, action_mask, env_global_state = self._unpack_observation(\n",
    "            samples[SampleBatch.CUR_OBS]\n",
    "        )\n",
    "        (\n",
    "            next_obs_batch,\n",
    "            next_action_mask,\n",
    "            next_env_global_state,\n",
    "        ) = self._unpack_observation(samples[SampleBatch.NEXT_OBS])\n",
    "        group_rewards = self._get_group_rewards(samples[SampleBatch.INFOS])\n",
    "\n",
    "        input_list = [\n",
    "            group_rewards,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            samples[SampleBatch.ACTIONS],\n",
    "            samples[SampleBatch.TERMINATEDS],\n",
    "            obs_batch,\n",
    "            next_obs_batch,\n",
    "        ]\n",
    "        if self.has_env_global_state:\n",
    "            input_list.extend([env_global_state, next_env_global_state])\n",
    "\n",
    "        output_list, _, seq_lens = chop_into_sequences(\n",
    "            episode_ids=samples[SampleBatch.EPS_ID],\n",
    "            unroll_ids=samples[SampleBatch.UNROLL_ID],\n",
    "            agent_indices=samples[SampleBatch.AGENT_INDEX],\n",
    "            feature_columns=input_list,\n",
    "            state_columns=[],  # RNN states not used here\n",
    "            max_seq_len=self.config[\"model\"][\"max_seq_len\"],\n",
    "            dynamic_max=True,\n",
    "        )\n",
    "        # These will be padded to shape [B * T, ...]\n",
    "        if self.has_env_global_state:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "                env_global_state,\n",
    "                next_env_global_state,\n",
    "            ) = output_list\n",
    "        else:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "            ) = output_list\n",
    "        B, T = len(seq_lens), max(seq_lens)\n",
    "\n",
    "        def to_batches(arr, dtype):\n",
    "            new_shape = [B, T] + list(arr.shape[1:])\n",
    "            return torch.as_tensor(\n",
    "                np.reshape(arr, new_shape), dtype=dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        rewards = to_batches(rew, torch.float)\n",
    "        actions = to_batches(act, torch.long)\n",
    "        obs = to_batches(obs, torch.float).reshape([B, T, self.n_agents, self.obs_size])\n",
    "        action_mask = to_batches(action_mask, torch.float)\n",
    "        next_obs = to_batches(next_obs, torch.float).reshape(\n",
    "            [B, T, self.n_agents, self.obs_size]\n",
    "        )\n",
    "        next_action_mask = to_batches(next_action_mask, torch.float)\n",
    "        if self.has_env_global_state:\n",
    "            env_global_state = to_batches(env_global_state, torch.float)\n",
    "            next_env_global_state = to_batches(next_env_global_state, torch.float)\n",
    "\n",
    "        # TODO(ekl) this treats group termination as individual termination\n",
    "        terminated = (\n",
    "            to_batches(terminateds, torch.float)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Create mask for where index is < unpadded sequence length\n",
    "        filled = np.reshape(\n",
    "            np.tile(np.arange(T, dtype=np.float32), B), [B, T]\n",
    "        ) < np.expand_dims(seq_lens, 1)\n",
    "        mask = (\n",
    "            torch.as_tensor(filled, dtype=torch.float, device=self.device)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss_out, mask, masked_td_error, chosen_action_qvals, targets = self.loss(\n",
    "            rewards,\n",
    "            actions,\n",
    "            terminated,\n",
    "            mask,\n",
    "            obs,\n",
    "            next_obs,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            env_global_state,\n",
    "            next_env_global_state,\n",
    "        )\n",
    "\n",
    "        # Optimise\n",
    "        self.rmsprop_optimizer.zero_grad()\n",
    "\n",
    "        loss_out.backward()\n",
    "        grad_norm_info = apply_grad_clipping(self, self.rmsprop_optimizer, loss_out)\n",
    "        self.rmsprop_optimizer.step()\n",
    "\n",
    "        mask_elems = mask.sum().item()\n",
    "        stats = {\n",
    "            \"loss\": loss_out.item(),\n",
    "            \"td_error_abs\": masked_td_error.abs().sum().item() / mask_elems,\n",
    "            \"q_taken_mean\": (chosen_action_qvals * mask).sum().item() / mask_elems,\n",
    "            \"target_mean\": (targets * mask).sum().item() / mask_elems,\n",
    "        }\n",
    "        stats.update(grad_norm_info)\n",
    "\n",
    "        return {LEARNER_STATS_KEY: stats}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_initial_state(self):  # initial RNN state\n",
    "        return [\n",
    "            s.expand([self.n_agents, -1]).cpu().numpy()\n",
    "            for s in self.model.get_initial_state()\n",
    "        ]\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            \"model\": self._cpu_dict(self.model.state_dict()),\n",
    "            \"target_model\": self._cpu_dict(self.target_model.state_dict()),\n",
    "            \"mixer\": self._cpu_dict(self.mixer.state_dict()) if self.mixer else None,\n",
    "            \"target_mixer\": self._cpu_dict(self.target_mixer.state_dict())\n",
    "            if self.mixer\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(self._device_dict(weights[\"model\"]))\n",
    "        self.target_model.load_state_dict(self._device_dict(weights[\"target_model\"]))\n",
    "        if weights[\"mixer\"] is not None:\n",
    "            self.mixer.load_state_dict(self._device_dict(weights[\"mixer\"]))\n",
    "            self.target_mixer.load_state_dict(\n",
    "                self._device_dict(weights[\"target_mixer\"])\n",
    "            )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_state(self):\n",
    "        state = self.get_weights()\n",
    "        state[\"cur_epsilon\"] = self.cur_epsilon\n",
    "        return state\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_state(self, state):\n",
    "        self.set_weights(state)\n",
    "        self.set_epsilon(state[\"cur_epsilon\"])\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        logger.debug(\"Updated target networks\")\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cur_epsilon = epsilon\n",
    "\n",
    "    def _get_group_rewards(self, info_batch):\n",
    "        group_rewards = np.array(\n",
    "            [info.get(GROUP_REWARDS, [0.0] * self.n_agents) for info in info_batch]\n",
    "        )\n",
    "        return group_rewards\n",
    "\n",
    "    def _device_dict(self, state_dict):\n",
    "        return {\n",
    "            k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _cpu_dict(state_dict):\n",
    "        return {k: v.cpu().detach().numpy() for k, v in state_dict.items()}\n",
    "\n",
    "    def _unpack_observation(self, obs_batch):\n",
    "        \"\"\"Unpacks the observation, action mask, and state (if present)\n",
    "        from agent grouping.\n",
    "        Returns:\n",
    "            obs (np.ndarray): obs tensor of shape [B, n_agents, obs_size]\n",
    "            mask (np.ndarray): action mask, if any\n",
    "            state (np.ndarray or None): state tensor of shape [B, state_size]\n",
    "                or None if it is not in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        unpacked = _unpack_obs(\n",
    "            np.array(obs_batch, dtype=np.float32),\n",
    "            self.observation_space.original_space,\n",
    "            tensorlib=np,\n",
    "        )\n",
    "\n",
    "        if isinstance(unpacked[0], dict):\n",
    "            assert \"obs\" in unpacked[0]\n",
    "            unpacked_obs = [np.concatenate(tree.flatten(u[\"obs\"]), 1) for u in unpacked]\n",
    "        else:\n",
    "            unpacked_obs = unpacked\n",
    "\n",
    "        obs = np.concatenate(unpacked_obs, axis=1).reshape(\n",
    "            [len(obs_batch), self.n_agents, self.obs_size]\n",
    "        )\n",
    "\n",
    "        if self.has_action_mask:\n",
    "            action_mask = np.concatenate(\n",
    "                [o[\"action_mask\"] for o in unpacked], axis=1\n",
    "            ).reshape([len(obs_batch), self.n_agents, self.n_actions])\n",
    "        else:\n",
    "            action_mask = np.ones(\n",
    "                [len(obs_batch), self.n_agents, self.n_actions], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.has_env_global_state:\n",
    "            state = np.concatenate(tree.flatten(unpacked[0][ENV_STATE]), 1)\n",
    "        else:\n",
    "            state = None\n",
    "        return obs, action_mask, state\n",
    "\n",
    "#@ti.func\n",
    "def _validate(obs_space, action_space):\n",
    "    if not hasattr(obs_space, \"original_space\") or not isinstance(\n",
    "        obs_space.original_space, gym.spaces.Tuple\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Obs space must be a Tuple, got {}. Use \".format(obs_space)\n",
    "            + \"MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space, gym.spaces.Tuple):\n",
    "        raise ValueError(\n",
    "            \"Action space must be a Tuple, got {}. \".format(action_space)\n",
    "            + \"Use MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space.spaces[0], gym.spaces.Discrete):\n",
    "        raise ValueError(\n",
    "            \"QMix requires a discrete action space, got {}\".format(\n",
    "                action_space.spaces[0]\n",
    "            )\n",
    "        )\n",
    "    if len({str(x) for x in obs_space.original_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: observations of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(obs_space.original_space.spaces)\n",
    "        )\n",
    "    if len({str(x) for x in action_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: action space of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(action_space.spaces)\n",
    "        )\n",
    "\n",
    "#@ti.func\n",
    "def _mac(model, obs, h):\n",
    "    \"\"\"Forward pass of the multi-agent controller.\n",
    "    Args:\n",
    "        model: TorchModelV2 class\n",
    "        obs: Tensor of shape [B, n_agents, obs_size]\n",
    "        h: List of tensors of shape [B, n_agents, h_size]\n",
    "    Returns:\n",
    "        q_vals: Tensor of shape [B, n_agents, n_actions]\n",
    "        h: Tensor of shape [B, n_agents, h_size]\n",
    "    \"\"\"\n",
    "    B, n_agents = obs.size(0), obs.size(1)\n",
    "    if not isinstance(obs, dict):\n",
    "        obs = {\"obs\": obs}\n",
    "    obs_agents_as_batches = {k: _drop_agent_dim(v) for k, v in obs.items()}\n",
    "    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n",
    "    q_flat, h_flat = model(obs_agents_as_batches, h_flat, None)\n",
    "    return q_flat.reshape([B, n_agents, -1]), [\n",
    "        s.reshape([B, n_agents, -1]) for s in h_flat\n",
    "    ]\n",
    "#@ti.func\n",
    "def _unroll_mac(model, obs_tensor):\n",
    "    \"\"\"Computes the estimated Q values for an entire trajectory batch\"\"\"\n",
    "    B = obs_tensor.size(0)\n",
    "    T = obs_tensor.size(1)\n",
    "    n_agents = obs_tensor.size(2)\n",
    "\n",
    "    mac_out = []\n",
    "    h = [s.expand([B, n_agents, -1]) for s in model.get_initial_state()]\n",
    "    for t in range(T):\n",
    "        q, h = _mac(model, obs_tensor[:, t], h)\n",
    "        mac_out.append(q)\n",
    "    mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "    return mac_out\n",
    "#@ti.func\n",
    "def _drop_agent_dim(T):\n",
    "    shape = list(T.shape)\n",
    "    B, n_agents = shape[0], shape[1]\n",
    "    return T.reshape([B * n_agents] + shape[2:])\n",
    "#@ti.func\n",
    "def _add_agent_dim(T, n_agents):\n",
    "    shape = list(T.shape)\n",
    "    B = shape[0] // n_agents\n",
    "    assert shape[0] % n_agents == 0\n",
    "    return T.reshape([B, n_agents] + shape[1:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e7b8a67",
   "metadata": {},
   "source": [
    "psuedo code for assigning prior\n",
    "\n",
    "if agent_id.startswith(\"high_level_\"):\n",
    "    assign non informative prior\n",
    "else:\n",
    "    prior = posterior\n",
    "\n",
    "\n",
    "our 3 kinds of agents will be \n",
    "\n",
    "excitatory, inhibitory and motor\n",
    "\n",
    "two kinds of priors: priors about states or precisions or structural priors about generative model\n",
    "\n",
    "policy priors depend on expected free energy which itself depends on posterior beliefs, the potential for information gain and prior preferences\n",
    "\n",
    "priors over policies can also be equipped with a fixed form term representing habitual biases \n",
    "\n",
    "we are going to have a global reward which is going to be it not falling over\n",
    "\n",
    "we are then going to have local rewards for each of the policies on each level. the reward at the higher levels will be less specified less granular conditions for recieving a reward  \n",
    "\n",
    "things left to do\n",
    "\n",
    "replace neuronal agent model prior with dirchlet prior\n",
    "\n",
    "integrate expeced free energy and variational free energy into loss and \n",
    "use gaussain process regression in conjunction with expected free energy\n",
    "\n",
    "create an expected free energy and integrate the VFE network into the evaluation and selection Q values respectively\n",
    "\n",
    "Pending a positive results from tasha put the neurons in random spaces on each level during rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ddfa939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hierarchicalcarpole(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self._skip_env_checking = True\n",
    "        self.flat_env = MultiAgentCartPole(env_config)\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.cur_obs, infos = self.flat_env.reset()\n",
    "        self.current_goal = None\n",
    "        self.steps_remaining_at_level = None\n",
    "        self.num_high_level_steps = 0\n",
    "        # current low level agent id. This must be unique for each high level\n",
    "        # step since agent ids cannot be reused.\n",
    "        self.low_level_agent_id = \"low_level_{}\".format(self.num_high_level_steps)\n",
    "        return {\n",
    "            \"high_level_agent\": self.cur_obs,\n",
    "        }, {\"high_level_agent\": infos}\n",
    "    def _high_level_step(self, action):\n",
    "        logger.debug(\"High level agent sets goal\")\n",
    "        self.current_goal = action\n",
    "        self.steps_remaining_at_level = 25\n",
    "        self.num_high_level_steps += 1\n",
    "        self.low_level_agent_id = \"low_level_{}\".format(self.num_high_level_steps)\n",
    "        obs = {self.low_level_agent_id: [self.cur_obs, self.current_goal]}\n",
    "        rew = {self.low_level_agent_id: 0}\n",
    "        done = truncated = {\"__all__\": False}\n",
    "        return obs, rew, done, truncated, {}\n",
    "\n",
    "    def _low_level_step(self, action):\n",
    "        reward = 1.0\n",
    "        logger.debug(\"Low level agent step {}\".format(action))\n",
    "        self.steps_remaining_at_level -= 1\n",
    "        cur_pos = tuple(self.cur_obs[0])\n",
    "        goal_pos = self.flat_env._get_new_pos(cur_pos, self.current_goal)\n",
    "\n",
    "        # Step in the actual env\n",
    "        f_obs, f_rew, f_terminated, f_truncated, info = self.flat_env.step(action)\n",
    "        new_pos = tuple(f_obs[0])\n",
    "        self.cur_obs = f_obs\n",
    "\n",
    "        # Calculate low-level agent observation and reward\n",
    "        obs = {self.low_level_agent_id: [f_obs, self.current_goal]}\n",
    "\n",
    "\n",
    "        # Handle env termination & transitions back to higher level.\n",
    "        terminated = {\"__all__\": False}\n",
    "        truncated = {\"__all__\": False}\n",
    "        if f_terminated or f_truncated:\n",
    "            terminated[\"__all__\"] = f_terminated\n",
    "            truncated[\"__all__\"] = f_truncated\n",
    "            logger.debug(\"high level final reward {}\".format(f_rew))\n",
    "            rew[\"high_level_agent\"] = f_rew\n",
    "            obs[\"high_level_agent\"] = f_obs\n",
    "        elif self.steps_remaining_at_level == 0:\n",
    "            terminated[self.low_level_agent_id] = True\n",
    "            truncated[self.low_level_agent_id] = False\n",
    "            rew[\"high_level_agent\"] = 0\n",
    "            obs[\"high_level_agent\"] = f_obs\n",
    "\n",
    "        return obs, rew, terminated, truncated, {self.low_level_agent_id: info}\n",
    "    def render():\n",
    "        X = np.random.random((5, 2))\n",
    "        Y = np.random.random((5, 2))\n",
    "        gui = ti.GUI(\"lines\", res=(400, 400))\n",
    "        while gui.running:\n",
    "            gui.lines(begin=X, end=Y, radius=2, color=0x068587)\n",
    "            gui.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e30cbd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 00:00:42,309\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from gymnasium.spaces import Discrete, Tuple\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.examples.env.windy_maze_env import WindyMazeEnv, HierarchicalWindyMazeEnv\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--flat\", action=\"store_true\")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=100000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=0.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    ray.init(local_mode=args.local_mode)\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "    }\n",
    "\n",
    "    if args.flat:\n",
    "        results = tune.Tuner(\n",
    "            \"PPO\",\n",
    "            run_config=air.RunConfig(stop=stop),\n",
    "            param_space=(\n",
    "                PPOConfig()\n",
    "                .environment(MultiAgentCartPole)\n",
    "                .rollouts(num_rollout_workers=0)\n",
    "                .framework(args.framework)\n",
    "            ).to_dict(),\n",
    "        ).fit()\n",
    "\n",
    "\n",
    "        def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "            if agent_id.startswith(\"low_level_\"):\n",
    "                return \"low_level_policy\"\n",
    "            else:\n",
    "                return \"high_level_policy\"\n",
    "        from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "        pbt_scheduler = PopulationBasedTraining(\n",
    "            time_attr='training_iteration',\n",
    "            metric='episode_reward_mean',#'loss',\n",
    "            mode='min',\n",
    "            perturbation_interval=1,\n",
    "            hyperparam_mutations={\n",
    "                \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "                \"alpha\": tune.uniform(0.0, 1.0),\n",
    "            }\n",
    "        )\n",
    "        config = (\n",
    "            PPOConfig()\n",
    "            .environment(Hierarchicalcarpole)\n",
    "            .framework(args.framework)\n",
    "            .rollouts(num_rollout_workers=0)\n",
    "            .training(entropy_coeff=0.01)\n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        maze.observation_space,\n",
    "                        Discrete(4),\n",
    "                        PPOConfig.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        Tuple([maze.observation_space, Discrete(4)]),\n",
    "                        maze.action_space,\n",
    "                        PPOConfig.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn,\n",
    "            )\n",
    "            # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "            .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "        )\n",
    "\n",
    "        results = tune.Tuner(\n",
    "            \"PPO\",\n",
    "            param_space=config.to_dict(),\n",
    "            run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "            tune_config=tune.TuneConfig(\n",
    "                num_samples=4,\n",
    "                scheduler=pbt_scheduler,\n",
    "            ),\n",
    "              \n",
    "        ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9164002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass TwoStepGameWithGroupedAgents(MultiAgentEnv):\\n    def __init__(self, env_config):\\n        super().__init__()\\n        env = TwoStepGame(env_config)\\n        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\\n        tuple_act_space = Tuple([env.action_space, env.action_space])\\n\\n        self.env = env.with_agent_groups(\\n            groups={\"agents\": [0, 1]},\\n            obs_space=tuple_obs_space,\\n            act_space=tuple_act_space,\\n        )\\n        self.observation_space = self.env.observation_space\\n        self.action_space = self.env.action_space\\n        self._agent_ids = {\"agents\"}\\n\\n    def reset(self, *, seed=None, options=None):\\n        return self.env.reset(seed=seed, options=options)\\n\\n    def step(self, actions):\\n        return self.env.step(actions)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@ti.func\n",
    "#@ti.kernel\n",
    "#@ti.data_oriented\n",
    "class TwoStepGame(MultiAgentEnv):\n",
    "    action_space = Discrete(2)\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(2)\n",
    "        self.state = None\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        #self.agent_3=2\n",
    "        self._skip_env_checking = True\n",
    "        # MADDPG emits action logits instead of actual discrete actions\n",
    "        self.actions_are_logits = env_config.get(\"actions_are_logits\", False)\n",
    "        self.one_hot_state_encoding = env_config.get(\"one_hot_state_encoding\", False)\n",
    "        self.with_state = env_config.get(\"separate_state_space\", False)\n",
    "        self._agent_ids = {0, 1}\n",
    "        if not self.one_hot_state_encoding:\n",
    "            self.observation_space = Discrete(6)\n",
    "            self.with_state = False\n",
    "        else:\n",
    "            # Each agent gets the full state (one-hot encoding of which of the\n",
    "            # three states are active) as input with the receiving agent's\n",
    "            # ID (1 or 2) concatenated onto the end.\n",
    "            if self.with_state:\n",
    "                self.observation_space = Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.observation_space = MultiDiscrete([2, 2, 2, 3])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = np.array([1, 0, 0])\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "\n",
    "        state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "        \n",
    "\n",
    "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
    "        obs = self._obs()\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        truncateds = {\"__all__\": False}\n",
    "        infos = {\n",
    "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
    "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "        }\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "\n",
    "    def _obs(self):\n",
    "        if self.with_state:\n",
    "            return {\n",
    "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
    "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
    "            }\n",
    "        else:\n",
    "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
    "\n",
    "    def agent_1_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [1]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0]\n",
    "\n",
    "    def agent_2_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [2]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0] + 3\n",
    "\n",
    "        #if self.render_mode == \"rgb_array\":\n",
    "            #return self._render_frame()\n",
    "\n",
    "\"\"\"\n",
    "class TwoStepGameWithGroupedAgents(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        env = TwoStepGame(env_config)\n",
    "        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\n",
    "        tuple_act_space = Tuple([env.action_space, env.action_space])\n",
    "\n",
    "        self.env = env.with_agent_groups(\n",
    "            groups={\"agents\": [0, 1]},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self._agent_ids = {\"agents\"}\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        return self.env.reset(seed=seed, options=options)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6662859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "enva = gym.make('InvertedPendulum-v4')\n",
    "\n",
    "#act of -3,3\n",
    "#obs of -inf, inf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a33345f",
   "metadata": {},
   "source": [
    "psuedo code for rendering \n",
    "\n",
    "for i in num_agents:\n",
    "pos = np.randint(num_agents)\n",
    "blueline = action space\n",
    "redline = obs space\n",
    "pinkline  = obs inhibitory space\n",
    "greenline = action inhibitory space \n",
    "\n",
    "brain has 6 layes of neurons \n",
    "\n",
    "cortical column is a vertical slice of the brain that encapsulates the entire hierarchy of the brain\n",
    "\n",
    "connections entering and leaving cortical column relate to likelihood distribtuions whereas transition probabilities and continuous dynamics depend on connections withen a microcircuit\n",
    "\n",
    "Cortical columns are proposed to be the canonical microcircuits for predictive coding\n",
    "\n",
    "we should integrate expected free energy into the exploratory config \n",
    "\n",
    "double q learning uses 1q to select and another q to evaluate an action\n",
    "\n",
    "latest opinion\n",
    "\n",
    "expected free energy selects data that will best optimize beliefs and variational free energy optimizes beliefs in relation to data that has already been gathered \n",
    "\n",
    "expected free energy is the policy selection and variational free energy should be the evaluation policy evaluation q\n",
    "\n",
    "we are going to first make predictions about what each policy. We are going to do this by doing a variational gaussian process output layer similar to what we have with jepps. Except the loss here will be loss for optimzing a policy that optimizes over finding data/BOED. We are then going to evaluate the policies with variational free energy with the target network for the second q value. meanwhile the network for the first q value for data aquisition will also be used for our explore config algorithm \n",
    "\n",
    "epistemic value is also called information gain\n",
    "\n",
    "pragmatic value is a kind of prior about probability of observations given certain policies. Uniform prior beliefs about beliefs about outcomes averaged over policies is uniform. This is equivalent to a pragmatic value of 0.\n",
    "\n",
    "psuedo code for emodel module for first q value\n",
    "\n",
    "e model loss = EIG #this should be the the qmix loss module for the primary q value network \n",
    "b model will become the target network \n",
    "\n",
    "emodel  will be integrated into our custom exploration module \n",
    "note: we are going to have a pragmatic value of 0. \n",
    "\n",
    "\n",
    "inputs\n",
    "action_distribution.inputs, self.model, temperature=self.temperature\n",
    "\n",
    "what is needed \n",
    "\n",
    "policies = Tensor of possible designs.\n",
    "\n",
    "guide = OutcomePredictor()\n",
    "\n",
    "from adaptive memory experiment\n",
    "\n",
    "eig = marginal_eig(model,\n",
    "                   candidate_designs,       # design, or in this case, tensor of possible designs\n",
    "                   \"y\",                     # site label of observations, could be a list\n",
    "                   \"theta\",                 # site label of 'targets' (latent variables), could also be list\n",
    "                   num_samples=100,         # number of samples to draw per step in the expectation\n",
    "                   num_steps=num_steps,     # number of gradient steps\n",
    "                   guide=marginal_guide,    # guide q(y)\n",
    "                   optim=optimizer,         # optimizer with learning rate decay\n",
    "                   final_num_samples=10000  # at the last step, we draw more samples\n",
    "                   eig (bool)               # for a more accurate EIG estimate\n",
    "                  )\n",
    "\n",
    "\n",
    "getting around position problem: we are going to use RL to create a connectivity structure. Then we can use assign the neuronal agents that have the strongest connections as being the closes together \n",
    "\n",
    "in our case the guide is going to determine what policy will minimize expected free energy\n",
    "\n",
    "first let us do it over 1 time step\n",
    "\n",
    "we will use variational gaussian process as part of the neural network for the guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238cf16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "design = torch.tensor(torch.tensor([[1.], [-1.]]))#policies in the end can only do two things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class OutcomePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_entropy = dist.Bernoulli(prior_w_prob).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class EFE(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a SoftQ Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        #self.model  is the first agent model we use to get the first q value used for model selection\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75cc5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our custom replacement for softq in explore config\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class SoftQ(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a SoftQ Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a160d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig, NotProvided\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    "    SAMPLE_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "#@ti.kernel\n",
    "class QMixConfig(SimpleQConfig):\n",
    "    \"\"\"Defines a configuration class from which QMix can be built.\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> config = QMixConfig()  # doctest: +SKIP\n",
    "        >>> config = config.training(gamma=0.9, lr=0.01, kl_coeff=0.3)  # doctest: +SKIP\n",
    "        >>> config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "        >>> config = config.rollouts(num_rollout_workers=4)  # doctest: +SKIP\n",
    "        >>> print(config.to_dict())  # doctest: +SKIP\n",
    "        >>> # Build an Algorithm object from the config and run 1 training iteration.\n",
    "        >>> algo = config.build(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> algo.train()  # doctest: +SKIP\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> from ray import air\n",
    "        >>> from ray import tune\n",
    "        >>> config = QMixConfig()\n",
    "        >>> # Print out some default values.\n",
    "        >>> print(config.optim_alpha)  # doctest: +SKIP\n",
    "        >>> # Update the config object.\n",
    "        >>> config.training(  # doctest: +SKIP\n",
    "        ...     lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97\n",
    "        ... )\n",
    "        >>> # Set the config object's env.\n",
    "        >>> config.environment(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> # Use to_dict() to get the old-style python config dict\n",
    "        >>> # when running with tune.\n",
    "        >>> tune.Tuner(  # doctest: +SKIP\n",
    "        ...     \"QMix\",\n",
    "        ...     run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "        ...     param_space=config.to_dict(),\n",
    "        ... ).fit()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PPOConfig instance.\"\"\"\n",
    "        super().__init__(algo_class=QMix)\n",
    "\n",
    "        # fmt: off\n",
    "        # __sphinx_doc_begin__\n",
    "        # QMix specific settings:\n",
    "        self.mixer = \"qmix\"\n",
    "        self.mixing_embed_dim = 32\n",
    "        self.double_q = True\n",
    "        self.optim_alpha = 0.99\n",
    "        self.optim_eps = 0.00001\n",
    "        self.grad_clip = 10\n",
    "        #self.render_mode = 'rgb_array'\n",
    "        # QMix-torch overrides the TorchPolicy's learn_on_batch w/o specifying a\n",
    "        # alternative `learn_on_loaded_batch` alternative for the GPU.\n",
    "        # TODO: This hack will be resolved once we move all algorithms to the new\n",
    "        #  RLModule/Learner APIs.\n",
    "        self.simple_optimizer = True\n",
    "\n",
    "        # Override some of AlgorithmConfig's default values with QMix-specific values.\n",
    "        # .training()\n",
    "        self.lr = 0.0005\n",
    "        self.train_batch_size = 32\n",
    "        self.target_network_update_freq = 500\n",
    "        self.num_steps_sampled_before_learning_starts = 1000\n",
    "        self.replay_buffer_config = {\n",
    "            \"type\": \"ReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "            \"prioritized_replay\": DEPRECATED_VALUE,\n",
    "            # Size of the replay buffer in batches (not timesteps!).\n",
    "            \"capacity\": 1000,\n",
    "            # Choosing `fragments` here makes it so that the buffer stores entire\n",
    "            # batches, instead of sequences, episodes or timesteps.\n",
    "            \"storage_unit\": \"fragments\",\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.model = {\n",
    "            \"lstm_cell_size\": 64,\n",
    "            \"max_seq_len\": 999999,\n",
    "        }\n",
    "        \"\"\"\n",
    "        # .framework()\n",
    "        self.framework_str = \"torch\"\n",
    "\n",
    "        # .rollouts()\n",
    "        self.rollout_fragment_length = 4\n",
    "        self.batch_mode = \"complete_episodes\"\n",
    "\n",
    "        # .reporting()\n",
    "        self.min_time_s_per_iteration = 1\n",
    "        self.min_sample_timesteps_per_iteration = 1000\n",
    "\n",
    "        # .exploration()\n",
    "        self.exploration_config = {\n",
    "            \"\"\"\n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\", #replace this with SoftQ\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.01,\n",
    "            # Timesteps over which to anneal epsilon.\n",
    "            \"epsilon_timesteps\": 40000,\n",
    "            \"\"\"\n",
    "            \"type\": \"SoftQ\",\n",
    "            \"temperature\": 1.0\n",
    "            # For soft_q, use:\n",
    "            # \"exploration_config\" = {\n",
    "            #   \"type\": \"SoftQ\"\n",
    "            #   \"temperature\": [float, e.g. 1.0]\n",
    "            # }\n",
    "        }\n",
    "\n",
    "        # .evaluation()\n",
    "        # Evaluate with epsilon=0 every `evaluation_interval` training iterations.\n",
    "        # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "        self.evaluation(\n",
    "            evaluation_config=AlgorithmConfig.overrides(explore=False)\n",
    "        )\n",
    "        # __sphinx_doc_end__\n",
    "        # fmt: on\n",
    "\n",
    "        self.worker_side_prioritization = DEPRECATED_VALUE\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        mixer: Optional[str] = NotProvided,\n",
    "        mixing_embed_dim: Optional[int] = NotProvided,\n",
    "        double_q: Optional[bool] = NotProvided,\n",
    "        target_network_update_freq: Optional[int] = NotProvided,\n",
    "        replay_buffer_config: Optional[dict] = NotProvided,\n",
    "        optim_alpha: Optional[float] = NotProvided,\n",
    "        optim_eps: Optional[float] = NotProvided,\n",
    "        grad_clip: Optional[float] = NotProvided,\n",
    "        # Deprecated args.\n",
    "        grad_norm_clipping=DEPRECATED_VALUE,\n",
    "        **kwargs,\n",
    "    ) -> \"QMixConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            mixer: Mixing network. Either \"qmix\", \"vdn\", or None.\n",
    "            mixing_embed_dim: Size of the mixing network embedding.\n",
    "            double_q: Whether to use Double_Q learning.\n",
    "            target_network_update_freq: Update the target network every\n",
    "                `target_network_update_freq` sample steps.\n",
    "            replay_buffer_config:\n",
    "            optim_alpha: RMSProp alpha.\n",
    "            optim_eps: RMSProp epsilon.\n",
    "            grad_clip: If not None, clip gradients during optimization at\n",
    "                this value.\n",
    "            grad_norm_clipping: Depcrecated in favor of grad_clip\n",
    "        Returns:\n",
    "            This updated AlgorithmConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if grad_norm_clipping != DEPRECATED_VALUE:\n",
    "            deprecation_warning(\n",
    "                old=\"grad_norm_clipping\",\n",
    "                new=\"grad_clip\",\n",
    "                help=\"Parameter `grad_norm_clipping` has been \"\n",
    "                \"deprecated in favor of grad_clip in QMix. \"\n",
    "                \"This is now the same parameter as in other \"\n",
    "                \"algorithms. `grad_clip` will be overwritten by \"\n",
    "                \"`grad_norm_clipping={}`\".format(grad_norm_clipping),\n",
    "                error=True,\n",
    "            )\n",
    "            grad_clip = grad_norm_clipping\n",
    "\n",
    "        if mixer is not NotProvided:\n",
    "            self.mixer = mixer\n",
    "        if mixing_embed_dim is not NotProvided:\n",
    "            self.mixing_embed_dim = mixing_embed_dim\n",
    "        if double_q is not NotProvided:\n",
    "            self.double_q = double_q\n",
    "        if target_network_update_freq is not NotProvided:\n",
    "            self.target_network_update_freq = target_network_update_freq\n",
    "        if replay_buffer_config is not NotProvided:\n",
    "            self.replay_buffer_config = replay_buffer_config\n",
    "        if optim_alpha is not NotProvided:\n",
    "            self.optim_alpha = optim_alpha\n",
    "        if optim_eps is not NotProvided:\n",
    "            self.optim_eps = optim_eps\n",
    "        if grad_clip is not NotProvided:\n",
    "            self.grad_clip = grad_clip\n",
    "\n",
    "        return self\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def validate(self) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate()\n",
    "\n",
    "        if self.framework_str != \"torch\":\n",
    "            raise ValueError(\n",
    "                \"Only `config.framework('torch')` supported so far for QMix!\"\n",
    "            )\n",
    "#@ti.kernel\n",
    "class QMix(SimpleQ):\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_config(cls) -> AlgorithmConfig:\n",
    "        return QMixConfig()\n",
    "\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_policy_class(\n",
    "        cls, config: AlgorithmConfig\n",
    "    ) -> Optional[Type[Policy]]:\n",
    "        return QMixTorchPolicy\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def training_step(self) -> ResultDict:\n",
    "        \"\"\"QMIX training iteration function.\n",
    "        - Sample n MultiAgentBatches from n workers synchronously.\n",
    "        - Store new samples in the replay buffer.\n",
    "        - Sample one training MultiAgentBatch from the replay buffer.\n",
    "        - Learn on the training batch.\n",
    "        - Update the target network every `target_network_update_freq` sample steps.\n",
    "        - Return all collected training metrics for the iteration.\n",
    "        Returns:\n",
    "            The results dict from executing the training iteration.\n",
    "        \"\"\"\n",
    "        # Sample n batches from n workers.\n",
    "        with self._timers[SAMPLE_TIMER]:\n",
    "            new_sample_batches = synchronous_parallel_sample(\n",
    "                worker_set=self.workers, concat=False\n",
    "            )\n",
    "\n",
    "        for batch in new_sample_batches:\n",
    "            # Update counters.\n",
    "            self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n",
    "            self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n",
    "            # Store new samples in the replay buffer.\n",
    "            self.local_replay_buffer.add(batch)\n",
    "\n",
    "        # Update target network every `target_network_update_freq` sample steps.\n",
    "        cur_ts = self._counters[\n",
    "            NUM_AGENT_STEPS_SAMPLED\n",
    "            if self.config.count_steps_by == \"agent_steps\"\n",
    "            else NUM_ENV_STEPS_SAMPLED\n",
    "        ]\n",
    "\n",
    "        train_results = {}\n",
    "\n",
    "        if cur_ts > self.config.num_steps_sampled_before_learning_starts:\n",
    "            # Sample n batches from replay buffer until the total number of timesteps\n",
    "            # reaches `train_batch_size`.\n",
    "            train_batch = sample_min_n_steps_from_buffer(\n",
    "                replay_buffer=self.local_replay_buffer,\n",
    "                min_steps=self.config.train_batch_size,\n",
    "                count_by_agent_steps=self.config.count_steps_by == \"agent_steps\",\n",
    "            )\n",
    "\n",
    "            # Learn on the training batch.\n",
    "            # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
    "            # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
    "            if self.config.get(\"simple_optimizer\") is True:\n",
    "                train_results = train_one_step(self, train_batch)\n",
    "            else:\n",
    "                train_results = multi_gpu_train_one_step(self, train_batch)\n",
    "\n",
    "            # Update target network every `target_network_update_freq` sample steps.\n",
    "            last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
    "            if cur_ts - last_update >= self.config.target_network_update_freq:\n",
    "                to_update = self.workers.local_worker().get_policies_to_train()\n",
    "                self.workers.local_worker().foreach_policy_to_train(\n",
    "                    lambda p, pid: pid in to_update and p.update_target()\n",
    "                )\n",
    "                self._counters[NUM_TARGET_UPDATES] += 1\n",
    "                self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
    "\n",
    "            update_priorities_in_replay_buffer(\n",
    "                self.local_replay_buffer, self.config, train_batch, train_results\n",
    "            )\n",
    "\n",
    "            # Update weights and global_vars - after learning on the local worker -\n",
    "            # on all remote workers.\n",
    "            global_vars = {\n",
    "                \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
    "            }\n",
    "            # Update remote workers' weights and global vars after learning on local\n",
    "            # worker.\n",
    "            with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
    "                self.workers.sync_weights(global_vars=global_vars)\n",
    "\n",
    "        # Return all collected metrics for the iteration.\n",
    "        return train_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50365aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naction_space = Tuple([\\n    \"agent1\": Dict(\\n        {\\n            #\"prob\": Discrete(100),\\n            \"action_potential\":  MultiDiscrete([1,1]), #excitatory neurons MultiDiscrete([2, 2, 2, 3])\\n            ENV_STATE: MultiDiscrete([2,2]),\\n        } ,dtype=np.float32),\\n    \"agent2\": Dict(\\n        {\\n            #\"prob\": Discrete(100),\\n            \"action_potential\": MultiDiscrete([1,2]), #inhibitory neurons MultiDiscrete([2, 2, 2, 3])\\n            ENV_STATE: MultiDiscrete([0,1]),\\n        } ,dtype=np.float32),\\n    \"agent3\": Dict(\\n        {\\n            #\"prob\": Discrete(100),\\n            \"outsideAction\":gym.spaces.Box(low=-1.0, high=1.0, shape=(10,)),#this should be replaced with the action space of inverted pendulum\\n            \"action_potential\": Discrete(1)# only excitatory\\n        }, dtype=np.float32)]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "#b = np.ones(3000).tolist()\n",
    "#obs_space= Tuple({MultiDiscrete([])})\n",
    "\n",
    "observation_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]), \n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]),\n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "action_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([1,3]), \n",
    "                ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,1]),\n",
    "                ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02c8cf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.rllib.utils.tf_utils.get_gpu_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44091fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "#ray.init(num_cpus=16,num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "138b875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19edeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = {\n",
    "    \"group_1\": [0],\n",
    "    \"group_2\": [0,1],\n",
    "    \"group_3\": [0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eae20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c647f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 12:58:58,750\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-04-07 13:03:13</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:10.57        </td></tr>\n",
       "<tr><td>Memory:      </td><td>29.1/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 230 checkpoints, 27 perturbs<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/21.76 GiB heap, 0.0/10.88 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">   alpha</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_a4f1f_00000</td><td>TERMINATED</td><td>127.0.0.1:2576 </td><td style=\"text-align: right;\">1.12505 </td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         71.2161</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_a4f1f_00001</td><td>TERMINATED</td><td>127.0.0.1:31324</td><td style=\"text-align: right;\">0.937545</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         71.3471</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_a4f1f_00002</td><td>TERMINATED</td><td>127.0.0.1:8416 </td><td style=\"text-align: right;\">0.247638</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         72.8707</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_a4f1f_00003</td><td>TERMINATED</td><td>127.0.0.1:116  </td><td style=\"text-align: right;\">0.206365</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         73.1691</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22792)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=22792)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=22792)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=22792)\u001b[0m [I 04/07/23 12:59:11.076 15680] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=22792)\u001b[0m 2023-04-07 12:59:11,668\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=22792)\u001b[0m 2023-04-07 12:59:11,908\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=22792)\u001b[0m 2023-04-07 12:59:11,921\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=6704)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=6704)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=6704)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=6704)\u001b[0m [I 04/07/23 12:59:19.669 27508] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=6704)\u001b[0m 2023-04-07 12:59:20,310\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=6704)\u001b[0m 2023-04-07 12:59:20,524\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=6704)\u001b[0m 2023-04-07 12:59:20,535\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=23744)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=23744)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=23744)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=23744)\u001b[0m [I 04/07/23 12:59:28.802 23288] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=23744)\u001b[0m 2023-04-07 12:59:29,349\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=23744)\u001b[0m 2023-04-07 12:59:29,588\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=23744)\u001b[0m 2023-04-07 12:59:29,601\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=33340)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33340)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33340)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33340)\u001b[0m [I 04/07/23 12:59:37.862 34220] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=33340)\u001b[0m 2023-04-07 12:59:38,440\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=33340)\u001b[0m 2023-04-07 12:59:38,687\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33340)\u001b[0m 2023-04-07 12:59:38,699\tWARNING env.py:53 -- Skipping env checking for this experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                         </th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                           </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                 </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                      </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </th><th>timers                                                                                                                                                                            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_a4f1f_00000</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.0029997825622558594, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.0675806999206543}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.742132842540741, &#x27;cur_lr&#x27;: 0.001}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 5.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}   </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 4.4, &#x27;ram_util_percent&#x27;: 45.7, &#x27;gpu_util_percent0&#x27;: 0.05, &#x27;vram_util_percent0&#x27;: 0.4573974609375}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.539268312755718, &#x27;mean_inference_ms&#x27;: 0.752543450988668, &#x27;mean_action_processing_ms&#x27;: 0.12400146332040998, &#x27;mean_env_wait_ms&#x27;: 0.024285372005722894, &#x27;mean_env_render_ms&#x27;: 0.0}  </td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.539268312755718, &#x27;mean_inference_ms&#x27;: 0.752543450988668, &#x27;mean_action_processing_ms&#x27;: 0.12400146332040998, &#x27;mean_env_wait_ms&#x27;: 0.024285372005722894, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.0029997825622558594, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.0675806999206543}} </td><td>{&#x27;training_iteration_time_ms&#x27;: 298.196, &#x27;load_time_ms&#x27;: 0.335, &#x27;load_throughput&#x27;: 596346.54, &#x27;learn_time_ms&#x27;: 7.367, &#x27;learn_throughput&#x27;: 27147.306, &#x27;synch_weights_time_ms&#x27;: 0.0} </td></tr>\n",
       "<tr><td>PG_TwoStepGame_a4f1f_00001</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.01304173469543457, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1111748218536377}  </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.6371487975120544, &#x27;cur_lr&#x27;: 0.0005}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 7.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000} </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 6.7, &#x27;ram_util_percent&#x27;: 45.8, &#x27;gpu_util_percent0&#x27;: 0.31, &#x27;vram_util_percent0&#x27;: 0.4573974609375}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6145523133200503, &#x27;mean_inference_ms&#x27;: 0.8291778493016847, &#x27;mean_action_processing_ms&#x27;: 0.11969565154610207, &#x27;mean_env_wait_ms&#x27;: 0.04014064012544133, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6145523133200503, &#x27;mean_inference_ms&#x27;: 0.8291778493016847, &#x27;mean_action_processing_ms&#x27;: 0.11969565154610207, &#x27;mean_env_wait_ms&#x27;: 0.04014064012544133, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.01304173469543457, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1111748218536377}}  </td><td>{&#x27;training_iteration_time_ms&#x27;: 333.475, &#x27;load_time_ms&#x27;: 0.0, &#x27;load_throughput&#x27;: 0.0, &#x27;learn_time_ms&#x27;: 7.838, &#x27;learn_throughput&#x27;: 25515.32, &#x27;synch_weights_time_ms&#x27;: 0.0}          </td></tr>\n",
       "<tr><td>PG_TwoStepGame_a4f1f_00002</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.01699995994567871, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.11254239082336426} </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.6785123944282532, &#x27;cur_lr&#x27;: 5e-05}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 33.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000} </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 6.7, &#x27;ram_util_percent&#x27;: 47.1, &#x27;gpu_util_percent0&#x27;: 0.09, &#x27;vram_util_percent0&#x27;: 0.4573974609375}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6464049943017105, &#x27;mean_inference_ms&#x27;: 0.8854012179465827, &#x27;mean_action_processing_ms&#x27;: 0.10774262475112155, &#x27;mean_env_wait_ms&#x27;: 0.039027600174825074, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6464049943017105, &#x27;mean_inference_ms&#x27;: 0.8854012179465827, &#x27;mean_action_processing_ms&#x27;: 0.10774262475112155, &#x27;mean_env_wait_ms&#x27;: 0.039027600174825074, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.01699995994567871, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.11254239082336426}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 335.112, &#x27;load_time_ms&#x27;: 0.301, &#x27;load_throughput&#x27;: 664812.807, &#x27;learn_time_ms&#x27;: 7.845, &#x27;learn_throughput&#x27;: 25495.506, &#x27;synch_weights_time_ms&#x27;: 0.0}</td></tr>\n",
       "<tr><td>PG_TwoStepGame_a4f1f_00003</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.01594257354736328, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12810635566711426} </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.8854095935821533, &#x27;cur_lr&#x27;: 0.0001}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 99.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 9.0, &#x27;ram_util_percent&#x27;: 47.9, &#x27;gpu_util_percent0&#x27;: 0.06, &#x27;vram_util_percent0&#x27;: 0.4573974609375}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6818334146352213, &#x27;mean_inference_ms&#x27;: 0.9060440248470879, &#x27;mean_action_processing_ms&#x27;: 0.1205827770036717, &#x27;mean_env_wait_ms&#x27;: 0.04495245112787306, &#x27;mean_env_render_ms&#x27;: 0.0}  </td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6818334146352213, &#x27;mean_inference_ms&#x27;: 0.9060440248470879, &#x27;mean_action_processing_ms&#x27;: 0.1205827770036717, &#x27;mean_env_wait_ms&#x27;: 0.04495245112787306, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.01594257354736328, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12810635566711426}}  </td><td>{&#x27;training_iteration_time_ms&#x27;: 353.363, &#x27;load_time_ms&#x27;: 0.245, &#x27;load_throughput&#x27;: 816012.451, &#x27;learn_time_ms&#x27;: 7.659, &#x27;learn_throughput&#x27;: 26114.024, &#x27;synch_weights_time_ms&#x27;: 0.0}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n",
      "2023-04-07 12:59:38,836\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:39,687\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00001\n",
      "2023-04-07 12:59:40,054\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00002 (score = -4.480000) into trial a4f1f_00001 (score = -5.180000)\n",
      "\n",
      "2023-04-07 12:59:40,054\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00001:\n",
      "lr : 0.0004 --- (resample) --> 0.0005\n",
      "alpha : 0.30507981516526006 --- (* 0.8) --> 0.24406385213220805\n",
      "\n",
      "2023-04-07 12:59:40,250\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00002 (score = -4.480000) into trial a4f1f_00003 (score = -5.410000)\n",
      "\n",
      "2023-04-07 12:59:40,251\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00003:\n",
      "lr : 0.0004 --- (resample) --> 0.0005\n",
      "alpha : 0.30507981516526006 --- (resample) --> 0.4796184754991636\n",
      "\n",
      "2023-04-07 12:59:40,863\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:41,240\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:41,269\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:41,973\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:42,368\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:42,400\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:42,765\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:43,154\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:43,548\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:43,886\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:44,278\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:44,667\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:44,697\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:45,036\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:45,057\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:45,427\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:45,776\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:46,140\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:46,495\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:46,928\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:46,952\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:47,732\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "\u001b[2m\u001b[36m(pid=33864)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33864)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=7952)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=7952)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-07 12:59:48,061\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:48,499\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33864)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=7952)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 12:59:48,961\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:49,257\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "\u001b[2m\u001b[36m(PG pid=33864)\u001b[0m [I 04/07/23 12:59:49.353 27360] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=7952)\u001b[0m [I 04/07/23 12:59:49.346 24108] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "2023-04-07 12:59:49,963\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 12:59:50,303\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "\u001b[2m\u001b[36m(PG pid=33864)\u001b[0m 2023-04-07 12:59:50,296\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=7952)\u001b[0m 2023-04-07 12:59:50,278\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=33864)\u001b[0m 2023-04-07 12:59:50,550\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33864)\u001b[0m 2023-04-07 12:59:50,563\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=7952)\u001b[0m 2023-04-07 12:59:50,531\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=7952)\u001b[0m 2023-04-07 12:59:50,545\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=33864)\u001b[0m 2023-04-07 12:59:50,661\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_960fd586afa54431bbed7d5c6f5d1156\n",
      "\u001b[2m\u001b[36m(PG pid=33864)\u001b[0m 2023-04-07 12:59:50,661\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 0.9978322982788086, '_episodes_total': 300}\n",
      "\u001b[2m\u001b[36m(PG pid=7952)\u001b[0m 2023-04-07 12:59:50,666\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f574e5570e6240a7b6d8ab089eb276fb\n",
      "\u001b[2m\u001b[36m(PG pid=7952)\u001b[0m 2023-04-07 12:59:50,666\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 0.9978322982788086, '_episodes_total': 300}\n",
      "2023-04-07 12:59:50,825\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 12:59:51,225\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -5.210000) into trial a4f1f_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-07 12:59:51,226\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0005 --- (shift left) --> 0.001\n",
      "alpha : 0.4796184754991636 --- (* 1.2) --> 0.5755421705989963\n",
      "\n",
      "2023-04-07 12:59:51,436\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -5.210000) into trial a4f1f_00002 (score = -7.010000)\n",
      "\n",
      "2023-04-07 12:59:51,437\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00002:\n",
      "lr : 0.0005 --- (shift left) --> 0.001\n",
      "alpha : 0.4796184754991636 --- (* 1.2) --> 0.5755421705989963\n",
      "\n",
      "2023-04-07 12:59:56,237\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00001\n",
      "2023-04-07 12:59:57,301\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00003\n",
      "2023-04-07 12:59:58,570\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00003\n",
      "\u001b[2m\u001b[36m(pid=29280)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=29280)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=33656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33656)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-07 12:59:58,940\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00001 (score = -7.000000) into trial a4f1f_00003 (score = -7.030000)\n",
      "\n",
      "2023-04-07 12:59:58,941\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00003:\n",
      "lr : 0.0005 --- (shift right) --> 0.0001\n",
      "alpha : 0.24406385213220805 --- (resample) --> 0.20636523692009068\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=29280)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=33656)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=29280)\u001b[0m [I 04/07/23 13:00:00.567 26084] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=33656)\u001b[0m [I 04/07/23 13:00:00.567 22588] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=29280)\u001b[0m 2023-04-07 13:00:01,375\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=33656)\u001b[0m 2023-04-07 13:00:01,406\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=29280)\u001b[0m 2023-04-07 13:00:01,601\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=29280)\u001b[0m 2023-04-07 13:00:01,613\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=33656)\u001b[0m 2023-04-07 13:00:01,634\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33656)\u001b[0m 2023-04-07 13:00:01,647\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=29280)\u001b[0m 2023-04-07 13:00:01,710\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_4095ef689fa342b5b14140f4c63c87ab\n",
      "\u001b[2m\u001b[36m(PG pid=29280)\u001b[0m 2023-04-07 13:00:01,711\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 4, '_timesteps_total': None, '_time_total': 1.346649169921875, '_episodes_total': 400}\n",
      "\u001b[2m\u001b[36m(PG pid=33656)\u001b[0m 2023-04-07 13:00:01,743\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_92f4db9da5b248e7986193082fa30a3d\n",
      "\u001b[2m\u001b[36m(PG pid=33656)\u001b[0m 2023-04-07 13:00:01,743\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 4, '_timesteps_total': None, '_time_total': 1.346649169921875, '_episodes_total': 400}\n",
      "\u001b[2m\u001b[36m(pid=5416)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=5416)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=2680)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=2680)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-07 13:00:07,784\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=5416)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=2680)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=5416)\u001b[0m [I 04/07/23 13:00:08.476 19392] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=2680)\u001b[0m [I 04/07/23 13:00:08.439 25572] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "2023-04-07 13:00:08,973\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00002 (score = -6.980000) into trial a4f1f_00000 (score = -7.040000)\n",
      "\n",
      "2023-04-07 13:00:08,974\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.001 --- (resample) --> 0.0001\n",
      "alpha : 0.5755421705989963 --- (* 0.8) --> 0.46043373647919705\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=5416)\u001b[0m 2023-04-07 13:00:09,432\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=2680)\u001b[0m 2023-04-07 13:00:09,410\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=5416)\u001b[0m 2023-04-07 13:00:09,717\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=5416)\u001b[0m 2023-04-07 13:00:09,730\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=2680)\u001b[0m 2023-04-07 13:00:09,696\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=2680)\u001b[0m 2023-04-07 13:00:09,708\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=5416)\u001b[0m 2023-04-07 13:00:09,837\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_70486e9a7ca540a786c91d541bc9ce71\n",
      "\u001b[2m\u001b[36m(PG pid=5416)\u001b[0m 2023-04-07 13:00:09,837\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 21, '_timesteps_total': None, '_time_total': 6.829457998275757, '_episodes_total': 2100}\n",
      "\u001b[2m\u001b[36m(PG pid=2680)\u001b[0m 2023-04-07 13:00:09,848\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_0a7f3be2bfe9485fb61acdab74d336be\n",
      "\u001b[2m\u001b[36m(PG pid=2680)\u001b[0m 2023-04-07 13:00:09,848\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 7.200775861740112, '_episodes_total': 2200}\n",
      "2023-04-07 13:00:13,224\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00001\n",
      "2023-04-07 13:00:16,991\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.010000) into trial a4f1f_00001 (score = -7.050000)\n",
      "\n",
      "2023-04-07 13:00:16,992\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00001:\n",
      "lr : 0.0001 --- (shift left) --> 0.0005\n",
      "alpha : 0.20636523692009068 --- (* 0.8) --> 0.16509218953607255\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=26340)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=26340)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=4284)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=4284)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=26340)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=4284)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=26340)\u001b[0m [I 04/07/23 13:00:19.102 34732] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=4284)\u001b[0m [I 04/07/23 13:00:19.073 12120] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=26340)\u001b[0m 2023-04-07 13:00:20,103\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=4284)\u001b[0m 2023-04-07 13:00:20,073\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26340)\u001b[0m 2023-04-07 13:00:20,400\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=26340)\u001b[0m 2023-04-07 13:00:20,414\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=4284)\u001b[0m 2023-04-07 13:00:20,356\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=4284)\u001b[0m 2023-04-07 13:00:20,371\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=26340)\u001b[0m 2023-04-07 13:00:20,535\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_c631bba34d9748f493db4dc0c426b15a\n",
      "\u001b[2m\u001b[36m(PG pid=26340)\u001b[0m 2023-04-07 13:00:20,535\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 6.4788336753845215, '_episodes_total': 2000}\n",
      "\u001b[2m\u001b[36m(PG pid=4284)\u001b[0m 2023-04-07 13:00:20,510\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_651009d6ad9343a1912d1d850ead6c0f\n",
      "\u001b[2m\u001b[36m(PG pid=4284)\u001b[0m 2023-04-07 13:00:20,510\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 21, '_timesteps_total': None, '_time_total': 6.819126844406128, '_episodes_total': 2100}\n",
      "2023-04-07 13:00:20,980\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.020000) into trial a4f1f_00000 (score = -7.050000)\n",
      "\n",
      "2023-04-07 13:00:20,981\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0001 --- (shift right) --> 5e-05\n",
      "alpha : 0.20636523692009068 --- (* 0.8) --> 0.16509218953607255\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=34220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34220)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m [I 04/07/23 13:00:27.182 33656] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:00:28,133\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:00:28,403\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:00:28,417\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:00:28,514\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_dcffa03e31c94580ae39cc200f96f252\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:00:28,514\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 36, '_timesteps_total': None, '_time_total': 12.180221557617188, '_episodes_total': 3600}\n",
      "\u001b[2m\u001b[36m(pid=18256)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=18256)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=3680)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=3680)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=18256)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=3680)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=18256)\u001b[0m [I 04/07/23 13:00:31.085 23384] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=3680)\u001b[0m [I 04/07/23 13:00:31.240 19856] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=18256)\u001b[0m 2023-04-07 13:00:32,378\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=3680)\u001b[0m 2023-04-07 13:00:32,428\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=18256)\u001b[0m 2023-04-07 13:00:32,705\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=18256)\u001b[0m 2023-04-07 13:00:32,724\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=3680)\u001b[0m 2023-04-07 13:00:32,754\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=3680)\u001b[0m 2023-04-07 13:00:32,775\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=18256)\u001b[0m 2023-04-07 13:00:32,851\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_aeb7d595d9d74be0b6b324ed3cd52c33\n",
      "\u001b[2m\u001b[36m(PG pid=18256)\u001b[0m 2023-04-07 13:00:32,851\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 7.199172496795654, '_episodes_total': 2200}\n",
      "\u001b[2m\u001b[36m(PG pid=3680)\u001b[0m 2023-04-07 13:00:32,907\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b9ac4ad1d7bf49da859e596581c1136b\n",
      "\u001b[2m\u001b[36m(PG pid=3680)\u001b[0m 2023-04-07 13:00:32,907\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 43, '_timesteps_total': None, '_time_total': 14.769606113433838, '_episodes_total': 4300}\n",
      "2023-04-07 13:00:33,438\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00002 (score = -6.980000) into trial a4f1f_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-07 13:00:33,439\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.001 --- (shift right) --> 0.0005\n",
      "alpha : 0.5755421705989963 --- (* 1.2) --> 0.6906506047187955\n",
      "\n",
      "2023-04-07 13:00:33,871\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00002 (score = -6.980000) into trial a4f1f_00001 (score = -7.010000)\n",
      "\n",
      "2023-04-07 13:00:33,872\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00001:\n",
      "lr : 0.001 --- (shift right) --> 0.0005\n",
      "alpha : 0.5755421705989963 --- (* 1.2) --> 0.6906506047187955\n",
      "\n",
      "2023-04-07 13:00:34,105\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "\u001b[2m\u001b[36m(pid=33100)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33100)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=28560)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=28560)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9272)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9272)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=2408)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=2408)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33100)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=28560)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=9272)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=2408)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33100)\u001b[0m [I 04/07/23 13:00:43.195 28404] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=28560)\u001b[0m [I 04/07/23 13:00:43.193 18308] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=9272)\u001b[0m [I 04/07/23 13:00:43.611 31372] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=2408)\u001b[0m [I 04/07/23 13:00:43.639 34712] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=33100)\u001b[0m 2023-04-07 13:00:44,190\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=28560)\u001b[0m 2023-04-07 13:00:44,086\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=28560)\u001b[0m 2023-04-07 13:00:44,345\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=28560)\u001b[0m 2023-04-07 13:00:44,361\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=33100)\u001b[0m 2023-04-07 13:00:44,444\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33100)\u001b[0m 2023-04-07 13:00:44,457\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=28560)\u001b[0m 2023-04-07 13:00:44,470\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_57349de095574f0cbc06d37361b81054\n",
      "\u001b[2m\u001b[36m(PG pid=28560)\u001b[0m 2023-04-07 13:00:44,470\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 66, '_timesteps_total': None, '_time_total': 23.4055597782135, '_episodes_total': 6600}\n",
      "\u001b[2m\u001b[36m(PG pid=33100)\u001b[0m 2023-04-07 13:00:44,573\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_c0e831ef4bce409d82897664006c0f72\n",
      "\u001b[2m\u001b[36m(PG pid=33100)\u001b[0m 2023-04-07 13:00:44,573\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 23, '_timesteps_total': None, '_time_total': 7.577331781387329, '_episodes_total': 2300}\n",
      "\u001b[2m\u001b[36m(PG pid=9272)\u001b[0m 2023-04-07 13:00:44,611\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=2408)\u001b[0m 2023-04-07 13:00:44,608\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=9272)\u001b[0m 2023-04-07 13:00:44,863\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=9272)\u001b[0m 2023-04-07 13:00:44,877\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=2408)\u001b[0m 2023-04-07 13:00:44,867\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=2408)\u001b[0m 2023-04-07 13:00:44,880\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=9272)\u001b[0m 2023-04-07 13:00:45,014\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_321cfc282438430187f744524a71195c\n",
      "\u001b[2m\u001b[36m(PG pid=9272)\u001b[0m 2023-04-07 13:00:45,014\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 23, '_timesteps_total': None, '_time_total': 7.577331781387329, '_episodes_total': 2300}\n",
      "\u001b[2m\u001b[36m(PG pid=2408)\u001b[0m 2023-04-07 13:00:45,028\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_13149429edc646f38e2dcdc4085abe10\n",
      "\u001b[2m\u001b[36m(PG pid=2408)\u001b[0m 2023-04-07 13:00:45,028\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 24, '_timesteps_total': None, '_time_total': 7.937309503555298, '_episodes_total': 2400}\n",
      "2023-04-07 13:00:45,427\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00000 (score = -6.970000) into trial a4f1f_00002 (score = -7.030000)\n",
      "\n",
      "2023-04-07 13:00:45,428\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00002:\n",
      "lr : 0.0005 --- (resample) --> 1e-05\n",
      "alpha : 0.6906506047187955 --- (* 0.8) --> 0.5525204837750364\n",
      "\n",
      "2023-04-07 13:00:45,656\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00000 (score = -6.970000) into trial a4f1f_00001 (score = -7.040000)\n",
      "\n",
      "2023-04-07 13:00:45,657\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00001:\n",
      "lr : 0.0005 --- (shift left) --> 0.001\n",
      "alpha : 0.6906506047187955 --- (resample) --> 0.46208138854932457\n",
      "\n",
      "2023-04-07 13:00:45,949\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 13:00:48,441\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.000000) into trial a4f1f_00000 (score = -7.040000)\n",
      "\n",
      "2023-04-07 13:00:48,442\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0001 --- (shift right) --> 5e-05\n",
      "alpha : 0.20636523692009068 --- (* 1.2) --> 0.2476382843041088\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=14808)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=14808)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=12328)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=12328)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=14808)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=12328)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=14808)\u001b[0m [I 04/07/23 13:00:55.993 29088] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=12328)\u001b[0m [I 04/07/23 13:00:56.019 33860] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(pid=32536)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=32536)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=14808)\u001b[0m 2023-04-07 13:00:57,037\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=12328)\u001b[0m 2023-04-07 13:00:57,049\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=14808)\u001b[0m 2023-04-07 13:00:57,283\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=14808)\u001b[0m 2023-04-07 13:00:57,297\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=12328)\u001b[0m 2023-04-07 13:00:57,297\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=14808)\u001b[0m 2023-04-07 13:00:57,406\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_28102e2611514ce0a6139756f9a8ba77\n",
      "\u001b[2m\u001b[36m(PG pid=14808)\u001b[0m 2023-04-07 13:00:57,406\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 24, '_timesteps_total': None, '_time_total': 7.972358703613281, '_episodes_total': 2400}\n",
      "\u001b[2m\u001b[36m(PG pid=12328)\u001b[0m 2023-04-07 13:00:57,313\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=12328)\u001b[0m 2023-04-07 13:00:57,433\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_5c389536fcc944878704eb097da8d795\n",
      "\u001b[2m\u001b[36m(PG pid=12328)\u001b[0m 2023-04-07 13:00:57,433\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 24, '_timesteps_total': None, '_time_total': 7.972358703613281, '_episodes_total': 2400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=32536)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=32536)\u001b[0m [I 04/07/23 13:00:58.436 29932] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=32536)\u001b[0m 2023-04-07 13:00:59,400\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=32536)\u001b[0m 2023-04-07 13:00:59,667\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=32536)\u001b[0m 2023-04-07 13:00:59,681\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=32536)\u001b[0m 2023-04-07 13:00:59,797\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b78e73daf1ff4a4dab22539c0d4d3213\n",
      "\u001b[2m\u001b[36m(PG pid=32536)\u001b[0m 2023-04-07 13:00:59,797\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 73, '_timesteps_total': None, '_time_total': 26.272619009017944, '_episodes_total': 7300}\n",
      "2023-04-07 13:01:00,356\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.010000) into trial a4f1f_00001 (score = -7.060000)\n",
      "\n",
      "2023-04-07 13:01:00,357\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00001:\n",
      "lr : 0.0001 --- (shift left) --> 0.0005\n",
      "alpha : 0.20636523692009068 --- (* 0.8) --> 0.16509218953607255\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=33260)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33260)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=34500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34500)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33260)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=34500)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33260)\u001b[0m [I 04/07/23 13:01:10.629 15824] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=34500)\u001b[0m [I 04/07/23 13:01:10.639 33172] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=33260)\u001b[0m 2023-04-07 13:01:11,696\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=34500)\u001b[0m 2023-04-07 13:01:11,670\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=33260)\u001b[0m 2023-04-07 13:01:11,941\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33260)\u001b[0m 2023-04-07 13:01:11,955\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=34500)\u001b[0m 2023-04-07 13:01:11,926\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34500)\u001b[0m 2023-04-07 13:01:11,941\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=33260)\u001b[0m 2023-04-07 13:01:12,116\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_1489fadde7594b30b9425a32a724e76b\n",
      "\u001b[2m\u001b[36m(PG pid=33260)\u001b[0m 2023-04-07 13:01:12,117\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 94, '_timesteps_total': None, '_time_total': 34.09998416900635, '_episodes_total': 9400}\n",
      "\u001b[2m\u001b[36m(PG pid=34500)\u001b[0m 2023-04-07 13:01:12,121\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_499f3e4df0cf49268fe5aeb0957a8099\n",
      "\u001b[2m\u001b[36m(PG pid=34500)\u001b[0m 2023-04-07 13:01:12,121\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 10.11820125579834, '_episodes_total': 3000}\n",
      "2023-04-07 13:01:12,896\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.000000) into trial a4f1f_00000 (score = -7.030000)\n",
      "\n",
      "2023-04-07 13:01:12,897\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0001 --- (shift right) --> 5e-05\n",
      "alpha : 0.20636523692009068 --- (* 0.8) --> 0.16509218953607255\n",
      "\n",
      "2023-04-07 13:01:19,325\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.000000) into trial a4f1f_00002 (score = -7.040000)\n",
      "\n",
      "2023-04-07 13:01:19,326\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00002:\n",
      "lr : 0.0001 --- (resample) --> 0.0005\n",
      "alpha : 0.20636523692009068 --- (* 1.2) --> 0.2476382843041088\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=19572)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19572)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=34468)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34468)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=34468)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m [I 04/07/23 13:01:23.017 26088] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=34468)\u001b[0m [I 04/07/23 13:01:23.123 30692] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=34468)\u001b[0m 2023-04-07 13:01:24,070\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:01:24,096\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:01:24,334\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34468)\u001b[0m 2023-04-07 13:01:24,319\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34468)\u001b[0m 2023-04-07 13:01:24,333\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:01:24,349\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:01:24,452\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_be26768478de4c69a35ae7ddc7310883\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:01:24,452\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 123, '_timesteps_total': None, '_time_total': 44.77521777153015, '_episodes_total': 12300}\n",
      "\u001b[2m\u001b[36m(PG pid=34468)\u001b[0m 2023-04-07 13:01:24,428\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_d0735f32427c47f4993fe8da232088cf\n",
      "\u001b[2m\u001b[36m(PG pid=34468)\u001b[0m 2023-04-07 13:01:24,428\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 96, '_timesteps_total': None, '_time_total': 34.86390781402588, '_episodes_total': 9600}\n",
      "2023-04-07 13:01:25,256\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -6.940000) into trial a4f1f_00001 (score = -7.040000)\n",
      "\n",
      "2023-04-07 13:01:25,257\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00001:\n",
      "lr : 0.0001 --- (shift left) --> 0.0005\n",
      "alpha : 0.20636523692009068 --- (resample) --> 0.9375447720095048\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=33584)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33584)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=32328)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=32328)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33584)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=32328)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33584)\u001b[0m [I 04/07/23 13:01:29.435 5280] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=32328)\u001b[0m [I 04/07/23 13:01:29.432 34728] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=33584)\u001b[0m 2023-04-07 13:01:30,462\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=32328)\u001b[0m 2023-04-07 13:01:30,477\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=33584)\u001b[0m 2023-04-07 13:01:30,749\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33584)\u001b[0m 2023-04-07 13:01:30,764\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=32328)\u001b[0m 2023-04-07 13:01:30,752\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=32328)\u001b[0m 2023-04-07 13:01:30,766\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=33584)\u001b[0m 2023-04-07 13:01:30,893\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_fa721bef6f69499e963bc9368c27ed14\n",
      "\u001b[2m\u001b[36m(PG pid=33584)\u001b[0m 2023-04-07 13:01:30,893\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 135, '_timesteps_total': None, '_time_total': 49.21681356430054, '_episodes_total': 13500}\n",
      "\u001b[2m\u001b[36m(PG pid=32328)\u001b[0m 2023-04-07 13:01:30,955\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_cb134fd2b4f84150bb3d17815beb9280\n",
      "\u001b[2m\u001b[36m(PG pid=32328)\u001b[0m 2023-04-07 13:01:30,955\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 134, '_timesteps_total': None, '_time_total': 48.828065156936646, '_episodes_total': 13400}\n",
      "\u001b[2m\u001b[36m(pid=34456)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34456)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=34220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34220)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=34456)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=34456)\u001b[0m [I 04/07/23 13:01:35.642 33668] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m [I 04/07/23 13:01:35.657 6820] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=34456)\u001b[0m 2023-04-07 13:01:36,662\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:01:36,671\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=34456)\u001b[0m 2023-04-07 13:01:36,912\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:01:36,917\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34456)\u001b[0m 2023-04-07 13:01:36,926\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:01:36,934\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:01:37,061\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_0b076c1648994e79920fc72edc5f3bbc\n",
      "\u001b[2m\u001b[36m(PG pid=34220)\u001b[0m 2023-04-07 13:01:37,062\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 135, '_timesteps_total': None, '_time_total': 49.21681356430054, '_episodes_total': 13500}\n",
      "\u001b[2m\u001b[36m(PG pid=34456)\u001b[0m 2023-04-07 13:01:37,092\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f41f8e872d4b425fa659536dc1717fdf\n",
      "\u001b[2m\u001b[36m(PG pid=34456)\u001b[0m 2023-04-07 13:01:37,092\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 125, '_timesteps_total': None, '_time_total': 45.502583742141724, '_episodes_total': 12500}\n",
      "2023-04-07 13:01:37,516\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.000000) into trial a4f1f_00000 (score = -7.030000)\n",
      "\n",
      "2023-04-07 13:01:37,517\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0001 --- (shift right) --> 5e-05\n",
      "alpha : 0.20636523692009068 --- (* 1.2) --> 0.2476382843041088\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=31892)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=31892)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=34412)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34412)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=34412)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=31892)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=34412)\u001b[0m [I 04/07/23 13:01:47.425 6632] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=31892)\u001b[0m [I 04/07/23 13:01:47.396 33992] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=34412)\u001b[0m 2023-04-07 13:01:48,396\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=31892)\u001b[0m 2023-04-07 13:01:48,356\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=34412)\u001b[0m 2023-04-07 13:01:48,637\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34412)\u001b[0m 2023-04-07 13:01:48,651\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=31892)\u001b[0m 2023-04-07 13:01:48,632\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=31892)\u001b[0m 2023-04-07 13:01:48,645\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=34412)\u001b[0m 2023-04-07 13:01:48,778\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3f0c8763a0de48f9b4265d7a6af836f7\n",
      "\u001b[2m\u001b[36m(PG pid=34412)\u001b[0m 2023-04-07 13:01:48,778\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 148, '_timesteps_total': None, '_time_total': 54.23439359664917, '_episodes_total': 14800}\n",
      "\u001b[2m\u001b[36m(PG pid=31892)\u001b[0m 2023-04-07 13:01:48,782\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_170cda86e8954246a02a6fef8e0a09d3\n",
      "\u001b[2m\u001b[36m(PG pid=31892)\u001b[0m 2023-04-07 13:01:48,782\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 147, '_timesteps_total': None, '_time_total': 53.86941409111023, '_episodes_total': 14700}\n",
      "2023-04-07 13:01:49,568\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.000000) into trial a4f1f_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-07 13:01:49,569\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0001 --- (resample) --> 5e-05\n",
      "alpha : 0.20636523692009068 --- (* 0.8) --> 0.16509218953607255\n",
      "\n",
      "2023-04-07 13:01:50,183\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.000000) into trial a4f1f_00002 (score = -7.010000)\n",
      "\n",
      "2023-04-07 13:01:50,183\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00002:\n",
      "lr : 0.0001 --- (shift right) --> 5e-05\n",
      "alpha : 0.20636523692009068 --- (* 1.2) --> 0.2476382843041088\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=116)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=116)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=10544)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=10544)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=34132)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34132)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=29912)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=29912)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=10544)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=116)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=34132)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=29912)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=116)\u001b[0m [I 04/07/23 13:02:00.105 11772] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=10544)\u001b[0m [I 04/07/23 13:02:00.105 34012] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=34132)\u001b[0m [I 04/07/23 13:02:00.186 27116] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=29912)\u001b[0m [I 04/07/23 13:02:00.220 30904] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=116)\u001b[0m 2023-04-07 13:02:00,983\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=10544)\u001b[0m 2023-04-07 13:02:01,010\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=34132)\u001b[0m 2023-04-07 13:02:01,096\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=29912)\u001b[0m 2023-04-07 13:02:01,130\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=116)\u001b[0m 2023-04-07 13:02:01,244\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=116)\u001b[0m 2023-04-07 13:02:01,257\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=10544)\u001b[0m 2023-04-07 13:02:01,255\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=10544)\u001b[0m 2023-04-07 13:02:01,269\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=116)\u001b[0m 2023-04-07 13:02:01,384\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_622e7278d53944b68f7e7c1ed463092f\n",
      "\u001b[2m\u001b[36m(PG pid=116)\u001b[0m 2023-04-07 13:02:01,384\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 150, '_timesteps_total': None, '_time_total': 54.99488806724548, '_episodes_total': 15000}\n",
      "\u001b[2m\u001b[36m(PG pid=10544)\u001b[0m 2023-04-07 13:02:01,447\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ba70abaefe1648aea3aa06c09afe3d87\n",
      "\u001b[2m\u001b[36m(PG pid=10544)\u001b[0m 2023-04-07 13:02:01,447\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 149, '_timesteps_total': None, '_time_total': 54.58522891998291, '_episodes_total': 14900}\n",
      "\u001b[2m\u001b[36m(PG pid=34132)\u001b[0m 2023-04-07 13:02:01,360\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34132)\u001b[0m 2023-04-07 13:02:01,381\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=29912)\u001b[0m 2023-04-07 13:02:01,396\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=29912)\u001b[0m 2023-04-07 13:02:01,410\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=34132)\u001b[0m 2023-04-07 13:02:01,493\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_8378ea2657f848659e647702bd8ae373\n",
      "\u001b[2m\u001b[36m(PG pid=34132)\u001b[0m 2023-04-07 13:02:01,494\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 150, '_timesteps_total': None, '_time_total': 54.99488806724548, '_episodes_total': 15000}\n",
      "\u001b[2m\u001b[36m(PG pid=29912)\u001b[0m 2023-04-07 13:02:01,513\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_0fc6053b021a4503a2feeaf215a2a9f7\n",
      "\u001b[2m\u001b[36m(PG pid=29912)\u001b[0m 2023-04-07 13:02:01,513\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 160, '_timesteps_total': None, '_time_total': 58.245938539505005, '_episodes_total': 16000}\n",
      "2023-04-07 13:02:01,966\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.000000) into trial a4f1f_00002 (score = -7.010000)\n",
      "\n",
      "2023-04-07 13:02:01,966\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00002:\n",
      "lr : 0.0001 --- (shift right) --> 5e-05\n",
      "alpha : 0.20636523692009068 --- (* 1.2) --> 0.2476382843041088\n",
      "\n",
      "2023-04-07 13:02:08,714\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.000000) into trial a4f1f_00000 (score = -7.010000)\n",
      "\n",
      "2023-04-07 13:02:08,715\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0001 --- (resample) --> 0.001\n",
      "alpha : 0.20636523692009068 --- (* 1.2) --> 0.2476382843041088\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=33068)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33068)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=32064)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=32064)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33068)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=32064)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=33068)\u001b[0m [I 04/07/23 13:02:12.227 34644] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=32064)\u001b[0m [I 04/07/23 13:02:12.302 8500] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=32064)\u001b[0m 2023-04-07 13:02:13,301\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=33068)\u001b[0m 2023-04-07 13:02:13,348\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=32064)\u001b[0m 2023-04-07 13:02:13,551\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=32064)\u001b[0m 2023-04-07 13:02:13,565\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=33068)\u001b[0m 2023-04-07 13:02:13,607\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33068)\u001b[0m 2023-04-07 13:02:13,625\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=32064)\u001b[0m 2023-04-07 13:02:13,713\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_591592e850414b919cadb7dd2bdd4052\n",
      "\u001b[2m\u001b[36m(PG pid=32064)\u001b[0m 2023-04-07 13:02:13,713\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 151, '_timesteps_total': None, '_time_total': 55.369802713394165, '_episodes_total': 15100}\n",
      "\u001b[2m\u001b[36m(PG pid=33068)\u001b[0m 2023-04-07 13:02:13,815\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_629639b6f0124922bc0f26cea8a514be\n",
      "\u001b[2m\u001b[36m(PG pid=33068)\u001b[0m 2023-04-07 13:02:13,815\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 161, '_timesteps_total': None, '_time_total': 58.61354947090149, '_episodes_total': 16100}\n",
      "\u001b[2m\u001b[36m(pid=22212)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=22212)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=22212)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=22212)\u001b[0m [I 04/07/23 13:02:18.534 28260] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "2023-04-07 13:02:19,075\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00003 (score = -7.000000) into trial a4f1f_00002 (score = -7.020000)\n",
      "\n",
      "2023-04-07 13:02:19,075\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00002:\n",
      "lr : 0.0001 --- (shift right) --> 5e-05\n",
      "alpha : 0.20636523692009068 --- (* 1.2) --> 0.2476382843041088\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=22212)\u001b[0m 2023-04-07 13:02:19,474\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=22212)\u001b[0m 2023-04-07 13:02:19,728\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=22212)\u001b[0m 2023-04-07 13:02:19,742\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=22212)\u001b[0m 2023-04-07 13:02:19,852\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b03c040d8c5f42228ba8d4c08c1cb6fc\n",
      "\u001b[2m\u001b[36m(PG pid=22212)\u001b[0m 2023-04-07 13:02:19,853\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 163, '_timesteps_total': None, '_time_total': 59.80077075958252, '_episodes_total': 16300}\n",
      "\u001b[2m\u001b[36m(pid=19604)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19604)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=8416)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=8416)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=19604)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=8416)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=19604)\u001b[0m [I 04/07/23 13:02:29.213 21908] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=8416)\u001b[0m [I 04/07/23 13:02:29.211 30976] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=19604)\u001b[0m 2023-04-07 13:02:30,142\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=8416)\u001b[0m 2023-04-07 13:02:30,060\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=19604)\u001b[0m 2023-04-07 13:02:30,393\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=19604)\u001b[0m 2023-04-07 13:02:30,408\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=8416)\u001b[0m 2023-04-07 13:02:30,318\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=8416)\u001b[0m 2023-04-07 13:02:30,331\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=19604)\u001b[0m 2023-04-07 13:02:30,525\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_4628a09188264636b32233444d7a2bc0\n",
      "\u001b[2m\u001b[36m(PG pid=19604)\u001b[0m 2023-04-07 13:02:30,525\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 172, '_timesteps_total': None, '_time_total': 62.58664011955261, '_episodes_total': 17200}\n",
      "\u001b[2m\u001b[36m(PG pid=8416)\u001b[0m 2023-04-07 13:02:30,457\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_00cd4ed65870434cb2fa283baee96d90\n",
      "\u001b[2m\u001b[36m(PG pid=8416)\u001b[0m 2023-04-07 13:02:30,457\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 183, '_timesteps_total': None, '_time_total': 66.95818305015564, '_episodes_total': 18300}\n",
      "2023-04-07 13:02:31,058\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00002 (score = -7.000000) into trial a4f1f_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-07 13:02:31,060\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 5e-05 --- (shift right) --> 1e-05\n",
      "alpha : 0.2476382843041088 --- (* 0.8) --> 0.19811062744328706\n",
      "\n",
      "2023-04-07 13:02:31,514\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 13:02:32,035\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 13:02:37,128\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "2023-04-07 13:02:38,692\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00002\n",
      "\u001b[2m\u001b[36m(pid=23776)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=23776)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=30072)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=30072)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=23776)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=30072)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=23776)\u001b[0m [I 04/07/23 13:02:40.490 19236] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=30072)\u001b[0m [I 04/07/23 13:02:40.474 21156] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=23776)\u001b[0m 2023-04-07 13:02:41,222\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=30072)\u001b[0m 2023-04-07 13:02:41,222\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=23776)\u001b[0m 2023-04-07 13:02:41,453\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=30072)\u001b[0m 2023-04-07 13:02:41,454\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=23776)\u001b[0m 2023-04-07 13:02:41,467\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=30072)\u001b[0m 2023-04-07 13:02:41,467\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=23776)\u001b[0m 2023-04-07 13:02:41,585\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_2f84a9d4db2c4d47aa592b110d607702\n",
      "\u001b[2m\u001b[36m(PG pid=23776)\u001b[0m 2023-04-07 13:02:41,586\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 184, '_timesteps_total': None, '_time_total': 67.35229706764221, '_episodes_total': 18400}\n",
      "\u001b[2m\u001b[36m(PG pid=30072)\u001b[0m 2023-04-07 13:02:41,649\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_8ae4435bfaed49e39ee102309a03a021\n",
      "\u001b[2m\u001b[36m(PG pid=30072)\u001b[0m 2023-04-07 13:02:41,650\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 173, '_timesteps_total': None, '_time_total': 62.97747588157654, '_episodes_total': 17300}\n",
      "2023-04-07 13:02:41,903\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a4f1f_00000\n",
      "2023-04-07 13:02:42,246\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00001 (score = -7.000000) into trial a4f1f_00000 (score = -7.010000)\n",
      "\n",
      "2023-04-07 13:02:42,246\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0005 --- (shift right) --> 0.0001\n",
      "alpha : 0.9375447720095048 --- (resample) --> 0.5950594397652048\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=2700)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=2700)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=2700)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=2700)\u001b[0m [I 04/07/23 13:02:50.785 34544] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=2700)\u001b[0m 2023-04-07 13:02:51,530\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=2700)\u001b[0m 2023-04-07 13:02:51,770\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=2700)\u001b[0m 2023-04-07 13:02:51,783\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=2700)\u001b[0m 2023-04-07 13:02:51,918\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_cbba56cdac804a7e94db72a885909c95\n",
      "\u001b[2m\u001b[36m(PG pid=2700)\u001b[0m 2023-04-07 13:02:51,918\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 174, '_timesteps_total': None, '_time_total': 63.27877354621887, '_episodes_total': 17400}\n",
      "2023-04-07 13:02:52,755\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00001 (score = -7.000000) into trial a4f1f_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-07 13:02:52,756\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0005 --- (shift left) --> 0.001\n",
      "alpha : 0.9375447720095048 --- (* 1.2) --> 1.1250537264114058\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=19572)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19572)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=31324)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=31324)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=31324)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m [I 04/07/23 13:03:01.693 33656] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=31324)\u001b[0m [I 04/07/23 13:03:01.739 26020] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:03:02,381\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=31324)\u001b[0m 2023-04-07 13:03:02,417\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:03:02,622\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:03:02,635\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=31324)\u001b[0m 2023-04-07 13:03:02,644\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=31324)\u001b[0m 2023-04-07 13:03:02,657\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=31324)\u001b[0m 2023-04-07 13:03:02,760\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_6fca31d4e7694d4bb7843245e6ddabbb\n",
      "\u001b[2m\u001b[36m(PG pid=31324)\u001b[0m 2023-04-07 13:03:02,760\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 196, '_timesteps_total': None, '_time_total': 70.01025223731995, '_episodes_total': 19600}\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:03:02,800\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_48e64cff96904470b532ab6510d843e5\n",
      "\u001b[2m\u001b[36m(PG pid=19572)\u001b[0m 2023-04-07 13:03:02,800\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 195, '_timesteps_total': None, '_time_total': 69.65321779251099, '_episodes_total': 19500}\n",
      "2023-04-07 13:03:03,273\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a4f1f_00001 (score = -7.000000) into trial a4f1f_00000 (score = -7.010000)\n",
      "\n",
      "2023-04-07 13:03:03,275\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala4f1f_00000:\n",
      "lr : 0.0005 --- (shift left) --> 0.001\n",
      "alpha : 0.9375447720095048 --- (* 1.2) --> 1.1250537264114058\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=2576)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=2576)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=2576)\u001b[0m [Taichi] version 1.5.0, llvm 15.0.1, commit 7b885c28, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=2576)\u001b[0m [I 04/07/23 13:03:11.460 33488] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=2576)\u001b[0m 2023-04-07 13:03:11,995\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=2576)\u001b[0m 2023-04-07 13:03:12,225\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=2576)\u001b[0m 2023-04-07 13:03:12,238\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=2576)\u001b[0m 2023-04-07 13:03:12,333\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_366585efe9034f27a888a19979dd92f6\n",
      "\u001b[2m\u001b[36m(PG pid=2576)\u001b[0m 2023-04-07 13:03:12,334\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 197, '_timesteps_total': None, '_time_total': 70.31953382492065, '_episodes_total': 19700}\n",
      "2023-04-07 13:03:13,821\tINFO tune.py:798 -- Total run time: 251.00 seconds (250.53 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "#from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "from gymnasium.spaces import Dict, Discrete, MultiDiscrete, Tuple,Box\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"PG\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=16)\n",
    "parser.add_argument(\"--num-gpus\", type=int, default=1)\n",
    "#parser.add_argument(\"render_mode\", type=int, default=1)\n",
    "parser.add_argument(\n",
    "    \"--mixer\",\n",
    "    type=str,\n",
    "    default=\"qmix\",\n",
    "    choices=[\"qmix\", \"vdn\", \"none\"],\n",
    "    help=\"The mixer model to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=70000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=8.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "            #if agent_id.startswith(\"low_level_\"):\n",
    "                #return \"low_level_policy\"\n",
    "            #else:\n",
    "                #return \"high_level_policy\"\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ray.init(num_cpus=16, num_gpus=1,local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0],\n",
    "        \"group_2\": [0,1],\n",
    "        \"group_3\": [0]\n",
    "    }\n",
    "\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=observation_space, act_space=action_space\n",
    "        ),\n",
    "    )\n",
    "    \"\"\"\n",
    "    from ray.tune import register_env\n",
    "    from ray.rllib.algorithms.dqn import DQN \n",
    "    YourExternalEnv = ... \n",
    "    register_env(\"my_env\", \n",
    "        lambda config: YourExternalEnv(config))\n",
    "    trainer = DQN(env=\"my_env\") \n",
    "    while True: \n",
    "        print(trainer.train()) \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(TwoStepGame)\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "    def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "        #if agent_id.startswith(\"low_level_\"):\n",
    "        if agent_id.startswith(\"group_1\"):\n",
    "            return \"low_level_policy\"\n",
    "        else:\n",
    "            return \"high_level_policy\"\n",
    "\n",
    "    if args.run == \"QMIX\":\n",
    "        \n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "\n",
    "            .training(mixer=args.mixer, train_batch_size=32)\n",
    "            \n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        observation_space,\n",
    "                        action_space,\n",
    "                        config.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        Tuple([observation_space,Box(-inf, inf, (1,), float64)]),#,Box()\n",
    "                        action_space,\n",
    "                        config.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn#lambda agent_id, episode, worker, **kwargs: \"pol2\"\n",
    "                #policy_mapping_fn=(lambda agent_id, episode, worker, **kw: (\"pol1\" if agent_id == \"agent1\" else \"pol2\")\n",
    "    #)#policy_mapping_fn,\n",
    "            )\n",
    "            .rollouts(num_rollout_workers=16, rollout_fragment_length=4)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "                }\n",
    "            )\n",
    "            .environment(\n",
    "                env=\"grouped_twostep\",\n",
    "                env_config={\n",
    "                    \"separate_state_space\": True,\n",
    "                    \"one_hot_state_encoding\": True,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr='training_iteration',\n",
    "        metric='episode_reward_mean',#'loss',\n",
    "        mode='min',\n",
    "        perturbation_interval=1,\n",
    "        hyperparam_mutations={\n",
    "            \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"alpha\": tune.uniform(0.0, 1.0),\n",
    "        }\n",
    "    )\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        \n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=4,\n",
    "            scheduler=pbt_scheduler,\n",
    "        ),\n",
    "        \n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a53598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best result based on a particular metric.\n",
    "best_result = results.get_best_result(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# Get the best checkpoint corresponding to the best result.\n",
    "best_checkpoint = best_result.checkpoint\n",
    "\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "algo = Algorithm.from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36041ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "while True:\n",
    "    print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a933b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = algo.get_policy()\n",
    "# <ray.rllib.policy.eager_tf_policy.PPOTFPolicy_eager object at 0x7fd020165470>\n",
    "\n",
    "# Run a forward pass to get model output logits. Note that complex observations\n",
    "# must be preprocessed as in the above code block.\n",
    "logits, _ = policy.model({\"obs\": np.array([[0.1, 0.2, 0.3, 0.4]])})\n",
    "# (<tf.Tensor: id=1274, shape=(1, 2), dtype=float32, numpy=...>, [])\n",
    "\n",
    "# Compute action distribution given logits\n",
    "policy.dist_class\n",
    "# <class_object 'ray.rllib.models.tf.tf_action_dist.Categorical'>\n",
    "dist = policy.dist_class(logits, policy.model)\n",
    "# <ray.rllib.models.tf.tf_action_dist.Categorical object at 0x7fd02301d710>\n",
    "\n",
    "# Query the distribution for samples, sample logps\n",
    "dist.sample()\n",
    "# <tf.Tensor: id=661, shape=(1,), dtype=int64, numpy=..>\n",
    "dist.logp([1])\n",
    "# <tf.Tensor: id=1298, shape=(1,), dtype=float32, numpy=...>\n",
    "\n",
    "# Get the estimated values for the most recent forward pass\n",
    "policy.model.value_function()\n",
    "# <tf.Tensor: id=670, shape=(1,), dtype=float32, numpy=...>\n",
    "\n",
    "policy.model.base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9664eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = Session(graph, env)\n",
    "sess.run(\n",
    "    n_steps=500000,\n",
    "    learn=True,\n",
    "    render=TwoStepGame.render,\n",
    "    success_reward=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3630fba4",
   "metadata": {},
   "source": [
    "0.37 policy loss with prior precision 100\n",
    "\n",
    "0.9271736741065979 policy loss with prior precision of 1\n",
    "0.55 policy loss prior precision of 0.0001\n",
    "\n",
    "0.96 policy loss with prior precision of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoStepGame.render()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a6e5079",
   "metadata": {},
   "source": [
    "psueudo code for visualization\n",
    "@ti.func\n",
    "render():\n",
    "for i in num_agents\n",
    "\n",
    "create agent\n",
    "\n",
    "if agent belongs to class 1 high level policy then put it in position 1 on the render screen\n",
    "\n",
    "if agent belongs to class 1 or 2 mid level policy then put it in position 2 on the render screen\n",
    "\n",
    "if agent belongs to class 1 low level policy then put it in position 3 on render screen\n",
    "\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "raw",
   "id": "482097b5",
   "metadata": {},
   "source": [
    "\n",
    "top level prior is going to be using expected free energy to create a dirchlet distribution. This will use something akin to Bayesian optimal experimental design\n",
    "design to create a policy that will explore what conditions cause a failiure of the objective and exploit this information to minimzie this. This prior will be constantly updated by prediction errors\n",
    "\n",
    "All other priors will be empirical priors. \n",
    "\n",
    "horizontal is dynamics where this can be interpreted as a graphical probabilistic model and vertical is from discrete to continuous from slower to finer time steps \n",
    "\n",
    "Note: when signals are sent to each other by neurons in the same level of the hierarchy this is just a probabilistic DAG like what is seen in the WISH presentation. When signals are sent up and down the hierarchy that is when predictions are sent down and prediction errors up from coarser to finer timescales \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pos = np.random.random((50, 2))\n",
    "# Create an array of 50 integer elements whose values are randomly 0, 1, 2\n",
    "# 0 corresponds to 0x068587\n",
    "# 1 corresponds to 0xED553B\n",
    "# 2 corresponds to 0xEEEEF0\n",
    "indices = np.random.randint(0, 2, size=(50,))\n",
    "gui = ti.GUI(\"circles\", res=(400, 400))\n",
    "while gui.running:\n",
    "    gui.circles(pos, radius=5, palette=[0x068587, 0xED553B, 0xEEEEF0], palette_indices=indices)\n",
    "    gui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bce991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "\n",
    "ti.init(arch=ti.cuda)\n",
    "\n",
    "N = 10\n",
    "\n",
    "particles_pos = ti.Vector.field(3, dtype=ti.f32, shape = N)\n",
    "points_pos = ti.Vector.field(3, dtype=ti.f32, shape = N)\n",
    "\n",
    "@ti.kernel\n",
    "def init_points_pos(points : ti.template()):\n",
    "    for i in range(points.shape[0]):\n",
    "        points[i] = [i for j in ti.static(range(3))]\n",
    "\n",
    "init_points_pos(particles_pos)\n",
    "init_points_pos(points_pos)\n",
    "\n",
    "window = ti.ui.Window(\"Test for Drawing 3d-lines\", (768, 768))\n",
    "canvas = window.get_canvas()\n",
    "scene = ti.ui.Scene()\n",
    "camera = ti.ui.Camera()\n",
    "camera.position(5, 2, 2)\n",
    "\n",
    "while window.running:\n",
    "    camera.track_user_inputs(window, movement_speed=0.03, hold_key=ti.ui.RMB)\n",
    "    scene.set_camera(camera)\n",
    "    scene.ambient_light((0.8, 0.8, 0.8))\n",
    "    scene.point_light(pos=(0.5, 1.5, 1.5), color=(1, 1, 1))\n",
    "\n",
    "    scene.particles(particles_pos, color = (0.68, 0.26, 0.19), radius = 0.1)\n",
    "    # Draw 3d-lines in the scene\n",
    "    scene.lines(points_pos, color = (0.28, 0.68, 0.99), width = 5.0)\n",
    "    canvas.scene(scene)\n",
    "    window.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212dfdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "[Taichi] Starting on arch=cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Window close button clicked, exiting... (use `while gui.running` to exit gracefully)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_12432\\2703488757.py\"\u001b[0m, line \u001b[0;32m29\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    gui.show()\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\taichi\\ui\\gui.py\"\u001b[1;36m, line \u001b[1;32m711\u001b[1;36m, in \u001b[1;35mshow\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.core.update()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m\u001b[1;31m:\u001b[0m Window close button clicked, exiting... (use `while gui.running` to exit gracefully)\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\n",
    "ti.init(arch=ti.gpu)\n",
    "\n",
    "n = 320\n",
    "pixels = ti.field(dtype=float, shape=(n * 2, n))\n",
    "\n",
    "@ti.func\n",
    "def complex_sqr(z):  # complex square of a 2D vector\n",
    "    return tm.vec2(z[0] * z[0] - z[1] * z[1], 2 * z[0] * z[1])\n",
    "\n",
    "@ti.kernel\n",
    "def paint(t: float):\n",
    "    for i, j in pixels:  # Parallelized over all pixels\n",
    "        c = tm.vec2(-0.8, tm.cos(t) * 0.2)\n",
    "        z = tm.vec2(i / n - 1, j / n - 0.5) * 2\n",
    "        iterations = 0\n",
    "        while z.norm() < 20 and iterations < 50:\n",
    "            z = complex_sqr(z) + c\n",
    "            iterations += 1\n",
    "        pixels[i, j] = 1 - iterations * 0.02\n",
    "\n",
    "gui = ti.GUI(\"Julia Set\", res=(n * 2, n))\n",
    "\n",
    "for i in range(1000000):\n",
    "    paint(i * 0.03)\n",
    "    gui.set_image(pixels)\n",
    "    gui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8692490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "\n",
    "#it lacks the full paremterizan of above cell. That my be the cause of the error\n",
    "\n",
    "\n",
    "config = QMixConfig()  \n",
    "config = config.training(gamma=0.9, lr=0.01)#, kl_coeff=0.3)  \n",
    "#config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=12)  \n",
    "print(config.to_dict())  \n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "register_env(\n",
    "    \"grouped_twostep\",\n",
    "    lambda config: TwoStepGame(config).with_agent_groups(\n",
    "        grouping, obs_space=obs_space, act_space=act_space\n",
    "    ),\n",
    ")\n",
    "\n",
    "algo = config.build(env=\"grouped_twostep\")  \n",
    "algo.train() "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5b2486f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
