{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c330eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "import argparse\n",
    "from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "import logging\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "#from ray.rllib.utils.torch_utils import \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77086353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b64f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CModel(torch.nn.Module,ivy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._avg_pool = torch.nn.AvgPool2d(4)\n",
    "        self._linear = torch.nn.Linear(3136, 1024)\n",
    "        self._relu = torch.nn.ReLU()\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        \n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x = self._avg_pool(x).mean(dim=-3).view(-1, 3136)\n",
    "        x = self._relu(self._linear(x))\n",
    "        return self._linears(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "246bdecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptimized_model = optimize_model(\\n    model, input_data=input_data, optimization_time=\"constrained\"\\n)\\nnote: under current code minimization of kullbacker-leibler divergence is already part of the code in the form the KLD divergence regularizer which means the loss simply needs to be the \"suprise\" \\n\\nw\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import tensorflow.compat.v2 as tf\n",
    "#tf.enable_v2_behavior()\n",
    "num_inducing_points=1\n",
    "#this is the agent network that will be used \n",
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "input_shape = [0,0]\n",
    "from numba import jit\n",
    "#@accelerate_model()\n",
    "\n",
    "class RNNModel(TorchModelV2,nn.Module,ivy.Module):\n",
    "  \n",
    "  def __init__(self):#,obs_space, action_space, num_outputs, model_config, name): #add this in later\n",
    "    \n",
    "    #self.obs_size = _get_size(obs_space)\n",
    "    #self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "\n",
    "    \"\"\"\n",
    "    what do we want our VAE to do?\n",
    "\n",
    "    our vae should: \n",
    "    1. infer a hidden state and \n",
    "    2. output an action   `\n",
    "\n",
    "    \"\"\"\n",
    "    tfk = tf.keras\n",
    "    tfkl = tf.keras.layers\n",
    "    \n",
    "    tfpl = tfp.layers\n",
    "    tfd = tfp.distributions\n",
    "\n",
    "    tfd = tfp.distributions\n",
    "    encoded_size = 16\n",
    "    \"\"\"\n",
    "    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),#This scale variable cannot be fixed but instead must be allowed to vary \n",
    "                                reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    The prior should be some kind of composite or dict type distribution with multiple dirchlet distributions. \n",
    "\n",
    "\n",
    "    \n",
    "    #note: the tfd Independent is for assumptions which are independent and unaffected or related to other assumptions. These distribtutions may need to be hand tuned based on domain knowledge and data analysis\n",
    "    #second note: this will be the top level prior. we will have to figure out a way to ensure that this prior is only inherited from posteriors of above signals\n",
    "\n",
    "    #solution: create an algorithm that tunes parameters of distribution and have it default to hand-written prior parameters if upper level observation space is empty and epoch is 0. \n",
    "    \n",
    "    the last layer needs to be a discrete (bernoulli) distribution layer for the high and midlevel layers and a continuous distribution for the lowest layer \n",
    "\n",
    "    dist = tfd.Normal(loc=0.5, scale=0.25). Note: we may want to consider using the normal for all agents and just discretize for higher level agents. \n",
    "\n",
    "    this is going to be where we consider things like lateral disinhibition where the prior pertaining to the probability carrying out an action is going to decrease if it has already carried out an action under certain circumstances\n",
    "    \n",
    "    Note: we are going to replace almost all the code in the init with the compiled model laplace CModel we have above\n",
    "    \"\"\"\n",
    "    #encoded_shape = 2\n",
    "    num_schools=2\n",
    "    \"\"\"\n",
    "    prior  = tfd.JointDistributionSequential([\n",
    "        tfd.Normal(loc=0., scale=10., name=\"avg_effect\"),  # `mu` above\n",
    "        tfd.Dirichlet(2),\n",
    "        tfd.Normal(loc=5., scale=1., name=\"avg_stddev\"),  # `log(tau)` above\n",
    "        tfd.Independent(tfd.Normal(loc=tf.zeros(num_schools),\n",
    "                                  scale=tf.ones(num_schools),\n",
    "                                  name=\"school_effects_standard\"),  # `theta_prime` \n",
    "                        reinterpreted_batch_ndims=1)\n",
    "    ])\n",
    "    \"\"\"\n",
    "    #prior= tfd.Normal(loc=0., scale=10., name=\"avg_effect\")\n",
    "    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                        reinterpreted_batch_ndims=1)\n",
    "    tfpl = tfp.layers\n",
    "    encoder = tfk.Sequential([\n",
    "        tfkl.InputLayer(input_shape=input_shape),\n",
    "        #tfkl.Dense(8)\n",
    "        tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                      activation=None),\n",
    "\n",
    "        tfpl.MultivariateNormalTriL(\n",
    "                encoded_size,\n",
    "                activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "    ])\n",
    "    decoder = tfk.Sequential([\n",
    "        tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "        tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "        #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "    ])\n",
    "    negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "    \"\"\"\n",
    "    #working theory as of 11/26/2022\n",
    "    \n",
    "    in order to convert from posterior -> action potential -> prior we need information theory specifically \n",
    "\n",
    "    probability of excitatory action potential = -log2(p(w)) if P(w) = 0.5 probability of excitatory action potential is 1\n",
    "\n",
    "    furthermore a prediction of w being 0.5 also yields a prediction of not(w) also being 0.5 \n",
    "\n",
    "    this is where I suspect inhibitory potentials come in since not(w) is expressed as log2(p(w)) which of 0.5 is -1 \n",
    "\n",
    "    we will probably do this translation from the observation space and action space\n",
    "\n",
    "    each action will actually correspond to a series of a 100 steps\n",
    "\n",
    "    if the algorithm tries to send a signal of 0.2 bits to a particular neuronal agent we will send \n",
    "    20 excitatory action potentials and 80 0s over 100 steps  or to put it another way we will say that 0.2 \n",
    "\n",
    "    is just the probability of it sending a excitatory potential at any given time \n",
    "\n",
    "    we will need to do an if else thing\n",
    "\n",
    "    if neurons is top neuron: \n",
    "      prior = <hand coded prior>\n",
    "    else: \n",
    "      if obs == 1:\n",
    "        prob = obs**2 #where obs will be the observed frequency over n previously observed values\n",
    "      if obs == -1:\n",
    "        prob = obs**2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    uncomment out later\n",
    "\n",
    "    loss = lambda y, rv_y: rv_y.variational_loss(\n",
    "        y, kl_weight=np.array(batch_size, tf.float64) / x.shape[0])\n",
    "    #tf.keras.optimizers.Adam(1e-4) tf.optimizers.Adam(learning_rate=0.011)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.011), loss=loss)#tf.optimizers.Adam(learning_rate=0.01)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    we are choosing to use a variational autoencoder because there are two things that need to be done: make predictions and select action that minimizes free energy \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    decoder = tfk.Sequential([\n",
    "        tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "        #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits),\n",
    "        ])\n",
    "    vae = tfk.Model(inputs=encoder.inputs,\n",
    "                        outputs=decoder(encoder.outputs[0]))\n",
    "    \"\"\"\n",
    "    ivy.Module.__init__(self)\n",
    "    \n",
    "  #@override(ModelV2)\n",
    "  @jit(nopython=True)\n",
    "  def get_initial_state(self):\n",
    "    # Place hidden states on same device as model.\n",
    "    return [\n",
    "        self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "    \n",
    "  #@override(ModelV2)\n",
    "  @jit(nopython=True)\n",
    "  def _forward(self, input_dict, hidden_state, seq_lens):# _forward\n",
    "    x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "    h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "    h = self.rnn(x, h_in)\n",
    "    q = self.fc2(h)#this may be the q-value \n",
    "    return q, [h]\n",
    "\n",
    "  \n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n",
    "\n",
    "ivy.set_framework('torch') \n",
    "model = RNNModel()#note that this particular line compiles if we get rid of parameters. at initialization we will call this with parameters \n",
    "#model = Laplace(model, 'regression', subset_of_weights='all', hessian_structure='full')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "optimized_model = optimize_model(\n",
    "    model, input_data=input_data, optimization_time=\"constrained\"\n",
    ")\n",
    "note: under current code minimization of kullbacker-leibler divergence is already part of the code in the form the KLD divergence regularizer which means the loss simply needs to be the \"suprise\" \n",
    "\n",
    "w\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b7550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ivy.set_framework('torch') \n",
    "model = CModel()\n",
    "model = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf119d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d29c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
