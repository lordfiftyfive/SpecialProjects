{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e69885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "if we do this carefully we can use taichi to carry out speedup\n",
    "\n",
    "The mixer and the bmodel would be ti.funcs\n",
    "\n",
    "qmixpolicy would also be a ti.func\n",
    "\n",
    "qmix would be the ti.kernel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import torch\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "#from nebulgym.decorators.torch_decorators import accel\n",
    "\n",
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "#from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE, make_multi_agent\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "\n",
    "import nitime\n",
    "from deeptime.sindy import SINDy\n",
    "\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51cee989",
   "metadata": {},
   "source": [
    "The expected free energy is minimized by selecting those observations that cause a large change in beliefs, in contrast to variational free energy that is minimized when observations comply wiht current beliefs\n",
    "\n",
    "This is the difference between optmizing beliefs in relation to data that have already been gathered (VFE) and selecting data that will best optimize beliefs \n",
    "\n",
    "Expected free energy furnishes our prior and is a prerequesite in minimzing variational free energy\n",
    "\n",
    "Note: the expected free energy will only be used on the level of the mixer network \n",
    "\n",
    "prior at top level agent will be: dirchlet by itself\n",
    "\n",
    "variational free energy according to friston determines the fit between generative model and past and current observations\n",
    "\n",
    "Variational free energy only depends on present and past observations. expected free energy is an extenstion that requires predicted future observations \n",
    "\n",
    "\"variational free neergy is at the core of Active inference. it measrues the fit between the generative model and current and past observations\"\n",
    "\n",
    "\"expected free energy is a way to score alternative policies for planning\"\n",
    "\n",
    "\"variational free energy minimzation is the outer loop of active inference... an active inference agent can also be endowed with a generative model of the consequences of its action that entails an evaluation of expected free energy (inside loop)\"\n",
    "\n",
    "\n",
    "First q will be produced using variational free energy while second q will be used to evaluate first q using expected free energy\n",
    "\n",
    "all of the neuronal agents will be rewarded based entirely on what happens in the inverse cartpole\n",
    "\n",
    "reward will be if the cartpole remains upright\n",
    "\n",
    "loss in qmix loss will deal with variational and expected free energy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e6d5b99",
   "metadata": {},
   "source": [
    "In the presentation we are going to characterize the brain as a natural experimenter\n",
    "\n",
    "Expected free energy is an extension that allows for planning. it essentially allows for what if and counterfactual simulations\n",
    "\n",
    "We may need a different model for each type of q in double q learning. the main q will be produced with variational free energy and the evaluation q will be produced by expected free energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97037571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pmodel(design):\n",
    "\n",
    "    # This line allows batching of designs, treating all batch dimensions as independent\n",
    "    with pyro.plate_stack(\"plate_stack\", design.shape):\n",
    "\n",
    "        # We use a dirchlet prior for theta\n",
    "        theta = pyro.sample(\"theta\", dist.Normal(torch.tensor(0.0), torch.tensor(1.0)))\n",
    "        #theta = pyro.sample(\"theta\", dist.Dirichlet())\n",
    "        # We use a simple logistic regression model for the likelihood\n",
    "        logit_p = theta - design\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p))\n",
    "\n",
    "        return y\n",
    "\n",
    "eig = nmc_eig(pmodel, design, observation_labels=[\"y\"], target_labels=[\"theta\"], N=2500, M=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "def pmodel(polling_allocation):\n",
    "    # This allows us to run many copies of the model in parallel\n",
    "    with pyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "        # Begin by sampling alpha\n",
    "        alpha = pyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "            prior_mean, covariance_matrix=prior_covariance))\n",
    "\n",
    "        # Sample y conditional on alpha\n",
    "        poll_results = pyro.sample(\"y\", dist.Binomial(\n",
    "            polling_allocation, logits=alpha).to_event(1))\n",
    "\n",
    "        # Now compute w according to the (approximate) electoral college formula\n",
    "        dem_win = election_winner(alpha)\n",
    "        pyro.sample(\"w\", dist.Delta(dem_win))\n",
    "\n",
    "        return poll_results, dem_win, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e750daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class OutcomePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_entropy = dist.Bernoulli(prior_w_prob).entropy()\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "poll_in_florida = torch.zeros(51)\n",
    "poll_in_florida[9] = 1000\n",
    "\n",
    "poll_in_dc = torch.zeros(51)\n",
    "poll_in_dc[8] = 1000\n",
    "\n",
    "uniform_poll = (1000 // 51) * torch.ones(51)\n",
    "\n",
    "# The swing score measures how close the state is to 50/50\n",
    "swing_score = 1. / (.5 - torch.tensor(prior_prob_dem.sort_values(\"State\").values).squeeze()).abs()\n",
    "swing_poll = 1000 * swing_score / swing_score.sum()\n",
    "swing_poll = swing_poll.round()\n",
    "\n",
    "poll_strategies = OrderedDict([(\"Florida\", poll_in_florida),\n",
    "                               (\"DC\", poll_in_dc),\n",
    "                               (\"Uniform\", uniform_poll),\n",
    "                               (\"Swing\", swing_poll)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be079b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3659c0a9",
   "metadata": {},
   "source": [
    "For expected free energy the prior entropy will come from a dirchlet distribution\n",
    "\n",
    "The \"guide\" will be a ML substitute for marginal density p(y|d). In the example election case the ml substitute binary classifier because of the w binary variable. \n",
    "\n",
    "in the example w is the target labels y is the observation labels and allocation is the design\n",
    "\n",
    "For project pegasus we will use a multi-label classifier with a dirchlet prior plugged into a posterior_eig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "\n",
    "eigs = {}\n",
    "best_strategy, best_eig = None, 0\n",
    "\n",
    "for strategy, allocation in poll_strategies.items():\n",
    "    print(strategy, end=\" \")\n",
    "    guide = OutcomePredictor()\n",
    "    pyro.clear_param_store()\n",
    "    # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "    # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "    # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "    ape = posterior_eig(pmodel, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "    eigs[strategy] = prior_entropy - ape\n",
    "    print(eigs[strategy].item())\n",
    "    if eigs[strategy] > best_eig:\n",
    "        best_strategy, best_eig = strategy, eigs[strategy]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8cf5f58",
   "metadata": {},
   "source": [
    "work on in 3/22/2023\n",
    "\n",
    "develop emodels into expected free energy and work on integration of variational free energy in the qmix loss module \n",
    "\n",
    "also work on visualization module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12369402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.504077, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#expected free energy\n",
    "\n",
    "class emodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        \"\"\"\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        #q = self.fc2(h)\n",
    "        vae = tfk.Model(inputs=input_dict[\"obs_flat\"],\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4c953be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "#@ti.func\n",
    "class bmodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        \"\"\"\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        #q = self.fc2(h)\n",
    "        vae = tfk.Model(inputs=input_dict[\"obs_flat\"],\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "479a3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "#guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "#elbo = elbo_(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4349a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "#@ti.func\n",
    "class QMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape, mixing_embed_dim):\n",
    "        super(QMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.embed_dim = mixing_embed_dim\n",
    "        self.state_dim = int(np.prod(state_shape))\n",
    "\n",
    "        self.hyper_w_1 = nn.Linear(self.state_dim, self.embed_dim * self.n_agents)\n",
    "        self.hyper_w_final = nn.Linear(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # State dependent bias for hidden layer\n",
    "        self.hyper_b_1 = nn.Linear(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # V(s) instead of a bias for the last layers\n",
    "        self.V = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embed_dim, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, agent_qs, states):\n",
    "        \"\"\"Forward pass for the mixer.\n",
    "        Args:\n",
    "            agent_qs: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            states: Tensor of shape [B, T, state_dim]\n",
    "        \"\"\"\n",
    "        bs = agent_qs.size(0)\n",
    "        states = states.reshape(-1, self.state_dim)\n",
    "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
    "        # First layer\n",
    "        w1 = torch.abs(self.hyper_w_1(states))\n",
    "        b1 = self.hyper_b_1(states)\n",
    "        w1 = w1.view(-1, self.n_agents, self.embed_dim)\n",
    "        b1 = b1.view(-1, 1, self.embed_dim)\n",
    "        hidden = nn.functional.elu(torch.bmm(agent_qs, w1) + b1)\n",
    "        # Second layer\n",
    "        w_final = torch.abs(self.hyper_w_final(states))\n",
    "        w_final = w_final.view(-1, self.embed_dim, 1)\n",
    "        # State-dependent bias\n",
    "        v = self.V(states).view(-1, 1, 1)\n",
    "        # Compute final output\n",
    "        y = torch.bmm(hidden, w_final) + v\n",
    "        # Reshape and return\n",
    "        q_tot = y.view(bs, -1, 1)\n",
    "        return q_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "278fda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ti.func\n",
    "class QMixLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        target_model,\n",
    "        mixer,\n",
    "        target_mixer,\n",
    "        n_agents,\n",
    "        n_actions,\n",
    "        double_q=True,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.mixer = mixer\n",
    "        self.target_mixer = target_mixer\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        self.double_q = double_q\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rewards,\n",
    "        actions,\n",
    "        terminated,\n",
    "        mask,\n",
    "        obs,\n",
    "        next_obs,\n",
    "        action_mask,\n",
    "        next_action_mask,\n",
    "        state=None,\n",
    "        next_state=None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the loss.\n",
    "        Args:\n",
    "            rewards: Tensor of shape [B, T, n_agents]\n",
    "            actions: Tensor of shape [B, T, n_agents]\n",
    "            terminated: Tensor of shape [B, T, n_agents]\n",
    "            mask: Tensor of shape [B, T, n_agents]\n",
    "            obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            next_obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            next_action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            state: Tensor of shape [B, T, state_dim] (optional)\n",
    "            next_state: Tensor of shape [B, T, state_dim] (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert either none or both of state and next_state are given\n",
    "        if state is None and next_state is None:\n",
    "            state = obs  # default to state being all agents' observations\n",
    "            next_state = next_obs\n",
    "        elif (state is None) != (next_state is None):\n",
    "            raise ValueError(\n",
    "                \"Expected either neither or both of `state` and \"\n",
    "                \"`next_state` to be given. Got: \"\n",
    "                \"\\n`state` = {}\\n`next_state` = {}\".format(state, next_state)\n",
    "            )\n",
    "\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = _unroll_mac(self.model, obs)\n",
    "\n",
    "        # Pick the Q-Values for the actions taken -> [B * n_agents, T]\n",
    "        chosen_action_qvals = torch.gather(\n",
    "            mac_out, dim=3, index=actions.unsqueeze(3)\n",
    "        ).squeeze(3)\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = _unroll_mac(self.target_model, next_obs)\n",
    "\n",
    "        # Mask out unavailable actions for the t+1 step\n",
    "        ignore_action_tp1 = (next_action_mask == 0) & (mask == 1).unsqueeze(-1)\n",
    "        target_mac_out[ignore_action_tp1] = -np.inf\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.double_q:\n",
    "            # Double Q learning computes the target Q values by selecting the\n",
    "            # t+1 timestep action according to the \"policy\" neural network and\n",
    "            # then estimating the Q-value of that action with the \"target\"\n",
    "            # neural network\n",
    "            \n",
    "            #target neural network does expected free energy while policy\n",
    "            #neural network will be variational free energy\n",
    "\n",
    "            # Compute the t+1 Q-values to be used in action selection\n",
    "            # using next_obs\n",
    "            mac_out_tp1 = _unroll_mac(self.model, next_obs)\n",
    "\n",
    "            # mask out unallowed actions\n",
    "            mac_out_tp1[ignore_action_tp1] = -np.inf\n",
    "\n",
    "            # obtain best actions at t+1 according to policy NN\n",
    "            cur_max_actions = mac_out_tp1.argmax(dim=3, keepdim=True)\n",
    "\n",
    "            # use the target network to estimate the Q-values of policy\n",
    "            # network's selected actions\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(\n",
    "                3\n",
    "            )\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "        assert (\n",
    "            target_max_qvals.min().item() != -np.inf\n",
    "        ), \"target_max_qvals contains a masked action; \\\n",
    "            there may be a state with no valid actions.\"\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state)\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, next_state)\n",
    "\n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n",
    "\n",
    "        # Td-error\n",
    "        #we need to replace this with a variational free energy error\n",
    "        td_error = chosen_action_qvals - targets.detach()\n",
    "\n",
    "        mask = mask.expand_as(td_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = td_error * mask\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        \"\"\"\n",
    "        guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        elbo_ = pyro.infer.Trace_ELBO(num_particles=1)\n",
    "\n",
    "        # Fix the model/guide pair\n",
    "        elbo = elbo_(model, guide)        \n",
    "        \"\"\"\n",
    "        #data = obs + preds \n",
    "        #loss = -elbo(data) + kl_loss(obs,preds)\n",
    "        loss = (masked_td_error**2).sum() / mask.sum()\n",
    "        return loss, mask, masked_td_error, chosen_action_qvals, targets\n",
    "\n",
    "    \n",
    "#this part just above is what we need to revise\n",
    "    \n",
    "    \n",
    "#@ti.func\n",
    "class QMixTorchPolicy(TorchPolicy):\n",
    "    \"\"\"QMix impl. Assumes homogeneous agents for now.\n",
    "    You must use MultiAgentEnv.with_agent_groups() to group agents\n",
    "    together for QMix. This creates the proper Tuple obs/action spaces and\n",
    "    populates the '_group_rewards' info field.\n",
    "    Action masking: to specify an action mask for individual agents, use a\n",
    "    dict space with an action_mask key, e.g. {\"obs\": ob, \"action_mask\": mask}.\n",
    "    The mask space must be `Box(0, 1, (n_actions,))`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        # We want to error out on instantiation and not on import, because tune\n",
    "        # imports all RLlib algorithms when registering them\n",
    "        # TODO (Artur): Find a way to only import algorithms when needed\n",
    "        if not torch:\n",
    "            raise ImportError(\"Could not import PyTorch, which QMix requires.\")\n",
    "\n",
    "        _validate(obs_space, action_space)\n",
    "        config = dict(ray.rllib.algorithms.qmix.qmix.DEFAULT_CONFIG, **config)\n",
    "        self.framework = \"torch\"\n",
    "\n",
    "        self.n_agents = len(obs_space.original_space.spaces)\n",
    "        config[\"model\"][\"n_agents\"] = self.n_agents\n",
    "        self.n_actions = action_space.spaces[0].n\n",
    "        self.h_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "        self.has_env_global_state = False\n",
    "        self.has_action_mask = False\n",
    "\n",
    "        agent_obs_space = obs_space.original_space.spaces[0]\n",
    "        if isinstance(agent_obs_space, gym.spaces.Dict):\n",
    "            space_keys = set(agent_obs_space.spaces.keys())\n",
    "            if \"obs\" not in space_keys:\n",
    "                raise ValueError(\"Dict obs space must have subspace labeled `obs`\")\n",
    "            self.obs_size = _get_size(agent_obs_space.spaces[\"obs\"])\n",
    "            if \"action_mask\" in space_keys:\n",
    "                mask_shape = tuple(agent_obs_space.spaces[\"action_mask\"].shape)\n",
    "                if mask_shape != (self.n_actions,):\n",
    "                    raise ValueError(\n",
    "                        \"Action mask shape must be {}, got {}\".format(\n",
    "                            (self.n_actions,), mask_shape\n",
    "                        )\n",
    "                    )\n",
    "                self.has_action_mask = True\n",
    "            if ENV_STATE in space_keys:\n",
    "                self.env_global_state_shape = _get_size(\n",
    "                    agent_obs_space.spaces[ENV_STATE]\n",
    "                )\n",
    "                self.has_env_global_state = True\n",
    "            else:\n",
    "                self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            # The real agent obs space is nested inside the dict\n",
    "            config[\"model\"][\"full_obs_space\"] = agent_obs_space\n",
    "            agent_obs_space = agent_obs_space.spaces[\"obs\"]\n",
    "        else:\n",
    "            self.obs_size = _get_size(agent_obs_space)\n",
    "            self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "        #model = bmodel()#CModel()#CModel()\n",
    "        #bmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#\n",
    "        ivy.set_framework('torch')\n",
    "        #model = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')\n",
    "        bmodel = bmodel()\n",
    "        bmodel = FullLaplace(bmodel,'regression',prior_precision=2)#0.0000000000000000000001)\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"model\",\n",
    "            default_model=bmodel#a#RNNModel#bmodel()#RNNModel,\n",
    "        )\n",
    "\n",
    "        super().__init__(obs_space, action_space, config, model=self.model)\n",
    "\n",
    "        self.target_model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"target_model\",\n",
    "            default_model=bmodel#bmodel()#RNNModel\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.exploration = self._create_exploration()\n",
    "\n",
    "        # Setup the mixer network.\n",
    "        if config[\"mixer\"] is None:\n",
    "            self.mixer = None\n",
    "            self.target_mixer = None\n",
    "        elif config[\"mixer\"] == \"qmix\":\n",
    "            self.mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "            self.target_mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.cur_epsilon = 1.0\n",
    "        self.update_target()  # initial sync\n",
    "\n",
    "        # Setup optimizer\n",
    "        self.params = list(self.model.parameters())\n",
    "        if self.mixer:\n",
    "            self.params += list(self.mixer.parameters())\n",
    "        self.loss = QMixLoss(\n",
    "            self.model,\n",
    "            self.target_model,\n",
    "            self.mixer,\n",
    "            self.target_mixer,\n",
    "            self.n_agents,\n",
    "            self.n_actions,\n",
    "            self.config[\"double_q\"],\n",
    "            self.config[\"gamma\"],\n",
    "        )\n",
    "        from torch.optim import RMSprop\n",
    "\n",
    "        self.rmsprop_optimizer = RMSprop(\n",
    "            params=self.params,\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"optim_alpha\"],\n",
    "            eps=config[\"optim_eps\"],\n",
    "        )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions_from_input_dict(\n",
    "        self,\n",
    "        input_dict: Dict[str, TensorType],\n",
    "        explore: bool = None,\n",
    "        timestep: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:\n",
    "\n",
    "        obs_batch = input_dict[SampleBatch.OBS]\n",
    "        state_batches = []\n",
    "        i = 0\n",
    "        while f\"state_in_{i}\" in input_dict:\n",
    "            state_batches.append(input_dict[f\"state_in_{i}\"])\n",
    "            i += 1\n",
    "\n",
    "        explore = explore if explore is not None else self.config[\"explore\"]\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        # We need to ensure we do not use the env global state\n",
    "        # to compute actions\n",
    "\n",
    "        # Compute actions\n",
    "        with torch.no_grad():\n",
    "            q_values, hiddens = _mac(\n",
    "                self.model,\n",
    "                torch.as_tensor(obs_batch, dtype=torch.float, device=self.device),\n",
    "                [\n",
    "                    torch.as_tensor(np.array(s), dtype=torch.float, device=self.device)\n",
    "                    for s in state_batches\n",
    "                ],\n",
    "            )\n",
    "            avail = torch.as_tensor(action_mask, dtype=torch.float, device=self.device)\n",
    "            masked_q_values = q_values.clone()\n",
    "            masked_q_values[avail == 0.0] = -float(\"inf\")\n",
    "            masked_q_values_folded = torch.reshape(\n",
    "                masked_q_values, [-1] + list(masked_q_values.shape)[2:]\n",
    "            )\n",
    "            actions, _ = self.exploration.get_exploration_action(\n",
    "                action_distribution=TorchCategorical(masked_q_values_folded),\n",
    "                timestep=timestep,\n",
    "                explore=explore,\n",
    "            )\n",
    "            actions = (\n",
    "                torch.reshape(actions, list(masked_q_values.shape)[:-1]).cpu().numpy()\n",
    "            )\n",
    "            hiddens = [s.cpu().numpy() for s in hiddens]\n",
    "\n",
    "        return tuple(actions.transpose([1, 0])), hiddens, {}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions(self, *args, **kwargs):\n",
    "        return self.compute_actions_from_input_dict(*args, **kwargs)\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "    ):\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        return np.zeros(obs_batch.size()[0])\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        obs_batch, action_mask, env_global_state = self._unpack_observation(\n",
    "            samples[SampleBatch.CUR_OBS]\n",
    "        )\n",
    "        (\n",
    "            next_obs_batch,\n",
    "            next_action_mask,\n",
    "            next_env_global_state,\n",
    "        ) = self._unpack_observation(samples[SampleBatch.NEXT_OBS])\n",
    "        group_rewards = self._get_group_rewards(samples[SampleBatch.INFOS])\n",
    "\n",
    "        input_list = [\n",
    "            group_rewards,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            samples[SampleBatch.ACTIONS],\n",
    "            samples[SampleBatch.TERMINATEDS],\n",
    "            obs_batch,\n",
    "            next_obs_batch,\n",
    "        ]\n",
    "        if self.has_env_global_state:\n",
    "            input_list.extend([env_global_state, next_env_global_state])\n",
    "\n",
    "        output_list, _, seq_lens = chop_into_sequences(\n",
    "            episode_ids=samples[SampleBatch.EPS_ID],\n",
    "            unroll_ids=samples[SampleBatch.UNROLL_ID],\n",
    "            agent_indices=samples[SampleBatch.AGENT_INDEX],\n",
    "            feature_columns=input_list,\n",
    "            state_columns=[],  # RNN states not used here\n",
    "            max_seq_len=self.config[\"model\"][\"max_seq_len\"],\n",
    "            dynamic_max=True,\n",
    "        )\n",
    "        # These will be padded to shape [B * T, ...]\n",
    "        if self.has_env_global_state:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "                env_global_state,\n",
    "                next_env_global_state,\n",
    "            ) = output_list\n",
    "        else:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "            ) = output_list\n",
    "        B, T = len(seq_lens), max(seq_lens)\n",
    "\n",
    "        def to_batches(arr, dtype):\n",
    "            new_shape = [B, T] + list(arr.shape[1:])\n",
    "            return torch.as_tensor(\n",
    "                np.reshape(arr, new_shape), dtype=dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        rewards = to_batches(rew, torch.float)\n",
    "        actions = to_batches(act, torch.long)\n",
    "        obs = to_batches(obs, torch.float).reshape([B, T, self.n_agents, self.obs_size])\n",
    "        action_mask = to_batches(action_mask, torch.float)\n",
    "        next_obs = to_batches(next_obs, torch.float).reshape(\n",
    "            [B, T, self.n_agents, self.obs_size]\n",
    "        )\n",
    "        next_action_mask = to_batches(next_action_mask, torch.float)\n",
    "        if self.has_env_global_state:\n",
    "            env_global_state = to_batches(env_global_state, torch.float)\n",
    "            next_env_global_state = to_batches(next_env_global_state, torch.float)\n",
    "\n",
    "        # TODO(ekl) this treats group termination as individual termination\n",
    "        terminated = (\n",
    "            to_batches(terminateds, torch.float)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Create mask for where index is < unpadded sequence length\n",
    "        filled = np.reshape(\n",
    "            np.tile(np.arange(T, dtype=np.float32), B), [B, T]\n",
    "        ) < np.expand_dims(seq_lens, 1)\n",
    "        mask = (\n",
    "            torch.as_tensor(filled, dtype=torch.float, device=self.device)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss_out, mask, masked_td_error, chosen_action_qvals, targets = self.loss(\n",
    "            rewards,\n",
    "            actions,\n",
    "            terminated,\n",
    "            mask,\n",
    "            obs,\n",
    "            next_obs,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            env_global_state,\n",
    "            next_env_global_state,\n",
    "        )\n",
    "\n",
    "        # Optimise\n",
    "        self.rmsprop_optimizer.zero_grad()\n",
    "        loss_out.backward()\n",
    "        grad_norm_info = apply_grad_clipping(self, self.rmsprop_optimizer, loss_out)\n",
    "        self.rmsprop_optimizer.step()\n",
    "\n",
    "        mask_elems = mask.sum().item()\n",
    "        stats = {\n",
    "            \"loss\": loss_out.item(),\n",
    "            \"td_error_abs\": masked_td_error.abs().sum().item() / mask_elems,\n",
    "            \"q_taken_mean\": (chosen_action_qvals * mask).sum().item() / mask_elems,\n",
    "            \"target_mean\": (targets * mask).sum().item() / mask_elems,\n",
    "        }\n",
    "        stats.update(grad_norm_info)\n",
    "\n",
    "        return {LEARNER_STATS_KEY: stats}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_initial_state(self):  # initial RNN state\n",
    "        return [\n",
    "            s.expand([self.n_agents, -1]).cpu().numpy()\n",
    "            for s in self.model.get_initial_state()\n",
    "        ]\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            \"model\": self._cpu_dict(self.model.state_dict()),\n",
    "            \"target_model\": self._cpu_dict(self.target_model.state_dict()),\n",
    "            \"mixer\": self._cpu_dict(self.mixer.state_dict()) if self.mixer else None,\n",
    "            \"target_mixer\": self._cpu_dict(self.target_mixer.state_dict())\n",
    "            if self.mixer\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(self._device_dict(weights[\"model\"]))\n",
    "        self.target_model.load_state_dict(self._device_dict(weights[\"target_model\"]))\n",
    "        if weights[\"mixer\"] is not None:\n",
    "            self.mixer.load_state_dict(self._device_dict(weights[\"mixer\"]))\n",
    "            self.target_mixer.load_state_dict(\n",
    "                self._device_dict(weights[\"target_mixer\"])\n",
    "            )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_state(self):\n",
    "        state = self.get_weights()\n",
    "        state[\"cur_epsilon\"] = self.cur_epsilon\n",
    "        return state\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_state(self, state):\n",
    "        self.set_weights(state)\n",
    "        self.set_epsilon(state[\"cur_epsilon\"])\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        logger.debug(\"Updated target networks\")\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cur_epsilon = epsilon\n",
    "\n",
    "    def _get_group_rewards(self, info_batch):\n",
    "        group_rewards = np.array(\n",
    "            [info.get(GROUP_REWARDS, [0.0] * self.n_agents) for info in info_batch]\n",
    "        )\n",
    "        return group_rewards\n",
    "\n",
    "    def _device_dict(self, state_dict):\n",
    "        return {\n",
    "            k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _cpu_dict(state_dict):\n",
    "        return {k: v.cpu().detach().numpy() for k, v in state_dict.items()}\n",
    "\n",
    "    def _unpack_observation(self, obs_batch):\n",
    "        \"\"\"Unpacks the observation, action mask, and state (if present)\n",
    "        from agent grouping.\n",
    "        Returns:\n",
    "            obs (np.ndarray): obs tensor of shape [B, n_agents, obs_size]\n",
    "            mask (np.ndarray): action mask, if any\n",
    "            state (np.ndarray or None): state tensor of shape [B, state_size]\n",
    "                or None if it is not in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        unpacked = _unpack_obs(\n",
    "            np.array(obs_batch, dtype=np.float32),\n",
    "            self.observation_space.original_space,\n",
    "            tensorlib=np,\n",
    "        )\n",
    "\n",
    "        if isinstance(unpacked[0], dict):\n",
    "            assert \"obs\" in unpacked[0]\n",
    "            unpacked_obs = [np.concatenate(tree.flatten(u[\"obs\"]), 1) for u in unpacked]\n",
    "        else:\n",
    "            unpacked_obs = unpacked\n",
    "\n",
    "        obs = np.concatenate(unpacked_obs, axis=1).reshape(\n",
    "            [len(obs_batch), self.n_agents, self.obs_size]\n",
    "        )\n",
    "\n",
    "        if self.has_action_mask:\n",
    "            action_mask = np.concatenate(\n",
    "                [o[\"action_mask\"] for o in unpacked], axis=1\n",
    "            ).reshape([len(obs_batch), self.n_agents, self.n_actions])\n",
    "        else:\n",
    "            action_mask = np.ones(\n",
    "                [len(obs_batch), self.n_agents, self.n_actions], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.has_env_global_state:\n",
    "            state = np.concatenate(tree.flatten(unpacked[0][ENV_STATE]), 1)\n",
    "        else:\n",
    "            state = None\n",
    "        return obs, action_mask, state\n",
    "\n",
    "#@ti.func\n",
    "def _validate(obs_space, action_space):\n",
    "    if not hasattr(obs_space, \"original_space\") or not isinstance(\n",
    "        obs_space.original_space, gym.spaces.Tuple\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Obs space must be a Tuple, got {}. Use \".format(obs_space)\n",
    "            + \"MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space, gym.spaces.Tuple):\n",
    "        raise ValueError(\n",
    "            \"Action space must be a Tuple, got {}. \".format(action_space)\n",
    "            + \"Use MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space.spaces[0], gym.spaces.Discrete):\n",
    "        raise ValueError(\n",
    "            \"QMix requires a discrete action space, got {}\".format(\n",
    "                action_space.spaces[0]\n",
    "            )\n",
    "        )\n",
    "    if len({str(x) for x in obs_space.original_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: observations of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(obs_space.original_space.spaces)\n",
    "        )\n",
    "    if len({str(x) for x in action_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: action space of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(action_space.spaces)\n",
    "        )\n",
    "\n",
    "#@ti.func\n",
    "def _mac(model, obs, h):\n",
    "    \"\"\"Forward pass of the multi-agent controller.\n",
    "    Args:\n",
    "        model: TorchModelV2 class\n",
    "        obs: Tensor of shape [B, n_agents, obs_size]\n",
    "        h: List of tensors of shape [B, n_agents, h_size]\n",
    "    Returns:\n",
    "        q_vals: Tensor of shape [B, n_agents, n_actions]\n",
    "        h: Tensor of shape [B, n_agents, h_size]\n",
    "    \"\"\"\n",
    "    B, n_agents = obs.size(0), obs.size(1)\n",
    "    if not isinstance(obs, dict):\n",
    "        obs = {\"obs\": obs}\n",
    "    obs_agents_as_batches = {k: _drop_agent_dim(v) for k, v in obs.items()}\n",
    "    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n",
    "    q_flat, h_flat = model(obs_agents_as_batches, h_flat, None)\n",
    "    return q_flat.reshape([B, n_agents, -1]), [\n",
    "        s.reshape([B, n_agents, -1]) for s in h_flat\n",
    "    ]\n",
    "#@ti.func\n",
    "def _unroll_mac(model, obs_tensor):\n",
    "    \"\"\"Computes the estimated Q values for an entire trajectory batch\"\"\"\n",
    "    B = obs_tensor.size(0)\n",
    "    T = obs_tensor.size(1)\n",
    "    n_agents = obs_tensor.size(2)\n",
    "\n",
    "    mac_out = []\n",
    "    h = [s.expand([B, n_agents, -1]) for s in model.get_initial_state()]\n",
    "    for t in range(T):\n",
    "        q, h = _mac(model, obs_tensor[:, t], h)\n",
    "        mac_out.append(q)\n",
    "    mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "    return mac_out\n",
    "#@ti.func\n",
    "def _drop_agent_dim(T):\n",
    "    shape = list(T.shape)\n",
    "    B, n_agents = shape[0], shape[1]\n",
    "    return T.reshape([B * n_agents] + shape[2:])\n",
    "#@ti.func\n",
    "def _add_agent_dim(T, n_agents):\n",
    "    shape = list(T.shape)\n",
    "    B = shape[0] // n_agents\n",
    "    assert shape[0] % n_agents == 0\n",
    "    return T.reshape([B, n_agents] + shape[1:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c2af3e9",
   "metadata": {},
   "source": [
    "psuedo code for assigning prior\n",
    "\n",
    "if agent_id.startswith(\"high_level_\"):\n",
    "    assign non informative prior\n",
    "else:\n",
    "    prior = posterior\n",
    "\n",
    "\n",
    "our 3 kinds of agents will be \n",
    "\n",
    "excitatory, inhibitory and motor\n",
    "\n",
    "two kinds of priors: priors about states or precisions or structural priors about generative model\n",
    "\n",
    "policy priors depend on expected free energy which itself depends on posterior beliefs, the potential for information gain and prior preferences\n",
    "\n",
    "priors over policies can also be equipped with a fixed form term representing habitual biases \n",
    "\n",
    "we are going to have a global reward which is going to be it not falling over\n",
    "\n",
    "we are then going to have local rewards for each of the policies on each level. the reward at the higher levels will be less specified less granular conditions for recieving a reward  \n",
    "\n",
    "things left to do\n",
    "\n",
    "replace neuronal agent model prior with dirchlet prior\n",
    "\n",
    "integrate expeced free energy and variational free energy into loss and \n",
    "use gaussain process regression in conjunction with expected free energy\n",
    "\n",
    "create an expected free energy and integrate the VFE network into the evaluation and selection Q values respectively\n",
    "\n",
    "Pending a positive results from tasha put the neurons in random spaces on each level during rendering\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3384f248",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b132d855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass TwoStepGameWithGroupedAgents(MultiAgentEnv):\\n    def __init__(self, env_config):\\n        super().__init__()\\n        env = TwoStepGame(env_config)\\n        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\\n        tuple_act_space = Tuple([env.action_space, env.action_space])\\n\\n        self.env = env.with_agent_groups(\\n            groups={\"agents\": [0, 1]},\\n            obs_space=tuple_obs_space,\\n            act_space=tuple_act_space,\\n        )\\n        self.observation_space = self.env.observation_space\\n        self.action_space = self.env.action_space\\n        self._agent_ids = {\"agents\"}\\n\\n    def reset(self, *, seed=None, options=None):\\n        return self.env.reset(seed=seed, options=options)\\n\\n    def step(self, actions):\\n        return self.env.step(actions)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@ti.func\n",
    "#@ti.kernel\n",
    "#@ti.data_oriented\n",
    "class TwoStepGame(MultiAgentEnv):\n",
    "    action_space = Discrete(2)\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(2)\n",
    "        self.state = None\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        #self.agent_3=2\n",
    "        self._skip_env_checking = True\n",
    "        # MADDPG emits action logits instead of actual discrete actions\n",
    "        self.actions_are_logits = env_config.get(\"actions_are_logits\", False)\n",
    "        self.one_hot_state_encoding = env_config.get(\"one_hot_state_encoding\", False)\n",
    "        self.with_state = env_config.get(\"separate_state_space\", False)\n",
    "        self._agent_ids = {0, 1}\n",
    "        if not self.one_hot_state_encoding:\n",
    "            self.observation_space = Discrete(6)\n",
    "            self.with_state = False\n",
    "        else:\n",
    "            # Each agent gets the full state (one-hot encoding of which of the\n",
    "            # three states are active) as input with the receiving agent's\n",
    "            # ID (1 or 2) concatenated onto the end.\n",
    "            if self.with_state:\n",
    "                self.observation_space = Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.observation_space = MultiDiscrete([2, 2, 2, 3])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = np.array([1, 0, 0])\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "\n",
    "        state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "            \n",
    "        \"\"\"\n",
    "        guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        elbo = elbo_(model, guide)\n",
    "        \n",
    "        el = elbo(data)\n",
    "        \n",
    "        \n",
    "        #rewards = {self.agent_1:kullbacker, self.agent_2: el}\n",
    "        #loss = lambda y, rv_y: rv_y.variational_loss(y, kl_weight=np.array(batch_size, x.dtype) / x.shape[0])\n",
    "        #kull = tf.keras.losses.KLDivergence()        \n",
    "        \n",
    "        local reward elbo is going to be assigned to agent 3 \n",
    "        \n",
    "        there is going to be a high policy, medium policy and low policy\n",
    "        \n",
    "        high is going to have agent 1, medium agent 1,2 and low agent 3\n",
    "        \n",
    "        agent 3 is only going to differ from agent 1 in having a continuous instead of discrete action space\n",
    "        \n",
    "        top level prior is going to be expected free energy\n",
    "        \"\"\"\n",
    "\n",
    "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
    "        obs = self._obs()\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        truncateds = {\"__all__\": False}\n",
    "        infos = {\n",
    "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
    "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "        }\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "\n",
    "    def _obs(self):\n",
    "        if self.with_state:\n",
    "            return {\n",
    "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
    "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
    "            }\n",
    "        else:\n",
    "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
    "\n",
    "    def agent_1_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [1]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0]\n",
    "\n",
    "    def agent_2_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [2]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0] + 3\n",
    "    def render():\n",
    "        X = np.random.random((5, 2))\n",
    "        Y = np.random.random((5, 2))\n",
    "        gui = ti.GUI(\"lines\", res=(400, 400))\n",
    "        while gui.running:\n",
    "            gui.lines(begin=X, end=Y, radius=2, color=0x068587)\n",
    "            gui.show()\n",
    "        #if self.render_mode == \"rgb_array\":\n",
    "            #return self._render_frame()\n",
    "\n",
    "\"\"\"\n",
    "class TwoStepGameWithGroupedAgents(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        env = TwoStepGame(env_config)\n",
    "        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\n",
    "        tuple_act_space = Tuple([env.action_space, env.action_space])\n",
    "\n",
    "        self.env = env.with_agent_groups(\n",
    "            groups={\"agents\": [0, 1]},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self._agent_ids = {\"agents\"}\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        return self.env.reset(seed=seed, options=options)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('InvertedPendulum-v4')\n",
    "\n",
    "#act of -3,3\n",
    "#obs of -inf, inf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d17fe3a8",
   "metadata": {},
   "source": [
    "psuedo code for rendering \n",
    "\n",
    "for i in num_agents:\n",
    "pos = np.randint(num_agents)\n",
    "blueline = action space\n",
    "redline = obs space\n",
    "pinkline  = obs inhibitory space\n",
    "greenline = action inhibitory space \n",
    "\n",
    "brain has 6 layes of neurons \n",
    "\n",
    "cortical column is a vertical slice of the brain that encapsulates the entire hierarchy of the brain\n",
    "\n",
    "connections entering and leaving cortical column relate to likelihood distribtuions whereas transition probabilities and continuous dynamics depend on connections withen a microcircuit\n",
    "\n",
    "Cortical columns are proposed to be the canonical microcircuits for predictive coding\n",
    "\n",
    "we should integrate expected free energy into the exploratory config \n",
    "\n",
    "double q learning uses 1q to select and another q to evaluate an action\n",
    "\n",
    "latest opinion\n",
    "\n",
    "expected free energy selects data that will best optimize beliefs and variational free energy optimizes beliefs in relation to data that has already been gathered \n",
    "\n",
    "expected free energy is the policy selection and variational free energy should be the evaluation policy evaluation q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our custom replacement for softq in explore config\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "\n",
    "@PublicAPI\n",
    "class SoftQ(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a SoftQ Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9545e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig, NotProvided\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    "    SAMPLE_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "#@ti.kernel\n",
    "class QMixConfig(SimpleQConfig):\n",
    "    \"\"\"Defines a configuration class from which QMix can be built.\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> config = QMixConfig()  # doctest: +SKIP\n",
    "        >>> config = config.training(gamma=0.9, lr=0.01, kl_coeff=0.3)  # doctest: +SKIP\n",
    "        >>> config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "        >>> config = config.rollouts(num_rollout_workers=4)  # doctest: +SKIP\n",
    "        >>> print(config.to_dict())  # doctest: +SKIP\n",
    "        >>> # Build an Algorithm object from the config and run 1 training iteration.\n",
    "        >>> algo = config.build(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> algo.train()  # doctest: +SKIP\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> from ray import air\n",
    "        >>> from ray import tune\n",
    "        >>> config = QMixConfig()\n",
    "        >>> # Print out some default values.\n",
    "        >>> print(config.optim_alpha)  # doctest: +SKIP\n",
    "        >>> # Update the config object.\n",
    "        >>> config.training(  # doctest: +SKIP\n",
    "        ...     lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97\n",
    "        ... )\n",
    "        >>> # Set the config object's env.\n",
    "        >>> config.environment(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> # Use to_dict() to get the old-style python config dict\n",
    "        >>> # when running with tune.\n",
    "        >>> tune.Tuner(  # doctest: +SKIP\n",
    "        ...     \"QMix\",\n",
    "        ...     run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "        ...     param_space=config.to_dict(),\n",
    "        ... ).fit()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PPOConfig instance.\"\"\"\n",
    "        super().__init__(algo_class=QMix)\n",
    "\n",
    "        # fmt: off\n",
    "        # __sphinx_doc_begin__\n",
    "        # QMix specific settings:\n",
    "        self.mixer = \"qmix\"\n",
    "        self.mixing_embed_dim = 32\n",
    "        self.double_q = True\n",
    "        self.optim_alpha = 0.99\n",
    "        self.optim_eps = 0.00001\n",
    "        self.grad_clip = 10\n",
    "        #self.render_mode = 'rgb_array'\n",
    "        # QMix-torch overrides the TorchPolicy's learn_on_batch w/o specifying a\n",
    "        # alternative `learn_on_loaded_batch` alternative for the GPU.\n",
    "        # TODO: This hack will be resolved once we move all algorithms to the new\n",
    "        #  RLModule/Learner APIs.\n",
    "        self.simple_optimizer = True\n",
    "\n",
    "        # Override some of AlgorithmConfig's default values with QMix-specific values.\n",
    "        # .training()\n",
    "        self.lr = 0.0005\n",
    "        self.train_batch_size = 32\n",
    "        self.target_network_update_freq = 500\n",
    "        self.num_steps_sampled_before_learning_starts = 1000\n",
    "        self.replay_buffer_config = {\n",
    "            \"type\": \"ReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "            \"prioritized_replay\": DEPRECATED_VALUE,\n",
    "            # Size of the replay buffer in batches (not timesteps!).\n",
    "            \"capacity\": 1000,\n",
    "            # Choosing `fragments` here makes it so that the buffer stores entire\n",
    "            # batches, instead of sequences, episodes or timesteps.\n",
    "            \"storage_unit\": \"fragments\",\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.model = {\n",
    "            \"lstm_cell_size\": 64,\n",
    "            \"max_seq_len\": 999999,\n",
    "        }\n",
    "        \"\"\"\n",
    "        # .framework()\n",
    "        self.framework_str = \"torch\"\n",
    "\n",
    "        # .rollouts()\n",
    "        self.rollout_fragment_length = 4\n",
    "        self.batch_mode = \"complete_episodes\"\n",
    "\n",
    "        # .reporting()\n",
    "        self.min_time_s_per_iteration = 1\n",
    "        self.min_sample_timesteps_per_iteration = 1000\n",
    "\n",
    "        # .exploration()\n",
    "        self.exploration_config = {\n",
    "            \"\"\"\n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\", #replace this with SoftQ\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.01,\n",
    "            # Timesteps over which to anneal epsilon.\n",
    "            \"epsilon_timesteps\": 40000,\n",
    "            \"\"\"\n",
    "            \"type\": \"SoftQ\",\n",
    "            \"temperature\": 1.0\n",
    "            # For soft_q, use:\n",
    "            # \"exploration_config\" = {\n",
    "            #   \"type\": \"SoftQ\"\n",
    "            #   \"temperature\": [float, e.g. 1.0]\n",
    "            # }\n",
    "        }\n",
    "\n",
    "        # .evaluation()\n",
    "        # Evaluate with epsilon=0 every `evaluation_interval` training iterations.\n",
    "        # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "        self.evaluation(\n",
    "            evaluation_config=AlgorithmConfig.overrides(explore=False)\n",
    "        )\n",
    "        # __sphinx_doc_end__\n",
    "        # fmt: on\n",
    "\n",
    "        self.worker_side_prioritization = DEPRECATED_VALUE\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        mixer: Optional[str] = NotProvided,\n",
    "        mixing_embed_dim: Optional[int] = NotProvided,\n",
    "        double_q: Optional[bool] = NotProvided,\n",
    "        target_network_update_freq: Optional[int] = NotProvided,\n",
    "        replay_buffer_config: Optional[dict] = NotProvided,\n",
    "        optim_alpha: Optional[float] = NotProvided,\n",
    "        optim_eps: Optional[float] = NotProvided,\n",
    "        grad_clip: Optional[float] = NotProvided,\n",
    "        # Deprecated args.\n",
    "        grad_norm_clipping=DEPRECATED_VALUE,\n",
    "        **kwargs,\n",
    "    ) -> \"QMixConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            mixer: Mixing network. Either \"qmix\", \"vdn\", or None.\n",
    "            mixing_embed_dim: Size of the mixing network embedding.\n",
    "            double_q: Whether to use Double_Q learning.\n",
    "            target_network_update_freq: Update the target network every\n",
    "                `target_network_update_freq` sample steps.\n",
    "            replay_buffer_config:\n",
    "            optim_alpha: RMSProp alpha.\n",
    "            optim_eps: RMSProp epsilon.\n",
    "            grad_clip: If not None, clip gradients during optimization at\n",
    "                this value.\n",
    "            grad_norm_clipping: Depcrecated in favor of grad_clip\n",
    "        Returns:\n",
    "            This updated AlgorithmConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if grad_norm_clipping != DEPRECATED_VALUE:\n",
    "            deprecation_warning(\n",
    "                old=\"grad_norm_clipping\",\n",
    "                new=\"grad_clip\",\n",
    "                help=\"Parameter `grad_norm_clipping` has been \"\n",
    "                \"deprecated in favor of grad_clip in QMix. \"\n",
    "                \"This is now the same parameter as in other \"\n",
    "                \"algorithms. `grad_clip` will be overwritten by \"\n",
    "                \"`grad_norm_clipping={}`\".format(grad_norm_clipping),\n",
    "                error=True,\n",
    "            )\n",
    "            grad_clip = grad_norm_clipping\n",
    "\n",
    "        if mixer is not NotProvided:\n",
    "            self.mixer = mixer\n",
    "        if mixing_embed_dim is not NotProvided:\n",
    "            self.mixing_embed_dim = mixing_embed_dim\n",
    "        if double_q is not NotProvided:\n",
    "            self.double_q = double_q\n",
    "        if target_network_update_freq is not NotProvided:\n",
    "            self.target_network_update_freq = target_network_update_freq\n",
    "        if replay_buffer_config is not NotProvided:\n",
    "            self.replay_buffer_config = replay_buffer_config\n",
    "        if optim_alpha is not NotProvided:\n",
    "            self.optim_alpha = optim_alpha\n",
    "        if optim_eps is not NotProvided:\n",
    "            self.optim_eps = optim_eps\n",
    "        if grad_clip is not NotProvided:\n",
    "            self.grad_clip = grad_clip\n",
    "\n",
    "        return self\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def validate(self) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate()\n",
    "\n",
    "        if self.framework_str != \"torch\":\n",
    "            raise ValueError(\n",
    "                \"Only `config.framework('torch')` supported so far for QMix!\"\n",
    "            )\n",
    "#@ti.kernel\n",
    "class QMix(SimpleQ):\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_config(cls) -> AlgorithmConfig:\n",
    "        return QMixConfig()\n",
    "\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_policy_class(\n",
    "        cls, config: AlgorithmConfig\n",
    "    ) -> Optional[Type[Policy]]:\n",
    "        return QMixTorchPolicy\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def training_step(self) -> ResultDict:\n",
    "        \"\"\"QMIX training iteration function.\n",
    "        - Sample n MultiAgentBatches from n workers synchronously.\n",
    "        - Store new samples in the replay buffer.\n",
    "        - Sample one training MultiAgentBatch from the replay buffer.\n",
    "        - Learn on the training batch.\n",
    "        - Update the target network every `target_network_update_freq` sample steps.\n",
    "        - Return all collected training metrics for the iteration.\n",
    "        Returns:\n",
    "            The results dict from executing the training iteration.\n",
    "        \"\"\"\n",
    "        # Sample n batches from n workers.\n",
    "        with self._timers[SAMPLE_TIMER]:\n",
    "            new_sample_batches = synchronous_parallel_sample(\n",
    "                worker_set=self.workers, concat=False\n",
    "            )\n",
    "\n",
    "        for batch in new_sample_batches:\n",
    "            # Update counters.\n",
    "            self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n",
    "            self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n",
    "            # Store new samples in the replay buffer.\n",
    "            self.local_replay_buffer.add(batch)\n",
    "\n",
    "        # Update target network every `target_network_update_freq` sample steps.\n",
    "        cur_ts = self._counters[\n",
    "            NUM_AGENT_STEPS_SAMPLED\n",
    "            if self.config.count_steps_by == \"agent_steps\"\n",
    "            else NUM_ENV_STEPS_SAMPLED\n",
    "        ]\n",
    "\n",
    "        train_results = {}\n",
    "\n",
    "        if cur_ts > self.config.num_steps_sampled_before_learning_starts:\n",
    "            # Sample n batches from replay buffer until the total number of timesteps\n",
    "            # reaches `train_batch_size`.\n",
    "            train_batch = sample_min_n_steps_from_buffer(\n",
    "                replay_buffer=self.local_replay_buffer,\n",
    "                min_steps=self.config.train_batch_size,\n",
    "                count_by_agent_steps=self.config.count_steps_by == \"agent_steps\",\n",
    "            )\n",
    "\n",
    "            # Learn on the training batch.\n",
    "            # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
    "            # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
    "            if self.config.get(\"simple_optimizer\") is True:\n",
    "                train_results = train_one_step(self, train_batch)\n",
    "            else:\n",
    "                train_results = multi_gpu_train_one_step(self, train_batch)\n",
    "\n",
    "            # Update target network every `target_network_update_freq` sample steps.\n",
    "            last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
    "            if cur_ts - last_update >= self.config.target_network_update_freq:\n",
    "                to_update = self.workers.local_worker().get_policies_to_train()\n",
    "                self.workers.local_worker().foreach_policy_to_train(\n",
    "                    lambda p, pid: pid in to_update and p.update_target()\n",
    "                )\n",
    "                self._counters[NUM_TARGET_UPDATES] += 1\n",
    "                self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
    "\n",
    "            update_priorities_in_replay_buffer(\n",
    "                self.local_replay_buffer, self.config, train_batch, train_results\n",
    "            )\n",
    "\n",
    "            # Update weights and global_vars - after learning on the local worker -\n",
    "            # on all remote workers.\n",
    "            global_vars = {\n",
    "                \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
    "            }\n",
    "            # Update remote workers' weights and global vars after learning on local\n",
    "            # worker.\n",
    "            with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
    "                self.workers.sync_weights(global_vars=global_vars)\n",
    "\n",
    "        # Return all collected metrics for the iteration.\n",
    "        return train_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329e83cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'MultiDiscrete'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_41384\\1753468262.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    obs_space= Tuple({MultiDiscrete([])})\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m\u001b[1;31m:\u001b[0m unhashable type: 'MultiDiscrete'\n"
     ]
    }
   ],
   "source": [
    "from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "b = np.ones(3000).tolist()\n",
    "obs_space= Tuple({MultiDiscrete([])})\n",
    "action_space = Tuple({\n",
    "    \"agent1\": Dict(\n",
    "        {\n",
    "            #\"prob\": Discrete(100),\n",
    "            \"action_potential\": Discrete(1) #excitatory neurons MultiDiscrete([2, 2, 2, 3])\n",
    "        } ,dtype=np.float32),\n",
    "    \"agent2\": Dict(\n",
    "        {\n",
    "            #\"prob\": Discrete(100),\n",
    "            \"action_potential\": Discrete(1) #inhibitory neurons MultiDiscrete([2, 2, 2, 3])\n",
    "        } ,dtype=np.float32),\n",
    "    \"agent3\": Dict(\n",
    "        {\n",
    "            #\"prob\": Discrete(100),\n",
    "            \"outsideAction\":gym.spaces.Box(low=-1.0, high=1.0, shape=(10,)),#this should be replaced with the action space of inverted pendulum\n",
    "            \"action_potential\": Discrete(1)# only excitatory\n",
    "        }, dtype=np.float32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eabdb5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.rllib.utils.tf_utils.get_gpu_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0951de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "#ray.init(num_cpus=16,num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e36c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99187b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 21:48:18,280\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\tune.py:562: UserWarning: Consider boosting PBT performance by enabling `reuse_actors` as well as implementing `reset_config` for Trainable.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-26 21:52:49</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:26.80        </td></tr>\n",
       "<tr><td>Memory:      </td><td>30.7/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 272 checkpoints, 25 perturbs<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/20.88 GiB heap, 0.0/10.44 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">   alpha</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_9a59b_00000</td><td>TERMINATED</td><td>127.0.0.1:43920</td><td style=\"text-align: right;\">0.195595</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         80.1971</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_9a59b_00001</td><td>TERMINATED</td><td>127.0.0.1:40436</td><td style=\"text-align: right;\">0.244493</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         80.3293</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_9a59b_00002</td><td>TERMINATED</td><td>127.0.0.1:12640</td><td style=\"text-align: right;\">1.19651 </td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         83.7088</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_9a59b_00003</td><td>TERMINATED</td><td>127.0.0.1:28248</td><td style=\"text-align: right;\">0.490298</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         82.8742</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30456)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=30456)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=30456)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=30456)\u001b[0m [I 03/26/23 21:48:31.544 13112] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=30456)\u001b[0m 2023-03-26 21:48:32,397\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=30456)\u001b[0m 2023-03-26 21:48:32,644\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=30456)\u001b[0m 2023-03-26 21:48:32,666\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=41188)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=41188)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=41188)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=41188)\u001b[0m [I 03/26/23 21:48:41.106 33384] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=41188)\u001b[0m 2023-03-26 21:48:42,036\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=41188)\u001b[0m 2023-03-26 21:48:42,277\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=41188)\u001b[0m 2023-03-26 21:48:42,298\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=41688)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=41688)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=41688)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=41688)\u001b[0m [I 03/26/23 21:48:50.789 22504] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=41688)\u001b[0m 2023-03-26 21:48:51,565\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=41688)\u001b[0m 2023-03-26 21:48:51,796\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=41688)\u001b[0m 2023-03-26 21:48:51,816\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=5448)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=5448)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=5448)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=5448)\u001b[0m [I 03/26/23 21:49:00.518 23564] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=5448)\u001b[0m 2023-03-26 21:49:01,297\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=5448)\u001b[0m 2023-03-26 21:49:01,544\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=5448)\u001b[0m 2023-03-26 21:49:01,559\tWARNING env.py:53 -- Skipping env checking for this experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                         </th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                           </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                 </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                     </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </th><th>timers                                                                                                                                                                            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_9a59b_00000</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.0, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.130462646484375}                   </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.9476379454135895, &#x27;cur_lr&#x27;: 0.0005}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 25.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{}                                                                                                                   </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6201865809644841, &#x27;mean_inference_ms&#x27;: 0.8241173305313481, &#x27;mean_action_processing_ms&#x27;: 0.09716689151234466, &#x27;mean_env_wait_ms&#x27;: 0.02414911630198572, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6201865809644841, &#x27;mean_inference_ms&#x27;: 0.8241173305313481, &#x27;mean_action_processing_ms&#x27;: 0.09716689151234466, &#x27;mean_env_wait_ms&#x27;: 0.02414911630198572, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.0, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.130462646484375}}                  </td><td>{&#x27;training_iteration_time_ms&#x27;: 322.635, &#x27;load_time_ms&#x27;: 0.2, &#x27;load_throughput&#x27;: 1000907.768, &#x27;learn_time_ms&#x27;: 7.127, &#x27;learn_throughput&#x27;: 28062.209, &#x27;synch_weights_time_ms&#x27;: 0.0} </td></tr>\n",
       "<tr><td>PG_TwoStepGame_9a59b_00001</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.009002208709716797, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.10541796684265137}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.9285473227500916, &#x27;cur_lr&#x27;: 0.001}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 73.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000} </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 3.5, &#x27;ram_util_percent&#x27;: 48.8, &#x27;gpu_util_percent0&#x27;: 0.04, &#x27;vram_util_percent0&#x27;: 0.4420166015625}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6321348575076096, &#x27;mean_inference_ms&#x27;: 0.853864103857689, &#x27;mean_action_processing_ms&#x27;: 0.11350273876605106, &#x27;mean_env_wait_ms&#x27;: 0.03950738565387991, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6321348575076096, &#x27;mean_inference_ms&#x27;: 0.853864103857689, &#x27;mean_action_processing_ms&#x27;: 0.11350273876605106, &#x27;mean_env_wait_ms&#x27;: 0.03950738565387991, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.009002208709716797, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.10541796684265137}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 327.071, &#x27;load_time_ms&#x27;: 0.0, &#x27;load_throughput&#x27;: 0.0, &#x27;learn_time_ms&#x27;: 7.609, &#x27;learn_throughput&#x27;: 26282.982, &#x27;synch_weights_time_ms&#x27;: 0.102}       </td></tr>\n",
       "<tr><td>PG_TwoStepGame_9a59b_00002</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.02104330062866211, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.15186429023742676} </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.4708160534501076, &#x27;cur_lr&#x27;: 5e-05}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 125.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 7.6, &#x27;ram_util_percent&#x27;: 50.6, &#x27;gpu_util_percent0&#x27;: 0.01, &#x27;vram_util_percent0&#x27;: 0.4420166015625}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.8140596054194608, &#x27;mean_inference_ms&#x27;: 1.078571083904987, &#x27;mean_action_processing_ms&#x27;: 0.1487997957339657, &#x27;mean_env_wait_ms&#x27;: 0.048520006685822306, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.8140596054194608, &#x27;mean_inference_ms&#x27;: 1.078571083904987, &#x27;mean_action_processing_ms&#x27;: 0.1487997957339657, &#x27;mean_env_wait_ms&#x27;: 0.048520006685822306, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.02104330062866211, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.15186429023742676}} </td><td>{&#x27;training_iteration_time_ms&#x27;: 414.682, &#x27;load_time_ms&#x27;: 0.362, &#x27;load_throughput&#x27;: 552209.071, &#x27;learn_time_ms&#x27;: 8.819, &#x27;learn_throughput&#x27;: 22678.227, &#x27;synch_weights_time_ms&#x27;: 0.0}</td></tr>\n",
       "<tr><td>PG_TwoStepGame_9a59b_00003</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.013077497482299805, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.13428306579589844}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.9313746690750122, &#x27;cur_lr&#x27;: 1e-05}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 41.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000} </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{}                                                                                                                   </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.7117695820896266, &#x27;mean_inference_ms&#x27;: 0.9402355901458891, &#x27;mean_action_processing_ms&#x27;: 0.1361796300316447, &#x27;mean_env_wait_ms&#x27;: 0.04178079869797671, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.7117695820896266, &#x27;mean_inference_ms&#x27;: 0.9402355901458891, &#x27;mean_action_processing_ms&#x27;: 0.1361796300316447, &#x27;mean_env_wait_ms&#x27;: 0.04178079869797671, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.013077497482299805, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.13428306579589844}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 356.08, &#x27;load_time_ms&#x27;: 0.0, &#x27;load_throughput&#x27;: 0.0, &#x27;learn_time_ms&#x27;: 7.969, &#x27;learn_throughput&#x27;: 25098.31, &#x27;synch_weights_time_ms&#x27;: 0.1}           </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n",
      "2023-03-26 21:49:01,695\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:02,222\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00003 (score = -4.120000) into trial 9a59b_00001 (score = -5.510000)\n",
      "\n",
      "2023-03-26 21:49:02,223\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00001:\n",
      "lr : 0.0004 --- (resample) --> 0.001\n",
      "alpha : 0.9970887429558626 --- (* 0.8) --> 0.7976709943646901\n",
      "\n",
      "2023-03-26 21:49:02,432\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00003 (score = -4.120000) into trial 9a59b_00002 (score = -5.610000)\n",
      "\n",
      "2023-03-26 21:49:02,433\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00002:\n",
      "lr : 0.0004 --- (resample) --> 5e-05\n",
      "alpha : 0.9970887429558626 --- (* 0.8) --> 0.7976709943646901\n",
      "\n",
      "2023-03-26 21:49:04,162\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:04,606\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:04,918\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:05,375\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:05,677\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:05,756\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:06,168\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:06,552\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:06,939\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:07,332\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:07,718\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:08,010\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:08,497\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:08,782\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:09,221\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:09,309\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:09,701\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:10,352\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:10,477\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "\u001b[2m\u001b[36m(pid=18696)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=18696)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-03-26 21:49:10,735\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:11,124\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:11,633\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=18696)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=19828)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 21:49:12,056\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:12,515\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=18696)\u001b[0m [I 03/26/23 21:49:12.275 35288] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=19828)\u001b[0m [I 03/26/23 21:49:12.276 17112] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 21:49:12,959\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:13,150\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:49:13,586\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "\u001b[2m\u001b[36m(PG pid=18696)\u001b[0m 2023-03-26 21:49:13,515\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=19828)\u001b[0m 2023-03-26 21:49:13,547\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=18696)\u001b[0m 2023-03-26 21:49:13,782\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=18696)\u001b[0m 2023-03-26 21:49:13,805\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=19828)\u001b[0m 2023-03-26 21:49:13,812\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=18696)\u001b[0m 2023-03-26 21:49:13,905\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_13a4e5eb998947f098966d3b15ad6175\n",
      "\u001b[2m\u001b[36m(PG pid=18696)\u001b[0m 2023-03-26 21:49:13,905\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.40579962730407715, '_episodes_total': 100}\n",
      "\u001b[2m\u001b[36m(PG pid=19828)\u001b[0m 2023-03-26 21:49:13,828\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=19828)\u001b[0m 2023-03-26 21:49:13,947\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_be088cf584994f93b0899a35214cfe16\n",
      "\u001b[2m\u001b[36m(PG pid=19828)\u001b[0m 2023-03-26 21:49:13,947\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.40579962730407715, '_episodes_total': 100}\n",
      "2023-03-26 21:49:14,226\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:49:14,522\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00002 (score = -4.500000) into trial 9a59b_00000 (score = -7.040000)\n",
      "\n",
      "2023-03-26 21:49:14,523\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 5e-05 --- (resample) --> 0.001\n",
      "alpha : 0.7976709943646901 --- (resample) --> 0.14242637111134082\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=8392)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=8392)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=28744)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=28744)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-03-26 21:49:23,344\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=8392)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=28744)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=8392)\u001b[0m [I 03/26/23 21:49:23.953 31680] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 21:49:24,293\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=28744)\u001b[0m [I 03/26/23 21:49:24.591 35512] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 21:49:25,005\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00001 (score = -6.940000) into trial 9a59b_00002 (score = -7.050000)\n",
      "\n",
      "2023-03-26 21:49:25,006\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00002:\n",
      "lr : 0.001 --- (shift left (noop)) --> 0.001\n",
      "alpha : 0.7976709943646901 --- (* 1.2) --> 0.9572051932376281\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=8392)\u001b[0m 2023-03-26 21:49:25,202\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=8392)\u001b[0m 2023-03-26 21:49:25,520\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=8392)\u001b[0m 2023-03-26 21:49:25,538\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=8392)\u001b[0m 2023-03-26 21:49:25,688\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_49102aefbfd04c678b93cc06154c7c81\n",
      "\u001b[2m\u001b[36m(PG pid=8392)\u001b[0m 2023-03-26 21:49:25,688\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 0.8102152347564697, '_episodes_total': 200}\n",
      "\u001b[2m\u001b[36m(PG pid=28744)\u001b[0m 2023-03-26 21:49:25,901\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=28744)\u001b[0m 2023-03-26 21:49:26,225\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=28744)\u001b[0m 2023-03-26 21:49:26,245\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=28744)\u001b[0m 2023-03-26 21:49:26,417\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_4b29787148df473c94721308c033fd63\n",
      "\u001b[2m\u001b[36m(PG pid=28744)\u001b[0m 2023-03-26 21:49:26,417\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 32, '_timesteps_total': None, '_time_total': 11.701532363891602, '_episodes_total': 3200}\n",
      "2023-03-26 21:49:30,183\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00000 (score = -6.080000) into trial 9a59b_00001 (score = -7.050000)\n",
      "\n",
      "2023-03-26 21:49:30,184\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00001:\n",
      "lr : 0.001 --- (shift left (noop)) --> 0.001\n",
      "alpha : 0.14242637111134082 --- (* 0.8) --> 0.11394109688907267\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=18064)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=18064)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=18064)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=18064)\u001b[0m [I 03/26/23 21:49:35.973 32636] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=18064)\u001b[0m 2023-03-26 21:49:37,306\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "2023-03-26 21:49:37,466\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00003 (score = -7.000000) into trial 9a59b_00000 (score = -7.070000)\n",
      "\n",
      "2023-03-26 21:49:37,466\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 0.0004 --- (resample) --> 0.0001\n",
      "alpha : 0.9970887429558626 --- (* 0.8) --> 0.7976709943646901\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=18064)\u001b[0m 2023-03-26 21:49:37,613\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=18064)\u001b[0m 2023-03-26 21:49:37,629\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=18064)\u001b[0m 2023-03-26 21:49:37,739\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_1de3f62edd154e7089306fcd5f186176\n",
      "\u001b[2m\u001b[36m(PG pid=18064)\u001b[0m 2023-03-26 21:49:37,739\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 23, '_timesteps_total': None, '_time_total': 8.32797884941101, '_episodes_total': 2300}\n",
      "\u001b[2m\u001b[36m(pid=24600)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=24600)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=24600)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=24600)\u001b[0m [I 03/26/23 21:49:41.065 8744] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=24600)\u001b[0m 2023-03-26 21:49:42,200\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=24600)\u001b[0m 2023-03-26 21:49:42,495\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=24600)\u001b[0m 2023-03-26 21:49:42,520\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=24600)\u001b[0m 2023-03-26 21:49:42,637\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_d3ad97aa8d05416f9568240a1131861a\n",
      "\u001b[2m\u001b[36m(PG pid=24600)\u001b[0m 2023-03-26 21:49:42,637\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 9, '_timesteps_total': None, '_time_total': 3.7660388946533203, '_episodes_total': 900}\n",
      "\u001b[2m\u001b[36m(pid=12732)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=12732)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=29256)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=29256)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=12732)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=29256)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=12732)\u001b[0m [I 03/26/23 21:49:49.011 23712] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=29256)\u001b[0m [I 03/26/23 21:49:49.040 2496] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=12732)\u001b[0m 2023-03-26 21:49:50,265\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=29256)\u001b[0m 2023-03-26 21:49:50,288\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=12732)\u001b[0m 2023-03-26 21:49:50,542\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=12732)\u001b[0m 2023-03-26 21:49:50,566\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=29256)\u001b[0m 2023-03-26 21:49:50,571\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=29256)\u001b[0m 2023-03-26 21:49:50,590\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=12732)\u001b[0m 2023-03-26 21:49:50,737\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_1f1f8175b72048aab8738bae4e1c46bc\n",
      "\u001b[2m\u001b[36m(PG pid=12732)\u001b[0m 2023-03-26 21:49:50,738\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 51, '_timesteps_total': None, '_time_total': 19.91071319580078, '_episodes_total': 5100}\n",
      "\u001b[2m\u001b[36m(PG pid=29256)\u001b[0m 2023-03-26 21:49:50,762\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_a9aca4c637fc46fe976834e55bb8db69\n",
      "\u001b[2m\u001b[36m(PG pid=29256)\u001b[0m 2023-03-26 21:49:50,762\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 50, '_timesteps_total': None, '_time_total': 19.489715099334717, '_episodes_total': 5000}\n",
      "2023-03-26 21:49:51,646\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00001 (score = -6.950000) into trial 9a59b_00002 (score = -7.010000)\n",
      "\n",
      "2023-03-26 21:49:51,648\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00002:\n",
      "lr : 0.001 --- (shift right) --> 0.0005\n",
      "alpha : 0.11394109688907267 --- (resample) --> 0.37882167963580304\n",
      "\n",
      "2023-03-26 21:49:52,276\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00001 (score = -6.970000) into trial 9a59b_00000 (score = -7.020000)\n",
      "\n",
      "2023-03-26 21:49:52,277\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 0.001 --- (shift left (noop)) --> 0.001\n",
      "alpha : 0.11394109688907267 --- (* 0.8) --> 0.09115287751125814\n",
      "\n",
      "2023-03-26 21:49:54,542\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:49:57,592\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:49:58,500\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "\u001b[2m\u001b[36m(pid=43124)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43124)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=38236)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=38236)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=6624)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=6624)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-03-26 21:50:02,767\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=43124)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=38236)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=6624)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=43124)\u001b[0m [I 03/26/23 21:50:03.646 41404] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=38236)\u001b[0m [I 03/26/23 21:50:03.643 38036] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=6624)\u001b[0m [I 03/26/23 21:50:03.652 31920] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=38236)\u001b[0m 2023-03-26 21:50:04,853\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43124)\u001b[0m 2023-03-26 21:50:04,877\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=6624)\u001b[0m 2023-03-26 21:50:04,888\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43124)\u001b[0m 2023-03-26 21:50:05,175\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43124)\u001b[0m 2023-03-26 21:50:05,192\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=38236)\u001b[0m 2023-03-26 21:50:05,166\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=38236)\u001b[0m 2023-03-26 21:50:05,181\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=6624)\u001b[0m 2023-03-26 21:50:05,197\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=6624)\u001b[0m 2023-03-26 21:50:05,212\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=38236)\u001b[0m 2023-03-26 21:50:05,334\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_50d8eb2b9845441dad2c1f88d7a7db23\n",
      "\u001b[2m\u001b[36m(PG pid=38236)\u001b[0m 2023-03-26 21:50:05,335\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 25, '_timesteps_total': None, '_time_total': 10.661375999450684, '_episodes_total': 2500}\n",
      "\u001b[2m\u001b[36m(PG pid=43124)\u001b[0m 2023-03-26 21:50:05,410\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f61f4470ecf443458054fba8c205b2f8\n",
      "\u001b[2m\u001b[36m(PG pid=43124)\u001b[0m 2023-03-26 21:50:05,410\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 24, '_timesteps_total': None, '_time_total': 10.280701160430908, '_episodes_total': 2400}\n",
      "\u001b[2m\u001b[36m(PG pid=6624)\u001b[0m 2023-03-26 21:50:05,361\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_fc70ff13d016416db4e14b07579f4eb0\n",
      "\u001b[2m\u001b[36m(PG pid=6624)\u001b[0m 2023-03-26 21:50:05,361\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 53, '_timesteps_total': None, '_time_total': 20.727547883987427, '_episodes_total': 5300}\n",
      "2023-03-26 21:50:05,903\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:50:06,452\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00000 (score = -6.970000) into trial 9a59b_00001 (score = -7.010000)\n",
      "\n",
      "2023-03-26 21:50:06,453\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00001:\n",
      "lr : 0.001 --- (shift left (noop)) --> 0.001\n",
      "alpha : 0.09115287751125814 --- (resample) --> 0.2563790287981156\n",
      "\n",
      "2023-03-26 21:50:06,894\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00000 (score = -6.970000) into trial 9a59b_00002 (score = -7.020000)\n",
      "\n",
      "2023-03-26 21:50:06,895\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00002:\n",
      "lr : 0.001 --- (resample) --> 0.0005\n",
      "alpha : 0.09115287751125814 --- (* 0.8) --> 0.07292230200900651\n",
      "\n",
      "2023-03-26 21:50:07,126\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "\u001b[2m\u001b[36m(pid=39272)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=39272)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=13880)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=13880)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=6276)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=6276)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=26248)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=26248)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=39272)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=13880)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=26248)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=6276)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=39272)\u001b[0m [I 03/26/23 21:50:17.217 24696] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=13880)\u001b[0m [I 03/26/23 21:50:17.247 16916] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=6276)\u001b[0m [I 03/26/23 21:50:17.643 41672] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=26248)\u001b[0m [I 03/26/23 21:50:17.623 15940] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=39272)\u001b[0m 2023-03-26 21:50:18,517\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=13880)\u001b[0m 2023-03-26 21:50:18,550\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=39272)\u001b[0m 2023-03-26 21:50:18,814\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=39272)\u001b[0m 2023-03-26 21:50:18,840\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=13880)\u001b[0m 2023-03-26 21:50:18,855\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=13880)\u001b[0m 2023-03-26 21:50:18,872\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=39272)\u001b[0m 2023-03-26 21:50:18,980\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_304397cbd48349eea865808bb7d24506\n",
      "\u001b[2m\u001b[36m(PG pid=39272)\u001b[0m 2023-03-26 21:50:18,980\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 55, '_timesteps_total': None, '_time_total': 21.614136695861816, '_episodes_total': 5500}\n",
      "\u001b[2m\u001b[36m(PG pid=13880)\u001b[0m 2023-03-26 21:50:18,991\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f6b817f2a9314c279dcd300a69829f68\n",
      "\u001b[2m\u001b[36m(PG pid=13880)\u001b[0m 2023-03-26 21:50:18,991\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 27, '_timesteps_total': None, '_time_total': 11.535414218902588, '_episodes_total': 2700}\n",
      "\u001b[2m\u001b[36m(PG pid=6276)\u001b[0m 2023-03-26 21:50:18,965\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26248)\u001b[0m 2023-03-26 21:50:18,923\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26248)\u001b[0m 2023-03-26 21:50:19,251\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=26248)\u001b[0m 2023-03-26 21:50:19,268\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=6276)\u001b[0m 2023-03-26 21:50:19,291\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=6276)\u001b[0m 2023-03-26 21:50:19,310\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=26248)\u001b[0m 2023-03-26 21:50:19,387\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_a6b343ceea944642aaf05b3f1279f080\n",
      "\u001b[2m\u001b[36m(PG pid=26248)\u001b[0m 2023-03-26 21:50:19,387\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 27, '_timesteps_total': None, '_time_total': 11.535414218902588, '_episodes_total': 2700}\n",
      "\u001b[2m\u001b[36m(PG pid=6276)\u001b[0m 2023-03-26 21:50:19,425\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f32f15d80867470db55b2b14b9d1a116\n",
      "\u001b[2m\u001b[36m(PG pid=6276)\u001b[0m 2023-03-26 21:50:19,425\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 11.884807586669922, '_episodes_total': 2800}\n",
      "2023-03-26 21:50:20,024\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00002 (score = -6.950000) into trial 9a59b_00000 (score = -7.020000)\n",
      "\n",
      "2023-03-26 21:50:20,025\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 0.0005 --- (shift left) --> 0.001\n",
      "alpha : 0.07292230200900651 --- (* 0.8) --> 0.05833784160720521\n",
      "\n",
      "2023-03-26 21:50:20,275\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00002 (score = -6.950000) into trial 9a59b_00001 (score = -7.040000)\n",
      "\n",
      "2023-03-26 21:50:20,276\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00001:\n",
      "lr : 0.0005 --- (shift left) --> 0.001\n",
      "alpha : 0.07292230200900651 --- (* 1.2) --> 0.0875067624108078\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27940)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=27940)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=40132)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=40132)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=40132)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=27940)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=40132)\u001b[0m [I 03/26/23 21:50:30.969 38392] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=27940)\u001b[0m [I 03/26/23 21:50:30.912 43268] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=40132)\u001b[0m 2023-03-26 21:50:32,150\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=27940)\u001b[0m 2023-03-26 21:50:32,096\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=27940)\u001b[0m 2023-03-26 21:50:32,367\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=27940)\u001b[0m 2023-03-26 21:50:32,382\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=40132)\u001b[0m 2023-03-26 21:50:32,422\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=40132)\u001b[0m 2023-03-26 21:50:32,439\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=27940)\u001b[0m 2023-03-26 21:50:32,505\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_e01fce48c2c64fa5a3592fe60419aaed\n",
      "\u001b[2m\u001b[36m(PG pid=27940)\u001b[0m 2023-03-26 21:50:32,505\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 11.945060729980469, '_episodes_total': 2800}\n",
      "\u001b[2m\u001b[36m(PG pid=40132)\u001b[0m 2023-03-26 21:50:32,601\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_bedb49f4389f47f0b07a7929f6c15c42\n",
      "\u001b[2m\u001b[36m(PG pid=40132)\u001b[0m 2023-03-26 21:50:32,601\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 11.945060729980469, '_episodes_total': 2800}\n",
      "2023-03-26 21:50:33,360\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00000 (score = -6.910000) into trial 9a59b_00001 (score = -7.050000)\n",
      "\n",
      "2023-03-26 21:50:33,361\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00001:\n",
      "lr : 0.001 --- (shift left (noop)) --> 0.001\n",
      "alpha : 0.05833784160720521 --- (resample) --> 0.18075917997800428\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=21548)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=21548)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=43908)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43908)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=21548)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=43908)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=21548)\u001b[0m [I 03/26/23 21:50:44.400 42696] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=43908)\u001b[0m [I 03/26/23 21:50:44.407 12292] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=43908)\u001b[0m 2023-03-26 21:50:45,617\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=21548)\u001b[0m 2023-03-26 21:50:45,731\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43908)\u001b[0m 2023-03-26 21:50:45,920\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43908)\u001b[0m 2023-03-26 21:50:45,936\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=21548)\u001b[0m 2023-03-26 21:50:46,040\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=21548)\u001b[0m 2023-03-26 21:50:46,060\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=43908)\u001b[0m 2023-03-26 21:50:46,054\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_6c8f39b415b647679a24c08299a0ac8e\n",
      "\u001b[2m\u001b[36m(PG pid=43908)\u001b[0m 2023-03-26 21:50:46,054\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 80, '_timesteps_total': None, '_time_total': 32.054208755493164, '_episodes_total': 8000}\n",
      "\u001b[2m\u001b[36m(PG pid=21548)\u001b[0m 2023-03-26 21:50:46,174\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b13051eff3084898a7f14898a2938323\n",
      "\u001b[2m\u001b[36m(PG pid=21548)\u001b[0m 2023-03-26 21:50:46,174\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 29, '_timesteps_total': None, '_time_total': 12.495647192001343, '_episodes_total': 2900}\n",
      "2023-03-26 21:50:46,721\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00003 (score = -7.000000) into trial 9a59b_00001 (score = -7.040000)\n",
      "\n",
      "2023-03-26 21:50:46,722\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00001:\n",
      "lr : 0.0004 --- (resample) --> 0.001\n",
      "alpha : 0.9970887429558626 --- (resample) --> 0.24449344840155807\n",
      "\n",
      "2023-03-26 21:50:52,395\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00000 (score = -6.910000) into trial 9a59b_00002 (score = -7.050000)\n",
      "\n",
      "2023-03-26 21:50:52,396\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00002:\n",
      "lr : 0.001 --- (resample) --> 5e-05\n",
      "alpha : 0.05833784160720521 --- (* 0.8) --> 0.046670273285764174\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=17380)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=17380)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=17380)\u001b[0m [I 03/26/23 21:50:58.277 13832] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=17380)\u001b[0m 2023-03-26 21:50:59,540\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=17380)\u001b[0m 2023-03-26 21:50:59,829\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=17380)\u001b[0m 2023-03-26 21:50:59,845\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=17380)\u001b[0m 2023-03-26 21:50:59,980\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_df75969a674744a8a75451441f9133f1\n",
      "\u001b[2m\u001b[36m(PG pid=17380)\u001b[0m 2023-03-26 21:50:59,980\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 81, '_timesteps_total': None, '_time_total': 32.46448588371277, '_episodes_total': 8100}\n",
      "\u001b[2m\u001b[36m(pid=29544)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=29544)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=29544)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=29544)\u001b[0m [I 03/26/23 21:51:03.611 24148] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=29544)\u001b[0m 2023-03-26 21:51:04,928\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=29544)\u001b[0m 2023-03-26 21:51:05,258\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=29544)\u001b[0m 2023-03-26 21:51:05,275\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=29544)\u001b[0m 2023-03-26 21:51:05,427\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f1f36efb20c94ed0abfea7a3df11bc9b\n",
      "\u001b[2m\u001b[36m(PG pid=29544)\u001b[0m 2023-03-26 21:51:05,427\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 63, '_timesteps_total': None, '_time_total': 26.981966257095337, '_episodes_total': 6300}\n",
      "2023-03-26 21:51:05,944\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00000 (score = -6.970000) into trial 9a59b_00002 (score = -7.030000)\n",
      "\n",
      "2023-03-26 21:51:05,944\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00002:\n",
      "lr : 0.001 --- (shift right) --> 0.0005\n",
      "alpha : 0.05833784160720521 --- (* 0.8) --> 0.046670273285764174\n",
      "\n",
      "2023-03-26 21:51:06,181\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "\u001b[2m\u001b[36m(pid=10412)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=10412)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=26972)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=26972)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=10412)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=26972)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=10412)\u001b[0m [I 03/26/23 21:51:16.951 9204] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=26972)\u001b[0m [I 03/26/23 21:51:16.958 12336] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=10412)\u001b[0m 2023-03-26 21:51:18,190\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26972)\u001b[0m 2023-03-26 21:51:18,210\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=10412)\u001b[0m 2023-03-26 21:51:18,470\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=10412)\u001b[0m 2023-03-26 21:51:18,488\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=26972)\u001b[0m 2023-03-26 21:51:18,497\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=26972)\u001b[0m 2023-03-26 21:51:18,513\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=10412)\u001b[0m 2023-03-26 21:51:18,656\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_080a51a76de846f3964263c07ce09ad3\n",
      "\u001b[2m\u001b[36m(PG pid=10412)\u001b[0m 2023-03-26 21:51:18,656\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 86, '_timesteps_total': None, '_time_total': 37.11512517929077, '_episodes_total': 8600}\n",
      "\u001b[2m\u001b[36m(PG pid=26972)\u001b[0m 2023-03-26 21:51:18,676\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3a05a46a420e43bbbe65e16707e6e0c4\n",
      "\u001b[2m\u001b[36m(PG pid=26972)\u001b[0m 2023-03-26 21:51:18,676\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 85, '_timesteps_total': None, '_time_total': 36.6580753326416, '_episodes_total': 8500}\n",
      "2023-03-26 21:51:19,293\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00003 (score = -7.000000) into trial 9a59b_00002 (score = -7.030000)\n",
      "\n",
      "2023-03-26 21:51:19,294\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00002:\n",
      "lr : 0.0004 --- (resample) --> 5e-05\n",
      "alpha : 0.9970887429558626 --- (* 1.2) --> 1.1965064915470351\n",
      "\n",
      "2023-03-26 21:51:24,401\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "\u001b[2m\u001b[36m(pid=12640)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=12640)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-03-26 21:51:29,351\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00003 (score = -7.000000) into trial 9a59b_00000 (score = -7.030000)\n",
      "\n",
      "2023-03-26 21:51:29,352\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 0.0004 --- (resample) --> 5e-05\n",
      "alpha : 0.9970887429558626 --- (resample) --> 0.8987568164647679\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=12640)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=12640)\u001b[0m [I 03/26/23 21:51:30.581 23736] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=12640)\u001b[0m 2023-03-26 21:51:31,694\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=12640)\u001b[0m 2023-03-26 21:51:31,981\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=12640)\u001b[0m 2023-03-26 21:51:31,999\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=12640)\u001b[0m 2023-03-26 21:51:32,109\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_13f53a5de88140ad96b3d61186b3ebdb\n",
      "\u001b[2m\u001b[36m(PG pid=12640)\u001b[0m 2023-03-26 21:51:32,109\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 137, '_timesteps_total': None, '_time_total': 56.44181728363037, '_episodes_total': 13700}\n",
      "\u001b[2m\u001b[36m(pid=7796)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=7796)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=37752)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=37752)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=7796)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=37752)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=7796)\u001b[0m [I 03/26/23 21:51:40.538 40872] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=37752)\u001b[0m [I 03/26/23 21:51:40.539 19228] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=7796)\u001b[0m 2023-03-26 21:51:41,719\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=37752)\u001b[0m 2023-03-26 21:51:41,715\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=7796)\u001b[0m 2023-03-26 21:51:41,980\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=7796)\u001b[0m 2023-03-26 21:51:41,997\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=37752)\u001b[0m 2023-03-26 21:51:41,986\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=37752)\u001b[0m 2023-03-26 21:51:42,000\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=7796)\u001b[0m 2023-03-26 21:51:42,170\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_10b11ca4f9a64b2e9e3c5061f30f9eb4\n",
      "\u001b[2m\u001b[36m(PG pid=7796)\u001b[0m 2023-03-26 21:51:42,170\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 133, '_timesteps_total': None, '_time_total': 54.458128452301025, '_episodes_total': 13300}\n",
      "\u001b[2m\u001b[36m(PG pid=37752)\u001b[0m 2023-03-26 21:51:42,140\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ca30c4d3b05d44f483e511127971b5a9\n",
      "\u001b[2m\u001b[36m(PG pid=37752)\u001b[0m 2023-03-26 21:51:42,140\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 155, '_timesteps_total': None, '_time_total': 64.08341860771179, '_episodes_total': 15500}\n",
      "2023-03-26 21:51:42,711\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00002 (score = -7.000000) into trial 9a59b_00000 (score = -7.010000)\n",
      "\n",
      "2023-03-26 21:51:42,712\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 5e-05 --- (shift left) --> 0.0001\n",
      "alpha : 1.1965064915470351 --- (resample) --> 0.31260400603421823\n",
      "\n",
      "2023-03-26 21:51:42,942\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00002 (score = -7.000000) into trial 9a59b_00003 (score = -7.020000)\n",
      "\n",
      "2023-03-26 21:51:42,944\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00003:\n",
      "lr : 5e-05 --- (shift right) --> 1e-05\n",
      "alpha : 1.1965064915470351 --- (* 0.8) --> 0.9572051932376282\n",
      "\n",
      "2023-03-26 21:51:44,459\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00002\n",
      "2023-03-26 21:51:49,856\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00002\n",
      "\u001b[2m\u001b[36m(pid=43604)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43604)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=33796)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33796)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=43604)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=33796)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=43604)\u001b[0m [I 03/26/23 21:51:54.273 10952] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=33796)\u001b[0m [I 03/26/23 21:51:54.314 42860] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=43604)\u001b[0m 2023-03-26 21:51:55,743\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=33796)\u001b[0m 2023-03-26 21:51:55,774\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43604)\u001b[0m 2023-03-26 21:51:56,034\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43604)\u001b[0m 2023-03-26 21:51:56,058\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=33796)\u001b[0m 2023-03-26 21:51:56,061\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33796)\u001b[0m 2023-03-26 21:51:56,077\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=43604)\u001b[0m 2023-03-26 21:51:56,179\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_a4592b0dc7554272a933e3bd3c6da727\n",
      "\u001b[2m\u001b[36m(PG pid=43604)\u001b[0m 2023-03-26 21:51:56,179\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 157, '_timesteps_total': None, '_time_total': 64.97367429733276, '_episodes_total': 15700}\n",
      "\u001b[2m\u001b[36m(PG pid=33796)\u001b[0m 2023-03-26 21:51:56,202\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_a3b8e96cbebf4128989f4babdeeaa8d3\n",
      "\u001b[2m\u001b[36m(PG pid=33796)\u001b[0m 2023-03-26 21:51:56,202\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 157, '_timesteps_total': None, '_time_total': 64.97367429733276, '_episodes_total': 15700}\n",
      "2023-03-26 21:51:56,661\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00002 (score = -7.000000) into trial 9a59b_00003 (score = -7.010000)\n",
      "\n",
      "2023-03-26 21:51:56,663\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00003:\n",
      "lr : 5e-05 --- (shift right) --> 1e-05\n",
      "alpha : 1.1965064915470351 --- (resample) --> 0.49029848780439966\n",
      "\n",
      "2023-03-26 21:51:57,989\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00002 (score = -7.000000) into trial 9a59b_00000 (score = -7.030000)\n",
      "\n",
      "2023-03-26 21:51:57,991\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 5e-05 --- (shift right) --> 1e-05\n",
      "alpha : 1.1965064915470351 --- (* 0.8) --> 0.9572051932376282\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=28248)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=28248)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=2460)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=2460)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=28248)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=2460)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=26468)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=26468)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=28248)\u001b[0m [I 03/26/23 21:52:07.619 41620] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=2460)\u001b[0m [I 03/26/23 21:52:07.622 41592] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=26468)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=26468)\u001b[0m [I 03/26/23 21:52:08.855 34188] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=28248)\u001b[0m 2023-03-26 21:52:08,892\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=2460)\u001b[0m 2023-03-26 21:52:08,859\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=28248)\u001b[0m 2023-03-26 21:52:09,225\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=28248)\u001b[0m 2023-03-26 21:52:09,240\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=2460)\u001b[0m 2023-03-26 21:52:09,191\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=2460)\u001b[0m 2023-03-26 21:52:09,208\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=2460)\u001b[0m 2023-03-26 21:52:09,345\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_4166d305fac540bb90ec8e3ec53e7818\n",
      "\u001b[2m\u001b[36m(PG pid=2460)\u001b[0m 2023-03-26 21:52:09,345\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 157, '_timesteps_total': None, '_time_total': 65.24305844306946, '_episodes_total': 15700}\n",
      "\u001b[2m\u001b[36m(PG pid=28248)\u001b[0m 2023-03-26 21:52:09,383\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_32fd0b7fcdb74a1db9f6b2ad8c70f279\n",
      "\u001b[2m\u001b[36m(PG pid=28248)\u001b[0m 2023-03-26 21:52:09,383\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 179, '_timesteps_total': None, '_time_total': 74.91732287406921, '_episodes_total': 17900}\n",
      "\u001b[2m\u001b[36m(PG pid=26468)\u001b[0m 2023-03-26 21:52:10,245\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26468)\u001b[0m 2023-03-26 21:52:10,556\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=26468)\u001b[0m 2023-03-26 21:52:10,573\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=26468)\u001b[0m 2023-03-26 21:52:10,739\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_aa77136b70e34541b01fc76594e38f5f\n",
      "\u001b[2m\u001b[36m(PG pid=26468)\u001b[0m 2023-03-26 21:52:10,739\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 181, '_timesteps_total': None, '_time_total': 75.7872109413147, '_episodes_total': 18100}\n",
      "2023-03-26 21:52:11,358\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:52:12,402\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00003 (score = -7.000000) into trial 9a59b_00000 (score = -7.010000)\n",
      "\n",
      "2023-03-26 21:52:12,403\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 1e-05 --- (shift left) --> 5e-05\n",
      "alpha : 0.49029848780439966 --- (resample) --> 0.15372735109634528\n",
      "\n",
      "2023-03-26 21:52:15,026\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "2023-03-26 21:52:15,812\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00003\n",
      "\u001b[2m\u001b[36m(pid=14540)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=14540)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=40436)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=40436)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=14540)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=40436)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=14540)\u001b[0m [I 03/26/23 21:52:22.000 42504] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "\u001b[2m\u001b[36m(PG pid=40436)\u001b[0m [I 03/26/23 21:52:22.028 34512] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=14540)\u001b[0m 2023-03-26 21:52:22,890\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=40436)\u001b[0m 2023-03-26 21:52:22,918\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=14540)\u001b[0m 2023-03-26 21:52:23,135\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=14540)\u001b[0m 2023-03-26 21:52:23,156\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=40436)\u001b[0m 2023-03-26 21:52:23,157\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=40436)\u001b[0m 2023-03-26 21:52:23,171\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=14540)\u001b[0m 2023-03-26 21:52:23,270\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_72b20e3c41dc4b0eb593a8009decef2d\n",
      "\u001b[2m\u001b[36m(PG pid=14540)\u001b[0m 2023-03-26 21:52:23,270\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 184, '_timesteps_total': None, '_time_total': 77.04365277290344, '_episodes_total': 18400}\n",
      "\u001b[2m\u001b[36m(PG pid=40436)\u001b[0m 2023-03-26 21:52:23,283\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_03511423b04647559116ddd9b504db9d\n",
      "\u001b[2m\u001b[36m(PG pid=40436)\u001b[0m 2023-03-26 21:52:23,283\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 163, '_timesteps_total': None, '_time_total': 67.77223515510559, '_episodes_total': 16300}\n",
      "2023-03-26 21:52:23,615\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00000\n",
      "2023-03-26 21:52:23,999\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00001 (score = -7.000000) into trial 9a59b_00000 (score = -7.000000)\n",
      "\n",
      "2023-03-26 21:52:24,000\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 0.001 --- (shift right) --> 0.0005\n",
      "alpha : 0.24449344840155807 --- (* 1.2) --> 0.29339213808186965\n",
      "\n",
      "2023-03-26 21:52:24,290\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:24,670\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:25,676\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:26,684\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:27,569\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:30,049\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:30,909\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "\u001b[2m\u001b[36m(pid=14684)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=14684)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=14684)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=14684)\u001b[0m [I 03/26/23 21:52:33.120 25224] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 21:52:33,875\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "\u001b[2m\u001b[36m(PG pid=14684)\u001b[0m 2023-03-26 21:52:33,999\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "2023-03-26 21:52:34,244\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "\u001b[2m\u001b[36m(PG pid=14684)\u001b[0m 2023-03-26 21:52:34,257\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=14684)\u001b[0m 2023-03-26 21:52:34,271\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=14684)\u001b[0m 2023-03-26 21:52:34,370\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_58f38fcdc698448ab799d1e7be48b3e8\n",
      "\u001b[2m\u001b[36m(PG pid=14684)\u001b[0m 2023-03-26 21:52:34,370\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 164, '_timesteps_total': None, '_time_total': 68.1123480796814, '_episodes_total': 16400}\n",
      "2023-03-26 21:52:34,786\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9a59b_00001 (score = -7.000000) into trial 9a59b_00000 (score = -7.000000)\n",
      "\n",
      "2023-03-26 21:52:34,787\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9a59b_00000:\n",
      "lr : 0.001 --- (shift right) --> 0.0005\n",
      "alpha : 0.24449344840155807 --- (* 0.8) --> 0.19559475872124646\n",
      "\n",
      "2023-03-26 21:52:35,157\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:36,084\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:37,480\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:38,863\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:39,745\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "2023-03-26 21:52:40,108\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_9a59b_00001\n",
      "\u001b[2m\u001b[36m(pid=43920)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43920)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=43920)\u001b[0m [Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "\u001b[2m\u001b[36m(PG pid=43920)\u001b[0m [I 03/26/23 21:52:43.627 10368] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PG pid=43920)\u001b[0m 2023-03-26 21:52:44,499\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43920)\u001b[0m 2023-03-26 21:52:44,733\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43920)\u001b[0m 2023-03-26 21:52:44,746\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=43920)\u001b[0m 2023-03-26 21:52:44,839\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_430e615e245249a6b1d0ed1280a4f9fa\n",
      "\u001b[2m\u001b[36m(PG pid=43920)\u001b[0m 2023-03-26 21:52:44,839\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 187, '_timesteps_total': None, '_time_total': 75.98753952980042, '_episodes_total': 18700}\n",
      "2023-03-26 21:52:50,197\tINFO tune.py:798 -- Total run time: 267.49 seconds (266.76 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "#from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "from gymnasium.spaces import Dict, Discrete, MultiDiscrete, Tuple\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"PG\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=16)\n",
    "parser.add_argument(\"--num-gpus\", type=int, default=1)\n",
    "#parser.add_argument(\"render_mode\", type=int, default=1)\n",
    "parser.add_argument(\n",
    "    \"--mixer\",\n",
    "    type=str,\n",
    "    default=\"qmix\",\n",
    "    choices=[\"qmix\", \"vdn\", \"none\"],\n",
    "    help=\"The mixer model to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=70000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=8.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "            #if agent_id.startswith(\"low_level_\"):\n",
    "                #return \"low_level_policy\"\n",
    "            #else:\n",
    "                #return \"high_level_policy\"\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ray.init(num_cpus=16, num_gpus=1,local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0],\n",
    "        \"group_2\": [0,1]\n",
    "        #\"group_3\": [0]\n",
    "    }\n",
    "    obs_space = Tuple(\n",
    "        [\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]), \n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2,1]),\n",
    "                }\n",
    "            ),\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2,1]),\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    #act_space =Tuple(\n",
    "    #[\n",
    "    #TwoStepGame.action_space,\n",
    "    #TwoStepGame.action_space,\n",
    "    #]\n",
    "    #)\n",
    "    act_space = Tuple(\n",
    "        [\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]), \n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2,1]),\n",
    "                }\n",
    "            ),\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2,1]),\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=obs_space, act_space=act_space\n",
    "        ),\n",
    "    )\n",
    "    \"\"\"\n",
    "    from ray.tune import register_env\n",
    "    from ray.rllib.algorithms.dqn import DQN \n",
    "    YourExternalEnv = ... \n",
    "    register_env(\"my_env\", \n",
    "        lambda config: YourExternalEnv(config))\n",
    "    trainer = DQN(env=\"my_env\") \n",
    "    while True: \n",
    "        print(trainer.train()) \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(TwoStepGame)\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "\n",
    "    if args.run == \"QMIX\":\n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "            .training(mixer=args.mixer, train_batch_size=32)\n",
    "            \n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        obs_space,\n",
    "                        act_space,\n",
    "                        config.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        Tuple([obs_space]),#, Discrete(4)]),\n",
    "                        act_space,\n",
    "                        config.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn,\n",
    "            )\n",
    "            .rollouts(num_rollout_workers=16, rollout_fragment_length=4)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "                }\n",
    "            )\n",
    "            .environment(\n",
    "                env=\"grouped_twostep\",\n",
    "                env_config={\n",
    "                    \"separate_state_space\": True,\n",
    "                    \"one_hot_state_encoding\": True,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr='training_iteration',\n",
    "        metric='episode_reward_mean',#'loss',\n",
    "        mode='min',\n",
    "        perturbation_interval=1,\n",
    "        hyperparam_mutations={\n",
    "            \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"alpha\": tune.uniform(0.0, 1.0),\n",
    "        }\n",
    "    )\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        \n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=4,\n",
    "            scheduler=pbt_scheduler,\n",
    "        ),\n",
    "        \n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best result based on a particular metric.\n",
    "best_result = results.get_best_result(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# Get the best checkpoint corresponding to the best result.\n",
    "best_checkpoint = best_result.checkpoint\n",
    "\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "algo = Algorithm.from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "while True:\n",
    "    print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e658e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = algo.get_policy()\n",
    "# <ray.rllib.policy.eager_tf_policy.PPOTFPolicy_eager object at 0x7fd020165470>\n",
    "\n",
    "# Run a forward pass to get model output logits. Note that complex observations\n",
    "# must be preprocessed as in the above code block.\n",
    "logits, _ = policy.model({\"obs\": np.array([[0.1, 0.2, 0.3, 0.4]])})\n",
    "# (<tf.Tensor: id=1274, shape=(1, 2), dtype=float32, numpy=...>, [])\n",
    "\n",
    "# Compute action distribution given logits\n",
    "policy.dist_class\n",
    "# <class_object 'ray.rllib.models.tf.tf_action_dist.Categorical'>\n",
    "dist = policy.dist_class(logits, policy.model)\n",
    "# <ray.rllib.models.tf.tf_action_dist.Categorical object at 0x7fd02301d710>\n",
    "\n",
    "# Query the distribution for samples, sample logps\n",
    "dist.sample()\n",
    "# <tf.Tensor: id=661, shape=(1,), dtype=int64, numpy=..>\n",
    "dist.logp([1])\n",
    "# <tf.Tensor: id=1298, shape=(1,), dtype=float32, numpy=...>\n",
    "\n",
    "# Get the estimated values for the most recent forward pass\n",
    "policy.model.value_function()\n",
    "# <tf.Tensor: id=670, shape=(1,), dtype=float32, numpy=...>\n",
    "\n",
    "policy.model.base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61730f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = Session(graph, env)\n",
    "sess.run(\n",
    "    n_steps=500000,\n",
    "    learn=True,\n",
    "    render=TwoStepGame.render,\n",
    "    success_reward=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00033161",
   "metadata": {},
   "source": [
    "0.37 policy loss with prior precision 100\n",
    "\n",
    "0.9271736741065979 policy loss with prior precision of 1\n",
    "0.55 policy loss prior precision of 0.0001\n",
    "\n",
    "0.96 policy loss with prior precision of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoStepGame.render()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d283e22e",
   "metadata": {},
   "source": [
    "psueudo code for visualization\n",
    "@ti.func\n",
    "render():\n",
    "for i in num_agents\n",
    "\n",
    "create agent\n",
    "\n",
    "if agent belongs to class 1 high level policy then put it in position 1 on the render screen\n",
    "\n",
    "if agent belongs to class 1 or 2 mid level policy then put it in position 2 on the render screen\n",
    "\n",
    "if agent belongs to class 1 low level policy then put it in position 3 on render screen\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "if obs == false\n",
    "\n",
    "reward function = expected free energy\n",
    "\n",
    "else obs == true and not top level neuronal agent\n",
    "\n",
    "for mixer network\n",
    "\n",
    "global_reward = variational free energy\n",
    "\n",
    "prediction errors will be generated by subtracting actions from env states or observations\n",
    "\n",
    "higher level neurons will have a kullbacker liebler local reward while the motor neurons will just have a surprise reward\n",
    "\n",
    "We will aquire these from the replay buffer "
   ]
  },
  {
   "cell_type": "raw",
   "id": "92cd1717",
   "metadata": {},
   "source": [
    "\n",
    "top level prior is going to be using expected free energy to create a dirchlet distribution. This will use something akin to Bayesian optimal experimental design\n",
    "design to create a policy that will explore what conditions cause a failiure of the objective and exploit this information to minimzie this. This prior will be constantly updated by prediction errors\n",
    "\n",
    "All other priors will be empirical priors. \n",
    "\n",
    "horizontal is dynamics where this can be interpreted as a graphical probabilistic model and vertical is from discrete to continuous from slower to finer time steps \n",
    "\n",
    "Note: when signals are sent to each other by neurons in the same level of the hierarchy this is just a probabilistic DAG like what is seen in the WISH presentation. When signals are sent up and down the hierarchy that is when predictions are sent down and prediction errors up from coarser to finer timescales \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c504050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pos = np.random.random((50, 2))\n",
    "# Create an array of 50 integer elements whose values are randomly 0, 1, 2\n",
    "# 0 corresponds to 0x068587\n",
    "# 1 corresponds to 0xED553B\n",
    "# 2 corresponds to 0xEEEEF0\n",
    "indices = np.random.randint(0, 2, size=(50,))\n",
    "gui = ti.GUI(\"circles\", res=(400, 400))\n",
    "while gui.running:\n",
    "    gui.circles(pos, radius=5, palette=[0x068587, 0xED553B, 0xEEEEF0], palette_indices=indices)\n",
    "    gui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc8a5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "\n",
    "ti.init(arch=ti.cuda)\n",
    "\n",
    "N = 10\n",
    "\n",
    "particles_pos = ti.Vector.field(3, dtype=ti.f32, shape = N)\n",
    "points_pos = ti.Vector.field(3, dtype=ti.f32, shape = N)\n",
    "\n",
    "@ti.kernel\n",
    "def init_points_pos(points : ti.template()):\n",
    "    for i in range(points.shape[0]):\n",
    "        points[i] = [i for j in ti.static(range(3))]\n",
    "\n",
    "init_points_pos(particles_pos)\n",
    "init_points_pos(points_pos)\n",
    "\n",
    "window = ti.ui.Window(\"Test for Drawing 3d-lines\", (768, 768))\n",
    "canvas = window.get_canvas()\n",
    "scene = ti.ui.Scene()\n",
    "camera = ti.ui.Camera()\n",
    "camera.position(5, 2, 2)\n",
    "\n",
    "while window.running:\n",
    "    camera.track_user_inputs(window, movement_speed=0.03, hold_key=ti.ui.RMB)\n",
    "    scene.set_camera(camera)\n",
    "    scene.ambient_light((0.8, 0.8, 0.8))\n",
    "    scene.point_light(pos=(0.5, 1.5, 1.5), color=(1, 1, 1))\n",
    "\n",
    "    scene.particles(particles_pos, color = (0.68, 0.26, 0.19), radius = 0.1)\n",
    "    # Draw 3d-lines in the scene\n",
    "    scene.lines(points_pos, color = (0.28, 0.68, 0.99), width = 5.0)\n",
    "    canvas.scene(scene)\n",
    "    window.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00f8f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.4.1, llvm 15.0.1, commit e67c674e, win, python 3.9.15\n",
      "[Taichi] Starting on arch=cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Window close button clicked, exiting... (use `while gui.running` to exit gracefully)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_12432\\2703488757.py\"\u001b[0m, line \u001b[0;32m29\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    gui.show()\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\taichi\\ui\\gui.py\"\u001b[1;36m, line \u001b[1;32m711\u001b[1;36m, in \u001b[1;35mshow\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.core.update()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m\u001b[1;31m:\u001b[0m Window close button clicked, exiting... (use `while gui.running` to exit gracefully)\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\n",
    "ti.init(arch=ti.gpu)\n",
    "\n",
    "n = 320\n",
    "pixels = ti.field(dtype=float, shape=(n * 2, n))\n",
    "\n",
    "@ti.func\n",
    "def complex_sqr(z):  # complex square of a 2D vector\n",
    "    return tm.vec2(z[0] * z[0] - z[1] * z[1], 2 * z[0] * z[1])\n",
    "\n",
    "@ti.kernel\n",
    "def paint(t: float):\n",
    "    for i, j in pixels:  # Parallelized over all pixels\n",
    "        c = tm.vec2(-0.8, tm.cos(t) * 0.2)\n",
    "        z = tm.vec2(i / n - 1, j / n - 0.5) * 2\n",
    "        iterations = 0\n",
    "        while z.norm() < 20 and iterations < 50:\n",
    "            z = complex_sqr(z) + c\n",
    "            iterations += 1\n",
    "        pixels[i, j] = 1 - iterations * 0.02\n",
    "\n",
    "gui = ti.GUI(\"Julia Set\", res=(n * 2, n))\n",
    "\n",
    "for i in range(1000000):\n",
    "    paint(i * 0.03)\n",
    "    gui.set_image(pixels)\n",
    "    gui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b405a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "\n",
    "#it lacks the full paremterizan of above cell. That my be the cause of the error\n",
    "\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "config = QMixConfig()  \n",
    "config = config.training(gamma=0.9, lr=0.01)#, kl_coeff=0.3)  \n",
    "#config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=12)  \n",
    "print(config.to_dict())  \n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "register_env(\n",
    "    \"grouped_twostep\",\n",
    "    lambda config: TwoStepGame(config).with_agent_groups(\n",
    "        grouping, obs_space=obs_space, act_space=act_space\n",
    "    ),\n",
    ")\n",
    "\n",
    "algo = config.build(env=\"grouped_twostep\")  \n",
    "algo.train() "
   ]
  },
  {
   "cell_type": "raw",
   "id": "980a7fc4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
