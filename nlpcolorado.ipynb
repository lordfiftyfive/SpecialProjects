{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:32:36.496990Z","iopub.execute_input":"2025-05-02T18:32:36.497528Z","iopub.status.idle":"2025-05-02T18:32:36.508479Z","shell.execute_reply.started":"2025-05-02T18:32:36.497494Z","shell.execute_reply":"2025-05-02T18:32:36.507354Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install keras-core --upgrade\n!pip install -q keras-nlp --upgrade\n\n# This sample uses Keras Core, the multi-backend version of Keras.\n# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\nimport os\nos.environ['KERAS_BACKEND'] = 'tensorflow'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:07:57.059750Z","iopub.execute_input":"2025-05-02T18:07:57.060113Z","iopub.status.idle":"2025-05-02T18:08:05.358093Z","shell.execute_reply.started":"2025-05-02T18:07:57.060090Z","shell.execute_reply":"2025-05-02T18:08:05.356231Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras-core in /usr/local/lib/python3.11/dist-packages (0.1.7)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras-core) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-core) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras-core) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras-core) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-core) (3.12.1)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras-core) (0.1.9)\nRequirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras-core) (25.3.0)\nRequirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras-core) (1.17.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->keras-core) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->keras-core) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->keras-core) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->keras-core) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->keras-core) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->keras-core) (2.4.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras-core) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras-core) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->keras-core) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->keras-core) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->keras-core) (2024.2.0)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:32:40.405715Z","iopub.execute_input":"2025-05-02T18:32:40.406199Z","iopub.status.idle":"2025-05-02T18:32:40.414982Z","shell.execute_reply.started":"2025-05-02T18:32:40.406171Z","shell.execute_reply":"2025-05-02T18:32:40.413955Z"}},"outputs":[{"name":"stdout","text":"TensorFlow version: 2.18.0\nKerasNLP version: 0.20.0\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"My task here is to take various tweets in text format and using NLP and deep learning, carry out binary classification to determine whether they are referencing a disaster or not. \n\nmy data cleaning procedure is a simple dropna in the even any rows have missing data. \n\nI believe that an LSTM based architecture would be suitable because the long term memory aspect of LSTMs allow for long term dependence of some tokens to affect the interpretationa and final classification of the tweets. \n\nI could in the future try to implement dimensionality reductions schemes like Isomap. I am also curious if applying LDA and including the subsequent output could improve the model. ","metadata":{}},{"cell_type":"code","source":"import keras_tuner\nimport keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:32:44.349309Z","iopub.execute_input":"2025-05-02T18:32:44.349624Z","iopub.status.idle":"2025-05-02T18:32:44.354487Z","shell.execute_reply.started":"2025-05-02T18:32:44.349597Z","shell.execute_reply":"2025-05-02T18:32:44.353124Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:32:48.664739Z","iopub.execute_input":"2025-05-02T18:32:48.665161Z","iopub.status.idle":"2025-05-02T18:32:48.715046Z","shell.execute_reply.started":"2025-05-02T18:32:48.665134Z","shell.execute_reply":"2025-05-02T18:32:48.713610Z"}},"outputs":[{"name":"stdout","text":"Training Set Shape = (7613, 5)\nTraining Set Memory Usage = 0.29 MB\nTest Set Shape = (3263, 4)\nTest Set Memory Usage = 0.10 MB\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"#data cleaning \ndf_train.dropna(axis=0,how='any')\nprint(df_train.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:32:51.077639Z","iopub.execute_input":"2025-05-02T18:32:51.078358Z","iopub.status.idle":"2025-05-02T18:32:51.091220Z","shell.execute_reply.started":"2025-05-02T18:32:51.078331Z","shell.execute_reply":"2025-05-02T18:32:51.089919Z"}},"outputs":[{"name":"stdout","text":"(7613, 5)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_TRAINING_EXAMPLES = df_train.shape[0]\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.2\nSTEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n\nEPOCHS = 2\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:32:52.847266Z","iopub.execute_input":"2025-05-02T18:32:52.847603Z","iopub.status.idle":"2025-05-02T18:32:52.853456Z","shell.execute_reply.started":"2025-05-02T18:32:52.847573Z","shell.execute_reply":"2025-05-02T18:32:52.852200Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_train[\"text\"]\ny = df_train[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_test = df_test[\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:32:55.229445Z","iopub.execute_input":"2025-05-02T18:32:55.229837Z","iopub.status.idle":"2025-05-02T18:32:55.239666Z","shell.execute_reply.started":"2025-05-02T18:32:55.229807Z","shell.execute_reply":"2025-05-02T18:32:55.238736Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"print(y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:16:36.191187Z","iopub.execute_input":"2025-05-01T16:16:36.192022Z","iopub.status.idle":"2025-05-01T16:16:36.197569Z","shell.execute_reply.started":"2025-05-01T16:16:36.191990Z","shell.execute_reply":"2025-05-01T16:16:36.196578Z"}},"outputs":[{"name":"stdout","text":"(7613,)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(y_train.shape)\n\nprint(X_train.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:17:57.681939Z","iopub.execute_input":"2025-05-01T16:17:57.682373Z","iopub.status.idle":"2025-05-01T16:17:57.688638Z","shell.execute_reply.started":"2025-05-01T16:17:57.682344Z","shell.execute_reply":"2025-05-01T16:17:57.687564Z"}},"outputs":[{"name":"stdout","text":"(6090,)\n(6090,)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:27:18.360461Z","iopub.status.idle":"2025-04-30T00:27:18.360923Z","shell.execute_reply.started":"2025-04-30T00:27:18.360716Z","shell.execute_reply":"2025-04-30T00:27:18.360735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(strip_accents=None,lowercase=False)\n\n#vectorizer.get_feature_names_out()\nprint(X.shape)\nxn = np.array(X_train)\n#this single line after specifying the text data will do all the preprocessing. \nX_train_preprocessed = vectorizer.fit_transform(xn)#preprocessor(X_train)\n\nX_val = vectorizer.fit_transform(X_val)\n#x_val = preprocessor(X_val)\n\nprint(X_train_preprocessed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T17:57:52.820546Z","iopub.execute_input":"2025-05-02T17:57:52.820897Z","iopub.status.idle":"2025-05-02T17:57:53.046185Z","shell.execute_reply.started":"2025-05-02T17:57:52.820873Z","shell.execute_reply":"2025-05-02T17:57:53.045176Z"}},"outputs":[{"name":"stdout","text":"(7613,)\n<Compressed Sparse Row sparse matrix of dtype 'float64'\n\twith 89916 stored elements and shape (6090, 22138)>\n  Coords\tValues\n  (0, 10974)\t0.2982535459993856\n  (0, 13383)\t0.054526920564421634\n  (0, 15960)\t0.12914874584715366\n  (0, 20436)\t0.2848486273381596\n  (0, 19107)\t0.2848486273381596\n  (0, 17412)\t0.20872378881345313\n  (0, 5846)\t0.24504456728569277\n  (0, 5266)\t0.2848486273381596\n  (0, 212)\t0.26193276752714595\n  (0, 16174)\t0.08432504935013056\n  (0, 2217)\t0.2390169077161323\n  (0, 1692)\t0.21897769600090522\n  (0, 21257)\t0.2122070703936803\n  (0, 20838)\t0.08484082575971075\n  (0, 17742)\t0.18990918366499138\n  (0, 17979)\t0.08515419869656861\n  (0, 12060)\t0.2982535459993856\n  (0, 15898)\t0.2848486273381596\n  (0, 12066)\t0.0947001431289348\n  (0, 3056)\t0.2982535459993856\n  (1, 12774)\t0.3356155786735253\n  (1, 15044)\t0.22147763598121362\n  (1, 14602)\t0.23365063602303468\n  (1, 12526)\t0.3015276599656971\n  (1, 13095)\t0.3205314343476497\n  :\t:\n  (6088, 18854)\t0.2612498294588477\n  (6088, 19547)\t0.204761146733252\n  (6088, 7778)\t0.2536626063134879\n  (6088, 16361)\t0.12020686362359159\n  (6088, 15959)\t0.06394802604898339\n  (6088, 12251)\t0.13772609518859955\n  (6088, 13383)\t0.06026614903112499\n  (6088, 16174)\t0.09320067846464569\n  (6089, 14439)\t0.3861923543344302\n  (6089, 17580)\t0.34696743625354814\n  (6089, 15032)\t0.31729493119628605\n  (6089, 3964)\t0.3268473442199061\n  (6089, 12539)\t0.2031152235262055\n  (6089, 21937)\t0.24671853302653315\n  (6089, 11947)\t0.2835423521351627\n  (6089, 20803)\t0.22827741602449986\n  (6089, 11297)\t0.23338645688730675\n  (6089, 21615)\t0.2855337984511892\n  (6089, 20839)\t0.23782982904811986\n  (6089, 15709)\t0.1910235641707581\n  (6089, 15959)\t0.07491740557293457\n  (6089, 16373)\t0.15638373312357054\n  (6089, 13383)\t0.07060395462129336\n  (6089, 20838)\t0.10985578774606544\n  (6089, 17979)\t0.11026155738030168\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"e = X_train_preprocessed \ncoo = e.tocoo()\nindices = np.mat([coo.row, coo.col]).transpose()\ne = indices\ne = tf.SparseTensor(indices, coo.data, coo.shape)\ne = tf.sparse.reorder(e)\ne = tf.sparse.to_dense(e)\ne = pd.DataFrame(e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:18:26.120722Z","iopub.execute_input":"2025-05-01T16:18:26.121035Z","iopub.status.idle":"2025-05-01T16:18:27.745606Z","shell.execute_reply.started":"2025-05-01T16:18:26.121012Z","shell.execute_reply":"2025-05-01T16:18:27.743661Z"}},"outputs":[{"name":"stderr","text":"2025-05-01 16:18:26.139343: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"f = X_val \ncoo = f.tocoo()\nindices = np.mat([coo.row, coo.col]).transpose()\nf = indices\nf = tf.SparseTensor(indices, coo.data, coo.shape)\nf = tf.sparse.reorder(f)\nf = tf.sparse.to_dense(f)\nf = pd.DataFrame(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:19:15.274761Z","iopub.execute_input":"2025-05-01T16:19:15.275200Z","iopub.status.idle":"2025-05-01T16:19:15.481925Z","shell.execute_reply.started":"2025-05-01T16:19:15.275167Z","shell.execute_reply":"2025-05-01T16:19:15.480268Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(strip_accents=None,lowercase=False)\n\n#vectorizer.get_feature_names_out()\n\n#this single line after specifying the text data will do all the preprocessing. \nX_test = vectorizer.fit_transform(X_test)#preprocessor(X_train)\n\nX_test = X_test \n\ncoo = X_test.tocoo()\nindices = np.mat([coo.row, coo.col]).transpose()\nX_test = indices\nX_test = tf.SparseTensor(X_test, coo.data, coo.shape)\nX_test = tf.sparse.reorder(X_test)\nX_test  = tf.sparse.to_dense(X_test)\nX_test = pd.DataFrame(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T17:57:57.498899Z","iopub.execute_input":"2025-05-02T17:57:57.499268Z","iopub.status.idle":"2025-05-02T17:57:57.974676Z","shell.execute_reply.started":"2025-05-02T17:57:57.499236Z","shell.execute_reply":"2025-05-02T17:57:57.973687Z"}},"outputs":[{"name":"stderr","text":"2025-05-02 17:57:57.589867: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:27:18.368103Z","iopub.status.idle":"2025-04-30T00:27:18.368483Z","shell.execute_reply.started":"2025-04-30T00:27:18.368335Z","shell.execute_reply":"2025-04-30T00:27:18.368354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import Isomap\n\n#X = Isomap.fit_transform(e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:00:07.259144Z","iopub.execute_input":"2025-05-02T18:00:07.259489Z","iopub.status.idle":"2025-05-02T18:00:07.429693Z","shell.execute_reply.started":"2025-05-02T18:00:07.259466Z","shell.execute_reply":"2025-05-02T18:00:07.428659Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=25,random_state=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:27:18.370911Z","iopub.status.idle":"2025-04-30T00:27:18.371183Z","shell.execute_reply.started":"2025-04-30T00:27:18.371056Z","shell.execute_reply":"2025-04-30T00:27:18.371069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#this gives back topics. This\n\n#lda = LatentDirichletAllocation(n_components=5,random_state=0)\n#lda.fit(X_train_preprocessed['token_ids'])\n#LatentDirichletAllocation(...)\n# get topics for some given samples:\nx_trainl = lda.transform(X_train_preprocessed['token_ids'])\nX_vall = lda.transform(x_val['token_ids'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:26:41.006096Z","iopub.status.idle":"2025-05-01T08:26:41.006777Z","shell.execute_reply.started":"2025-05-01T08:26:41.006426Z","shell.execute_reply":"2025-05-01T08:26:41.006455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import Isomap\n\n#x_trainl = lda.transform(X_train_preprocessed['token_ids'])\n#X_vall = lda.transform(x_val['token_ids'])\n\n\n#xx = Isomap.fit_transform(X[:100])\n#x = xx.transform(x)\n#l= xx.transform(l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:27:18.373753Z","iopub.status.idle":"2025-04-30T00:27:18.374028Z","shell.execute_reply.started":"2025-04-30T00:27:18.373904Z","shell.execute_reply":"2025-04-30T00:27:18.373915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.Input(shape=(25798,), dtype=tf.float64),#4360# 25\n    tf.keras.layers.Reshape((25798, 1)),\n    tf.keras.layers.LSTM(25, kernel_initializer='ones', use_bias=False, dtype=tf.float64),\n    #tf.keras.layers.Dense(25,kernel_initializer='ones',use_bias=False),\n    #tf.keras.layers.InputLayer(input_shape=(10),dtype=x.dtype),#put a 1 before the 9 later\n    tf.keras.layers.Dense(50,kernel_initializer='ones', use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(100,kernel_initializer='ones', use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(175,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(250,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(275,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(300,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(275,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(250,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n    #goal is to eventually replace the first dense layer with an LSTM layer\n    #tf.keras.layers.LSTM\n    #tf.keras.layers.TimeDistributed(Dense(vocabulary)))\n    #tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(100,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(50,kernel_initializer='ones',use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(25,kernel_initializer='ones',use_bias=False,),\n    tf.keras.layers.Dense(1,activation='sigmoid', kernel_regularizer='l1',kernel_initializer='ones',use_bias=False,)\n])\nmodel.compile(optimizer='Adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'])\nmodel.fit(x=e,y=y,epochs=1750,validation_split=0.1)#batch_size=BATCH_SIZE,validation_data=(X_vall, y_val)\n\nPrediction = model.predict(l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:26:41.007888Z","iopub.status.idle":"2025-05-01T08:26:41.008310Z","shell.execute_reply.started":"2025-05-01T08:26:41.008150Z","shell.execute_reply":"2025-05-01T08:26:41.008164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\nimport numpy as np\n\n# Parameters\nmax_vocab_size = 10000\nsequence_length = 100\n\n# Create text vectorizer\nvectorizer = TextVectorization(max_tokens=max_vocab_size, output_sequence_length=sequence_length)\n\n# Fit on training text\nvectorizer.adapt(X_train)  # text_data is a list/array of raw tweets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:33:07.149468Z","iopub.execute_input":"2025-05-02T18:33:07.150949Z","iopub.status.idle":"2025-05-02T18:33:07.306234Z","shell.execute_reply.started":"2025-05-02T18:33:07.150900Z","shell.execute_reply":"2025-05-02T18:33:07.304231Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def build_model(hp=None):\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(1,), dtype=tf.string),  # Input is raw text\n\n        # Text vectorization layer\n        vectorizer,\n\n        # Embedding layer maps tokens to dense vectors\n        tf.keras.layers.Embedding(input_dim=max_vocab_size, output_dim=128),\n\n        # LSTM to process sequence\n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n\n        tf.keras.layers.Dense(50,kernel_initializer='ones',activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]), use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(100,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(175,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(250,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(275,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(300,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(275,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(250,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n        #goal is to eventually replace the first dense layer with an LSTM layer\n        #tf.keras.layers.LSTM\n        #tf.keras.layers.TimeDistributed(Dense(vocabulary)))\n        #tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(100,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(50,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(25,kernel_initializer='ones',use_bias=False,),\n        tf.keras.layers.Dense(1,activation='sigmoid', kernel_regularizer='l1',kernel_initializer='ones',use_bias=False,)\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        metrics=['accuracy']\n    )\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:33:10.720159Z","iopub.execute_input":"2025-05-02T18:33:10.720503Z","iopub.status.idle":"2025-05-02T18:33:10.737848Z","shell.execute_reply.started":"2025-05-02T18:33:10.720481Z","shell.execute_reply":"2025-05-02T18:33:10.736857Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"model = build_model()\n#model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:00:39.360164Z","iopub.execute_input":"2025-05-02T18:00:39.360442Z","iopub.status.idle":"2025-05-02T18:01:21.606043Z","shell.execute_reply.started":"2025-05-02T18:00:39.360423Z","shell.execute_reply":"2025-05-02T18:01:21.604173Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6767 - loss: 210.1533","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_30/865552007.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    543\u001b[0m                         \u001b[0;34m\"When using `TextVectorization` to tokenize strings, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                         \u001b[0;34m\"the input rank must be 1 or the last shape dimension \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling TextVectorization.call().\n\n\u001b[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 8338) with rank=2\u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(None, 8338), dtype=string)"],"ename":"ValueError","evalue":"Exception encountered when calling TextVectorization.call().\n\n\u001b[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 8338) with rank=2\u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(None, 8338), dtype=string)","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"tuner = keras_tuner.RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=5,\n    directory='my_dir',\n    project_name='my_project',\n    overwrite=True\n)\ntuner.search(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\nbest_hp = tuner.get_best_hyperparameters(1)[0]\nprint(best_hp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:33:24.847135Z","iopub.execute_input":"2025-05-02T18:33:24.847976Z","iopub.status.idle":"2025-05-02T18:37:28.079613Z","shell.execute_reply.started":"2025-05-02T18:33:24.847930Z","shell.execute_reply":"2025-05-02T18:37:28.078666Z"}},"outputs":[{"name":"stdout","text":"Trial 2 Complete [00h 02m 01s]\nval_loss: 7.566357612609863\n\nBest val_loss So Far: 2.833733081817627\nTotal elapsed time: 00h 04m 03s\n<keras_tuner.src.engine.hyperparameters.hyperparameters.HyperParameters object at 0x79ad3ad8a2d0>\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"best_model = build_model(best_hp)\n\n# Optional: retrain the best model using all data or just validation set\nbest_model.fit(X_train, y_train, epochs=1, validation_data=(X_val, y_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:37:53.600117Z","iopub.execute_input":"2025-05-02T18:37:53.600522Z","iopub.status.idle":"2025-05-02T18:38:40.615849Z","shell.execute_reply.started":"2025-05-02T18:37:53.600496Z","shell.execute_reply":"2025-05-02T18:38:40.615075Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 101ms/step - accuracy: 0.6534 - loss: 172.6660 - val_accuracy: 0.5883 - val_loss: 942.2491\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x79ad3ececa50>"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission.head()\n\nsample_submission[\"target\"] = np.argmax(best_model.predict(X_test), axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:13:13.630964Z","iopub.execute_input":"2025-05-02T18:13:13.632141Z","iopub.status.idle":"2025-05-02T18:13:18.681520Z","shell.execute_reply.started":"2025-05-02T18:13:13.632109Z","shell.execute_reply":"2025-05-02T18:13:18.680553Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:16:31.648732Z","iopub.execute_input":"2025-05-02T18:16:31.649444Z","iopub.status.idle":"2025-05-02T18:16:31.659865Z","shell.execute_reply.started":"2025-05-02T18:16:31.649417Z","shell.execute_reply":"2025-05-02T18:16:31.658919Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def build_model(hp): \n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(1,25798), dtype=tf.float64),#4360# 25\n        #tf.keras.layers.Reshape((25798)),\n        tf.keras.layers.LSTM(25, kernel_initializer='ones', use_bias=False, dtype=tf.float64),\n        #tf.keras.layers.Dense(25,kernel_initializer='ones',use_bias=False),\n        #tf.keras.layers.InputLayer(input_shape=(10),dtype=x.dtype),#put a 1 before the 9 later\n        tf.keras.layers.Dense(50,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(100,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(175,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(250,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(275,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(300,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(275,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(250,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\n        #goal is to eventually replace the first dense layer with an LSTM layer\n        #tf.keras.layers.LSTM\n        #tf.keras.layers.TimeDistributed(Dense(vocabulary)))\n        #tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(100,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(50,kernel_initializer='ones',use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(25,kernel_initializer='ones',use_bias=False,),\n        tf.keras.layers.Dense(1,activation='sigmoid', kernel_regularizer='l1',kernel_initializer='ones',use_bias=False,)\n    ])\n    model.compile(optimizer='Adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:30:16.469960Z","iopub.execute_input":"2025-05-01T16:30:16.470414Z","iopub.status.idle":"2025-05-01T16:30:16.488538Z","shell.execute_reply.started":"2025-05-01T16:30:16.470384Z","shell.execute_reply":"2025-05-01T16:30:16.487504Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"build_model(keras_tuner.HyperParameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:31:34.616219Z","iopub.execute_input":"2025-05-01T17:31:34.616560Z","iopub.status.idle":"2025-05-01T17:31:34.788005Z","shell.execute_reply.started":"2025-05-01T17:31:34.616535Z","shell.execute_reply":"2025-05-01T17:31:34.786972Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<Sequential name=sequential, built=True>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"tuner = keras_tuner.RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=5,\n    directory='my_dir',\n    project_name='my_project',\n    overwrite=True\n)\ntuner.search(e, y_train, epochs=5, validation_data=(f, y_val))\nbest_hp = tuner.get_best_hyperparameters(1)[0]\nbest_model = build_model(best_hp)\n\n# Optional: retrain the best model using all data or just validation set\nbest_model.fit(e, y_train, epochs=5, validation_data=(f, y_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:31:37.118506Z","iopub.execute_input":"2025-05-01T17:31:37.118794Z","iopub.status.idle":"2025-05-01T17:31:37.245845Z","shell.execute_reply.started":"2025-05-01T17:31:37.118775Z","shell.execute_reply":"2025-05-01T17:31:37.244472Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/4213841208.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mbest_hp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_hp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"],"ename":"NameError","evalue":"name 'e' is not defined","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"tunerh = keras_tuner.Hyperband(\n    hypermodel=build_model,\n    objective=\"val_accuracy\",\n    max_epochs=2,\n    factor=3,\n    hyperband_iterations=1,\n    #distribution_strategy=tf.distribute.MirroredStrategy(),\n    #directory=\"results_dir\",\n    #project_name=\"mnist\",\n    #overwrite=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:40:49.908234Z","iopub.execute_input":"2025-05-01T08:40:49.908608Z","iopub.status.idle":"2025-05-01T08:40:49.952019Z","shell.execute_reply.started":"2025-05-01T08:40:49.908586Z","shell.execute_reply":"2025-05-01T08:40:49.950203Z"}},"outputs":[{"name":"stdout","text":"Reloading Tuner from ./untitled_project/tuner0.json\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_oracle_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/stateful.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/tuners/hyperband.py\u001b[0m in \u001b[0;36mset_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperband_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hyperband_iterations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'hyperband_iterations'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2602670636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tunerh = keras_tuner.Hyperband(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mhypermodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/tuners/hyperband.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hypermodel, objective, max_epochs, factor, hyperband_iterations, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         )\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypermodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             )\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0moracle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mhypermodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tuner_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reloading Tuner from {self._get_tuner_fname()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# Only populate initial space if not reloading.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;34m\"\"\"Reloads this object from its project directory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tuner_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_oracle_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    762\u001b[0m                 \u001b[0;34m\"Error reloading `Oracle` from existing project. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                 \u001b[0;34m\"If you did not mean to reload from an existing project, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error reloading `Oracle` from existing project. If you did not mean to reload from an existing project, change the `project_name` or pass `overwrite=True` when creating the `Tuner`. Found existing project at: ./untitled_project"],"ename":"RuntimeError","evalue":"Error reloading `Oracle` from existing project. If you did not mean to reload from an existing project, change the `project_name` or pass `overwrite=True` when creating the `Tuner`. Found existing project at: ./untitled_project","output_type":"error"}],"execution_count":47},{"cell_type":"code","source":"tuner.search(\n    e,#x_train,\n    y_train,\n    steps_per_epoch=600,\n    validation_data=(f, y_val),\n    validation_steps=100,\n    callbacks=[keras.callbacks.EarlyStopping(\"val_accuracy\")],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:40:55.001896Z","iopub.execute_input":"2025-05-01T08:40:55.002268Z","iopub.status.idle":"2025-05-01T08:40:55.009890Z","shell.execute_reply.started":"2025-05-01T08:40:55.002234Z","shell.execute_reply":"2025-05-01T08:40:55.008389Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"tuner.search(e, y_train, epochs=2, validation_data=(f, y_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:40:59.292658Z","iopub.execute_input":"2025-05-01T08:40:59.292993Z","iopub.status.idle":"2025-05-01T08:40:59.299057Z","shell.execute_reply.started":"2025-05-01T08:40:59.292971Z","shell.execute_reply":"2025-05-01T08:40:59.297954Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"#NLP_1_model = tuner.get_best_models(num_models=1)#[0]\n#NLP_1_model.summary()\n#print(NLP_1_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:34:11.443513Z","iopub.execute_input":"2025-05-01T08:34:11.444649Z","iopub.status.idle":"2025-05-01T08:34:11.450423Z","shell.execute_reply.started":"2025-05-01T08:34:11.444600Z","shell.execute_reply":"2025-05-01T08:34:11.449075Z"}},"outputs":[{"name":"stdout","text":"[]\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"models = tuner.get_best_models(num_models=2)\nbest_model = models[0]\nbest_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:41:02.192193Z","iopub.execute_input":"2025-05-01T08:41:02.192505Z","iopub.status.idle":"2025-05-01T08:41:03.514218Z","shell.execute_reply.started":"2025-05-01T08:41:02.192484Z","shell.execute_reply":"2025-05-01T08:41:03.512594Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1192383524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mget_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \"\"\"\n\u001b[1;32m    399\u001b[0m         \u001b[0;31m# Method only exists in this class for the docstring override.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36mget_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[1;32m    365\u001b[0m         \u001b[0mbest_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_trials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[1;32m    365\u001b[0m         \u001b[0mbest_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_trials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# Only load weights to avoid loading `custom_objects`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_distribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_checkpoint_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = './untitled_project/trial_0/checkpoint.weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"],"ename":"FileNotFoundError","evalue":"[Errno 2] Unable to synchronously open file (unable to open file: name = './untitled_project/trial_0/checkpoint.weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"import numpy as np\nfrom sklearn import random_projection\nX = np.random.rand(100, 10000)\nprint(X.shape)# random projection can reduce the shape of something from 100,10000 to 100,3947. \n#memory requirements can be reduced further if it is sparse projection\n\n#transformer = random_projection.SparseRandomProjection()\n#X_new = transformer.fit_transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:27:18.378761Z","iopub.status.idle":"2025-04-30T00:27:18.379136Z","shell.execute_reply.started":"2025-04-30T00:27:18.378944Z","shell.execute_reply":"2025-04-30T00:27:18.378959Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"note: Do NOT use Transformer based models. USe a custom LSTM neural network based architecture to satisfy the rubric\n\nUse a tfidf vectorizer instead. Look into possibly using word2vec as well.  \n\nShould also determine whether isomap, or TSNE would be effective. \n\nList of confirmed core preprocessing: \n\nstandard scalar, vectorizer\n\nAlso required by assignment: hyperparameter tuning different architectures \n\nList of preprocessing things to consider: \n\n, LDA, random projection, isomap or TSNE\n\nalso implement gaussian random projection\n\nGOals accomplished\n\nneural network with LSTM is working\n\nvectorizer is working\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}