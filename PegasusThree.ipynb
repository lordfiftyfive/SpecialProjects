{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994ab004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "from ray.rllib.examples.env.gpu_requiring_env import GPURequiringEnv\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "if we do this carefully we can use taichi to carry out speedup\n",
    "\n",
    "The mixer and the bmodel would be ti.funcs\n",
    "\n",
    "qmixpolicy would also be a ti.func\n",
    "\n",
    "qmix would be the ti.kernel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import torch\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "#from nebulgym.decorators.torch_decorators import accel\n",
    "\n",
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "#from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE, make_multi_agent\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "\n",
    "import nitime\n",
    "from deeptime.sindy import SINDy\n",
    "\n",
    "#counterfactual Causal inference\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers # helper functions\n",
    "\n",
    "#data visualization\n",
    "import pygwalker as pyg\n",
    "\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2001f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmodel(design):\n",
    "\n",
    "    # This line allows batching of designs, treating all batch dimensions as independent\n",
    "    with pyro.plate_stack(\"plate_stack\", design.shape):\n",
    "\n",
    "        # We use a dirchlet prior for theta\n",
    "        theta = pyro.sample(\"theta\", dist.Normal(torch.tensor(0.0), torch.tensor(1.0)))\n",
    "        #theta = pyro.sample(\"theta\", dist.Dirichlet())\n",
    "        # We use a simple logistic regression model for the likelihood\n",
    "        logit_p = theta - design\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p))\n",
    "\n",
    "        return y\n",
    "\n",
    "eig = nmc_eig(pmodel, design, observation_labels=[\"y\"], target_labels=[\"theta\"], N=2500, M=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1164000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pmodel(polling_allocation):\n",
    "    # This allows us to run many copies of the model in parallel\n",
    "    with pyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "        # Begin by sampling alpha\n",
    "        alpha = pyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "            prior_mean, covariance_matrix=prior_covariance))\n",
    "\n",
    "        # Sample y conditional on alpha\n",
    "        poll_results = pyro.sample(\"y\", dist.Binomial(\n",
    "            polling_allocation, logits=alpha).to_event(1))\n",
    "\n",
    "        # Now compute w according to the (approximate) electoral college formula\n",
    "        dem_win = election_winner(alpha)\n",
    "        pyro.sample(\"w\", dist.Delta(dem_win))\n",
    "\n",
    "        return poll_results, dem_win, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutcomePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbe9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_entropy = dist.Bernoulli(prior_w_prob).entropy()\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "poll_in_florida = torch.zeros(51)\n",
    "poll_in_florida[9] = 1000\n",
    "\n",
    "poll_in_dc = torch.zeros(51)\n",
    "poll_in_dc[8] = 1000\n",
    "\n",
    "uniform_poll = (1000 // 51) * torch.ones(51)\n",
    "\n",
    "# The swing score measures how close the state is to 50/50\n",
    "swing_score = 1. / (.5 - torch.tensor(prior_prob_dem.sort_values(\"State\").values).squeeze()).abs()\n",
    "swing_poll = 1000 * swing_score / swing_score.sum()\n",
    "swing_poll = swing_poll.round()\n",
    "\n",
    "poll_strategies = OrderedDict([(\"Florida\", poll_in_florida),\n",
    "                               (\"DC\", poll_in_dc),\n",
    "                               (\"Uniform\", uniform_poll),\n",
    "                               (\"Swing\", swing_poll)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "\n",
    "eigs = {}\n",
    "best_strategy, best_eig = None, 0\n",
    "\n",
    "for strategy, allocation in poll_strategies.items():\n",
    "    print(strategy, end=\" \")\n",
    "    guide = OutcomePredictor()\n",
    "    pyro.clear_param_store()\n",
    "    # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "    # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "    # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "    ape = posterior_eig(pmodel, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "    eigs[strategy] = prior_entropy - ape\n",
    "    print(eigs[strategy].item())\n",
    "    if eigs[strategy] > best_eig:\n",
    "        best_strategy, best_eig = strategy, eigs[strategy]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e017468",
   "metadata": {},
   "source": [
    "For the ESM the observations are stuff from observation space, x is the reward\n",
    "\n",
    "The epistemic value has form expectation of entropy of p(outcomes given states)-Q(outcomes given policy) \n",
    "\n",
    "pragmatic value has lnP(outcomes|c)\n",
    "\n",
    "APE = expected entropy of probability theta given observations and design\n",
    "\n",
    "Theta is q as defined by the guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849df685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create this like the outcome predictor. This is the expected free energy sub-module ESM\n",
    "#this will essentially calculate the q(x|y)\n",
    "class ESM(nn.Module,ivy.Module):\n",
    "     def __init__(self):\n",
    "        super().__init__()\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior_entropy = dist.Bernoulli(prior_w_prob).entropy()\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        #model = tf.keras.Sequential([\n",
    "        u = tfkl.InputLayer(input_shape=input_shape),\n",
    "        \"\"\"\n",
    "        u = tf.keras.layers.LSTM(25,kernel_initializer='zeros',activation='tanh', dtype = x.dtype, use_bias=True)(u),\n",
    "        u = tfp.layers.VariationalGaussianProcess(\n",
    "                num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\n",
    "                inducing_index_points_initializer=tf.compat.v1.constant_initializer(\n",
    "                    np.linspace(0,x_range, num=1125,\n",
    "                                dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))), mean_fn=None,\n",
    "                jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)(u)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    #in unconstrained thing replace astype with tf.dtype thing.    #tf.initializers.constant(-10.0)\n",
    "    #])\n",
    "    def compute_probability(self, y):\n",
    "        #fk.Model()\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        #o = tf.nn.relu(u)\n",
    "        return self.h3(z)\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        \n",
    "        #bmodel = FullLaplace(bmodel,'regression',prior_precision=2)\n",
    "        dem_prob = self.compute_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d54d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob)).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7074a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.oed.eig import marginal_eig\n",
    "def ESM(design, observation_labels, target_labels):\n",
    "    # This shape allows us to learn a different parameter for each candidate design l\n",
    "    q_logit = pyro.param(\"q_logit\", torch.zeros(design.shape[-2:])).entropy()\n",
    "    pyro.sample(\"y\", dist.Bernoulli(logits=q_logit).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class emodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "\n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        guide = ESM()# marginal_guide = posterior predictive entropy d can also be expressed as pi \n",
    "        pyro.clear_param_store()\n",
    "        # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "        # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "        # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)] where w = x  \n",
    "        with pyro.plate_stack(\"plate\", l.shape[:-1]):\n",
    "            theta = pyro.sample(\"theta\", dist.Normal(0, 1))\n",
    "            # Share theta across the number of rounds of the experiment\n",
    "            # This represents repeatedly testing the same participant\n",
    "            theta = theta.unsqueeze(-1)\n",
    "            # This define a *logistic regression* model for y\n",
    "            logit_p = sensitivity * (theta - l)\n",
    "            # The event shape represents responses from the same participant\n",
    "            y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n",
    "            return y\n",
    "            #ape = average posterior entropy\n",
    "        \"\"\"\n",
    "        ape = posterior_eig(model, allocation, \"y\", \"x\", 10, 12500, guide,\n",
    "                            Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "        eigs[strategy] = prior_entropy - ape\n",
    "        print(eigs[strategy].item())\n",
    "        \"\"\"\n",
    "        if eigs[strategy] > best_eig:\n",
    "            best_strategy, best_eig = strategy, eigs[strategy]\n",
    "        \n",
    "        \n",
    "        eig = marginal_eig(model,\n",
    "                           candidate_designs,       # design, or in this case, tensor of possible designs\n",
    "                           \"y\",                     # site label of observations, could be a list\n",
    "                           \"theta\",                 # site label of 'targets' (latent variables), could also be list\n",
    "                           num_samples=100,         # number of samples to draw per step in the expectation\n",
    "                           num_steps=num_steps,     # number of gradient steps\n",
    "                           guide=marginal_guide,    # guide q(y)\n",
    "                           optim=optimizer,         # optimizer with learning rate decay\n",
    "                           final_num_samples=10000  # at the last step, we draw more samples\n",
    "                                                    # for a more accurate EIG estimate\n",
    "                          )\n",
    "        \"\"\"\n",
    "        expected free energy is minimized when when observations are slected that cause a large change in beliefs \n",
    "        \n",
    "        Expected free energy will consist of 3 components\n",
    "        \n",
    "        ESM -> q(y|d)\n",
    "        |\n",
    "        emodels -> pragmatic value neural network. This is going to be neural network for log(p(y|prior))\n",
    "        |\n",
    "        EFE -> emodels + (epistemic value = ESM - expected ambiguity) -> expected free energy  \n",
    "        \n",
    "        pragmatic value will be calculated using a variational gaussian process regression tensorflow neural network \n",
    "        \n",
    "        We will actually calculate both epistemic value and define the neural network for pragmatic value in emodels\n",
    "        \n",
    "        but inputs, parameterization and outputs of pragmatic value network will happen on EFE\n",
    "        \n",
    "        Next objective: determine the probaility of hidden state given observation\n",
    "        \n",
    "        we should use a neural network to calculate the probability of reward given action \n",
    "        \n",
    "        if there is 0 correlation, that is to say reward does not change given a certain action then p(s|y) = 0\n",
    "        \n",
    "        pragmatic value observations expecteation of log probability of outcomes given prior \n",
    "        \n",
    "        theta = s=w=x is the hidden latent variable state. Here it is going to be the reward\n",
    "        \n",
    "        design is going to be action, outcome is observation, state is the reward\n",
    "        \n",
    "        also -H(P(s)) = E(lnP(s))\n",
    "        \n",
    "        in EFE we are going to use a relaxed bernoulli with temperature for distribution of entropy of p(o|s)\n",
    " \n",
    "        average predictive entropy = H(posteririor pdf) = entropy of entire epsitemic value \n",
    "        \n",
    "        for our purposes p(theta) = q(theta|design)\n",
    "        \n",
    "        pragmatic value is -E(ln(p(y|prior)) which could be converted to prior entropy \n",
    "        \n",
    "        we will implement enhanced expected free energy later to include learning. \n",
    "        \n",
    "        we for now can set the pragmatic value to 0 \n",
    "        \n",
    "        NOte: to clear up confusion theta can refer to states or to beliefs about parameters in model \n",
    "        \n",
    "        later for enhanced expected free energy we will have to include theta side by side with states outcomes,\n",
    "        designs and prior \n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        #q = self.fc2(h)\n",
    "        vae = tfk.Model(inputs=encoder(input_dict[\"obs_flat\"]),\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n",
    "ivy.set_framework('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da6747ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "#@ti.func\n",
    "class bmodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        #under advice from Tasha and Dimitrov and given what we know about\n",
    "        #long term dependencce int he brain we are using LSTMs for neuronal agents\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        \n",
    "        #tf.keras.Input(shape=(1,19), dtype=x.dtype),\n",
    "        #tf.keras.layers.RNN(ltc_cell, return_sequences=True),\n",
    "        #x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(25,kernel_initializer='zeros',activation='tanh', dtype = x.dtype, use_bias=True)),\n",
    "        #Attention(50)#note: only use attention when Gaussian process layer is not being used\n",
    "        #tf.keras.layers.InputLayer(input_shape=(10),dtype=x.dtype),#put a 1 before the 9 later\n",
    "        #x = tf.keras.layers.BatchNormalization(),\n",
    "        #x = tf.keras.layers.Dense(50,kernel_initializer='zeros', use_bias=False),\n",
    "        #x = tf.keras.layers.Dropout(0.1),\n",
    "        #x = tf.keras.layers.BatchNormalization(),\n",
    "        #x = tf.keras.layers.Dense(1,activation='elu',kernel_initializer='zeros', use_bias=False),\n",
    "        \"\"\"\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(priora, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))#nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        #\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))\n",
    "        \n",
    "        h = self.rnn(x,h_in)\n",
    "        q = self.fc2(h)\n",
    "        #h = self.rnn(x, h_in)\n",
    "        #q = self.fc2(h)\n",
    "        \"\"\"\n",
    "        vae = tfk.Model(inputs=encoder(input_dict[\"obs_flat\"]),\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        \"\"\"\n",
    "        kl = tf.keras.losses.KLDivergence()\n",
    "        model.add_loss(kl)\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "496698dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\"\"\"\n",
    "next objective for april 8th 2023: we are going to have to modify the mixer network \n",
    "\n",
    "The mixer network is going to be a bayesian tensorflow network with a -ELBO builtin loss and a kullbacker leibeler loss\n",
    "that will be seen in the QMIX loss function\n",
    "\n",
    "We will have a configurable parameter that depending on whether the policy is high or low will have either -elbo or \n",
    "kullbacker leibler as builtin losses with a generic bayesian neural network . \n",
    "\n",
    "finally we will have emodel which will have a neural network guide\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#@ti.func\n",
    "class QMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape, mixing_embed_dim):\n",
    "        super(QMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.embed_dim = mixing_embed_dim\n",
    "        self.state_dim = int(np.prod(state_shape))\n",
    "\n",
    "        self.hyper_w_1 = nn.Linear(self.state_dim, self.embed_dim * self.n_agents)#tfkl.Embedding(self.state_dim,  self.embed_dim * self.n_agents)#\n",
    "        self.hyper_w_final = nn.Linear(self.state_dim, self.embed_dim)#tfkl.Embedding(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # State dependent bias for hidden layer\n",
    "        self.hyper_b_1 = nn.Linear(self.state_dim, self.embed_dim)#tfkl.Embedding(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # V(s) instead of a bias for the last layers\n",
    "        self.V = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.embed_dim),#tfkl.Embedding(self.state_dim, self.embed_dim)\n",
    "            nn.ReLU(),\n",
    "            # tf.nn.relu\n",
    "            nn.Linear(self.embed_dim, 1),#tfkl.Embedding(self.embed_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, agent_qs, states):\n",
    "        \"\"\"Forward pass for the mixer.\n",
    "        Args:\n",
    "            agent_qs: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            states: Tensor of shape [B, T, state_dim]\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        nn.Sequential(\n",
    "            torch.abs(self.hyper_w_1(states))\n",
    "            self.hyper_b_1(states)\n",
    "            nn.functional.elu(torch.bmm(agent_qs, w1) + b1)\n",
    "            torch.abs(self.hyper_w_final(states))\n",
    "            self.V\n",
    "            \n",
    "        )\n",
    "        tfkl.InputLayer(self.state_dim,self.embed_dim*self.n_agents)\n",
    "        \n",
    "        self.hyper_w_1 = tf.keras.activations.linear(self.state_dim, self.embed_dim * self.n_agents)\n",
    "        \n",
    "        tfkl.Embedding(1000, 64)#, input_length=10)\n",
    "        \n",
    "        x.numpy()\n",
    "        \n",
    "        #batch matrix multiplication\n",
    "        tf.matmul()\n",
    "        \n",
    "        model = keras.Model(inputs=[states, agent_qs, tags_input], outputs=output\n",
    "        \n",
    "        \n",
    "        #################################################################################################\n",
    "        w1 = tf.math.abs(tfkl.Embedding(self.state_dim,  self.embed_dim * self.n_agents)(states))\n",
    "        b1 = tfkl.Embedding(self.state_dim, self.embed_dim)(states)\n",
    "        w1 = tf.reshape(w1, [-1, self.n_agents, self.embed_dim])\n",
    "        b1 = tf.reshape(b1,[-1, 1, self.embed_dim])\n",
    "        hidden = tf.nn.elu(tf.matmul(agent_qs, w1)+ b1)\n",
    "        w_final = tf.math.abs(tfkl.Embedding(self.state_dim, self.embed_dim)(states))\n",
    "        w_final = tf.reshape(-1, self.embed_dim, 1)\n",
    "        v =  tfkl.Embedding(self.V(states),-1, 1, 1)\n",
    "        y = tf.matmul(hidden, w_final) + v\n",
    "        q_tot = tf.reshape(y,[bs, -1, 1])\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        bs = agent_qs.size(0)\n",
    "        states = states.reshape(-1, self.state_dim)#tf.keras.layers.Reshape((3, 4), input_shape=(12,))\n",
    "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
    "        # First layer\n",
    "        \n",
    "        w1 = torch.abs(self.hyper_w_1(states))\n",
    "        \n",
    "        b1 = self.hyper_b_1(states)\n",
    "        w1 = w1.view(-1, self.n_agents, self.embed_dim)\n",
    "        b1 = b1.view(-1, 1, self.embed_dim)\n",
    "        hidden = nn.functional.elu(torch.bmm(agent_qs, w1) + b1)#tf.nn.elu(tf.matmul(agent_qs, w1)+ b1)\n",
    "        # Second layer #tf.nn.elu()\n",
    "        w_final = torch.abs(self.hyper_w_final(states))\n",
    "        w_final = w_final.view(-1, self.embed_dim, 1)\n",
    "        # State-dependent bias\n",
    "        v = self.V(states).view(-1, 1, 1)\n",
    "        # Compute final output\n",
    "        y = torch.bmm(hidden, w_final) + v#tf.matmul(hidden, w_final)\n",
    "        \n",
    "        # Reshape and return\n",
    "        q_tot = y.view(bs, -1, 1)\n",
    "        return q_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4f7ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dadaptation import DAdaptAdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6a03c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMixLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        target_model,\n",
    "        mixer,\n",
    "        target_mixer,\n",
    "        n_agents,\n",
    "        n_actions,\n",
    "        double_q=True,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.mixer = mixer\n",
    "        self.target_mixer = target_mixer\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        self.double_q = double_q\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rewards,\n",
    "        actions,\n",
    "        terminated,\n",
    "        mask,\n",
    "        obs,\n",
    "        next_obs,\n",
    "        action_mask,\n",
    "        next_action_mask,\n",
    "        state=None,\n",
    "        next_state=None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the loss.\n",
    "        Args:\n",
    "            rewards: Tensor of shape [B, T, n_agents]\n",
    "            actions: Tensor of shape [B, T, n_agents]\n",
    "            terminated: Tensor of shape [B, T, n_agents]\n",
    "            mask: Tensor of shape [B, T, n_agents]\n",
    "            obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            next_obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            next_action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            state: Tensor of shape [B, T, state_dim] (optional)\n",
    "            next_state: Tensor of shape [B, T, state_dim] (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert either none or both of state and next_state are given\n",
    "        if state is None and next_state is None:\n",
    "            state = obs  # default to state being all agents' observations\n",
    "            next_state = next_obs\n",
    "        elif (state is None) != (next_state is None):\n",
    "            raise ValueError(\n",
    "                \"Expected either neither or both of `state` and \"\n",
    "                \"`next_state` to be given. Got: \"\n",
    "                \"\\n`state` = {}\\n`next_state` = {}\".format(state, next_state)\n",
    "            )\n",
    "\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = _unroll_mac(self.model, obs)\n",
    "\n",
    "        # Pick the Q-Values for the actions taken -> [B * n_agents, T]\n",
    "        chosen_action_qvals = torch.gather(\n",
    "            mac_out, dim=3, index=actions.unsqueeze(3)\n",
    "        ).squeeze(3)\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = _unroll_mac(self.target_model, next_obs)\n",
    "\n",
    "        # Mask out unavailable actions for the t+1 step\n",
    "        ignore_action_tp1 = (next_action_mask == 0) & (mask == 1).unsqueeze(-1)\n",
    "        target_mac_out[ignore_action_tp1] = -np.inf\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.double_q:\n",
    "            # Double Q learning computes the target Q values by selecting the\n",
    "            # t+1 timestep action according to the \"policy\" neural network and\n",
    "            # then estimating the Q-value of that action with the \"target\"\n",
    "            # neural network\n",
    "            \n",
    "            #target neural network does expected free energy while policy\n",
    "            #neural network will be variational free energy\n",
    "\n",
    "            # Compute the t+1 Q-values to be used in action selection\n",
    "            # using next_obs\n",
    "            mac_out_tp1 = _unroll_mac(self.model, next_obs)\n",
    "\n",
    "            # mask out unallowed actions\n",
    "            mac_out_tp1[ignore_action_tp1] = -np.inf\n",
    "\n",
    "            # obtain best actions at t+1 according to policy NN\n",
    "            cur_max_actions = mac_out_tp1.argmax(dim=3, keepdim=True)\n",
    "\n",
    "            # use the target network to estimate the Q-values of policy\n",
    "            # network's selected actions\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(\n",
    "                3\n",
    "            )\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "        assert (\n",
    "            target_max_qvals.min().item() != -np.inf\n",
    "        ), \"target_max_qvals contains a masked action; \\\n",
    "            there may be a state with no valid actions.\"\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state)\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, next_state)\n",
    "\n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n",
    "        \"\"\"\n",
    "        \n",
    "        guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        elbo = elbo_(model, guide)\n",
    "        \n",
    "        el = elbo(data)\n",
    "        \n",
    "        #rewards = {self.agent_1:kullbacker, self.agent_2: el}\n",
    "        #loss = lambda y, rv_y: rv_y.variational_loss(y, kl_weight=np.array(batch_size, x.dtype) / x.shape[0])\n",
    "                \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Td-error\n",
    "        #we need to replace this with a variational free energy error\n",
    "        td_error = chosen_action_qvals - targets.detach()\n",
    "        te_error= tf.keras.losses.KLDivergence(chosen_action_qvals - targets.detach()).numpy()\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)#this is ELBO \n",
    "        \n",
    "        #we are going to ahve a kldivergence loss and a -ELBO regularizer for the mixer network \n",
    "        \n",
    "        mask = mask.expand_as(te_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = te_error * mask\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        \n",
    "        #guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        #elbo_ = pyro.infer.Trace_ELBO(num_particles=1)\n",
    "\n",
    "        # Fix the model/guide pair\n",
    "        #elbo = elbo_(model, guide)        \n",
    "        \n",
    "        #data = obs + preds \n",
    "        #loss = -ln()\n",
    "        #loss = lambda y, rv_y: -rv_y.log_prob(y)\n",
    "        loss = masked_error#(masked_td_error**2).sum() / mask.sum()\n",
    "        return loss, mask, masked_td_error, chosen_action_qvals, targets\n",
    "\n",
    "    \n",
    "#this part just above is what we need to revise\n",
    "    \n",
    "#@ti.func\n",
    "class QMixTorchPolicy(TorchPolicy):\n",
    "    \"\"\"QMix impl. Assumes homogeneous agents for now.\n",
    "    You must use MultiAgentEnv.with_agent_groups() to group agents\n",
    "    together for QMix. This creates the proper Tuple obs/action spaces and\n",
    "    populates the '_group_rewards' info field.\n",
    "    Action masking: to specify an action mask for individual agents, use a\n",
    "    dict space with an action_mask key, e.g. {\"obs\": ob, \"action_mask\": mask}.\n",
    "    The mask space must be `Box(0, 1, (n_actions,))`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        # We want to error out on instantiation and not on import, because tune\n",
    "        # imports all RLlib algorithms when registering them\n",
    "        # TODO (Artur): Find a way to only import algorithms when needed\n",
    "        if not torch:\n",
    "            raise ImportError(\"Could not import PyTorch, which QMix requires.\")\n",
    "\n",
    "        _validate(obs_space, action_space)\n",
    "        config = dict(ray.rllib.algorithms.qmix.qmix.DEFAULT_CONFIG, **config)\n",
    "        self.framework = \"torch\"\n",
    "\n",
    "        self.n_agents = 3000#len(obs_space.original_space.spaces)\n",
    "        config[\"model\"][\"n_agents\"] = self.n_agents\n",
    "        self.n_actions = action_space.spaces[0].n\n",
    "        self.h_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "        self.has_env_global_state = False\n",
    "        self.has_action_mask = False\n",
    "\n",
    "        agent_obs_space = obs_space.original_space.spaces[0]\n",
    "        if isinstance(agent_obs_space, gym.spaces.Dict):\n",
    "            space_keys = set(agent_obs_space.spaces.keys())\n",
    "            if \"obs\" not in space_keys:\n",
    "                raise ValueError(\"Dict obs space must have subspace labeled `obs`\")\n",
    "            self.obs_size = _get_size(agent_obs_space.spaces[\"obs\"])\n",
    "            if \"action_mask\" in space_keys:\n",
    "                mask_shape = tuple(agent_obs_space.spaces[\"action_mask\"].shape)\n",
    "                if mask_shape != (self.n_actions,):\n",
    "                    raise ValueError(\n",
    "                        \"Action mask shape must be {}, got {}\".format(\n",
    "                            (self.n_actions,), mask_shape\n",
    "                        )\n",
    "                    )\n",
    "                self.has_action_mask = True\n",
    "            if ENV_STATE in space_keys:\n",
    "                self.env_global_state_shape = _get_size(\n",
    "                    agent_obs_space.spaces[ENV_STATE]\n",
    "                )\n",
    "                self.has_env_global_state = True\n",
    "            else:\n",
    "                self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            # The real agent obs space is nested inside the dict\n",
    "            config[\"model\"][\"full_obs_space\"] = agent_obs_space\n",
    "            agent_obs_space = agent_obs_space.spaces[\"obs\"]\n",
    "        else:\n",
    "            self.obs_size = _get_size(agent_obs_space)\n",
    "            self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "        #model = bmodel()#CModel()#CModel()\n",
    "        #bmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#\n",
    "        ivy.set_framework('torch')\n",
    "        #model = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')\n",
    "        bmodel = bmodel()\n",
    "        bmodel = FullLaplace(bmodel,'regression',prior_precision=0.00000000000000000000001)#0.00000000000000000000001)#10000000000000000000000000)\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"model\",\n",
    "            default_model=bmodel#a#RNNModel#bmodel()#RNNModel,\n",
    "        )\n",
    "\n",
    "        super().__init__(obs_space, action_space, config, model=self.model)\n",
    "\n",
    "        self.target_model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"target_model\",\n",
    "            default_model=bmodel#bmodel()#RNNModel\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.exploration = self._create_exploration()\n",
    "        \n",
    "        # Setup the mixer network.\n",
    "        if config[\"mixer\"] is None:\n",
    "            self.mixer = None\n",
    "            self.target_mixer = None\n",
    "        elif config[\"mixer\"] == \"qmix\":\n",
    "            self.mixer = FullLaplace(QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ),prior_precision=1).to(self.device)\n",
    "            self.target_mixer = FullLaplace(QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ),prior_precision=1).to(self.device)\n",
    "\n",
    "        self.cur_epsilon = 1.0\n",
    "        self.update_target()  # initial sync\n",
    "\n",
    "        # Setup optimizer\n",
    "        self.params = list(self.model.parameters())\n",
    "        if self.mixer:\n",
    "            self.params += list(self.mixer.parameters())\n",
    "        self.loss = QMixLoss(\n",
    "            self.model,\n",
    "            self.target_model,\n",
    "            self.mixer,\n",
    "            self.target_mixer,\n",
    "            self.n_agents,\n",
    "            self.n_actions,\n",
    "            self.config[\"double_q\"],\n",
    "            self.config[\"gamma\"],\n",
    "        )\n",
    "        from torch.optim import RMSprop\n",
    "        \n",
    "        self.rmsprop_optimizer = RMSprop(\n",
    "            params=self.params,\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"optim_alpha\"],\n",
    "            eps=config[\"optim_eps\"],\n",
    "        )#replace with dadptation \n",
    "        self.rs_prop_optimizer = DAdaptAdaGrad(\n",
    "            params=self.params,\n",
    "            alpha=config(\"optim_alpha\"),\n",
    "            eps = config[\"optim_eps\"],\n",
    "        \n",
    "        )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions_from_input_dict(\n",
    "        self,\n",
    "        input_dict: Dict[str, TensorType],\n",
    "        explore: bool = None,\n",
    "        timestep: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:\n",
    "\n",
    "        obs_batch = input_dict[SampleBatch.OBS]\n",
    "        state_batches = []\n",
    "        i = 0\n",
    "        while f\"state_in_{i}\" in input_dict:\n",
    "            state_batches.append(input_dict[f\"state_in_{i}\"])\n",
    "            i += 1\n",
    "\n",
    "        explore = explore if explore is not None else self.config[\"explore\"]\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        # We need to ensure we do not use the env global state\n",
    "        # to compute actions\n",
    "\n",
    "        # Compute actions\n",
    "        with torch.no_grad():\n",
    "            q_values, hiddens = _mac(\n",
    "                self.model,\n",
    "                torch.as_tensor(obs_batch, dtype=torch.float, device=self.device),\n",
    "                [\n",
    "                    torch.as_tensor(np.array(s), dtype=torch.float, device=self.device)\n",
    "                    for s in state_batches\n",
    "                ],\n",
    "            )\n",
    "            avail = torch.as_tensor(action_mask, dtype=torch.float, device=self.device)\n",
    "            masked_q_values = q_values.clone()\n",
    "            masked_q_values[avail == 0.0] = -float(\"inf\")\n",
    "            masked_q_values_folded = torch.reshape(\n",
    "                masked_q_values, [-1] + list(masked_q_values.shape)[2:]\n",
    "            )\n",
    "            actions, _ = self.exploration.get_exploration_action(\n",
    "                action_distribution=TorchCategorical(masked_q_values_folded),\n",
    "                timestep=timestep,\n",
    "                explore=explore,\n",
    "            )\n",
    "            actions = (\n",
    "                torch.reshape(actions, list(masked_q_values.shape)[:-1]).cpu().numpy()\n",
    "            )\n",
    "            hiddens = [s.cpu().numpy() for s in hiddens]\n",
    "\n",
    "        return tuple(actions.transpose([1, 0])), hiddens, {}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions(self, *args, **kwargs):\n",
    "        return self.compute_actions_from_input_dict(*args, **kwargs)\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "    ):\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        return np.zeros(obs_batch.size()[0])\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        obs_batch, action_mask, env_global_state = self._unpack_observation(\n",
    "            samples[SampleBatch.CUR_OBS]\n",
    "        )\n",
    "        (\n",
    "            next_obs_batch,\n",
    "            next_action_mask,\n",
    "            next_env_global_state,\n",
    "        ) = self._unpack_observation(samples[SampleBatch.NEXT_OBS])\n",
    "        group_rewards = self._get_group_rewards(samples[SampleBatch.INFOS])\n",
    "\n",
    "        input_list = [\n",
    "            group_rewards,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            samples[SampleBatch.ACTIONS],\n",
    "            samples[SampleBatch.TERMINATEDS],\n",
    "            obs_batch,\n",
    "            next_obs_batch,\n",
    "        ]\n",
    "        if self.has_env_global_state:\n",
    "            input_list.extend([env_global_state, next_env_global_state])\n",
    "\n",
    "        output_list, _, seq_lens = chop_into_sequences(\n",
    "            episode_ids=samples[SampleBatch.EPS_ID],\n",
    "            unroll_ids=samples[SampleBatch.UNROLL_ID],\n",
    "            agent_indices=samples[SampleBatch.AGENT_INDEX],\n",
    "            feature_columns=input_list,\n",
    "            state_columns=[],  # RNN states not used here\n",
    "            max_seq_len=self.config[\"model\"][\"max_seq_len\"],\n",
    "            dynamic_max=True,\n",
    "        )\n",
    "        # These will be padded to shape [B * T, ...]\n",
    "        if self.has_env_global_state:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "                env_global_state,\n",
    "                next_env_global_state,\n",
    "            ) = output_list\n",
    "        else:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "            ) = output_list\n",
    "        B, T = len(seq_lens), max(seq_lens)\n",
    "\n",
    "        def to_batches(arr, dtype):\n",
    "            new_shape = [B, T] + list(arr.shape[1:])\n",
    "            return torch.as_tensor(\n",
    "                np.reshape(arr, new_shape), dtype=dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        rewards = to_batches(rew, torch.float)\n",
    "        actions = to_batches(act, torch.long)\n",
    "        obs = to_batches(obs, torch.float).reshape([B, T, self.n_agents, self.obs_size])\n",
    "        action_mask = to_batches(action_mask, torch.float)\n",
    "        next_obs = to_batches(next_obs, torch.float).reshape(\n",
    "            [B, T, self.n_agents, self.obs_size]\n",
    "        )\n",
    "        next_action_mask = to_batches(next_action_mask, torch.float)\n",
    "        if self.has_env_global_state:\n",
    "            env_global_state = to_batches(env_global_state, torch.float)\n",
    "            next_env_global_state = to_batches(next_env_global_state, torch.float)\n",
    "\n",
    "        # TODO(ekl) this treats group termination as individual termination\n",
    "        terminated = (\n",
    "            to_batches(terminateds, torch.float)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Create mask for where index is < unpadded sequence length\n",
    "        filled = np.reshape(\n",
    "            np.tile(np.arange(T, dtype=np.float32), B), [B, T]\n",
    "        ) < np.expand_dims(seq_lens, 1)\n",
    "        mask = (\n",
    "            torch.as_tensor(filled, dtype=torch.float, device=self.device)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss_out, mask, masked_td_error, chosen_action_qvals, targets = self.loss(\n",
    "            rewards,\n",
    "            actions,\n",
    "            terminated,\n",
    "            mask,\n",
    "            obs,\n",
    "            next_obs,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            env_global_state,\n",
    "            next_env_global_state,\n",
    "        )\n",
    "\n",
    "        # Optimise\n",
    "        self.rmsprop_optimizer.zero_grad()\n",
    "\n",
    "        loss_out.backward()\n",
    "        grad_norm_info = apply_grad_clipping(self, self.rmsprop_optimizer, loss_out)\n",
    "        self.rmsprop_optimizer.step()\n",
    "\n",
    "        mask_elems = mask.sum().item()\n",
    "        stats = {\n",
    "            \"loss\": loss_out.item(),\n",
    "            \"td_error_abs\": masked_td_error.abs().sum().item() / mask_elems,\n",
    "            \"q_taken_mean\": (chosen_action_qvals * mask).sum().item() / mask_elems,\n",
    "            \"target_mean\": (targets * mask).sum().item() / mask_elems,\n",
    "        }\n",
    "        stats.update(grad_norm_info)\n",
    "\n",
    "        return {LEARNER_STATS_KEY: stats}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_initial_state(self):  # initial RNN state\n",
    "        return [\n",
    "            s.expand([self.n_agents, -1]).cpu().numpy()\n",
    "            for s in self.model.get_initial_state()\n",
    "        ]\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            \"model\": self._cpu_dict(self.model.state_dict()),\n",
    "            \"target_model\": self._cpu_dict(self.target_model.state_dict()),\n",
    "            \"mixer\": self._cpu_dict(self.mixer.state_dict()) if self.mixer else None,\n",
    "            \"target_mixer\": self._cpu_dict(self.target_mixer.state_dict())\n",
    "            if self.mixer\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(self._device_dict(weights[\"model\"]))\n",
    "        self.target_model.load_state_dict(self._device_dict(weights[\"target_model\"]))\n",
    "        if weights[\"mixer\"] is not None:\n",
    "            self.mixer.load_state_dict(self._device_dict(weights[\"mixer\"]))\n",
    "            self.target_mixer.load_state_dict(\n",
    "                self._device_dict(weights[\"target_mixer\"])\n",
    "            )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_state(self):\n",
    "        state = self.get_weights()\n",
    "        state[\"cur_epsilon\"] = self.cur_epsilon\n",
    "        return state\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_state(self, state):\n",
    "        self.set_weights(state)\n",
    "        self.set_epsilon(state[\"cur_epsilon\"])\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        logger.debug(\"Updated target networks\")\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cur_epsilon = epsilon\n",
    "\n",
    "    def _get_group_rewards(self, info_batch):\n",
    "        group_rewards = np.array(\n",
    "            [info.get(GROUP_REWARDS, [0.0] * self.n_agents) for info in info_batch]\n",
    "        )\n",
    "        return group_rewards\n",
    "\n",
    "    def _device_dict(self, state_dict):\n",
    "        return {\n",
    "            k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _cpu_dict(state_dict):\n",
    "        return {k: v.cpu().detach().numpy() for k, v in state_dict.items()}\n",
    "\n",
    "    def _unpack_observation(self, obs_batch):\n",
    "        \"\"\"Unpacks the observation, action mask, and state (if present)\n",
    "        from agent grouping.\n",
    "        Returns:\n",
    "            obs (np.ndarray): obs tensor of shape [B, n_agents, obs_size]\n",
    "            mask (np.ndarray): action mask, if any\n",
    "            state (np.ndarray or None): state tensor of shape [B, state_size]\n",
    "                or None if it is not in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        unpacked = _unpack_obs(\n",
    "            np.array(obs_batch, dtype=np.float32),\n",
    "            self.observation_space.original_space,\n",
    "            tensorlib=np,\n",
    "        )\n",
    "\n",
    "        if isinstance(unpacked[0], dict):\n",
    "            assert \"obs\" in unpacked[0]\n",
    "            unpacked_obs = [np.concatenate(tree.flatten(u[\"obs\"]), 1) for u in unpacked]\n",
    "        else:\n",
    "            unpacked_obs = unpacked\n",
    "\n",
    "        obs = np.concatenate(unpacked_obs, axis=1).reshape(\n",
    "            [len(obs_batch), self.n_agents, self.obs_size]\n",
    "        )\n",
    "\n",
    "        if self.has_action_mask:\n",
    "            action_mask = np.concatenate(\n",
    "                [o[\"action_mask\"] for o in unpacked], axis=1\n",
    "            ).reshape([len(obs_batch), self.n_agents, self.n_actions])\n",
    "        else:\n",
    "            action_mask = np.ones(\n",
    "                [len(obs_batch), self.n_agents, self.n_actions], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.has_env_global_state:\n",
    "            state = np.concatenate(tree.flatten(unpacked[0][ENV_STATE]), 1)\n",
    "        else:\n",
    "            state = None\n",
    "        return obs, action_mask, state\n",
    "\n",
    "#@ti.func\n",
    "def _validate(obs_space, action_space):\n",
    "    if not hasattr(obs_space, \"original_space\") or not isinstance(\n",
    "        obs_space.original_space, gym.spaces.Tuple\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Obs space must be a Tuple, got {}. Use \".format(obs_space)\n",
    "            + \"MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space, gym.spaces.Tuple):\n",
    "        raise ValueError(\n",
    "            \"Action space must be a Tuple, got {}. \".format(action_space)\n",
    "            + \"Use MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space.spaces[0], gym.spaces.Discrete):\n",
    "        raise ValueError(\n",
    "            \"QMix requires a discrete action space, got {}\".format(\n",
    "                action_space.spaces[0]\n",
    "            )\n",
    "        )\n",
    "    if len({str(x) for x in obs_space.original_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: observations of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(obs_space.original_space.spaces)\n",
    "        )\n",
    "    if len({str(x) for x in action_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: action space of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(action_space.spaces)\n",
    "        )\n",
    "\n",
    "#@ti.func\n",
    "def _mac(model, obs, h):\n",
    "    \"\"\"Forward pass of the multi-agent controller.\n",
    "    Args:\n",
    "        model: TorchModelV2 class\n",
    "        obs: Tensor of shape [B, n_agents, obs_size]\n",
    "        h: List of tensors of shape [B, n_agents, h_size]\n",
    "    Returns:\n",
    "        q_vals: Tensor of shape [B, n_agents, n_actions]\n",
    "        h: Tensor of shape [B, n_agents, h_size]\n",
    "    \"\"\"\n",
    "    B, n_agents = obs.size(0), obs.size(1)\n",
    "    if not isinstance(obs, dict):\n",
    "        obs = {\"obs\": obs}\n",
    "    obs_agents_as_batches = {k: _drop_agent_dim(v) for k, v in obs.items()}\n",
    "    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n",
    "    q_flat, h_flat = model(obs_agents_as_batches, h_flat, None)\n",
    "    return q_flat.reshape([B, n_agents, -1]), [\n",
    "        s.reshape([B, n_agents, -1]) for s in h_flat\n",
    "    ]\n",
    "#@ti.func\n",
    "def _unroll_mac(model, obs_tensor):\n",
    "    \"\"\"Computes the estimated Q values for an entire trajectory batch\"\"\"\n",
    "    B = obs_tensor.size(0)\n",
    "    T = obs_tensor.size(1)\n",
    "    n_agents = obs_tensor.size(2)\n",
    "\n",
    "    mac_out = []\n",
    "    h = [s.expand([B, n_agents, -1]) for s in model.get_initial_state()]\n",
    "    for t in range(T):\n",
    "        q, h = _mac(model, obs_tensor[:, t], h)\n",
    "        mac_out.append(q)\n",
    "    mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "    return mac_out\n",
    "#@ti.func\n",
    "def _drop_agent_dim(T):\n",
    "    shape = list(T.shape)\n",
    "    B, n_agents = shape[0], shape[1]\n",
    "    return T.reshape([B * n_agents] + shape[2:])\n",
    "#@ti.func\n",
    "def _add_agent_dim(T, n_agents):\n",
    "    shape = list(T.shape)\n",
    "    B = shape[0] // n_agents\n",
    "    assert shape[0] % n_agents == 0\n",
    "    return T.reshape([B, n_agents] + shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6aeb0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass TwoStepGameWithGroupedAgents(MultiAgentEnv):\\n    def __init__(self, env_config):\\n        super().__init__()\\n        env = TwoStepGame(env_config)\\n        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\\n        tuple_act_space = Tuple([env.action_space, env.action_space])\\n\\n        self.env = env.with_agent_groups(\\n            groups={\"agents\": [0, 1]},\\n            obs_space=tuple_obs_space,\\n            act_space=tuple_act_space,\\n        )\\n        self.observation_space = self.env.observation_space\\n        self.action_space = self.env.action_space\\n        self._agent_ids = {\"agents\"}\\n\\n    def reset(self, *, seed=None, options=None):\\n        return self.env.reset(seed=seed, options=options)\\n\\n    def step(self, actions):\\n        return self.env.step(actions)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TwoStepGame(MultiAgentEnv):\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(2)\n",
    "        self.state = None\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        #self.agent_3=2\n",
    "        self._skip_env_checking = True\n",
    "        gpus_available = ray.get_gpu_ids()\n",
    "        # MADDPG emits action logits instead of actual discrete actions\n",
    "        self.actions_are_logits = env_config.get(\"actions_are_logits\", False)\n",
    "        self.one_hot_state_encoding = env_config.get(\"one_hot_state_encoding\", False)\n",
    "        self.with_state = env_config.get(\"separate_state_space\", False)\n",
    "        self._agent_ids = {0, 1}\n",
    "        if not self.one_hot_state_encoding:\n",
    "            self.observation_space = Discrete(6)\n",
    "            self.with_state = False\n",
    "        else:\n",
    "            # Each agent gets the full state (one-hot encoding of which of the\n",
    "            # three states are active) as input with the receiving agent's\n",
    "            # ID (1 or 2) concatenated onto the end.\n",
    "            if self.with_state:\n",
    "                self.observation_space = Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.observation_space = MultiDiscrete([2, 2, 2, 3])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = np.array([1, 0, 0])\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "\n",
    "        state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "        \n",
    "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
    "        obs = self._obs()\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        truncateds = {\"__all__\": False}\n",
    "        infos = {\n",
    "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
    "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "        }\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "\n",
    "    def _obs(self):\n",
    "        if self.with_state:\n",
    "            return {\n",
    "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
    "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
    "            }\n",
    "        else:\n",
    "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
    "\n",
    "    def agent_1_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [1]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0]\n",
    "\n",
    "    def agent_2_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [2]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0] + 3\n",
    "\n",
    "        #if self.render_mode == \"rgb_array\":\n",
    "            #return self._render_frame()\n",
    "\n",
    "\"\"\"\n",
    "class TwoStepGameWithGroupedAgents(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        env = TwoStepGame(env_config)\n",
    "        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\n",
    "        tuple_act_space = Tuple([env.action_space, env.action_space])\n",
    "\n",
    "        self.env = env.with_agent_groups(\n",
    "            groups={\"agents\": [0, 1]},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self._agent_ids = {\"agents\"}\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        return self.env.reset(seed=seed, options=options)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEnv(MultiAgentEnv):\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        print(\"asdf\")\n",
    "    #maps env states to observations. we may want to have observations that are correlated to but not the same as env st\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        terminated = 0\n",
    "        reward = 1 if terminated else 0  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec12a627",
   "metadata": {},
   "source": [
    "1.finish modifying mixer network - status: basically complete\n",
    "2. visualization - status: ongoing\n",
    "3. expected free energy module Status: 80% done \n",
    "4. integrate reloadium to reduce testing time \n",
    "\n",
    "note: there are actually two mixers: a regular mixer network and a target mixer network. self.target_mixer and self.mixer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "604357ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class EFE(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \n",
    "    In our case we will be this is where we will create the pragmatic value and together with the epistemic value\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a EFE Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        eig = marginal_eig(model,\n",
    "                           candidate_designs,       # design, or in this case, tensor of possible designs\n",
    "                           \"y\",                     # site label of observations, could be a list\n",
    "                           \"theta\",                 # site label of 'targets' (latent variables), could also be list\n",
    "                           num_samples=100,         # number of samples to draw per step in the expectation\n",
    "                           num_steps=num_steps,     # number of gradient steps\n",
    "                           guide=marginal_guide,    # guide q(y)\n",
    "                           optim=optimizer,         # optimizer with learning rate decay\n",
    "                           final_num_samples=10000  # at the last step, we draw more samples\n",
    "                                                    # for a more accurate EIG estimate\n",
    "                          )\n",
    "        #self.model  is the first agent model we use to get the first q value used for model selection\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37d644b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our custom replacement for softq in explore config\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class SoftQ(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a SoftQ Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9980dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig, NotProvided\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    "    SAMPLE_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "#@ti.kernel\n",
    "class QMixConfig(SimpleQConfig):\n",
    "    \"\"\"Defines a configuration class from which QMix can be built.\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> config = QMixConfig()  # doctest: +SKIP\n",
    "        >>> config = config.training(gamma=0.9, lr=0.01, kl_coeff=0.3)  # doctest: +SKIP\n",
    "        >>> config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "        >>> config = config.rollouts(num_rollout_workers=4)  # doctest: +SKIP\n",
    "        >>> print(config.to_dict())  # doctest: +SKIP\n",
    "        >>> # Build an Algorithm object from the config and run 1 training iteration.\n",
    "        >>> algo = config.build(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> algo.train()  # doctest: +SKIP\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> from ray import air\n",
    "        >>> from ray import tune\n",
    "        >>> config = QMixConfig()\n",
    "        >>> # Print out some default values.\n",
    "        >>> print(config.optim_alpha)  # doctest: +SKIP\n",
    "        >>> # Update the config object.\n",
    "        >>> config.training(  # doctest: +SKIP\n",
    "        ...     lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97\n",
    "        ... )\n",
    "        >>> # Set the config object's env.\n",
    "        >>> config.environment(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> # Use to_dict() to get the old-style python config dict\n",
    "        >>> # when running with tune.\n",
    "        >>> tune.Tuner(  # doctest: +SKIP\n",
    "        ...     \"QMix\",\n",
    "        ...     run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "        ...     param_space=config.to_dict(),\n",
    "        ... ).fit()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PPOConfig instance.\"\"\"\n",
    "        super().__init__(algo_class=QMix)\n",
    "\n",
    "        # fmt: off\n",
    "        # __sphinx_doc_begin__\n",
    "        # QMix specific settings:\n",
    "        self.mixer = \"qmix\"\n",
    "        self.mixing_embed_dim = 32\n",
    "        self.double_q = True\n",
    "        self.optim_alpha = 0.99\n",
    "        self.optim_eps = 0.00001\n",
    "        self.grad_clip = 10\n",
    "        #self.render_mode = 'rgb_array'\n",
    "        # QMix-torch overrides the TorchPolicy's learn_on_batch w/o specifying a\n",
    "        # alternative `learn_on_loaded_batch` alternative for the GPU.\n",
    "        # TODO: This hack will be resolved once we move all algorithms to the new\n",
    "        #  RLModule/Learner APIs.\n",
    "        self.simple_optimizer = True\n",
    "\n",
    "        # Override some of AlgorithmConfig's default values with QMix-specific values.\n",
    "        # .training()\n",
    "        self.lr = 0.0005\n",
    "        self.train_batch_size = 32\n",
    "        self.target_network_update_freq = 500\n",
    "        self.num_steps_sampled_before_learning_starts = 1000\n",
    "        self.replay_buffer_config = {\n",
    "            \"type\": \"ReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "            \"prioritized_replay\": DEPRECATED_VALUE,\n",
    "            # Size of the replay buffer in batches (not timesteps!).\n",
    "            \"capacity\": 1000,\n",
    "            # Choosing `fragments` here makes it so that the buffer stores entire\n",
    "            # batches, instead of sequences, episodes or timesteps.\n",
    "            \"storage_unit\": \"fragments\",\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.model = {\n",
    "            \"lstm_cell_size\": 64,\n",
    "            \"max_seq_len\": 999999,\n",
    "        }\n",
    "        \"\"\"\n",
    "        # .framework()\n",
    "        self.framework_str = \"torch\"\n",
    "\n",
    "        # .rollouts()\n",
    "        self.rollout_fragment_length = 4\n",
    "        self.batch_mode = \"complete_episodes\"\n",
    "\n",
    "        # .reporting()\n",
    "        self.min_time_s_per_iteration = 1\n",
    "        self.min_sample_timesteps_per_iteration = 1000\n",
    "\n",
    "        # .exploration()\n",
    "        self.exploration_config = {\n",
    "            \"\"\"\n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\", #replace this with SoftQ\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.01,\n",
    "            # Timesteps over which to anneal epsilon.\n",
    "            \"epsilon_timesteps\": 40000,\n",
    "            \"\"\"\n",
    "            \"type\": \"SoftQ\",\n",
    "            \"temperature\": 1.0\n",
    "            # For soft_q, use:\n",
    "            # \"exploration_config\" = {\n",
    "            #   \"type\": \"SoftQ\"\n",
    "            #   \"temperature\": [float, e.g. 1.0]\n",
    "            # }\n",
    "        }\n",
    "\n",
    "        # .evaluation()\n",
    "        # Evaluate with epsilon=0 every `evaluation_interval` training iterations.\n",
    "        # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "        self.evaluation(\n",
    "            evaluation_config=AlgorithmConfig.overrides(explore=True)#False\n",
    "        )\n",
    "        # __sphinx_doc_end__\n",
    "        # fmt: on\n",
    "\n",
    "        self.worker_side_prioritization = DEPRECATED_VALUE\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        mixer: Optional[str] = NotProvided,\n",
    "        mixing_embed_dim: Optional[int] = NotProvided,\n",
    "        double_q: Optional[bool] = NotProvided,\n",
    "        target_network_update_freq: Optional[int] = NotProvided,\n",
    "        replay_buffer_config: Optional[dict] = NotProvided,\n",
    "        optim_alpha: Optional[float] = NotProvided,\n",
    "        optim_eps: Optional[float] = NotProvided,\n",
    "        grad_clip: Optional[float] = NotProvided,\n",
    "        # Deprecated args.\n",
    "        grad_norm_clipping=DEPRECATED_VALUE,\n",
    "        **kwargs,\n",
    "    ) -> \"QMixConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            mixer: Mixing network. Either \"qmix\", \"vdn\", or None.\n",
    "            mixing_embed_dim: Size of the mixing network embedding.\n",
    "            double_q: Whether to use Double_Q learning.\n",
    "            target_network_update_freq: Update the target network every\n",
    "                `target_network_update_freq` sample steps.\n",
    "            replay_buffer_config:\n",
    "            optim_alpha: RMSProp alpha.\n",
    "            optim_eps: RMSProp epsilon.\n",
    "            grad_clip: If not None, clip gradients during optimization at\n",
    "                this value.\n",
    "            grad_norm_clipping: Depcrecated in favor of grad_clip\n",
    "        Returns:\n",
    "            This updated AlgorithmConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if grad_norm_clipping != DEPRECATED_VALUE:\n",
    "            deprecation_warning(\n",
    "                old=\"grad_norm_clipping\",\n",
    "                new=\"grad_clip\",\n",
    "                help=\"Parameter `grad_norm_clipping` has been \"\n",
    "                \"deprecated in favor of grad_clip in QMix. \"\n",
    "                \"This is now the same parameter as in other \"\n",
    "                \"algorithms. `grad_clip` will be overwritten by \"\n",
    "                \"`grad_norm_clipping={}`\".format(grad_norm_clipping),\n",
    "                error=True,\n",
    "            )\n",
    "            grad_clip = grad_norm_clipping\n",
    "\n",
    "        if mixer is not NotProvided:\n",
    "            self.mixer = mixer\n",
    "        if mixing_embed_dim is not NotProvided:\n",
    "            self.mixing_embed_dim = mixing_embed_dim\n",
    "        if double_q is not NotProvided:\n",
    "            self.double_q = double_q\n",
    "        if target_network_update_freq is not NotProvided:\n",
    "            self.target_network_update_freq = target_network_update_freq\n",
    "        if replay_buffer_config is not NotProvided:\n",
    "            self.replay_buffer_config = replay_buffer_config\n",
    "        if optim_alpha is not NotProvided:\n",
    "            self.optim_alpha = optim_alpha\n",
    "        if optim_eps is not NotProvided:\n",
    "            self.optim_eps = optim_eps\n",
    "        if grad_clip is not NotProvided:\n",
    "            self.grad_clip = grad_clip\n",
    "\n",
    "        return self\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def validate(self) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate()\n",
    "\n",
    "        if self.framework_str != \"torch\":\n",
    "            raise ValueError(\n",
    "                \"Only `config.framework('torch')` supported so far for QMix!\"\n",
    "            )\n",
    "#@ti.kernel\n",
    "class QMix(SimpleQ):\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_config(cls) -> AlgorithmConfig:\n",
    "        return QMixConfig()\n",
    "\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_policy_class(\n",
    "        cls, config: AlgorithmConfig\n",
    "    ) -> Optional[Type[Policy]]:\n",
    "        return QMixTorchPolicy\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def training_step(self) -> ResultDict:\n",
    "        \"\"\"QMIX training iteration function.\n",
    "        - Sample n MultiAgentBatches from n workers synchronously.\n",
    "        - Store new samples in the replay buffer.\n",
    "        - Sample one training MultiAgentBatch from the replay buffer.\n",
    "        - Learn on the training batch.\n",
    "        - Update the target network every `target_network_update_freq` sample steps.\n",
    "        - Return all collected training metrics for the iteration.\n",
    "        Returns:\n",
    "            The results dict from executing the training iteration.\n",
    "        \"\"\"\n",
    "        # Sample n batches from n workers.\n",
    "        with self._timers[SAMPLE_TIMER]:\n",
    "            new_sample_batches = synchronous_parallel_sample(\n",
    "                worker_set=self.workers, concat=False\n",
    "            )\n",
    "\n",
    "        for batch in new_sample_batches:\n",
    "            # Update counters.\n",
    "            self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n",
    "            self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n",
    "            # Store new samples in the replay buffer.\n",
    "            self.local_replay_buffer.add(batch)\n",
    "\n",
    "        # Update target network every `target_network_update_freq` sample steps.\n",
    "        cur_ts = self._counters[\n",
    "            NUM_AGENT_STEPS_SAMPLED\n",
    "            if self.config.count_steps_by == \"agent_steps\"\n",
    "            else NUM_ENV_STEPS_SAMPLED\n",
    "        ]\n",
    "\n",
    "        train_results = {}\n",
    "\n",
    "        if cur_ts > self.config.num_steps_sampled_before_learning_starts:\n",
    "            # Sample n batches from replay buffer until the total number of timesteps\n",
    "            # reaches `train_batch_size`.\n",
    "            train_batch = sample_min_n_steps_from_buffer(\n",
    "                replay_buffer=self.local_replay_buffer,\n",
    "                min_steps=self.config.train_batch_size,\n",
    "                count_by_agent_steps=self.config.count_steps_by == \"agent_steps\",\n",
    "            )\n",
    "\n",
    "            # Learn on the training batch.\n",
    "            # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
    "            # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
    "            if self.config.get(\"simple_optimizer\") is True:\n",
    "                train_results = train_one_step(self, train_batch)\n",
    "            else:\n",
    "                train_results = multi_gpu_train_one_step(self, train_batch)\n",
    "\n",
    "            # Update target network every `target_network_update_freq` sample steps.\n",
    "            last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
    "            if cur_ts - last_update >= self.config.target_network_update_freq:\n",
    "                to_update = self.workers.local_worker().get_policies_to_train()\n",
    "                self.workers.local_worker().foreach_policy_to_train(\n",
    "                    lambda p, pid: pid in to_update and p.update_target()\n",
    "                )\n",
    "                self._counters[NUM_TARGET_UPDATES] += 1\n",
    "                self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
    "\n",
    "            update_priorities_in_replay_buffer(\n",
    "                self.local_replay_buffer, self.config, train_batch, train_results\n",
    "            )\n",
    "\n",
    "            # Update weights and global_vars - after learning on the local worker -\n",
    "            # on all remote workers.\n",
    "            global_vars = {\n",
    "                \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
    "            }\n",
    "            # Update remote workers' weights and global vars after learning on local\n",
    "            # worker.\n",
    "            with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
    "                self.workers.sync_weights(global_vars=global_vars)\n",
    "\n",
    "        # Return all collected metrics for the iteration.\n",
    "        return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8210c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "#b = np.ones(3000).tolist()\n",
    "#obs_space= Tuple({MultiDiscrete([])})\n",
    "\n",
    "observation_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]), \n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]),\n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "action_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([1,3]), \n",
    "                ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,1]),\n",
    "                ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "obs_space = Tuple([\n",
    "    Dict({\n",
    "        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    \n",
    "    }\n",
    "    )\n",
    "]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4088a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 22:19:02,195\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\tune.py:562: UserWarning: Consider boosting PBT performance by enabling `reuse_actors` as well as implementing `reset_config` for Trainable.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-04-23 22:24:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:05:17.41        </td></tr>\n",
       "<tr><td>Memory:      </td><td>27.7/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 238 checkpoints, 41 perturbs<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/22.72 GiB heap, 0.0/11.36 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">   alpha</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_88e23_00000</td><td>TERMINATED</td><td>127.0.0.1:5784 </td><td style=\"text-align: right;\">0.325744</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         96.4706</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_88e23_00001</td><td>TERMINATED</td><td>127.0.0.1:9580 </td><td style=\"text-align: right;\">0.407179</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         96.7603</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_88e23_00002</td><td>TERMINATED</td><td>127.0.0.1:38208</td><td style=\"text-align: right;\">0.271453</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         97.37  </td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_88e23_00003</td><td>TERMINATED</td><td>127.0.0.1:28460</td><td style=\"text-align: right;\">0.339316</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         98.2132</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    7.01</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=34064)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34064)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=34064)\u001b[0m 2023-04-23 22:19:14,407\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=34064)\u001b[0m 2023-04-23 22:19:14,645\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34064)\u001b[0m 2023-04-23 22:19:14,669\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=29048)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=29048)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=29048)\u001b[0m 2023-04-23 22:19:22,259\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=29048)\u001b[0m 2023-04-23 22:19:22,488\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=29048)\u001b[0m 2023-04-23 22:19:22,509\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=16524)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=16524)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=16524)\u001b[0m 2023-04-23 22:19:30,098\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=16524)\u001b[0m 2023-04-23 22:19:30,340\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=16524)\u001b[0m 2023-04-23 22:19:30,361\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=7224)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=7224)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=7224)\u001b[0m 2023-04-23 22:19:38,063\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=7224)\u001b[0m 2023-04-23 22:19:38,297\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=7224)\u001b[0m 2023-04-23 22:19:38,318\tWARNING env.py:53 -- Skipping env checking for this experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                         </th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                           </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                          </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                     </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </th><th>timers                                                                                                                                                                             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_88e23_00000</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.012359619140625, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12152767181396484}   </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.5677143186330795, &#x27;cur_lr&#x27;: 0.0004}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 33.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{}                                                                                                            </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.5508146367329634, &#x27;mean_inference_ms&#x27;: 0.7260175916385176, &#x27;mean_action_processing_ms&#x27;: 0.09829203755110372, &#x27;mean_env_wait_ms&#x27;: 0.03324722900211163, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.5508146367329634, &#x27;mean_inference_ms&#x27;: 0.7260175916385176, &#x27;mean_action_processing_ms&#x27;: 0.09829203755110372, &#x27;mean_env_wait_ms&#x27;: 0.03324722900211163, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.012359619140625, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12152767181396484}}   </td><td>{&#x27;training_iteration_time_ms&#x27;: 288.639, &#x27;load_time_ms&#x27;: 0.0, &#x27;load_throughput&#x27;: 0.0, &#x27;learn_time_ms&#x27;: 6.289, &#x27;learn_throughput&#x27;: 31799.844, &#x27;synch_weights_time_ms&#x27;: 0.0}          </td></tr>\n",
       "<tr><td>PG_TwoStepGame_88e23_00001</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.014212369918823242, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12501907348632812}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.388309508562088, &#x27;cur_lr&#x27;: 0.0004}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 35.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000} </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 3.6, &#x27;ram_util_percent&#x27;: 44.5, &#x27;gpu_util_percent0&#x27;: 0.0, &#x27;vram_util_percent0&#x27;: 0.5234375}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.5803704096257306, &#x27;mean_inference_ms&#x27;: 0.7581283105343324, &#x27;mean_action_processing_ms&#x27;: 0.11002093810102936, &#x27;mean_env_wait_ms&#x27;: 0.03863638952287558, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.5803704096257306, &#x27;mean_inference_ms&#x27;: 0.7581283105343324, &#x27;mean_action_processing_ms&#x27;: 0.11002093810102936, &#x27;mean_env_wait_ms&#x27;: 0.03863638952287558, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.014212369918823242, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12501907348632812}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 310.909, &#x27;load_time_ms&#x27;: 0.0, &#x27;load_throughput&#x27;: 0.0, &#x27;learn_time_ms&#x27;: 6.737, &#x27;learn_throughput&#x27;: 29688.092, &#x27;synch_weights_time_ms&#x27;: 0.0}          </td></tr>\n",
       "<tr><td>PG_TwoStepGame_88e23_00002</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.02540755271911621, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.09464406967163086} </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.8856923878192902, &#x27;cur_lr&#x27;: 0.0004}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 49.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 3.1, &#x27;ram_util_percent&#x27;: 45.1, &#x27;gpu_util_percent0&#x27;: 0.0, &#x27;vram_util_percent0&#x27;: 0.5234375}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6639485930328581, &#x27;mean_inference_ms&#x27;: 0.8790528767110345, &#x27;mean_action_processing_ms&#x27;: 0.12046326353320644, &#x27;mean_env_wait_ms&#x27;: 0.03713136957874539, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6639485930328581, &#x27;mean_inference_ms&#x27;: 0.8790528767110345, &#x27;mean_action_processing_ms&#x27;: 0.12046326353320644, &#x27;mean_env_wait_ms&#x27;: 0.03713136957874539, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.02540755271911621, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.09464406967163086}} </td><td>{&#x27;training_iteration_time_ms&#x27;: 333.128, &#x27;load_time_ms&#x27;: 0.0, &#x27;load_throughput&#x27;: 0.0, &#x27;learn_time_ms&#x27;: 7.212, &#x27;learn_throughput&#x27;: 27733.411, &#x27;synch_weights_time_ms&#x27;: 0.0}          </td></tr>\n",
       "<tr><td>PG_TwoStepGame_88e23_00003</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.026968002319335938, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.11197948455810547}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                 7.01</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 1.296194612979889, &#x27;cur_lr&#x27;: 0.0004}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 213.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{}                                                                                                            </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.7397995714350096, &#x27;mean_inference_ms&#x27;: 0.967874810392737, &#x27;mean_action_processing_ms&#x27;: 0.1367839060845283, &#x27;mean_env_wait_ms&#x27;: 0.042656638745832455, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 8.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.01, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.7397995714350096, &#x27;mean_inference_ms&#x27;: 0.967874810392737, &#x27;mean_action_processing_ms&#x27;: 0.1367839060845283, &#x27;mean_env_wait_ms&#x27;: 0.042656638745832455, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.026968002319335938, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.11197948455810547}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 383.687, &#x27;load_time_ms&#x27;: 0.102, &#x27;load_throughput&#x27;: 1955842.388, &#x27;learn_time_ms&#x27;: 8.673, &#x27;learn_throughput&#x27;: 23058.803, &#x27;synch_weights_time_ms&#x27;: 0.0}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n",
      "2023-04-23 22:19:38,454\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:19:38,485\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "2023-04-23 22:19:39,076\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -4.500000) into trial 88e23_00002 (score = -4.710000)\n",
      "\n",
      "2023-04-23 22:19:39,077\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.25785258121068966 --- (* 0.8) --> 0.20628206496855173\n",
      "\n",
      "2023-04-23 22:19:39,270\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -4.500000) into trial 88e23_00003 (score = -5.190000)\n",
      "\n",
      "2023-04-23 22:19:39,271\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00003:\n",
      "alpha : 0.25785258121068966 --- (* 1.2) --> 0.3094230974528276\n",
      "\n",
      "2023-04-23 22:19:39,490\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -4.500000) into trial 88e23_00000 (score = -5.240000)\n",
      "\n",
      "2023-04-23 22:19:39,490\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.25785258121068966 --- (* 0.8) --> 0.20628206496855173\n",
      "\n",
      "2023-04-23 22:19:39,676\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "\u001b[2m\u001b[36m(pid=24832)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=24832)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=3456)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=3456)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=26592)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=26592)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=36756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=36756)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=3456)\u001b[0m 2023-04-23 22:19:47,706\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=24832)\u001b[0m 2023-04-23 22:19:47,710\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=24832)\u001b[0m 2023-04-23 22:19:47,949\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=24832)\u001b[0m 2023-04-23 22:19:47,970\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=3456)\u001b[0m 2023-04-23 22:19:47,949\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=3456)\u001b[0m 2023-04-23 22:19:47,971\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=24832)\u001b[0m 2023-04-23 22:19:48,105\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_0a25a54907d44a3abada9a73acebe5fb\n",
      "\u001b[2m\u001b[36m(PG pid=24832)\u001b[0m 2023-04-23 22:19:48,105\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 0.7424437999725342, '_episodes_total': 200}\n",
      "\u001b[2m\u001b[36m(PG pid=3456)\u001b[0m 2023-04-23 22:19:48,104\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_2918f6fd54974a6a87cd03be9de19253\n",
      "\u001b[2m\u001b[36m(PG pid=3456)\u001b[0m 2023-04-23 22:19:48,105\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 0.7424437999725342, '_episodes_total': 200}\n",
      "\u001b[2m\u001b[36m(PG pid=26592)\u001b[0m 2023-04-23 22:19:48,064\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=36756)\u001b[0m 2023-04-23 22:19:48,065\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26592)\u001b[0m 2023-04-23 22:19:48,312\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=26592)\u001b[0m 2023-04-23 22:19:48,335\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=36756)\u001b[0m 2023-04-23 22:19:48,323\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=36756)\u001b[0m 2023-04-23 22:19:48,336\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=26592)\u001b[0m 2023-04-23 22:19:48,447\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_1f269b17486d4d33bb4829876944ecab\n",
      "\u001b[2m\u001b[36m(PG pid=26592)\u001b[0m 2023-04-23 22:19:48,447\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 1.1108062267303467, '_episodes_total': 300}\n",
      "\u001b[2m\u001b[36m(PG pid=36756)\u001b[0m 2023-04-23 22:19:48,435\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_9bb4b89df5d4485ebf3b180ad04eb689\n",
      "\u001b[2m\u001b[36m(PG pid=36756)\u001b[0m 2023-04-23 22:19:48,435\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 0.7424437999725342, '_episodes_total': 200}\n",
      "2023-04-23 22:19:48,815\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:19:48,839\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00003\n",
      "2023-04-23 22:19:49,237\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00003\n",
      "2023-04-23 22:19:49,253\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:19:49,599\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00002 (score = -5.110000) into trial 88e23_00003 (score = -5.850000)\n",
      "\n",
      "2023-04-23 22:19:49,600\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00003:\n",
      "alpha : 0.20628206496855173 --- (* 0.8) --> 0.1650256519748414\n",
      "\n",
      "2023-04-23 22:19:50,153\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00000 (score = -5.070000) into trial 88e23_00002 (score = -5.930000)\n",
      "\n",
      "2023-04-23 22:19:50,154\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.20628206496855173 --- (* 0.8) --> 0.1650256519748414\n",
      "\n",
      "2023-04-23 22:19:51,256\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00000 (score = -5.070000) into trial 88e23_00001 (score = -6.430000)\n",
      "\n",
      "2023-04-23 22:19:51,257\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00001:\n",
      "alpha : 0.20628206496855173 --- (* 0.8) --> 0.1650256519748414\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20908)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=20908)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=8660)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=8660)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=27348)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=27348)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=20908)\u001b[0m 2023-04-23 22:19:58,591\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=8660)\u001b[0m 2023-04-23 22:19:58,593\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=20908)\u001b[0m 2023-04-23 22:19:58,829\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=20908)\u001b[0m 2023-04-23 22:19:58,850\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=8660)\u001b[0m 2023-04-23 22:19:58,833\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=8660)\u001b[0m 2023-04-23 22:19:58,850\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=27348)\u001b[0m 2023-04-23 22:19:58,815\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=20908)\u001b[0m 2023-04-23 22:19:59,055\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_42b3ca18e5394b6f9d85c90b85d45429\n",
      "\u001b[2m\u001b[36m(PG pid=20908)\u001b[0m 2023-04-23 22:19:59,055\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 1.8185808658599854, '_episodes_total': 500}\n",
      "\u001b[2m\u001b[36m(PG pid=8660)\u001b[0m 2023-04-23 22:19:59,057\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_6d9d814f3db9401aa27fe3653c161b83\n",
      "\u001b[2m\u001b[36m(PG pid=8660)\u001b[0m 2023-04-23 22:19:59,058\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 1.8100712299346924, '_episodes_total': 500}\n",
      "\u001b[2m\u001b[36m(PG pid=27348)\u001b[0m 2023-04-23 22:19:59,080\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=27348)\u001b[0m 2023-04-23 22:19:59,094\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=27348)\u001b[0m 2023-04-23 22:19:59,211\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b5c26e637e2c42c6b21eb3e28115506c\n",
      "\u001b[2m\u001b[36m(PG pid=27348)\u001b[0m 2023-04-23 22:19:59,211\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 1.8100712299346924, '_episodes_total': 500}\n",
      "\u001b[2m\u001b[36m(pid=9404)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9404)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=9404)\u001b[0m 2023-04-23 22:19:59,903\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=9404)\u001b[0m 2023-04-23 22:20:00,153\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=9404)\u001b[0m 2023-04-23 22:20:00,176\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=9404)\u001b[0m 2023-04-23 22:20:00,261\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b231995202054cdbb2ac804b17c6abb4\n",
      "\u001b[2m\u001b[36m(PG pid=9404)\u001b[0m 2023-04-23 22:20:00,261\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 1.8100712299346924, '_episodes_total': 500}\n",
      "2023-04-23 22:20:00,846\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "2023-04-23 22:20:01,223\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -6.150000) into trial 88e23_00002 (score = -6.440000)\n",
      "\n",
      "2023-04-23 22:20:01,224\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.1650256519748414 --- (* 0.8) --> 0.13202052157987312\n",
      "\n",
      "2023-04-23 22:20:01,408\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -6.150000) into trial 88e23_00003 (score = -6.490000)\n",
      "\n",
      "2023-04-23 22:20:01,409\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00003:\n",
      "alpha : 0.1650256519748414 --- (* 1.2) --> 0.19803078236980967\n",
      "\n",
      "2023-04-23 22:20:02,393\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -6.080000) into trial 88e23_00000 (score = -6.520000)\n",
      "\n",
      "2023-04-23 22:20:02,393\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.1650256519748414 --- (* 0.8) --> 0.13202052157987312\n",
      "\n",
      "2023-04-23 22:20:03,340\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:20:03,701\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:20:04,127\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:20:04,991\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:20:06,002\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:20:06,982\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:20:07,984\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:20:08,965\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "\u001b[2m\u001b[36m(pid=12836)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=12836)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=31160)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=31160)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-23 22:20:09,978\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "\u001b[2m\u001b[36m(PG pid=12836)\u001b[0m 2023-04-23 22:20:10,396\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=31160)\u001b[0m 2023-04-23 22:20:10,396\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=12836)\u001b[0m 2023-04-23 22:20:10,644\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=12836)\u001b[0m 2023-04-23 22:20:10,657\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=31160)\u001b[0m 2023-04-23 22:20:10,644\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=31160)\u001b[0m 2023-04-23 22:20:10,657\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=12836)\u001b[0m 2023-04-23 22:20:10,787\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_eded63f34c8e49b7a037e50400b2f1d7\n",
      "\u001b[2m\u001b[36m(PG pid=12836)\u001b[0m 2023-04-23 22:20:10,787\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 7, '_timesteps_total': None, '_time_total': 2.5459327697753906, '_episodes_total': 700}\n",
      "\u001b[2m\u001b[36m(PG pid=31160)\u001b[0m 2023-04-23 22:20:10,777\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_bc92b050e6b04651b2c66d89c9e22430\n",
      "\u001b[2m\u001b[36m(PG pid=31160)\u001b[0m 2023-04-23 22:20:10,777\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 7, '_timesteps_total': None, '_time_total': 2.5459327697753906, '_episodes_total': 700}\n",
      "2023-04-23 22:20:10,972\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "\u001b[2m\u001b[36m(pid=39712)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=39712)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=39712)\u001b[0m 2023-04-23 22:20:11,557\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=39712)\u001b[0m 2023-04-23 22:20:11,804\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=39712)\u001b[0m 2023-04-23 22:20:11,819\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-04-23 22:20:11,926\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00002 (score = -5.880000) into trial 88e23_00001 (score = -7.030000)\n",
      "\n",
      "2023-04-23 22:20:11,927\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00001:\n",
      "alpha : 0.13202052157987312 --- (* 1.2) --> 0.15842462589584774\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=39712)\u001b[0m 2023-04-23 22:20:11,916\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ccaecfb2f1b64a2087ad05f5a5dba52a\n",
      "\u001b[2m\u001b[36m(PG pid=39712)\u001b[0m 2023-04-23 22:20:11,916\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 9, '_timesteps_total': None, '_time_total': 3.2731494903564453, '_episodes_total': 900}\n",
      "2023-04-23 22:20:24,131\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00002 (score = -6.940000) into trial 88e23_00003 (score = -7.050000)\n",
      "\n",
      "2023-04-23 22:20:24,133\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00003:\n",
      "alpha : 0.13202052157987312 --- (* 0.8) --> 0.1056164172638985\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3316)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=3316)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=3316)\u001b[0m 2023-04-23 22:20:26,213\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=3316)\u001b[0m 2023-04-23 22:20:26,856\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=3316)\u001b[0m 2023-04-23 22:20:26,887\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=3316)\u001b[0m 2023-04-23 22:20:27,046\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_538c4cad9a6a477891949306b1584ce8\n",
      "\u001b[2m\u001b[36m(PG pid=3316)\u001b[0m 2023-04-23 22:20:27,046\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 8, '_timesteps_total': None, '_time_total': 2.8908181190490723, '_episodes_total': 800}\n",
      "\u001b[2m\u001b[36m(pid=22176)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=22176)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=3548)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=3548)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=22176)\u001b[0m 2023-04-23 22:20:39,285\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=3548)\u001b[0m 2023-04-23 22:20:39,487\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=22176)\u001b[0m 2023-04-23 22:20:39,784\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=22176)\u001b[0m 2023-04-23 22:20:39,816\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=22176)\u001b[0m 2023-04-23 22:20:39,988\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_1360f35a98fd42aeb4541cf5d08e0c11\n",
      "\u001b[2m\u001b[36m(PG pid=22176)\u001b[0m 2023-04-23 22:20:39,989\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 18, '_timesteps_total': None, '_time_total': 12.716545343399048, '_episodes_total': 1800}\n",
      "\u001b[2m\u001b[36m(PG pid=3548)\u001b[0m 2023-04-23 22:20:39,989\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=3548)\u001b[0m 2023-04-23 22:20:40,022\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=3548)\u001b[0m 2023-04-23 22:20:40,192\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_08e8ddc4b7084a0b8f0713469aed5501\n",
      "\u001b[2m\u001b[36m(PG pid=3548)\u001b[0m 2023-04-23 22:20:40,192\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 21, '_timesteps_total': None, '_time_total': 14.560564756393433, '_episodes_total': 2100}\n",
      "2023-04-23 22:20:42,165\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00000 (score = -6.850000) into trial 88e23_00003 (score = -6.960000)\n",
      "\n",
      "2023-04-23 22:20:42,166\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00003:\n",
      "alpha : 0.13202052157987312 --- (* 1.2) --> 0.15842462589584774\n",
      "\n",
      "2023-04-23 22:20:42,690\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00000 (score = -6.850000) into trial 88e23_00001 (score = -7.030000)\n",
      "\n",
      "2023-04-23 22:20:42,692\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00001:\n",
      "alpha : 0.13202052157987312 --- (* 1.2) --> 0.15842462589584774\n",
      "\n",
      "2023-04-23 22:20:46,631\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00002 (score = -6.950000) into trial 88e23_00000 (score = -7.030000)\n",
      "\n",
      "2023-04-23 22:20:46,633\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.13202052157987312 --- (* 0.8) --> 0.1056164172638985\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=22000)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=22000)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=34296)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34296)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=22000)\u001b[0m 2023-04-23 22:20:52,043\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=34296)\u001b[0m 2023-04-23 22:20:52,044\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=22000)\u001b[0m 2023-04-23 22:20:52,309\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=22000)\u001b[0m 2023-04-23 22:20:52,331\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=34296)\u001b[0m 2023-04-23 22:20:52,307\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34296)\u001b[0m 2023-04-23 22:20:52,332\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=22000)\u001b[0m 2023-04-23 22:20:52,449\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b7c83e3f749c4fc292db0502870b2836\n",
      "\u001b[2m\u001b[36m(PG pid=22000)\u001b[0m 2023-04-23 22:20:52,449\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 15.471824645996094, '_episodes_total': 2200}\n",
      "\u001b[2m\u001b[36m(PG pid=34296)\u001b[0m 2023-04-23 22:20:52,450\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_bb096a369f4a403aadb87236708d052d\n",
      "\u001b[2m\u001b[36m(PG pid=34296)\u001b[0m 2023-04-23 22:20:52,450\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 15.471824645996094, '_episodes_total': 2200}\n",
      "2023-04-23 22:20:54,043\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00003\n",
      "2023-04-23 22:20:54,248\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00002 (score = -7.010000) into trial 88e23_00001 (score = -7.040000)\n",
      "\n",
      "2023-04-23 22:20:54,249\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00001:\n",
      "alpha : 0.13202052157987312 --- (* 1.2) --> 0.15842462589584774\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=13804)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=13804)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=13804)\u001b[0m 2023-04-23 22:20:56,975\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=13804)\u001b[0m 2023-04-23 22:20:57,288\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=13804)\u001b[0m 2023-04-23 22:20:57,304\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=13804)\u001b[0m 2023-04-23 22:20:57,426\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_d8bf0e5bcca143978348766e34d344f7\n",
      "\u001b[2m\u001b[36m(PG pid=13804)\u001b[0m 2023-04-23 22:20:57,426\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 38, '_timesteps_total': None, '_time_total': 31.735483646392822, '_episodes_total': 3800}\n",
      "\u001b[2m\u001b[36m(pid=25908)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=25908)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=25908)\u001b[0m 2023-04-23 22:21:04,026\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=25908)\u001b[0m 2023-04-23 22:21:04,299\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=25908)\u001b[0m 2023-04-23 22:21:04,315\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=25908)\u001b[0m 2023-04-23 22:21:04,420\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b78c8253b49549ba902f5adade7ad948\n",
      "\u001b[2m\u001b[36m(PG pid=25908)\u001b[0m 2023-04-23 22:21:04,421\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 54, '_timesteps_total': None, '_time_total': 38.6900680065155, '_episodes_total': 5400}\n",
      "2023-04-23 22:21:05,732\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -6.950000) into trial 88e23_00000 (score = -7.010000)\n",
      "\n",
      "2023-04-23 22:21:05,732\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.15842462589584774 --- (resample) --> 0.3933084143966358\n",
      "\n",
      "2023-04-23 22:21:07,105\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:21:07,519\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:21:10,756\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "2023-04-23 22:21:11,993\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:21:13,182\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "2023-04-23 22:21:13,208\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:21:13,712\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:21:14,464\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "\u001b[2m\u001b[36m(pid=18524)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=18524)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=10260)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=10260)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=18524)\u001b[0m 2023-04-23 22:21:15,583\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=10260)\u001b[0m 2023-04-23 22:21:15,583\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=18524)\u001b[0m 2023-04-23 22:21:15,846\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=18524)\u001b[0m 2023-04-23 22:21:15,861\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=10260)\u001b[0m 2023-04-23 22:21:15,847\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=10260)\u001b[0m 2023-04-23 22:21:15,861\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=18524)\u001b[0m 2023-04-23 22:21:15,972\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_6ed5525ffbab4a20aee4a235fc601580\n",
      "\u001b[2m\u001b[36m(PG pid=18524)\u001b[0m 2023-04-23 22:21:15,972\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 47, '_timesteps_total': None, '_time_total': 25.627387523651123, '_episodes_total': 4700}\n",
      "\u001b[2m\u001b[36m(PG pid=10260)\u001b[0m 2023-04-23 22:21:16,030\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_d8df3d671ecf41cfb422882626a8ba74\n",
      "\u001b[2m\u001b[36m(PG pid=10260)\u001b[0m 2023-04-23 22:21:16,030\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 56, '_timesteps_total': None, '_time_total': 39.54473829269409, '_episodes_total': 5600}\n",
      "2023-04-23 22:21:16,347\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "2023-04-23 22:21:16,530\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -6.900000) into trial 88e23_00000 (score = -7.020000)\n",
      "\n",
      "2023-04-23 22:21:16,531\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.15842462589584774 --- (* 0.8) --> 0.1267397007166782\n",
      "\n",
      "2023-04-23 22:21:16,938\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00003\n",
      "2023-04-23 22:21:17,511\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -6.880000) into trial 88e23_00002 (score = -7.030000)\n",
      "\n",
      "2023-04-23 22:21:17,512\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.15842462589584774 --- (resample) --> 0.30078291595969087\n",
      "\n",
      "2023-04-23 22:21:18,916\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00003\n",
      "2023-04-23 22:21:23,326\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00003\n",
      "\u001b[2m\u001b[36m(pid=32716)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=32716)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=31288)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=31288)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=32716)\u001b[0m 2023-04-23 22:21:26,799\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=31288)\u001b[0m 2023-04-23 22:21:26,788\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(pid=20224)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=20224)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=32716)\u001b[0m 2023-04-23 22:21:27,121\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=32716)\u001b[0m 2023-04-23 22:21:27,136\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=31288)\u001b[0m 2023-04-23 22:21:27,118\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=31288)\u001b[0m 2023-04-23 22:21:27,134\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=32716)\u001b[0m 2023-04-23 22:21:27,332\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_69ee821f8cc641398bf7434b90f2d41b\n",
      "\u001b[2m\u001b[36m(PG pid=32716)\u001b[0m 2023-04-23 22:21:27,333\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 82, '_timesteps_total': None, '_time_total': 49.575700521469116, '_episodes_total': 8200}\n",
      "\u001b[2m\u001b[36m(PG pid=31288)\u001b[0m 2023-04-23 22:21:27,332\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_d8d89fd754b24674be486d6f41a5c523\n",
      "\u001b[2m\u001b[36m(PG pid=31288)\u001b[0m 2023-04-23 22:21:27,333\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 48, '_timesteps_total': None, '_time_total': 25.999882698059082, '_episodes_total': 4800}\n",
      "\u001b[2m\u001b[36m(PG pid=20224)\u001b[0m 2023-04-23 22:21:27,686\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "2023-04-23 22:21:27,804\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -6.960000) into trial 88e23_00000 (score = -7.030000)\n",
      "\n",
      "2023-04-23 22:21:27,805\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.15842462589584774 --- (* 1.2) --> 0.19010955107501729\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=20224)\u001b[0m 2023-04-23 22:21:28,011\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=20224)\u001b[0m 2023-04-23 22:21:28,034\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=20224)\u001b[0m 2023-04-23 22:21:28,145\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_c880376f0c214c6f8d5fbf338bdbbb12\n",
      "\u001b[2m\u001b[36m(PG pid=20224)\u001b[0m 2023-04-23 22:21:28,146\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 50, '_timesteps_total': None, '_time_total': 26.738523244857788, '_episodes_total': 5000}\n",
      "2023-04-23 22:21:28,857\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00002 (score = -6.960000) into trial 88e23_00003 (score = -7.040000)\n",
      "\n",
      "2023-04-23 22:21:28,859\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00003:\n",
      "alpha : 0.30078291595969087 --- (* 0.8) --> 0.2406263327677527\n",
      "\n",
      "2023-04-23 22:21:33,828\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "2023-04-23 22:21:35,871\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "\u001b[2m\u001b[36m(pid=16236)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=16236)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=16236)\u001b[0m 2023-04-23 22:21:38,236\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=16236)\u001b[0m 2023-04-23 22:21:38,584\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=16236)\u001b[0m 2023-04-23 22:21:38,604\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=16236)\u001b[0m 2023-04-23 22:21:38,739\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_a1194d851fc94614978c5c82ab8e57ed\n",
      "\u001b[2m\u001b[36m(PG pid=16236)\u001b[0m 2023-04-23 22:21:38,739\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 70, '_timesteps_total': None, '_time_total': 35.1742525100708, '_episodes_total': 7000}\n",
      "\u001b[2m\u001b[36m(pid=2104)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=2104)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=15324)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=15324)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-23 22:21:38,846\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "\u001b[2m\u001b[36m(PG pid=2104)\u001b[0m 2023-04-23 22:21:39,691\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=15324)\u001b[0m 2023-04-23 22:21:39,672\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=2104)\u001b[0m 2023-04-23 22:21:40,188\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=2104)\u001b[0m 2023-04-23 22:21:40,215\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=15324)\u001b[0m 2023-04-23 22:21:40,186\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=15324)\u001b[0m 2023-04-23 22:21:40,214\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=2104)\u001b[0m 2023-04-23 22:21:40,420\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_5a502b4ed34244d1927bd1060a512250\n",
      "\u001b[2m\u001b[36m(PG pid=2104)\u001b[0m 2023-04-23 22:21:40,421\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 85, '_timesteps_total': None, '_time_total': 50.97311496734619, '_episodes_total': 8500}\n",
      "\u001b[2m\u001b[36m(PG pid=15324)\u001b[0m 2023-04-23 22:21:40,394\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_00dcb94ebc3a4e7ea61d619389e6961b\n",
      "\u001b[2m\u001b[36m(PG pid=15324)\u001b[0m 2023-04-23 22:21:40,394\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 51, '_timesteps_total': None, '_time_total': 27.22745656967163, '_episodes_total': 5100}\n",
      "2023-04-23 22:21:40,686\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00002 (score = -7.000000) into trial 88e23_00000 (score = -7.040000)\n",
      "\n",
      "2023-04-23 22:21:40,687\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.30078291595969087 --- (* 1.2) --> 0.36093949915162904\n",
      "\n",
      "2023-04-23 22:21:45,445\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00001 (score = -7.060000)\n",
      "\n",
      "2023-04-23 22:21:45,446\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00001:\n",
      "alpha : 0.2406263327677527 --- (* 1.2) --> 0.28875159932130323\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=32408)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=32408)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=32408)\u001b[0m 2023-04-23 22:21:56,396\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=32408)\u001b[0m 2023-04-23 22:21:56,946\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=32408)\u001b[0m 2023-04-23 22:21:56,970\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=32408)\u001b[0m 2023-04-23 22:21:57,147\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f383a783035f445eab60678af34880ca\n",
      "\u001b[2m\u001b[36m(PG pid=32408)\u001b[0m 2023-04-23 22:21:57,148\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 71, '_timesteps_total': None, '_time_total': 36.211949825286865, '_episodes_total': 7100}\n",
      "\u001b[2m\u001b[36m(pid=22616)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=22616)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=11828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=11828)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=22616)\u001b[0m 2023-04-23 22:22:02,746\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=11828)\u001b[0m 2023-04-23 22:22:02,758\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=22616)\u001b[0m 2023-04-23 22:22:03,232\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=22616)\u001b[0m 2023-04-23 22:22:03,272\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=11828)\u001b[0m 2023-04-23 22:22:03,253\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=11828)\u001b[0m 2023-04-23 22:22:03,285\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=22616)\u001b[0m 2023-04-23 22:22:03,488\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_857ecfced930433c81f167462a286695\n",
      "\u001b[2m\u001b[36m(PG pid=22616)\u001b[0m 2023-04-23 22:22:03,488\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 77, '_timesteps_total': None, '_time_total': 40.048946380615234, '_episodes_total': 7700}\n",
      "\u001b[2m\u001b[36m(PG pid=11828)\u001b[0m 2023-04-23 22:22:03,497\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_7b3dc737cf4d4b089e28f9c3927f40db\n",
      "\u001b[2m\u001b[36m(PG pid=11828)\u001b[0m 2023-04-23 22:22:03,498\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 57, '_timesteps_total': None, '_time_total': 30.90528416633606, '_episodes_total': 5700}\n",
      "2023-04-23 22:22:04,579\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00000 (score = -6.940000) into trial 88e23_00001 (score = -7.020000)\n",
      "\n",
      "2023-04-23 22:22:04,580\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00001:\n",
      "alpha : 0.36093949915162904 --- (* 1.2) --> 0.43312739898195485\n",
      "\n",
      "2023-04-23 22:22:06,769\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00002 (score = -7.030000)\n",
      "\n",
      "2023-04-23 22:22:06,770\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.2406263327677527 --- (resample) --> 0.5780081511021058\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=10648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=10648)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=10648)\u001b[0m 2023-04-23 22:22:18,270\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=10648)\u001b[0m 2023-04-23 22:22:18,569\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=10648)\u001b[0m 2023-04-23 22:22:18,584\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=10648)\u001b[0m 2023-04-23 22:22:18,690\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_9d4d9a8c852f4f67ad06e8728463847c\n",
      "\u001b[2m\u001b[36m(PG pid=10648)\u001b[0m 2023-04-23 22:22:18,691\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 80, '_timesteps_total': None, '_time_total': 42.81612968444824, '_episodes_total': 8000}\n",
      "\u001b[2m\u001b[36m(pid=22116)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=22116)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=17596)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=17596)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=22116)\u001b[0m 2023-04-23 22:22:19,624\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=17596)\u001b[0m 2023-04-23 22:22:19,628\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=22116)\u001b[0m 2023-04-23 22:22:19,945\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=22116)\u001b[0m 2023-04-23 22:22:19,961\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=17596)\u001b[0m 2023-04-23 22:22:19,960\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=17596)\u001b[0m 2023-04-23 22:22:19,977\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=22116)\u001b[0m 2023-04-23 22:22:20,102\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_fc3a79585d9944609623044bb5fda87f\n",
      "\u001b[2m\u001b[36m(PG pid=22116)\u001b[0m 2023-04-23 22:22:20,102\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 82, '_timesteps_total': None, '_time_total': 49.74801278114319, '_episodes_total': 8200}\n",
      "\u001b[2m\u001b[36m(PG pid=17596)\u001b[0m 2023-04-23 22:22:20,141\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_89ea53216992460a91f6fc96854b91b3\n",
      "\u001b[2m\u001b[36m(PG pid=17596)\u001b[0m 2023-04-23 22:22:20,142\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 83, '_timesteps_total': None, '_time_total': 50.54631519317627, '_episodes_total': 8300}\n",
      "2023-04-23 22:22:21,036\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00000\n",
      "2023-04-23 22:22:21,692\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00000 (score = -7.020000)\n",
      "\n",
      "2023-04-23 22:22:21,693\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.2406263327677527 --- (* 0.8) --> 0.19250106621420215\n",
      "\n",
      "2023-04-23 22:22:26,411\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.010000) into trial 88e23_00001 (score = -7.030000)\n",
      "\n",
      "2023-04-23 22:22:26,412\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00001:\n",
      "alpha : 0.2406263327677527 --- (* 1.2) --> 0.28875159932130323\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=26472)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=26472)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=21944)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=21944)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=26472)\u001b[0m 2023-04-23 22:22:37,952\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=21944)\u001b[0m 2023-04-23 22:22:38,004\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26472)\u001b[0m 2023-04-23 22:22:38,373\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=26472)\u001b[0m 2023-04-23 22:22:38,391\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=21944)\u001b[0m 2023-04-23 22:22:38,406\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=21944)\u001b[0m 2023-04-23 22:22:38,424\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=26472)\u001b[0m 2023-04-23 22:22:38,526\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_94ea1076d2eb4e81a0fbe64602691251\n",
      "\u001b[2m\u001b[36m(PG pid=26472)\u001b[0m 2023-04-23 22:22:38,526\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 85, '_timesteps_total': None, '_time_total': 51.62012076377869, '_episodes_total': 8500}\n",
      "\u001b[2m\u001b[36m(PG pid=21944)\u001b[0m 2023-04-23 22:22:38,562\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_5d8b34672c6a49c7ab5952c2d97ff4e4\n",
      "\u001b[2m\u001b[36m(PG pid=21944)\u001b[0m 2023-04-23 22:22:38,563\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 85, '_timesteps_total': None, '_time_total': 51.34108543395996, '_episodes_total': 8500}\n",
      "\u001b[2m\u001b[36m(pid=31996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=31996)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=31996)\u001b[0m 2023-04-23 22:22:41,649\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=31996)\u001b[0m 2023-04-23 22:22:41,961\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=31996)\u001b[0m 2023-04-23 22:22:41,978\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=31996)\u001b[0m 2023-04-23 22:22:42,085\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_549bdfc37a694ddb98a2da74bc0f4400\n",
      "\u001b[2m\u001b[36m(PG pid=31996)\u001b[0m 2023-04-23 22:22:42,086\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 90, '_timesteps_total': None, '_time_total': 55.18072009086609, '_episodes_total': 9000}\n",
      "2023-04-23 22:22:43,118\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-23 22:22:43,118\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.2406263327677527 --- (* 1.2) --> 0.28875159932130323\n",
      "\n",
      "2023-04-23 22:22:43,686\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00002 (score = -7.010000)\n",
      "\n",
      "2023-04-23 22:22:43,686\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.2406263327677527 --- (* 0.8) --> 0.19250106621420215\n",
      "\n",
      "2023-04-23 22:22:47,615\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00003\n",
      "\u001b[2m\u001b[36m(pid=25076)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=25076)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=11640)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=11640)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=30448)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=30448)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=25076)\u001b[0m 2023-04-23 22:22:52,935\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=11640)\u001b[0m 2023-04-23 22:22:52,909\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=25076)\u001b[0m 2023-04-23 22:22:53,206\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=25076)\u001b[0m 2023-04-23 22:22:53,220\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=11640)\u001b[0m 2023-04-23 22:22:53,177\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=11640)\u001b[0m 2023-04-23 22:22:53,192\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=30448)\u001b[0m 2023-04-23 22:22:53,249\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=25076)\u001b[0m 2023-04-23 22:22:53,334\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ded2f315a9724b92baec172c9a07b2e6\n",
      "\u001b[2m\u001b[36m(PG pid=25076)\u001b[0m 2023-04-23 22:22:53,335\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 110, '_timesteps_total': None, '_time_total': 67.70397472381592, '_episodes_total': 11000}\n",
      "\u001b[2m\u001b[36m(PG pid=11640)\u001b[0m 2023-04-23 22:22:53,287\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_7ce2504a07614ec0a223bcaed0854ec4\n",
      "\u001b[2m\u001b[36m(PG pid=11640)\u001b[0m 2023-04-23 22:22:53,287\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 92, '_timesteps_total': None, '_time_total': 56.00706100463867, '_episodes_total': 9200}\n",
      "\u001b[2m\u001b[36m(PG pid=30448)\u001b[0m 2023-04-23 22:22:53,555\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=30448)\u001b[0m 2023-04-23 22:22:53,575\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-04-23 22:22:53,698\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00003\n",
      "\u001b[2m\u001b[36m(PG pid=30448)\u001b[0m 2023-04-23 22:22:53,705\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_868b4bbdb276430891cbd908e7cc1514\n",
      "\u001b[2m\u001b[36m(PG pid=30448)\u001b[0m 2023-04-23 22:22:53,705\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 111, '_timesteps_total': None, '_time_total': 68.06284070014954, '_episodes_total': 11100}\n",
      "2023-04-23 22:22:54,121\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -6.940000) into trial 88e23_00003 (score = -7.020000)\n",
      "\n",
      "2023-04-23 22:22:54,122\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00003:\n",
      "alpha : 0.28875159932130323 --- (resample) --> 0.33931624374174263\n",
      "\n",
      "2023-04-23 22:22:56,464\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -7.000000) into trial 88e23_00002 (score = -7.020000)\n",
      "\n",
      "2023-04-23 22:22:56,465\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.28875159932130323 --- (* 0.8) --> 0.2310012794570426\n",
      "\n",
      "2023-04-23 22:22:57,787\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -7.000000) into trial 88e23_00000 (score = -7.030000)\n",
      "\n",
      "2023-04-23 22:22:57,788\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.28875159932130323 --- (* 1.2) --> 0.3465019191855639\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=24404)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=24404)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=28460)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=28460)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=24404)\u001b[0m 2023-04-23 22:23:04,119\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=28460)\u001b[0m 2023-04-23 22:23:04,114\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=24404)\u001b[0m 2023-04-23 22:23:04,378\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=24404)\u001b[0m 2023-04-23 22:23:04,391\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=28460)\u001b[0m 2023-04-23 22:23:04,374\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=28460)\u001b[0m 2023-04-23 22:23:04,389\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=24404)\u001b[0m 2023-04-23 22:23:04,509\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_be634071edf842ca80dac0ab6d28ec58\n",
      "\u001b[2m\u001b[36m(PG pid=24404)\u001b[0m 2023-04-23 22:23:04,509\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 94, '_timesteps_total': None, '_time_total': 56.79519987106323, '_episodes_total': 9400}\n",
      "\u001b[2m\u001b[36m(PG pid=28460)\u001b[0m 2023-04-23 22:23:04,494\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_13aaadedd60545698bdc3e89cdd03d24\n",
      "\u001b[2m\u001b[36m(PG pid=28460)\u001b[0m 2023-04-23 22:23:04,494\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 93, '_timesteps_total': None, '_time_total': 56.429341077804565, '_episodes_total': 9300}\n",
      "\u001b[2m\u001b[36m(pid=33828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33828)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=33828)\u001b[0m 2023-04-23 22:23:06,276\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(pid=25672)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=25672)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=33828)\u001b[0m 2023-04-23 22:23:06,537\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33828)\u001b[0m 2023-04-23 22:23:06,551\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=33828)\u001b[0m 2023-04-23 22:23:06,648\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_949ceb5fe83d43c2a72b8f4aaae924bd\n",
      "\u001b[2m\u001b[36m(PG pid=33828)\u001b[0m 2023-04-23 22:23:06,648\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 94, '_timesteps_total': None, '_time_total': 56.79519987106323, '_episodes_total': 9400}\n",
      "\u001b[2m\u001b[36m(PG pid=25672)\u001b[0m 2023-04-23 22:23:07,190\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=25672)\u001b[0m 2023-04-23 22:23:07,462\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=25672)\u001b[0m 2023-04-23 22:23:07,479\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=25672)\u001b[0m 2023-04-23 22:23:07,576\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_5cbf068607c74f538d0238d1ec4ad958\n",
      "\u001b[2m\u001b[36m(PG pid=25672)\u001b[0m 2023-04-23 22:23:07,577\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 94, '_timesteps_total': None, '_time_total': 56.79519987106323, '_episodes_total': 9400}\n",
      "2023-04-23 22:23:08,420\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00000 (score = -7.010000)\n",
      "\n",
      "2023-04-23 22:23:08,421\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.33931624374174263 --- (* 0.8) --> 0.2714529949933941\n",
      "\n",
      "2023-04-23 22:23:16,890\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00002 (score = -7.020000)\n",
      "\n",
      "2023-04-23 22:23:16,891\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.33931624374174263 --- (* 1.2) --> 0.40717949249009117\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=36284)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=36284)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=4744)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=4744)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=36284)\u001b[0m 2023-04-23 22:23:18,458\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=4744)\u001b[0m 2023-04-23 22:23:18,458\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=36284)\u001b[0m 2023-04-23 22:23:18,745\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=36284)\u001b[0m 2023-04-23 22:23:18,762\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=4744)\u001b[0m 2023-04-23 22:23:18,740\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=4744)\u001b[0m 2023-04-23 22:23:18,754\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=36284)\u001b[0m 2023-04-23 22:23:18,898\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_912d2d8d857a4ac082051647abd2f21e\n",
      "\u001b[2m\u001b[36m(PG pid=36284)\u001b[0m 2023-04-23 22:23:18,899\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': None, '_time_total': 59.01331424713135, '_episodes_total': 10000}\n",
      "\u001b[2m\u001b[36m(PG pid=4744)\u001b[0m 2023-04-23 22:23:18,903\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_bda0c9744487440c935e856540bf5880\n",
      "\u001b[2m\u001b[36m(PG pid=4744)\u001b[0m 2023-04-23 22:23:18,903\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 102, '_timesteps_total': None, '_time_total': 59.723816871643066, '_episodes_total': 10200}\n",
      "2023-04-23 22:23:21,782\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00000 (score = -7.020000)\n",
      "\n",
      "2023-04-23 22:23:21,783\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.33931624374174263 --- (resample) --> 0.5152640947838293\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30888)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=30888)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=30888)\u001b[0m 2023-04-23 22:23:26,578\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=30888)\u001b[0m 2023-04-23 22:23:26,861\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=30888)\u001b[0m 2023-04-23 22:23:26,883\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=30888)\u001b[0m 2023-04-23 22:23:26,986\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_a4c1bb5ba617458b8fa5cae46c990cfb\n",
      "\u001b[2m\u001b[36m(PG pid=30888)\u001b[0m 2023-04-23 22:23:26,986\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 115, '_timesteps_total': None, '_time_total': 64.9323182106018, '_episodes_total': 11500}\n",
      "\u001b[2m\u001b[36m(pid=19896)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19896)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=7328)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=7328)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=19896)\u001b[0m 2023-04-23 22:23:31,722\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=7328)\u001b[0m 2023-04-23 22:23:31,739\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=19896)\u001b[0m 2023-04-23 22:23:31,975\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=19896)\u001b[0m 2023-04-23 22:23:31,988\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=7328)\u001b[0m 2023-04-23 22:23:31,993\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=7328)\u001b[0m 2023-04-23 22:23:32,009\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=19896)\u001b[0m 2023-04-23 22:23:32,099\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_2799c85a4155486384c65aec3b1bcf97\n",
      "\u001b[2m\u001b[36m(PG pid=19896)\u001b[0m 2023-04-23 22:23:32,099\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 124, '_timesteps_total': None, '_time_total': 68.55538415908813, '_episodes_total': 12400}\n",
      "\u001b[2m\u001b[36m(PG pid=7328)\u001b[0m 2023-04-23 22:23:32,118\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_23640d2e2056431294d82806fff553f7\n",
      "\u001b[2m\u001b[36m(PG pid=7328)\u001b[0m 2023-04-23 22:23:32,118\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 108, '_timesteps_total': None, '_time_total': 62.06421422958374, '_episodes_total': 10800}\n",
      "2023-04-23 22:23:32,845\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00003\n",
      "2023-04-23 22:23:32,928\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00000\n",
      "2023-04-23 22:23:33,414\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00000 (score = -7.010000)\n",
      "\n",
      "2023-04-23 22:23:33,415\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.33931624374174263 --- (* 0.8) --> 0.2714529949933941\n",
      "\n",
      "2023-04-23 22:23:35,159\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -6.940000) into trial 88e23_00002 (score = -7.020000)\n",
      "\n",
      "2023-04-23 22:23:35,160\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.28875159932130323 --- (* 1.2) --> 0.3465019191855639\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3768)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=3768)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=4372)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=4372)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=3768)\u001b[0m 2023-04-23 22:23:43,437\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=4372)\u001b[0m 2023-04-23 22:23:43,450\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=3768)\u001b[0m 2023-04-23 22:23:43,704\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=3768)\u001b[0m 2023-04-23 22:23:43,725\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=4372)\u001b[0m 2023-04-23 22:23:43,710\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=4372)\u001b[0m 2023-04-23 22:23:43,727\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=3768)\u001b[0m 2023-04-23 22:23:43,846\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3ab2c320b6484987979dcb590223dd22\n",
      "\u001b[2m\u001b[36m(PG pid=3768)\u001b[0m 2023-04-23 22:23:43,846\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 111, '_timesteps_total': None, '_time_total': 63.22013568878174, '_episodes_total': 11100}\n",
      "\u001b[2m\u001b[36m(PG pid=4372)\u001b[0m 2023-04-23 22:23:43,849\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_2cb3faf426474ca18855a0688604e9cb\n",
      "\u001b[2m\u001b[36m(PG pid=4372)\u001b[0m 2023-04-23 22:23:43,849\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 146, '_timesteps_total': None, '_time_total': 77.1307680606842, '_episodes_total': 14600}\n",
      "\u001b[2m\u001b[36m(pid=34608)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34608)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=34608)\u001b[0m 2023-04-23 22:23:45,161\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=34608)\u001b[0m 2023-04-23 22:23:45,448\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=34608)\u001b[0m 2023-04-23 22:23:45,475\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=34608)\u001b[0m 2023-04-23 22:23:45,587\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_258f1e14034342328ac3a9ab754dc1ee\n",
      "\u001b[2m\u001b[36m(PG pid=34608)\u001b[0m 2023-04-23 22:23:45,587\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 111, '_timesteps_total': None, '_time_total': 63.22013568878174, '_episodes_total': 11100}\n",
      "2023-04-23 22:23:46,565\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-23 22:23:46,566\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.33931624374174263 --- (resample) --> 0.9116697336941954\n",
      "\n",
      "2023-04-23 22:23:46,754\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00002 (score = -7.010000)\n",
      "\n",
      "2023-04-23 22:23:46,755\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00002:\n",
      "alpha : 0.33931624374174263 --- (* 0.8) --> 0.2714529949933941\n",
      "\n",
      "2023-04-23 22:23:47,630\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00001\n",
      "2023-04-23 22:23:49,379\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00003 (score = -7.000000) into trial 88e23_00001 (score = -7.010000)\n",
      "\n",
      "2023-04-23 22:23:49,380\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00001:\n",
      "alpha : 0.33931624374174263 --- (* 1.2) --> 0.40717949249009117\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=38208)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=38208)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=880)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=880)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=38208)\u001b[0m 2023-04-23 22:23:56,338\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=880)\u001b[0m 2023-04-23 22:23:56,333\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=38208)\u001b[0m 2023-04-23 22:23:56,603\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=38208)\u001b[0m 2023-04-23 22:23:56,624\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=880)\u001b[0m 2023-04-23 22:23:56,602\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=880)\u001b[0m 2023-04-23 22:23:56,624\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=38208)\u001b[0m 2023-04-23 22:23:56,717\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_27a01350119044d8921a03ca3ff18077\n",
      "\u001b[2m\u001b[36m(PG pid=38208)\u001b[0m 2023-04-23 22:23:56,717\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 175, '_timesteps_total': None, '_time_total': 88.56567430496216, '_episodes_total': 17500}\n",
      "\u001b[2m\u001b[36m(PG pid=880)\u001b[0m 2023-04-23 22:23:56,809\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f6c6eb2d9b624377a6d5b3afc66a55a0\n",
      "\u001b[2m\u001b[36m(PG pid=880)\u001b[0m 2023-04-23 22:23:56,810\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 175, '_timesteps_total': None, '_time_total': 88.56567430496216, '_episodes_total': 17500}\n",
      "\u001b[2m\u001b[36m(pid=26780)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=26780)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=26780)\u001b[0m 2023-04-23 22:23:58,890\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26780)\u001b[0m 2023-04-23 22:23:59,201\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=26780)\u001b[0m 2023-04-23 22:23:59,218\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=26780)\u001b[0m 2023-04-23 22:23:59,345\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_fb48dc3ac2a34acd86c9ad46ab9048d6\n",
      "\u001b[2m\u001b[36m(PG pid=26780)\u001b[0m 2023-04-23 22:23:59,345\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 180, '_timesteps_total': None, '_time_total': 90.46134781837463, '_episodes_total': 18000}\n",
      "2023-04-23 22:24:00,285\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00002 (score = -7.000000) into trial 88e23_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-23 22:24:00,286\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.2714529949933941 --- (* 0.8) --> 0.2171623959947153\n",
      "\n",
      "2023-04-23 22:24:04,618\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "2023-04-23 22:24:06,012\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00002\n",
      "\u001b[2m\u001b[36m(pid=9580)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9580)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=32736)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=32736)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=9580)\u001b[0m 2023-04-23 22:24:09,203\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=32736)\u001b[0m 2023-04-23 22:24:09,201\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=9580)\u001b[0m 2023-04-23 22:24:09,432\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=9580)\u001b[0m 2023-04-23 22:24:09,452\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=32736)\u001b[0m 2023-04-23 22:24:09,431\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=32736)\u001b[0m 2023-04-23 22:24:09,452\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=9580)\u001b[0m 2023-04-23 22:24:09,553\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f308e639b9f04eba898571b0ad5c4aed\n",
      "\u001b[2m\u001b[36m(PG pid=9580)\u001b[0m 2023-04-23 22:24:09,553\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 182, '_timesteps_total': None, '_time_total': 91.20218634605408, '_episodes_total': 18200}\n",
      "\u001b[2m\u001b[36m(PG pid=32736)\u001b[0m 2023-04-23 22:24:09,552\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ba4a521cda4342f69e314479dbbd6bdb\n",
      "\u001b[2m\u001b[36m(PG pid=32736)\u001b[0m 2023-04-23 22:24:09,552\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 182, '_timesteps_total': None, '_time_total': 91.19070148468018, '_episodes_total': 18200}\n",
      "2023-04-23 22:24:09,875\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_88e23_00000\n",
      "2023-04-23 22:24:10,208\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 88e23_00001 (score = -7.000000) into trial 88e23_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-23 22:24:10,209\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial88e23_00000:\n",
      "alpha : 0.40717949249009117 --- (* 0.8) --> 0.325743593992073\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=5784)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=5784)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=5784)\u001b[0m 2023-04-23 22:24:18,168\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=5784)\u001b[0m 2023-04-23 22:24:18,412\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=5784)\u001b[0m 2023-04-23 22:24:18,425\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=5784)\u001b[0m 2023-04-23 22:24:18,525\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_1e9eaa6926f24a918cd0784100c7c21c\n",
      "\u001b[2m\u001b[36m(PG pid=5784)\u001b[0m 2023-04-23 22:24:18,525\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 183, '_timesteps_total': None, '_time_total': 91.51413893699646, '_episodes_total': 18300}\n",
      "2023-04-23 22:24:23,882\tINFO tune.py:798 -- Total run time: 317.42 seconds (317.37 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "#from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "from gymnasium.spaces import Dict, Discrete, MultiDiscrete, Tuple,Box\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"PG\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=16)\n",
    "#parser.add_argument(\"--num-gpus\", type=int, default=1)\n",
    "parser.add_argument(\"--num-gpus\", type=float, default=0)#0.25)\n",
    "\n",
    "parser.add_argument(\"--num-workers\", type=int, default=6)\n",
    "parser.add_argument(\"--num-gpus-per-worker\", type=float, default=0.0)\n",
    "#parser.add_argument(\"render_mode\", type=int, default=1)\n",
    "parser.add_argument(\n",
    "    \"--mixer\",\n",
    "    type=str,\n",
    "    default=\"qmix\",\n",
    "    choices=[\"qmix\", \"vdn\", \"none\"],\n",
    "    help=\"The mixer model to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=70000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=9.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "            #if agent_id.startswith(\"low_level_\"):\n",
    "                #return \"low_level_policy\"\n",
    "            #else:\n",
    "                #return \"high_level_policy\"\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ray.init(num_cpus=16, num_gpus=0,local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0],\n",
    "        \"group_2\": [0,1],\n",
    "        #\"group_3\": [0]\n",
    "    }\n",
    "\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=observation_space, act_space=action_space\n",
    "        ),\n",
    "    )\n",
    "    \"\"\"\n",
    "    from ray.tune import register_env\n",
    "    from ray.rllib.algorithms.dqn import DQN \n",
    "    YourExternalEnv = ... \n",
    "    register_env(\"my_env\", \n",
    "        lambda config: YourExternalEnv(config))\n",
    "    trainer = DQN(env=\"my_env\") \n",
    "    while True: \n",
    "        print(trainer.train()) \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(TwoStepGame)\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=args.num_gpus,num_gpus_per_worker=args.num_gpus_per_worker,)\n",
    "        #.reset_config(reuse_actors=True)\n",
    "    )\n",
    "    def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "        #if agent_id.startswith(\"low_level_\"):\n",
    "        if agent_id.startswith(\"group_1\"):\n",
    "            return \"low_level_policy\"\n",
    "        else:\n",
    "            return \"high_level_policy\"\n",
    "\n",
    "    if args.run == \"QMIX\":\n",
    "        \n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "\n",
    "            .training(mixer=args.mixer, train_batch_size=32)\n",
    "            .resources(\n",
    "                # How many GPUs does the local worker (driver) need? For most algos,\n",
    "                # this is where the learning updates happen.\n",
    "                # Set this to > 1 for multi-GPU learning.\n",
    "                num_gpus=args.num_gpus,\n",
    "                # How many GPUs does each RolloutWorker (`num_workers`) need?\n",
    "                num_gpus_per_worker=args.num_gpus_per_worker,#0.25,\n",
    "        )           \n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        observation_space,\n",
    "                        action_space,\n",
    "                        config.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        #Tuple([observation_space,Box(-inf, inf, (1,), float64)]),#,Box()\n",
    "                        Tuple([observation_space,obs_space]),\n",
    "                        Tuple([action_space,obs_space]),#action_space,\n",
    "                        config.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn#lambda agent_id, episode, worker, **kwargs: \"pol2\"\n",
    "                #policy_mapping_fn=(lambda agent_id, episode, worker, **kw: (\"pol1\" if agent_id == \"agent1\" else \"pol2\")\n",
    "    #)#policy_mapping_fn,\n",
    "            )\n",
    "            .rollouts(num_rollout_workers=args.num_workers, num_envs_per_worker=args.num_envs_per_worker,)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "                }\n",
    "            )\n",
    "            .environment(\n",
    "                env=\"grouped_twostep\",\n",
    "                env_config={\n",
    "                    \"separate_state_space\": True,\n",
    "                    \"one_hot_state_encoding\": True,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr='training_iteration',\n",
    "        metric='episode_reward_mean',#'loss',\n",
    "        mode='min',\n",
    "        perturbation_interval=1,\n",
    "        hyperparam_mutations={\n",
    "            #\"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"alpha\": tune.uniform(0.0, 1.0),\n",
    "        }\n",
    "    )\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=4,\n",
    "            scheduler=pbt_scheduler,\n",
    "            #reuse_actors=True,\n",
    "        ),\n",
    "        \n",
    "        \n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf39e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ac0b1cf",
   "metadata": {},
   "source": [
    "#episode reward mean is 4.64 and policy loss is 1.6 under prior precision 20\n",
    "#policy loss increased to 1.65 and epsidoe reward mean increased to 4.79 under prior precison 0.0000000001\n",
    "#under prior precision 2 episdoe reward mean is 5.05 and loss is 1.7. second trial of same prior precison yielded 5.15 reward mean and 1.8 loss \n",
    "#trial 3 under prior precision 2 loss is 1.73 and episdoe reward mean is 5.04. trial 4 loss is 1.65 and epsidoe reward mean and reward mean 4.79\n",
    "#trial 5 is 1.43 policy loss and reward of 4.18 for prior precision 2 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3562efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.train.rl import RLTrainer\n",
    "\n",
    "trainer = RLTrainer(\n",
    "    run_config=air.RunConfig(stop=stop, verbose=2),#RunConfig(stop={\"training_iteration\": 5}),\n",
    "    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),\n",
    "    algorithm=\"QMIX\",\n",
    "    config=config\n",
    "\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    {\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"framework\": \"tf\",\n",
    "        \"evaluation_num_workers\": 1,\n",
    "        \"evaluation_interval\": 1,\n",
    "        \"evaluation_config\": {\"input\": \"sampler\"},\n",
    "    },\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63892b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#offline learning\n",
    "\n",
    "import ray\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.train.rl import RLTrainer\n",
    "from ray.rllib.algorithms.bc.bc import BC\n",
    "\"\"\"\n",
    "dataset = ray.data.read_json(\n",
    "    \"/tmp/data-dir\", parallelism=2, ray_remote_args={\"num_cpus\": 1}\n",
    ")\n",
    "\n",
    "trainer = RLTrainer(\n",
    "    run_config=RunConfig(stop={\"training_iteration\": 5}),\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    datasets={\"train\": dataset},\n",
    "    algorithm=BCTrainer,\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"framework\": \"tf\",\n",
    "        \"evaluation_num_workers\": 1,\n",
    "        \"evaluation_interval\": 1,\n",
    "        \"evaluation_config\": {\"input\": \"sampler\"},\n",
    "    },\n",
    ")\n",
    "result = trainer.fit()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "48bb5d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 11:10:27,763\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-04-15 11:10:36,355\tINFO trainable.py:172 -- Trainable.setup took 13.594 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7622193694114685, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.13, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 0.0, 0.0, 8.0, 1.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 0.0, 1.0, 1.0, 0.0, 1.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 1.0, 0.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0889644053444933, 'mean_inference_ms': 1.6715585888914801, 'mean_action_processing_ms': 0.1555366895685148, 'mean_env_wait_ms': 0.0685827055973793, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02419821421305339, 'ViewRequirementAgentConnector_ms': 0.2105109426710341}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.13, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 0.0, 0.0, 8.0, 1.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 0.0, 1.0, 1.0, 0.0, 1.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 1.0, 0.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0889644053444933, 'mean_inference_ms': 1.6715585888914801, 'mean_action_processing_ms': 0.1555366895685148, 'mean_env_wait_ms': 0.0685827055973793, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02419821421305339, 'ViewRequirementAgentConnector_ms': 0.2105109426710341}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 624.493, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 19.055, 'learn_throughput': 10495.731, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-36', 'timestamp': 1681582236, 'time_this_iter_s': 0.6264936923980713, 'time_total_s': 0.6264936923980713, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B1274533A0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.6264936923980713, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 15.0, 'ram_util_percent': 51.9, 'gpu_util_percent0': 0.0, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7117332816123962, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 3.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.02, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 0.0, 8.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 0.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 0.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 1.0, 0.0, 8.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.131954930369693, 'mean_inference_ms': 1.690488206478129, 'mean_action_processing_ms': 0.194280819405344, 'mean_env_wait_ms': 0.06201558576854982, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.04031777381896973, 'ViewRequirementAgentConnector_ms': 0.242478609085083}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.02, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 0.0, 8.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 0.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 0.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 1.0, 0.0, 8.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.131954930369693, 'mean_inference_ms': 1.690488206478129, 'mean_action_processing_ms': 0.194280819405344, 'mean_env_wait_ms': 0.06201558576854982, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.04031777381896973, 'ViewRequirementAgentConnector_ms': 0.242478609085083}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 800, 'timers': {'training_iteration_time_ms': 640.844, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 16.785, 'learn_throughput': 11915.721, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'done': False, 'episodes_total': 200, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-37', 'timestamp': 1681582237, 'time_this_iter_s': 0.658195972442627, 'time_total_s': 1.2846896648406982, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B130C0EB80>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.2846896648406982, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 19.5, 'ram_util_percent': 52.1, 'gpu_util_percent0': 0.07, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7356886267662048, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 5.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.17, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 1.0, 1.0, 1.0, 8.0, 7.0, 1.0, 1.0, 8.0, 0.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 0.0, 0.0, 7.0, 7.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 7.0, 8.0, 0.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0731271816767787, 'mean_inference_ms': 1.6306672437417138, 'mean_action_processing_ms': 0.19447577376532274, 'mean_env_wait_ms': 0.05301223221713811, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008994340896606445, 'ViewRequirementAgentConnector_ms': 0.21720051765441895}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.17, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 1.0, 1.0, 1.0, 8.0, 7.0, 1.0, 1.0, 8.0, 0.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 0.0, 0.0, 7.0, 7.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 7.0, 8.0, 0.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0731271816767787, 'mean_inference_ms': 1.6306672437417138, 'mean_action_processing_ms': 0.19447577376532274, 'mean_env_wait_ms': 0.05301223221713811, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008994340896606445, 'ViewRequirementAgentConnector_ms': 0.21720051765441895}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1200, 'timers': {'training_iteration_time_ms': 613.278, 'load_time_ms': 0.333, 'load_throughput': 600043.491, 'learn_time_ms': 15.523, 'learn_throughput': 12884.077, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'done': False, 'episodes_total': 300, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-38', 'timestamp': 1681582238, 'time_this_iter_s': 0.5591464042663574, 'time_total_s': 1.8438360691070557, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B12C398E50>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.8438360691070557, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'warmup_time': 13.599156856536865, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6982377171516418, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 7.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.13, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 8.0, 8.0, 7.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 0.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.037116056673238, 'mean_inference_ms': 1.5966630309410912, 'mean_action_processing_ms': 0.17776501163858893, 'mean_env_wait_ms': 0.05970733442556546, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.027981042861938477, 'ViewRequirementAgentConnector_ms': 0.15546607971191406}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.13, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 8.0, 8.0, 7.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 0.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.037116056673238, 'mean_inference_ms': 1.5966630309410912, 'mean_action_processing_ms': 0.17776501163858893, 'mean_env_wait_ms': 0.05970733442556546, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.027981042861938477, 'ViewRequirementAgentConnector_ms': 0.15546607971191406}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1600, 'timers': {'training_iteration_time_ms': 595.497, 'load_time_ms': 0.25, 'load_throughput': 800057.988, 'learn_time_ms': 14.142, 'learn_throughput': 14142.473, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'done': False, 'episodes_total': 400, 'training_iteration': 4, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-38', 'timestamp': 1681582238, 'time_this_iter_s': 0.5441555976867676, 'time_total_s': 2.3879916667938232, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B10F00D670>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.3879916667938232, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 12.5, 'ram_util_percent': 52.1, 'gpu_util_percent0': 0.1, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7672795057296753, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 9.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.39, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 8.0, 7.0, 8.0, 7.0, 8.0, 8.0, 8.0, 7.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0361888191916726, 'mean_inference_ms': 1.6117643762182647, 'mean_action_processing_ms': 0.1854994199373624, 'mean_env_wait_ms': 0.05881126586731142, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.026178359985351562, 'ViewRequirementAgentConnector_ms': 0.19472694396972656}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.39, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 8.0, 7.0, 8.0, 7.0, 8.0, 8.0, 8.0, 7.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0361888191916726, 'mean_inference_ms': 1.6117643762182647, 'mean_action_processing_ms': 0.1854994199373624, 'mean_env_wait_ms': 0.05881126586731142, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.026178359985351562, 'ViewRequirementAgentConnector_ms': 0.19472694396972656}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 600.055, 'load_time_ms': 0.2, 'load_throughput': 1000072.485, 'learn_time_ms': 14.114, 'learn_throughput': 14170.233, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'episodes_total': 500, 'training_iteration': 5, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-39', 'timestamp': 1681582239, 'time_this_iter_s': 0.6192898750305176, 'time_total_s': 3.007281541824341, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B130BFA250>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.007281541824341, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 28.6, 'ram_util_percent': 52.1, 'gpu_util_percent0': 0.0, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7927850484848022, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 11.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.61, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.029419958541832, 'mean_inference_ms': 1.578336552120466, 'mean_action_processing_ms': 0.17962388253827366, 'mean_env_wait_ms': 0.05886536851512898, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.020925521850585938, 'ViewRequirementAgentConnector_ms': 0.19593262672424316}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.61, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.029419958541832, 'mean_inference_ms': 1.578336552120466, 'mean_action_processing_ms': 0.17962388253827366, 'mean_env_wait_ms': 0.05886536851512898, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.020925521850585938, 'ViewRequirementAgentConnector_ms': 0.19593262672424316}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2400, 'timers': {'training_iteration_time_ms': 590.246, 'load_time_ms': 0.167, 'load_throughput': 1200086.981, 'learn_time_ms': 13.763, 'learn_throughput': 14531.725, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'done': False, 'episodes_total': 600, 'training_iteration': 6, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-40', 'timestamp': 1681582240, 'time_this_iter_s': 0.5431962013244629, 'time_total_s': 3.5504777431488037, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B12742EA90>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.5504777431488037, 'timesteps_since_restore': 0, 'iterations_since_restore': 6, 'warmup_time': 13.599156856536865, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.862657368183136, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 13.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.98, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0451639149548073, 'mean_inference_ms': 1.5994893236725263, 'mean_action_processing_ms': 0.18306513669914554, 'mean_env_wait_ms': 0.06176930168200864, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0386810302734375, 'ViewRequirementAgentConnector_ms': 0.25295305252075195}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.98, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0451639149548073, 'mean_inference_ms': 1.5994893236725263, 'mean_action_processing_ms': 0.18306513669914554, 'mean_env_wait_ms': 0.06176930168200864, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0386810302734375, 'ViewRequirementAgentConnector_ms': 0.25295305252075195}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2800, 'timers': {'training_iteration_time_ms': 597.495, 'load_time_ms': 0.143, 'load_throughput': 1400101.478, 'learn_time_ms': 13.225, 'learn_throughput': 15122.628, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'done': False, 'episodes_total': 700, 'training_iteration': 7, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-40', 'timestamp': 1681582240, 'time_this_iter_s': 0.6419870853424072, 'time_total_s': 4.192464828491211, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B12C46FA60>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 4.192464828491211, 'timesteps_since_restore': 0, 'iterations_since_restore': 7, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 19.5, 'ram_util_percent': 52.3, 'gpu_util_percent0': 0.0, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9490025639533997, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 15.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.23, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0629878201982064, 'mean_inference_ms': 1.6329474630838332, 'mean_action_processing_ms': 0.18956093844736022, 'mean_env_wait_ms': 0.06399357192297415, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.03689146041870117, 'ViewRequirementAgentConnector_ms': 0.21502208709716797}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.23, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0629878201982064, 'mean_inference_ms': 1.6329474630838332, 'mean_action_processing_ms': 0.18956093844736022, 'mean_env_wait_ms': 0.06399357192297415, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.03689146041870117, 'ViewRequirementAgentConnector_ms': 0.21502208709716797}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3200, 'timers': {'training_iteration_time_ms': 610.001, 'load_time_ms': 0.125, 'load_throughput': 1600115.975, 'learn_time_ms': 13.762, 'learn_throughput': 14532.334, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'done': False, 'episodes_total': 800, 'training_iteration': 8, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-41', 'timestamp': 1681582241, 'time_this_iter_s': 0.6985442638397217, 'time_total_s': 4.891009092330933, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B12C398E20>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 4.891009092330933, 'timesteps_since_restore': 0, 'iterations_since_restore': 8, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 7.5, 'ram_util_percent': 51.8, 'gpu_util_percent0': 0.0, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8179271221160889, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 17.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.04, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.050259920042929, 'mean_inference_ms': 1.6232797134458716, 'mean_action_processing_ms': 0.18790561182508203, 'mean_env_wait_ms': 0.06514095982070235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023944616317749023, 'ViewRequirementAgentConnector_ms': 0.19333314895629883}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.04, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.050259920042929, 'mean_inference_ms': 1.6232797134458716, 'mean_action_processing_ms': 0.18790561182508203, 'mean_env_wait_ms': 0.06514095982070235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023944616317749023, 'ViewRequirementAgentConnector_ms': 0.19333314895629883}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3600, 'timers': {'training_iteration_time_ms': 605.295, 'load_time_ms': 0.111, 'load_throughput': 1800130.472, 'learn_time_ms': 13.789, 'learn_throughput': 14504.101, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'done': False, 'episodes_total': 900, 'training_iteration': 9, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-42', 'timestamp': 1681582242, 'time_this_iter_s': 0.5686533451080322, 'time_total_s': 5.459662437438965, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B10EEF36A0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 5.459662437438965, 'timesteps_since_restore': 0, 'iterations_since_restore': 9, 'warmup_time': 13.599156856536865, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9234033226966858, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 19.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.38, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0634884126540245, 'mean_inference_ms': 1.6418654343177537, 'mean_action_processing_ms': 0.18953979164287488, 'mean_env_wait_ms': 0.0646500394440841, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.039147377014160156, 'ViewRequirementAgentConnector_ms': 0.21978545188903809}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.38, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0634884126540245, 'mean_inference_ms': 1.6418654343177537, 'mean_action_processing_ms': 0.18953979164287488, 'mean_env_wait_ms': 0.0646500394440841, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.039147377014160156, 'ViewRequirementAgentConnector_ms': 0.21978545188903809}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 2000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 611.693, 'load_time_ms': 0.1, 'load_throughput': 2000144.969, 'learn_time_ms': 13.81, 'learn_throughput': 14481.844, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 1000, 'training_iteration': 10, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-43', 'timestamp': 1681582243, 'time_this_iter_s': 0.6702642440795898, 'time_total_s': 6.129926681518555, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B130BF5490>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 6.129926681518555, 'timesteps_since_restore': 0, 'iterations_since_restore': 10, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 8.3, 'ram_util_percent': 51.7, 'gpu_util_percent0': 0.08, 'vram_util_percent0': 0.5096435546875}}\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()\n",
    "\n",
    "#moderate prior precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b77ea42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 23:54:09,009\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-04-14 23:54:16,566\tINFO trainable.py:172 -- Trainable.setup took 12.545 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6795865297317505, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.88, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 1.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.039853736535827, 'mean_inference_ms': 1.5531155600476623, 'mean_action_processing_ms': 0.17674170916353288, 'mean_env_wait_ms': 0.04500773415636661, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.012067079544067383, 'ViewRequirementAgentConnector_ms': 0.22479356659783256}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.88, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 1.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.039853736535827, 'mean_inference_ms': 1.5531155600476623, 'mean_action_processing_ms': 0.17674170916353288, 'mean_env_wait_ms': 0.04500773415636661, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.012067079544067383, 'ViewRequirementAgentConnector_ms': 0.22479356659783256}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 577.822, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.064, 'learn_throughput': 16578.932, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-17', 'timestamp': 1681541657, 'time_this_iter_s': 0.5788228511810303, 'time_total_s': 0.5788228511810303, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BA90>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.5788228511810303, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 31.1, 'ram_util_percent': 47.4, 'gpu_util_percent0': 0.12, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7314061522483826, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 3.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.11, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.919299827252243, 'mean_inference_ms': 1.4818142774396406, 'mean_action_processing_ms': 0.1818664056107291, 'mean_env_wait_ms': 0.062393725958845535, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021955251693725586, 'ViewRequirementAgentConnector_ms': 0.16707277297973633}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.11, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.919299827252243, 'mean_inference_ms': 1.4818142774396406, 'mean_action_processing_ms': 0.1818664056107291, 'mean_env_wait_ms': 0.062393725958845535, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021955251693725586, 'ViewRequirementAgentConnector_ms': 0.16707277297973633}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 800, 'timers': {'training_iteration_time_ms': 545.348, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.532, 'learn_throughput': 15958.998, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'done': False, 'episodes_total': 200, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-17', 'timestamp': 1681541657, 'time_this_iter_s': 0.5138733386993408, 'time_total_s': 1.092696189880371, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BE20>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.092696189880371, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 46.2, 'ram_util_percent': 47.3, 'gpu_util_percent0': 0.07, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.5058264136314392, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 5.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.49, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 0.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 8.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8828798665381505, 'mean_inference_ms': 1.4100138240566666, 'mean_action_processing_ms': 0.1712384914201429, 'mean_env_wait_ms': 0.06160482193983334, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.017079591751098633, 'ViewRequirementAgentConnector_ms': 0.14663290977478027}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.49, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 0.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 8.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8828798665381505, 'mean_inference_ms': 1.4100138240566666, 'mean_action_processing_ms': 0.1712384914201429, 'mean_env_wait_ms': 0.06160482193983334, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.017079591751098633, 'ViewRequirementAgentConnector_ms': 0.14663290977478027}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1200, 'timers': {'training_iteration_time_ms': 521.626, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 11.69, 'learn_throughput': 17109.25, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'done': False, 'episodes_total': 300, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-18', 'timestamp': 1681541658, 'time_this_iter_s': 0.47518205642700195, 'time_total_s': 1.567878246307373, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BD60>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.567878246307373, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'warmup_time': 12.548984050750732, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6492184400558472, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 7.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.0, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9334712439262023, 'mean_inference_ms': 1.4415602856658671, 'mean_action_processing_ms': 0.1759609479582711, 'mean_env_wait_ms': 0.0549618819828486, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023952960968017578, 'ViewRequirementAgentConnector_ms': 0.22032475471496582}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.0, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9334712439262023, 'mean_inference_ms': 1.4415602856658671, 'mean_action_processing_ms': 0.1759609479582711, 'mean_env_wait_ms': 0.0549618819828486, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023952960968017578, 'ViewRequirementAgentConnector_ms': 0.22032475471496582}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1600, 'timers': {'training_iteration_time_ms': 538.458, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.656, 'learn_throughput': 15803.187, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'done': False, 'episodes_total': 400, 'training_iteration': 4, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-18', 'timestamp': 1681541658, 'time_this_iter_s': 0.5899109840393066, 'time_total_s': 2.1577892303466797, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B1382D1820>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.1577892303466797, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 37.9, 'ram_util_percent': 47.2, 'gpu_util_percent0': 0.11, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7665503025054932, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 9.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.52, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9231414947357327, 'mean_inference_ms': 1.4200236771132928, 'mean_action_processing_ms': 0.15987335266052308, 'mean_env_wait_ms': 0.058973943079625474, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.046994924545288086, 'ViewRequirementAgentConnector_ms': 0.14233922958374023}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.52, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9231414947357327, 'mean_inference_ms': 1.4200236771132928, 'mean_action_processing_ms': 0.15987335266052308, 'mean_env_wait_ms': 0.058973943079625474, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.046994924545288086, 'ViewRequirementAgentConnector_ms': 0.14233922958374023}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 529.815, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.929, 'learn_throughput': 15469.659, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'episodes_total': 500, 'training_iteration': 5, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-19', 'timestamp': 1681541659, 'time_this_iter_s': 0.4962425231933594, 'time_total_s': 2.654031753540039, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13FD5B580>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.654031753540039, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'warmup_time': 12.548984050750732, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8569368720054626, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 11.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9555395794947082, 'mean_inference_ms': 1.4787746607314338, 'mean_action_processing_ms': 0.17303054676167076, 'mean_env_wait_ms': 0.0628914860861188, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02704167366027832, 'ViewRequirementAgentConnector_ms': 0.2059648036956787}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9555395794947082, 'mean_inference_ms': 1.4787746607314338, 'mean_action_processing_ms': 0.17303054676167076, 'mean_env_wait_ms': 0.0628914860861188, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02704167366027832, 'ViewRequirementAgentConnector_ms': 0.2059648036956787}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2400, 'timers': {'training_iteration_time_ms': 550.907, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.771, 'learn_throughput': 15660.273, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'done': False, 'episodes_total': 600, 'training_iteration': 6, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-20', 'timestamp': 1681541660, 'time_this_iter_s': 0.6573679447174072, 'time_total_s': 3.3113996982574463, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13826E400>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.3113996982574463, 'timesteps_since_restore': 0, 'iterations_since_restore': 6, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 0.0, 'ram_util_percent': 47.1, 'gpu_util_percent0': 0.27, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7892510890960693, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 13.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.64, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9581208824685945, 'mean_inference_ms': 1.4988720885010631, 'mean_action_processing_ms': 0.17073016605744787, 'mean_env_wait_ms': 0.06317990239733548, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02521538734436035, 'ViewRequirementAgentConnector_ms': 0.1761481761932373}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.64, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9581208824685945, 'mean_inference_ms': 1.4988720885010631, 'mean_action_processing_ms': 0.17073016605744787, 'mean_env_wait_ms': 0.06317990239733548, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02521538734436035, 'ViewRequirementAgentConnector_ms': 0.1761481761932373}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2800, 'timers': {'training_iteration_time_ms': 555.635, 'load_time_ms': 0.143, 'load_throughput': 1398767.413, 'learn_time_ms': 13.375, 'learn_throughput': 14952.879, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'done': False, 'episodes_total': 700, 'training_iteration': 7, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-20', 'timestamp': 1681541660, 'time_this_iter_s': 0.5860385894775391, 'time_total_s': 3.8974382877349854, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BDF0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.8974382877349854, 'timesteps_since_restore': 0, 'iterations_since_restore': 7, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 0.0, 'ram_util_percent': 47.0, 'gpu_util_percent0': 0.41, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8371381759643555, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 15.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.03, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.944832427139806, 'mean_inference_ms': 1.4962974300539398, 'mean_action_processing_ms': 0.16498297620459987, 'mean_env_wait_ms': 0.06601082243672166, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.01466989517211914, 'ViewRequirementAgentConnector_ms': 0.12430882453918457}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.03, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.944832427139806, 'mean_inference_ms': 1.4962974300539398, 'mean_action_processing_ms': 0.16498297620459987, 'mean_env_wait_ms': 0.06601082243672166, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.01466989517211914, 'ViewRequirementAgentConnector_ms': 0.12430882453918457}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3200, 'timers': {'training_iteration_time_ms': 552.196, 'load_time_ms': 0.125, 'load_throughput': 1598591.329, 'learn_time_ms': 13.453, 'learn_throughput': 14866.137, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'done': False, 'episodes_total': 800, 'training_iteration': 8, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-21', 'timestamp': 1681541661, 'time_this_iter_s': 0.529125452041626, 'time_total_s': 4.426563739776611, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B138313160>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 4.426563739776611, 'timesteps_since_restore': 0, 'iterations_since_restore': 8, 'warmup_time': 12.548984050750732, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8821440935134888, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 17.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.31, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.960271534557014, 'mean_inference_ms': 1.5386448245390063, 'mean_action_processing_ms': 0.16615392101400103, 'mean_env_wait_ms': 0.06758842383537736, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.022125959396362305, 'ViewRequirementAgentConnector_ms': 0.2020738124847412}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.31, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.960271534557014, 'mean_inference_ms': 1.5386448245390063, 'mean_action_processing_ms': 0.16615392101400103, 'mean_env_wait_ms': 0.06758842383537736, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.022125959396362305, 'ViewRequirementAgentConnector_ms': 0.2020738124847412}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3600, 'timers': {'training_iteration_time_ms': 565.449, 'load_time_ms': 0.111, 'load_throughput': 1798415.245, 'learn_time_ms': 14.18, 'learn_throughput': 14104.005, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'done': False, 'episodes_total': 900, 'training_iteration': 9, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-22', 'timestamp': 1681541662, 'time_this_iter_s': 0.6734707355499268, 'time_total_s': 5.100034475326538, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B137FF9AF0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 5.100034475326538, 'timesteps_since_restore': 0, 'iterations_since_restore': 9, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 8.0, 'ram_util_percent': 47.1, 'gpu_util_percent0': 0.03, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8496705293655396, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 19.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.59, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9562232862526865, 'mean_inference_ms': 1.5362027524293271, 'mean_action_processing_ms': 0.17369788387666524, 'mean_env_wait_ms': 0.06433000331041754, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.013000011444091797, 'ViewRequirementAgentConnector_ms': 0.17013216018676758}}, 'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.59, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9562232862526865, 'mean_inference_ms': 1.5362027524293271, 'mean_action_processing_ms': 0.17369788387666524, 'mean_env_wait_ms': 0.06433000331041754, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.013000011444091797, 'ViewRequirementAgentConnector_ms': 0.17013216018676758}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 2000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 565.019, 'load_time_ms': 0.2, 'load_throughput': 1000430.292, 'learn_time_ms': 14.163, 'learn_throughput': 14121.76, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 1000, 'training_iteration': 10, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-22', 'timestamp': 1681541662, 'time_this_iter_s': 0.5621469020843506, 'time_total_s': 5.662181377410889, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13B131CD0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 5.662181377410889, 'timesteps_since_restore': 0, 'iterations_since_restore': 10, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 11.6, 'ram_util_percent': 47.1, 'gpu_util_percent0': 0.16, 'vram_util_percent0': 0.56298828125}}\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()\n",
    "\n",
    "#prior precision is large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e19fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()\n",
    "\n",
    "#small prior precision\n",
    "#episode reward mean is 4.64 and policy loss is 1.6 under prior precision 20\n",
    "#policy loss increased to 1.65 and epsidoe reward mean increased to 4.79 under prior precison 0.0000000001\n",
    "#under prior precision 2 episdoe reward mean is 5.05 and loss is 1.7. second trial of same prior precison yielded 5.15 reward mean and 1.8 loss \n",
    "#trial 3 under prior precision 2 loss is 1.73 and episdoe reward mean is 5.04. trial 4 loss is 1.65 and epsidoe reward mean and reward mean 4.79\n",
    "#trial 5 is 1.43 policy loss and reward of 4.18 for prior precision 2 \n",
    "\n",
    "#reward is increasing but policy loss is also increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bdd352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 23:46:46,491\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2023-04-19 23:46:50,066\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-04-19 23:46:55,310\tWARNING worker.py:846 -- `ray.get_gpu_ids()` will always return the empty list when called from the driver. This is because Ray does not manage GPU allocations to the driver process.\n",
      "2023-04-19 23:46:55,310\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-04-19 23:46:55,312\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property worker_index not supported.\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6482515335083008, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.79, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 0.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7618090406579167, 'mean_inference_ms': 1.0791999190600952, 'mean_action_processing_ms': 0.11994945469187263, 'mean_env_wait_ms': 0.031213855268943377, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.024434328079223633, 'ViewRequirementAgentConnector_ms': 0.15880955590142143}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.79, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 0.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7618090406579167, 'mean_inference_ms': 1.0791999190600952, 'mean_action_processing_ms': 0.11994945469187263, 'mean_env_wait_ms': 0.031213855268943377, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.024434328079223633, 'ViewRequirementAgentConnector_ms': 0.15880955590142143}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 411.886, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 9.463, 'learn_throughput': 21135.851, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-55', 'timestamp': 1681973215, 'time_this_iter_s': 0.4128868579864502, 'time_total_s': 0.4128868579864502, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB647F0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.4128868579864502, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 3.2, 'ram_util_percent': 40.3, 'gpu_util_percent0': 0.47, 'vram_util_percent0': 0.53369140625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.5196247100830078, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 3.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.47, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 8.0, 0.0, 8.0, 7.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 8.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 0.0, 0.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6959997210419389, 'mean_inference_ms': 1.0471641274164445, 'mean_action_processing_ms': 0.1251792669890825, 'mean_env_wait_ms': 0.03438102931453106, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008956432342529297, 'ViewRequirementAgentConnector_ms': 0.10797858238220215}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.47, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 8.0, 0.0, 8.0, 7.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 8.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 0.0, 0.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6959997210419389, 'mean_inference_ms': 1.0471641274164445, 'mean_action_processing_ms': 0.1251792669890825, 'mean_env_wait_ms': 0.03438102931453106, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008956432342529297, 'ViewRequirementAgentConnector_ms': 0.10797858238220215}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 800, 'timers': {'training_iteration_time_ms': 391.23, 'load_time_ms': 0.502, 'load_throughput': 398698.099, 'learn_time_ms': 8.234, 'learn_throughput': 24289.812, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'done': False, 'episodes_total': 200, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-56', 'timestamp': 1681973216, 'time_this_iter_s': 0.37207984924316406, 'time_total_s': 0.7849667072296143, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB53FD0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.7849667072296143, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 4.1, 'ram_util_percent': 40.4, 'gpu_util_percent0': 0.01, 'vram_util_percent0': 0.52978515625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.55398428440094, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 5.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.61, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 1.0, 8.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 0.0, 8.0, 0.0, 0.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 8.0, 8.0, 1.0, 8.0, 0.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6706936783877867, 'mean_inference_ms': 1.033522721733309, 'mean_action_processing_ms': 0.12198105429650936, 'mean_env_wait_ms': 0.029594053246217236, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.016854286193847656, 'ViewRequirementAgentConnector_ms': 0.1025397777557373}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.61, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 1.0, 8.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 0.0, 8.0, 0.0, 0.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 8.0, 8.0, 1.0, 8.0, 0.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6706936783877867, 'mean_inference_ms': 1.033522721733309, 'mean_action_processing_ms': 0.12198105429650936, 'mean_env_wait_ms': 0.029594053246217236, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.016854286193847656, 'ViewRequirementAgentConnector_ms': 0.1025397777557373}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1200, 'timers': {'training_iteration_time_ms': 381.445, 'load_time_ms': 0.334, 'load_throughput': 598047.148, 'learn_time_ms': 8.001, 'learn_throughput': 24998.335, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'done': False, 'episodes_total': 300, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-56', 'timestamp': 1681973216, 'time_this_iter_s': 0.3629014492034912, 'time_total_s': 1.1478681564331055, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB64430>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.1478681564331055, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8084396123886108, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 7.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.42, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6679702787363574, 'mean_inference_ms': 1.0462500778179191, 'mean_action_processing_ms': 0.12052639593345839, 'mean_env_wait_ms': 0.03543328703119514, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0290679931640625, 'ViewRequirementAgentConnector_ms': 0.12602901458740234}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.42, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6679702787363574, 'mean_inference_ms': 1.0462500778179191, 'mean_action_processing_ms': 0.12052639593345839, 'mean_env_wait_ms': 0.03543328703119514, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0290679931640625, 'ViewRequirementAgentConnector_ms': 0.12602901458740234}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1600, 'timers': {'training_iteration_time_ms': 384.338, 'load_time_ms': 0.251, 'load_throughput': 797396.198, 'learn_time_ms': 8.061, 'learn_throughput': 24811.026, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'done': False, 'episodes_total': 400, 'training_iteration': 4, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-56', 'timestamp': 1681973216, 'time_this_iter_s': 0.39403510093688965, 'time_total_s': 1.5419032573699951, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB57850>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.5419032573699951, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 2.8, 'ram_util_percent': 40.4, 'gpu_util_percent0': 0.04, 'vram_util_percent0': 0.52978515625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7519081234931946, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 9.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.36, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626207273561401, 'mean_inference_ms': 1.0419222977492484, 'mean_action_processing_ms': 0.1175179705395923, 'mean_env_wait_ms': 0.039889024092362724, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.009093046188354492, 'ViewRequirementAgentConnector_ms': 0.125152587890625}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.36, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626207273561401, 'mean_inference_ms': 1.0419222977492484, 'mean_action_processing_ms': 0.1175179705395923, 'mean_env_wait_ms': 0.039889024092362724, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.009093046188354492, 'ViewRequirementAgentConnector_ms': 0.125152587890625}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 383.847, 'load_time_ms': 0.201, 'load_throughput': 996745.247, 'learn_time_ms': 8.964, 'learn_throughput': 22311.433, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'episodes_total': 500, 'training_iteration': 5, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-57', 'timestamp': 1681973217, 'time_this_iter_s': 0.3828868865966797, 'time_total_s': 1.9247901439666748, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB946A0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.9247901439666748, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7848349213600159, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 11.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.51, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6609894056105791, 'mean_inference_ms': 1.0494113861770056, 'mean_action_processing_ms': 0.12151188497043072, 'mean_env_wait_ms': 0.03701780956055501, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021304607391357422, 'ViewRequirementAgentConnector_ms': 0.14121270179748535}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.51, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6609894056105791, 'mean_inference_ms': 1.0494113861770056, 'mean_action_processing_ms': 0.12151188497043072, 'mean_env_wait_ms': 0.03701780956055501, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021304607391357422, 'ViewRequirementAgentConnector_ms': 0.14121270179748535}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2400, 'timers': {'training_iteration_time_ms': 385.455, 'load_time_ms': 0.338, 'load_throughput': 592137.035, 'learn_time_ms': 8.901, 'learn_throughput': 22470.188, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'done': False, 'episodes_total': 600, 'training_iteration': 6, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-57', 'timestamp': 1681973217, 'time_this_iter_s': 0.394512414932251, 'time_total_s': 2.319302558898926, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB57F10>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.319302558898926, 'timesteps_since_restore': 0, 'iterations_since_restore': 6, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 3.7, 'ram_util_percent': 40.5, 'gpu_util_percent0': 0.05, 'vram_util_percent0': 0.52978515625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8710399270057678, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 13.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 1.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6585515966422213, 'mean_inference_ms': 1.055777711071856, 'mean_action_processing_ms': 0.12322258387694271, 'mean_env_wait_ms': 0.03422387236786434, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008939266204833984, 'ViewRequirementAgentConnector_ms': 0.12508893013000488}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 1.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6585515966422213, 'mean_inference_ms': 1.055777711071856, 'mean_action_processing_ms': 0.12322258387694271, 'mean_env_wait_ms': 0.03422387236786434, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008939266204833984, 'ViewRequirementAgentConnector_ms': 0.12508893013000488}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2800, 'timers': {'training_iteration_time_ms': 385.937, 'load_time_ms': 0.43, 'load_throughput': 465332.087, 'learn_time_ms': 8.849, 'learn_throughput': 22602.622, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'done': False, 'episodes_total': 700, 'training_iteration': 7, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-58', 'timestamp': 1681973218, 'time_this_iter_s': 0.38883185386657715, 'time_total_s': 2.708134412765503, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB64D00>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.708134412765503, 'timesteps_since_restore': 0, 'iterations_since_restore': 7, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.85731041431427, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 15.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.07, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6657106886797587, 'mean_inference_ms': 1.0772254748466532, 'mean_action_processing_ms': 0.12328861505816983, 'mean_env_wait_ms': 0.03620254330751226, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.010513544082641602, 'ViewRequirementAgentConnector_ms': 0.12743854522705078}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.07, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6657106886797587, 'mean_inference_ms': 1.0772254748466532, 'mean_action_processing_ms': 0.12328861505816983, 'mean_env_wait_ms': 0.03620254330751226, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.010513544082641602, 'ViewRequirementAgentConnector_ms': 0.12743854522705078}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3200, 'timers': {'training_iteration_time_ms': 393.099, 'load_time_ms': 0.376, 'load_throughput': 531808.099, 'learn_time_ms': 9.651, 'learn_throughput': 20723.678, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'done': False, 'episodes_total': 800, 'training_iteration': 8, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-58', 'timestamp': 1681973218, 'time_this_iter_s': 0.44522714614868164, 'time_total_s': 3.1533615589141846, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB53520>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.1533615589141846, 'timesteps_since_restore': 0, 'iterations_since_restore': 8, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 3.6, 'ram_util_percent': 40.6, 'gpu_util_percent0': 0.07, 'vram_util_percent0': 0.5296630859375}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9545664191246033, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 17.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.57, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6633380728917012, 'mean_inference_ms': 1.066249718208038, 'mean_action_processing_ms': 0.11748092032882122, 'mean_env_wait_ms': 0.038373476925457534, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008992910385131836, 'ViewRequirementAgentConnector_ms': 0.14661622047424316}}, 'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.57, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6633380728917012, 'mean_inference_ms': 1.066249718208038, 'mean_action_processing_ms': 0.11748092032882122, 'mean_env_wait_ms': 0.038373476925457534, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008992910385131836, 'ViewRequirementAgentConnector_ms': 0.14661622047424316}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3600, 'timers': {'training_iteration_time_ms': 389.767, 'load_time_ms': 0.334, 'load_throughput': 598284.111, 'learn_time_ms': 9.587, 'learn_throughput': 20862.513, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'done': False, 'episodes_total': 900, 'training_iteration': 9, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-59', 'timestamp': 1681973219, 'time_this_iter_s': 0.36411547660827637, 'time_total_s': 3.517477035522461, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB94910>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.517477035522461, 'timesteps_since_restore': 0, 'iterations_since_restore': 9, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9812411069869995, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 19.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.62, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626733001144689, 'mean_inference_ms': 1.061902172502311, 'mean_action_processing_ms': 0.11869277553758524, 'mean_env_wait_ms': 0.039210503009603596, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0190579891204834, 'ViewRequirementAgentConnector_ms': 0.1209249496459961}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.62, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626733001144689, 'mean_inference_ms': 1.061902172502311, 'mean_action_processing_ms': 0.11869277553758524, 'mean_env_wait_ms': 0.039210503009603596, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0190579891204834, 'ViewRequirementAgentConnector_ms': 0.1209249496459961}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 2000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 389.165, 'load_time_ms': 0.301, 'load_throughput': 664760.124, 'learn_time_ms': 9.579, 'learn_throughput': 20878.714, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 1000, 'training_iteration': 10, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-59', 'timestamp': 1681973219, 'time_this_iter_s': 0.3837430477142334, 'time_total_s': 3.9012200832366943, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F0002B7280>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.9012200832366943, 'timesteps_since_restore': 0, 'iterations_since_restore': 10, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 5.5, 'ram_util_percent': 40.4, 'gpu_util_percent0': 0.05, 'vram_util_percent0': 0.5296630859375}}\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ef5e8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.models.torch.torch_action_dist.TorchCategorical'>\n"
     ]
    }
   ],
   "source": [
    "policy = algo.get_policy()\n",
    "print(policy.dist_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "308f866c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PG' object has no attribute 'observation_space'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_15792\\2622574447.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    prep = get_preprocessor(algo.observation_space)#(algo.observation_space)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m\u001b[1;31m:\u001b[0m 'PG' object has no attribute 'observation_space'\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "prep = get_preprocessor(algo.observation_space)#(algo.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c5ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
