{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntrained_pop, fitnesses = train_multi_agent(\\n    pop=pop_agent,\\n    env=env,\\n    algo=\"MATD3\",\\n    env_name=\"cichyenv\",\\n    memory=memory,\\n    swap_channels=False,\\n    max_steps=500000,\\n    evo_steps=10000,\\n    eval_steps=None,\\n    eval_loop=1,\\n    target=200.0,\\n    tournament=tournament,\\n    wb=False,\\n)\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from pettingzoo import ParallelEnv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from agilerl.utils.utils import create_population as Population\n",
    "from agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\n",
    "from agilerl.training.train_offline import train_offline\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.training.train_multi_agent import train_multi_agent\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "#set device to the dual t4s. \n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "# -------------------------------\n",
    "# Environment Definition\n",
    "# -------------------------------\n",
    "class CichyEnv(ParallelEnv):\n",
    "    metadata = {\"name\": \"cichyenv\"}\n",
    "\n",
    "    def __init__(self, images, y1, y2):\n",
    "        self.images = images.reshape(92, 175, 175)  # Ensure correct shape\n",
    "        self.y1 = y1  # Rewards for agent IT\n",
    "        self.y2 = y2  # Rewards for agent EVC\n",
    "        self.agents = [\"IT\", \"EVC\"]\n",
    "        self.agent_ids = [\"IT\", \"EVC\"]\n",
    "        self.current_step = 0  # Initialize current_step\n",
    "        \n",
    "        # Merging observation spaces into a single space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"image\": spaces.Box(low=0, high=255, shape=(175, 175), dtype=np.uint8),  # Expecting image shape of (175, 175)\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(93,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0  # Ensure it's set to 0 at the start of each episode\n",
    "        self.actions = {agent: [] for agent in self.agent_ids}\n",
    "        \n",
    "        # Creating observation dictionary\n",
    "        obs = {}\n",
    "        for agent in self.agent_ids:\n",
    "            image = self.images[self.current_step]  # Get the image for the current step\n",
    "            if image.shape != (175, 175):\n",
    "                raise ValueError(f\"Unexpected image shape: {image.shape}, expected (175, 175)\")\n",
    "\n",
    "            # Expand the image dimensions to make it 5D for Conv3D (batch, channels, depth, height, width)\n",
    "            image_expanded = np.expand_dims(image, axis=0)  # Adds a batch dimension, (1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds a channel dimension, (1, 1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds depth dimension, (1, 1, 1, 175, 175)\n",
    "\n",
    "            obs[agent] = {\n",
    "                \"image\": image_expanded,  # Now shape (1, 1, 1, 175, 175)\n",
    "                \"other_action\": np.array([0.0]),  # Dummy value for the action of the other agent\n",
    "            }\n",
    "            \"\"\"\n",
    "            uncomment out soon \n",
    "\n",
    "            obs = {\n",
    "                agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "                for agent_id in self.agent_ids\n",
    "                }\n",
    "            \"\"\"\n",
    "                    \n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        if self.current_step >= len(self.images):\n",
    "            self.current_step = len(self.images) - 1\n",
    "\n",
    "        obs, rewards, dones, infos = {}, {}, {}, {}\n",
    "\n",
    "        for agent_id in self.agent_ids:\n",
    "            other_agent_id = \"EVC\" if agent_id == \"IT\" else \"IT\"\n",
    "\n",
    "            full_action = action_dict.get(agent_id, np.zeros((93,)))\n",
    "            agent_actions = full_action[:92].reshape(-1, 1)#(full_action[:92]).reshape((92, 1))  \n",
    "            other_action = full_action[92:]  \n",
    "\n",
    "            self.actions[agent_id].append(agent_actions)  \n",
    "\n",
    "            other_actions = action_dict.get(other_agent_id, np.zeros((93,)))[92:]  \n",
    "\n",
    "            obs[agent_id] = {\n",
    "                \"image\": torch.tensor(self.images[self.current_step], dtype=torch.float32),  # Convert to tensor\n",
    "                \"other_action\": torch.tensor(other_actions, dtype=torch.float32),  # Convert to tensor\n",
    "            }\n",
    "\n",
    "            #rewards[agent_id] = self._calculate_reward(agent_id)  \n",
    "\n",
    "            rewards[agent_id] = np.array([self._calculate_reward(agent_id)], dtype=np.float32)\n",
    "            dones[agent_id] = self.current_step >= len(self.images) - 1\n",
    "            infos[agent_id] = {}\n",
    "\n",
    "        dones[\"__all__\"] = all(dones.values())\n",
    "        self.current_step += 1  \n",
    "        return obs, rewards, dones, {}, infos\n",
    "    def _calculate_reward(self, agent_id):\n",
    "        \"\"\"Computes the reward based on similarity to the expert RDM using all 92 actions per step.\"\"\"\n",
    "        actions = np.array(self.actions[agent_id])  # Shape: (num_steps, 92, 1)\n",
    "        \n",
    "        num_steps = actions.shape[0]  \n",
    "        num_images = actions.shape[1]  \n",
    "\n",
    "        if num_steps < 2:  \n",
    "            return 0  \n",
    "\n",
    "        simulated_rdm = np.zeros((num_images, num_images))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for j in range(num_images):\n",
    "                if i != j:\n",
    "                    sim = cosine_similarity(actions[:, i].reshape(-1, 1), actions[:, j].reshape(-1, 1))[0][0]\n",
    "                    simulated_rdm[i, j] = 1 - sim\n",
    "\n",
    "        expert_rdm = self.y1 if agent_id == \"IT\" else self.y2\n",
    "\n",
    "        min_size = min(simulated_rdm.shape[0], expert_rdm.shape[0])\n",
    "        reward = -np.sum((simulated_rdm[:min_size, :min_size] - expert_rdm[:min_size, :min_size]) ** 2)\n",
    "        return reward\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training Setup with AgileRL\n",
    "# -------------------------------8-=\n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "\n",
    "env = CichyEnv(x_train, y_train1, y_train2)\n",
    "\n",
    "# Initial Hyperparameters\n",
    "INIT_HP = {\n",
    "    \"DOUBLE\": True,\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"POPULATION_SIZE\": 2,\n",
    "    \"O_U_NOISE\": 0.2,\n",
    "    \"EXPL_NOISE\": 0.1,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"LR\": 0.001,\n",
    "    \"LR_ACTOR\": 0.002,\n",
    "    \"LR_CRITIC\": 0.002,\n",
    "    \"TAU\": 0.5,\n",
    "    \"GAMMA\": 1.0,\n",
    "    \"LAMBDA\": 1.0,\n",
    "    \"REG\": 0.0625,\n",
    "    \"LEARN_STEP\": 2,\n",
    "    \"MEAN_NOISE\": 1,\n",
    "    \"THETA\": 1,\n",
    "    \"DT\": 1,\n",
    "    \"POLICY_FREQ\": 2,\n",
    "    \"AGENT_IDS\": [\"IT\",\"EVC\"],\n",
    "    \"MEMORY_SIZE\": 100000\n",
    "        \n",
    "}\n",
    "\n",
    "hp_config = HyperparameterConfig(\n",
    "    #lr=RLParameter(min=6.25e-5, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n",
    ")\n",
    "\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "\n",
    "# Create populations for each agent\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# Tournament selection\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,\n",
    "    elitism=True,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    eval_loop=1,\n",
    ")\n",
    "\n",
    "# Mutation settings\n",
    "mutations = Mutations(\n",
    "    no_mutation=0.4,\n",
    "    architecture=0.2,\n",
    "    new_layer_prob=0.2,\n",
    "    parameters=0.2,\n",
    "    activation=0,\n",
    "    rl_hp=0.2,\n",
    "    mutation_sd=0.1,\n",
    "    rand_seed=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Offline training\n",
    "trained_pop, pop_fitnesses = train_multi_agent(#train_on_policy(#train_offline(\n",
    "    pop=[pop_agent1, pop_agent2],\n",
    "    env=env,\n",
    "    algo=\"MATD3\",\n",
    "    env_name=\"cichyenv\",\n",
    "    #dataset=[x_train, y_train1, y_train2],\n",
    "    memory=memory,  # Replay buffer if needed\n",
    "    swap_channels=False,  # Ensure channel order is correct\n",
    "    max_steps=500000,\n",
    "    evo_steps=10000,\n",
    "    eval_steps=None,\n",
    "    eval_loop=1,\n",
    "    target=200.0,\n",
    "    tournament=tournament,\n",
    "    #mutation=mutations,\n",
    "    wb=False,  # Weights & Biases logging\n",
    "    #accelerator=device\n",
    ")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "trained_pop, fitnesses = train_multi_agent(\n",
    "    pop=pop_agent,\n",
    "    env=env,\n",
    "    algo=\"MATD3\",\n",
    "    env_name=\"cichyenv\",\n",
    "    memory=memory,\n",
    "    swap_channels=False,\n",
    "    max_steps=500000,\n",
    "    evo_steps=10000,\n",
    "    eval_steps=None,\n",
    "    eval_loop=1,\n",
    "    target=200.0,\n",
    "    tournament=tournament,\n",
    "    wb=False,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research team suggests that the number of representative agents that are needed are 5 agents for EVC and 3 agents for IT. Therefore we will use 9 agents total since 1 needs to represent the occipital lobe \n",
    "\n",
    "Other TODOs\n",
    "\n",
    "TODO: figure out visualization and figure out how to increase number of enviroments\n",
    "\n",
    "other TODO: switch over from RDMs to MEG \n",
    "\n",
    "In order to do this switch over the research team needs to figure out which channels correspond to which parts of the brain so that irrelavent channels can be removed. \n",
    "\n",
    "how to do groupings? the answer is to use hierarchical learning \n",
    "\n",
    "other TODO: swap out classical neural network for UODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'IT': {'image': tensor([[ 37., 244., 193.,  ..., 205.,  63., 145.],\n",
      "        [ 60.,  80.,  61.,  ...,  56.,  26.,   5.],\n",
      "        [109.,  26., 153.,  ...,  11.,  52., 114.],\n",
      "        ...,\n",
      "        [252.,  93.,  67.,  ..., 226.,  78.,  35.],\n",
      "        [246.,  24.,  80.,  ..., 183., 222., 165.],\n",
      "        [ 34., 136., 152.,  ..., 175., 230., 136.]]), 'other_action': tensor([0.7162])}, 'EVC': {'image': tensor([[ 37., 244., 193.,  ..., 205.,  63., 145.],\n",
      "        [ 60.,  80.,  61.,  ...,  56.,  26.,   5.],\n",
      "        [109.,  26., 153.,  ...,  11.,  52., 114.],\n",
      "        ...,\n",
      "        [252.,  93.,  67.,  ..., 226.,  78.,  35.],\n",
      "        [246.,  24.,  80.,  ..., 183., 222., 165.],\n",
      "        [ 34., 136., 152.,  ..., 175., 230., 136.]]), 'other_action': tensor([0.6542])}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000000step [01:01, 405757.86step/s] ep/s]\n",
      "24985000step [00:14, 1484579.37step/s]     "
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected torch.Tensor, got <class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m     pop_episode_scores\u001b[38;5;241m.\u001b[39mappend(completed_episode_scores)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Evaluate population\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    129\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtest(\n\u001b[1;32m    130\u001b[0m         env,\n\u001b[1;32m    131\u001b[0m         swap_channels\u001b[38;5;241m=\u001b[39mINIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHANNELS_LAST\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    132\u001b[0m         max_steps\u001b[38;5;241m=\u001b[39meval_steps,\n\u001b[1;32m    133\u001b[0m         loop\u001b[38;5;241m=\u001b[39meval_loop,\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m pop_agent\n\u001b[1;32m    136\u001b[0m ]\n\u001b[1;32m    137\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         np\u001b[38;5;241m.\u001b[39mmean(episode_scores)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode_scores \u001b[38;5;129;01min\u001b[39;00m pop_episode_scores\n\u001b[1;32m    144\u001b[0m ]\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Global steps \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m     pop_episode_scores\u001b[38;5;241m.\u001b[39mappend(completed_episode_scores)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Evaluate population\u001b[39;00m\n\u001b[1;32m    128\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 129\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mswap_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINIT_HP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCHANNELS_LAST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_loop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m pop_agent\n\u001b[1;32m    136\u001b[0m ]\n\u001b[1;32m    137\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         np\u001b[38;5;241m.\u001b[39mmean(episode_scores)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode_scores \u001b[38;5;129;01min\u001b[39;00m pop_episode_scores\n\u001b[1;32m    144\u001b[0m ]\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Global steps \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/algorithms/matd3.py:964\u001b[0m, in \u001b[0;36mMATD3.test\u001b[0;34m(self, env, swap_channels, max_steps, loop, sum_scores)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         obs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    961\u001b[0m             agent_id: np\u001b[38;5;241m.\u001b[39mmoveaxis(np\u001b[38;5;241m.\u001b[39mexpand_dims(s, \u001b[38;5;241m0\u001b[39m), [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    962\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m agent_id, s \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    963\u001b[0m         }\n\u001b[0;32m--> 964\u001b[0m cont_actions, discrete_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscrete_actions:\n\u001b[1;32m    970\u001b[0m     action \u001b[38;5;241m=\u001b[39m discrete_action\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/algorithms/matd3.py:536\u001b[0m, in \u001b[0;36mMATD3.get_action\u001b[0;34m(self, obs, training, infos)\u001b[0m\n\u001b[1;32m    533\u001b[0m action_masks, env_defined_actions, agent_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_infos(infos)\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Preprocess observations\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m preprocessed_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    538\u001b[0m action_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (agent_id, obs, actor) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ids, preprocessed_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactors)\n\u001b[1;32m    541\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/algorithms/core/base.py:1218\u001b[0m, in \u001b[0;36mMultiAgentRLAlgorithm.preprocess_observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m   1216\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_id, obs \u001b[38;5;129;01min\u001b[39;00m observation\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 1218\u001b[0m     preprocessed[agent_id] \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_observation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preprocessed\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/utils/algo_utils.py:513\u001b[0m, in \u001b[0;36mpreprocess_observation\u001b[0;34m(observation, observation_space, device, normalize_images)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    506\u001b[0m         observation, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    507\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tuple, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(observation)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    509\u001b[0m         preprocess_observation(_obs, _space, device, normalize_images)\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _obs, _space \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(observation, observation_space\u001b[38;5;241m.\u001b[39mspaces)\n\u001b[1;32m    511\u001b[0m     )\n\u001b[0;32m--> 513\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    514\u001b[0m     observation, torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m    515\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected torch.Tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(observation)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# Normalize images if applicable and specified\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation_space\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m normalize_images:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected torch.Tensor, got <class 'dict'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000000step [00:26, 1484579.37step/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "max_steps = 20000 \n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "training_steps = 6\n",
    "env = CichyEnv(x_train, y_train1, y_train2)#.parallel_env()\n",
    "action_dict = {\n",
    "    \"IT\": np.random.uniform(-1, 1, 93),  # Random action for agent IT (size 93)\n",
    "    \"EVC\": np.random.uniform(-1, 1, 93)  # Random action for agent EVC (size 93)\n",
    "}\n",
    "#env.step(action_dict)\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "obs = env.reset()  # This ensures self.actions is initialized\n",
    "obs, rewards, dones,bb, infos = env.step(action_dict)\n",
    "print(type(obs), obs)\n",
    "\n",
    "\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "#print(type(obs), obs)\n",
    "num_envs = 4\n",
    "learning_delay = 0  # Steps before starting learning\n",
    "evo_steps = 10000  # Evolution frequency\n",
    "eval_steps = None  # Evaluation steps per episode - go until done\n",
    "eval_loop = 1  \n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "agent_ids = [\"IT\", \"EVC\"]\n",
    "#agent = pop_agent[0]\n",
    "#print(agent)\n",
    "pbar = trange(max_steps, unit=\"step\")\n",
    "while np.less([agent.steps[-1] for agent in pop_agent], max_steps).all():\n",
    "    pop_episode_scores = []\n",
    "    for agent in pop_agent:  # Loop through population\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        scores = np.zeros(num_envs)\n",
    "        completed_episode_scores = []\n",
    "        steps = 0\n",
    "        processed_obs = {\n",
    "            agent_id: spaces.flatten(env.observation_space, obs[agent_id]) \n",
    "            for agent_id in agent_ids\n",
    "            }\n",
    "\n",
    "    for idx_step in range(training_steps // num_envs):\n",
    "        # Ensure obs is flattened before passing to agent\n",
    "        cont_actions, discrete_action = agent.get_action(obs=processed_obs, training=True, infos=info)\n",
    "        if agent.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "\n",
    "        # Act in environment\n",
    "        next_state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "        scores += np.sum(np.array(list(reward.values())).transpose(), axis=-1)\n",
    "        total_steps += num_envs\n",
    "        steps += num_envs\n",
    "        \"\"\"\n",
    "        # Save experiences to replay buffer\n",
    "\n",
    "        #erroring out must fix\n",
    "\n",
    "        memory.save_to_memory(\n",
    "            state,\n",
    "            cont_actions,\n",
    "            reward,\n",
    "            next_state,\n",
    "            termination,\n",
    "            is_vectorised=True,\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Learn according to learning frequency\n",
    "        # Handle learn steps > num_envs\n",
    "        if agent.learn_step > num_envs:\n",
    "            learn_step = agent.learn_step // num_envs\n",
    "            if (\n",
    "                idx_step % learn_step == 0\n",
    "                and len(memory) >= agent.batch_size\n",
    "                and memory.counter > learning_delay\n",
    "            ):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "        # Handle num_envs > learn step; learn multiple times per step in env\n",
    "        elif (\n",
    "            len(memory) >= agent.batch_size and memory.counter > learning_delay\n",
    "        ):\n",
    "            for _ in range(num_envs // agent.learn_step):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Calculate scores and reset noise for finished episodes\n",
    "        reset_noise_indices = []\n",
    "        term_array = np.array(list(termination.values())).transpose()\n",
    "        trunc_array = np.array(list(truncation.values())).transpose()\n",
    "        for idx, (d, t) in enumerate(zip(term_array, trunc_array)):\n",
    "            if np.any(d) or np.any(t):\n",
    "                completed_episode_scores.append(scores[idx])\n",
    "                agent.scores.append(scores[idx])\n",
    "                scores[idx] = 0\n",
    "                reset_noise_indices.append(idx)\n",
    "        agent.reset_action_noise(reset_noise_indices)\n",
    "\n",
    "    pbar.update(evo_steps // len(pop_agent))\n",
    "\n",
    "    agent.steps[-1] += steps\n",
    "    pop_episode_scores.append(completed_episode_scores)\n",
    "\n",
    "# Evaluate population\n",
    "fitnesses = [\n",
    "    agent.test(\n",
    "        env,\n",
    "        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "        max_steps=eval_steps,\n",
    "        loop=eval_loop,\n",
    "    )\n",
    "    for agent in pop_agent\n",
    "]\n",
    "mean_scores = [\n",
    "    (\n",
    "        np.mean(episode_scores)\n",
    "        if len(episode_scores) > 0\n",
    "        else \"0 completed episodes\"\n",
    "    )\n",
    "    for episode_scores in pop_episode_scores\n",
    "]\n",
    "\n",
    "print(f\"--- Global steps {total_steps} ---\")\n",
    "print(f\"Steps {[agent.steps[-1] for agent in pop_agent]}\")\n",
    "print(f\"Scores: {mean_scores}\")\n",
    "print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n",
    "print(\n",
    "    f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop_agent]}'\n",
    ")\n",
    "\n",
    "# Tournament selection and population mutation\n",
    "elite, pop = tournament.select(pop_agent)\n",
    "pop = mutations.mutation(pop)\n",
    "\n",
    "# Update step counter\n",
    "for agent in pop:\n",
    "    agent.steps.append(agent.steps[-1])\n",
    "\n",
    "pbar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from pettingzoo import ParallelEnv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from agilerl.utils.utils import create_population as Population\n",
    "from agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\n",
    "from agilerl.training.train_offline import train_offline\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.training.train_multi_agent import train_multi_agent\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "#set device to the dual t4s. \n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "# -------------------------------\n",
    "# Environment Definition\n",
    "# -------------------------------\n",
    "class CichyEnv(ParallelEnv):\n",
    "    metadata = {\"name\": \"cichyenv\"}\n",
    "\n",
    "    def __init__(self, images, y1, y2):\n",
    "        self.images = images.reshape(92, 175, 175)  # Ensure correct shape\n",
    "        self.y1 = y1  # Rewards for agent IT\n",
    "        self.y2 = y2  # Rewards for agent EVC\n",
    "        self.agents = [\"IT1\", \"IT2\", \"IT3\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\"]\n",
    "        self.agent_ids = [\"IT1\", \"IT2\", \"IT3\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\"]#[\"IT\", \"EVC\"]\n",
    "        self.current_step = 0  # Initialize current_step\n",
    "        \n",
    "        # Merging observation spaces into a single space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"image\": spaces.Box(low=0, high=255, shape=(175, 175), dtype=np.uint8),  # Expecting image shape of (175, 175)\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(93,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0  # Ensure it's set to 0 at the start of each episode\n",
    "        self.actions = {agent: [] for agent in self.agent_ids}\n",
    "        \n",
    "        # Creating observation dictionary\n",
    "        obs = {}\n",
    "        for agent in self.agent_ids:\n",
    "            image = self.images[self.current_step]  # Get the image for the current step\n",
    "            if image.shape != (175, 175):\n",
    "                raise ValueError(f\"Unexpected image shape: {image.shape}, expected (175, 175)\")\n",
    "\n",
    "            # Expand the image dimensions to make it 5D for Conv3D (batch, channels, depth, height, width)\n",
    "            image_expanded = np.expand_dims(image, axis=0)  # Adds a batch dimension, (1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds a channel dimension, (1, 1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds depth dimension, (1, 1, 1, 175, 175)\n",
    "\n",
    "            obs[agent] = {\n",
    "                \"image\": image_expanded,  # Now shape (1, 1, 1, 175, 175)\n",
    "                \"other_action\": np.array([0.0]),  # Dummy value for the action of the other agent\n",
    "            }\n",
    "            \"\"\"\n",
    "            uncomment out soon \n",
    "\n",
    "            obs = {\n",
    "                agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "                for agent_id in self.agent_ids\n",
    "                }\n",
    "            \"\"\"\n",
    "                    \n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        if self.current_step >= len(self.images):\n",
    "            self.current_step = len(self.images) - 1\n",
    "\n",
    "        obs, rewards, dones, infos = {}, {}, {}, {}\n",
    "\n",
    "        for agent_id in self.agent_ids:\n",
    "            other_agent_id = \"EVC\" if agent_id == \"IT\" else \"IT\"\n",
    "\n",
    "            full_action = action_dict.get(agent_id, np.zeros((93,)))\n",
    "            agent_actions = full_action[:92].reshape(-1, 1)#(full_action[:92]).reshape((92, 1))  \n",
    "            other_action = full_action[92:]  \n",
    "\n",
    "            self.actions[agent_id].append(agent_actions)  \n",
    "\n",
    "            other_actions = action_dict.get(other_agent_id, np.zeros((93,)))[92:]  \n",
    "\n",
    "            obs[agent_id] = {\n",
    "                \"image\": torch.tensor(self.images[self.current_step], dtype=torch.float32),  # Convert to tensor\n",
    "                \"other_action\": torch.tensor(other_actions, dtype=torch.float32),  # Convert to tensor\n",
    "            }\n",
    "\n",
    "            #rewards[agent_id] = self._calculate_reward(agent_id)  \n",
    "\n",
    "            rewards[agent_id] = np.array([self._calculate_reward(agent_id)], dtype=np.float32)\n",
    "            dones[agent_id] = self.current_step >= len(self.images) - 1\n",
    "            infos[agent_id] = {}\n",
    "\n",
    "        dones[\"__all__\"] = all(dones.values())\n",
    "        self.current_step += 1  \n",
    "        return obs, rewards, dones, {}, infos\n",
    "    def _calculate_reward(self, agent_id):\n",
    "        \"\"\"Computes the reward based on similarity to the expert RDM using all 92 actions per step.\"\"\"\n",
    "        actions = np.array(self.actions[agent_id])  # Shape: (num_steps, 92, 1)\n",
    "        \n",
    "        num_steps = actions.shape[0]  \n",
    "        num_images = actions.shape[1]  \n",
    "\n",
    "        if num_steps < 2:  \n",
    "            return 0  \n",
    "\n",
    "        simulated_rdm = np.zeros((num_images, num_images))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for j in range(num_images):\n",
    "                if i != j:\n",
    "                    sim = cosine_similarity(actions[:, i].reshape(-1, 1), actions[:, j].reshape(-1, 1))[0][0]\n",
    "                    simulated_rdm[i, j] = 1 - sim\n",
    "\n",
    "        expert_rdm = self.y1 if agent_id == \"IT\" else self.y2\n",
    "\n",
    "        min_size = min(simulated_rdm.shape[0], expert_rdm.shape[0])\n",
    "        reward = -np.sum((simulated_rdm[:min_size, :min_size] - expert_rdm[:min_size, :min_size]) ** 2)\n",
    "        return reward\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training Setup with AgileRL\n",
    "# -------------------------------8-=\n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "\n",
    "env = CichyEnv(x_train, y_train1, y_train2)\n",
    "\n",
    "# Initial Hyperparameters\n",
    "INIT_HP = {\n",
    "    \"DOUBLE\": True,\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"POPULATION_SIZE\": 8,\n",
    "    \"O_U_NOISE\": 0.2,\n",
    "    \"EXPL_NOISE\": 0.1,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"LR\": 0.001,\n",
    "    \"LR_ACTOR\": 0.002,\n",
    "    \"LR_CRITIC\": 0.002,\n",
    "    \"TAU\": 0.5,\n",
    "    \"GAMMA\": 1.0,\n",
    "    \"LAMBDA\": 1.0,\n",
    "    \"REG\": 0.0625,\n",
    "    \"LEARN_STEP\": 2,\n",
    "    \"MEAN_NOISE\": 1,\n",
    "    \"THETA\": 1,\n",
    "    \"DT\": 1,\n",
    "    \"POLICY_FREQ\": 2,\n",
    "    \"AGENT_IDS\": [\"IT1\", \"IT2\", \"IT3\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\"],\n",
    "    \"MEMORY_SIZE\": 100000\n",
    "        \n",
    "}\n",
    "\n",
    "hp_config = HyperparameterConfig(\n",
    "    #lr=RLParameter(min=6.25e-5, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n",
    ")\n",
    "\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "\n",
    "# Create populations for each agent\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# Tournament selection\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,\n",
    "    elitism=True,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    eval_loop=1,\n",
    ")\n",
    "\n",
    "# Mutation settings\n",
    "mutations = Mutations(\n",
    "    no_mutation=0.4,\n",
    "    architecture=0.2,\n",
    "    new_layer_prob=0.2,\n",
    "    parameters=0.2,\n",
    "    activation=0,\n",
    "    rl_hp=0.2,\n",
    "    mutation_sd=0.1,\n",
    "    rand_seed=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'IT1': {'image': tensor([[105., 105.,  42.,  ..., 243., 246., 226.],\n",
      "        [ 35.,  32., 211.,  ...,  69., 152.,  22.],\n",
      "        [160., 106.,  60.,  ..., 233.,   2.,  88.],\n",
      "        ...,\n",
      "        [143.,   8., 218.,  ..., 159., 206.,  47.],\n",
      "        [ 43.,  52.,  43.,  ...,  32., 136., 207.],\n",
      "        [128.,  81., 162.,  ..., 209., 136., 111.]]), 'other_action': tensor([-0.5256])}, 'IT2': {'image': tensor([[105., 105.,  42.,  ..., 243., 246., 226.],\n",
      "        [ 35.,  32., 211.,  ...,  69., 152.,  22.],\n",
      "        [160., 106.,  60.,  ..., 233.,   2.,  88.],\n",
      "        ...,\n",
      "        [143.,   8., 218.,  ..., 159., 206.,  47.],\n",
      "        [ 43.,  52.,  43.,  ...,  32., 136., 207.],\n",
      "        [128.,  81., 162.,  ..., 209., 136., 111.]]), 'other_action': tensor([-0.5256])}, 'IT3': {'image': tensor([[105., 105.,  42.,  ..., 243., 246., 226.],\n",
      "        [ 35.,  32., 211.,  ...,  69., 152.,  22.],\n",
      "        [160., 106.,  60.,  ..., 233.,   2.,  88.],\n",
      "        ...,\n",
      "        [143.,   8., 218.,  ..., 159., 206.,  47.],\n",
      "        [ 43.,  52.,  43.,  ...,  32., 136., 207.],\n",
      "        [128.,  81., 162.,  ..., 209., 136., 111.]]), 'other_action': tensor([-0.5256])}, 'EVC1': {'image': tensor([[105., 105.,  42.,  ..., 243., 246., 226.],\n",
      "        [ 35.,  32., 211.,  ...,  69., 152.,  22.],\n",
      "        [160., 106.,  60.,  ..., 233.,   2.,  88.],\n",
      "        ...,\n",
      "        [143.,   8., 218.,  ..., 159., 206.,  47.],\n",
      "        [ 43.,  52.,  43.,  ...,  32., 136., 207.],\n",
      "        [128.,  81., 162.,  ..., 209., 136., 111.]]), 'other_action': tensor([-0.5256])}, 'EVC2': {'image': tensor([[105., 105.,  42.,  ..., 243., 246., 226.],\n",
      "        [ 35.,  32., 211.,  ...,  69., 152.,  22.],\n",
      "        [160., 106.,  60.,  ..., 233.,   2.,  88.],\n",
      "        ...,\n",
      "        [143.,   8., 218.,  ..., 159., 206.,  47.],\n",
      "        [ 43.,  52.,  43.,  ...,  32., 136., 207.],\n",
      "        [128.,  81., 162.,  ..., 209., 136., 111.]]), 'other_action': tensor([-0.5256])}, 'EVC3': {'image': tensor([[105., 105.,  42.,  ..., 243., 246., 226.],\n",
      "        [ 35.,  32., 211.,  ...,  69., 152.,  22.],\n",
      "        [160., 106.,  60.,  ..., 233.,   2.,  88.],\n",
      "        ...,\n",
      "        [143.,   8., 218.,  ..., 159., 206.,  47.],\n",
      "        [ 43.,  52.,  43.,  ...,  32., 136., 207.],\n",
      "        [128.,  81., 162.,  ..., 209., 136., 111.]]), 'other_action': tensor([-0.5256])}, 'EVC4': {'image': tensor([[105., 105.,  42.,  ..., 243., 246., 226.],\n",
      "        [ 35.,  32., 211.,  ...,  69., 152.,  22.],\n",
      "        [160., 106.,  60.,  ..., 233.,   2.,  88.],\n",
      "        ...,\n",
      "        [143.,   8., 218.,  ..., 159., 206.,  47.],\n",
      "        [ 43.,  52.,  43.,  ...,  32., 136., 207.],\n",
      "        [128.,  81., 162.,  ..., 209., 136., 111.]]), 'other_action': tensor([-0.5256])}, 'EVC5': {'image': tensor([[105., 105.,  42.,  ..., 243., 246., 226.],\n",
      "        [ 35.,  32., 211.,  ...,  69., 152.,  22.],\n",
      "        [160., 106.,  60.,  ..., 233.,   2.,  88.],\n",
      "        ...,\n",
      "        [143.,   8., 218.,  ..., 159., 206.,  47.],\n",
      "        [ 43.,  52.,  43.,  ...,  32., 136., 207.],\n",
      "        [128.,  81., 162.,  ..., 209., 136., 111.]]), 'other_action': tensor([-0.5256])}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3750/20000 [05:51<25:24, 10.66step/s]\n",
      "6247500step [00:37, 166485.00step/s]                       "
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected torch.Tensor, got <class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m     pop_episode_scores\u001b[38;5;241m.\u001b[39mappend(completed_episode_scores)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Evaluate population\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    129\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtest(\n\u001b[1;32m    130\u001b[0m         env,\n\u001b[1;32m    131\u001b[0m         swap_channels\u001b[38;5;241m=\u001b[39mINIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHANNELS_LAST\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    132\u001b[0m         max_steps\u001b[38;5;241m=\u001b[39meval_steps,\n\u001b[1;32m    133\u001b[0m         loop\u001b[38;5;241m=\u001b[39meval_loop,\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m pop_agent\n\u001b[1;32m    136\u001b[0m ]\n\u001b[1;32m    137\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         np\u001b[38;5;241m.\u001b[39mmean(episode_scores)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode_scores \u001b[38;5;129;01min\u001b[39;00m pop_episode_scores\n\u001b[1;32m    144\u001b[0m ]\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Global steps \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[64], line 129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m     pop_episode_scores\u001b[38;5;241m.\u001b[39mappend(completed_episode_scores)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Evaluate population\u001b[39;00m\n\u001b[1;32m    128\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 129\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mswap_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINIT_HP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCHANNELS_LAST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_loop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m pop_agent\n\u001b[1;32m    136\u001b[0m ]\n\u001b[1;32m    137\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         np\u001b[38;5;241m.\u001b[39mmean(episode_scores)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode_scores \u001b[38;5;129;01min\u001b[39;00m pop_episode_scores\n\u001b[1;32m    144\u001b[0m ]\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Global steps \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/algorithms/matd3.py:964\u001b[0m, in \u001b[0;36mMATD3.test\u001b[0;34m(self, env, swap_channels, max_steps, loop, sum_scores)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         obs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    961\u001b[0m             agent_id: np\u001b[38;5;241m.\u001b[39mmoveaxis(np\u001b[38;5;241m.\u001b[39mexpand_dims(s, \u001b[38;5;241m0\u001b[39m), [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    962\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m agent_id, s \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    963\u001b[0m         }\n\u001b[0;32m--> 964\u001b[0m cont_actions, discrete_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscrete_actions:\n\u001b[1;32m    970\u001b[0m     action \u001b[38;5;241m=\u001b[39m discrete_action\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/algorithms/matd3.py:536\u001b[0m, in \u001b[0;36mMATD3.get_action\u001b[0;34m(self, obs, training, infos)\u001b[0m\n\u001b[1;32m    533\u001b[0m action_masks, env_defined_actions, agent_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_infos(infos)\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Preprocess observations\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m preprocessed_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    538\u001b[0m action_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (agent_id, obs, actor) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ids, preprocessed_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactors)\n\u001b[1;32m    541\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/algorithms/core/base.py:1218\u001b[0m, in \u001b[0;36mMultiAgentRLAlgorithm.preprocess_observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m   1216\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_id, obs \u001b[38;5;129;01min\u001b[39;00m observation\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 1218\u001b[0m     preprocessed[agent_id] \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_observation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preprocessed\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/utils/algo_utils.py:513\u001b[0m, in \u001b[0;36mpreprocess_observation\u001b[0;34m(observation, observation_space, device, normalize_images)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    506\u001b[0m         observation, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    507\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tuple, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(observation)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    509\u001b[0m         preprocess_observation(_obs, _space, device, normalize_images)\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _obs, _space \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(observation, observation_space\u001b[38;5;241m.\u001b[39mspaces)\n\u001b[1;32m    511\u001b[0m     )\n\u001b[0;32m--> 513\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    514\u001b[0m     observation, torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m    515\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected torch.Tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(observation)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# Normalize images if applicable and specified\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation_space\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m normalize_images:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected torch.Tensor, got <class 'dict'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6250000step [00:54, 166485.00step/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "max_steps = 20000 \n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "training_steps = 6\n",
    "env = CichyEnv(x_train, y_train1, y_train2)#.parallel_env()\n",
    "action_dict = {\n",
    "    \"IT\": np.random.uniform(-1, 1, 93),  # Random action for agent IT (size 93)\n",
    "    \"EVC\": np.random.uniform(-1, 1, 93)  # Random action for agent EVC (size 93)\n",
    "}\n",
    "#env.step(action_dict)\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "obs = env.reset()  # This ensures self.actions is initialized\n",
    "obs, rewards, dones,bb, infos = env.step(action_dict)\n",
    "print(type(obs), obs)\n",
    "\n",
    "\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "#print(type(obs), obs)\n",
    "num_envs = 4\n",
    "learning_delay = 0  # Steps before starting learning\n",
    "evo_steps = 10000  # Evolution frequency\n",
    "eval_steps = None  # Evaluation steps per episode - go until done\n",
    "eval_loop = 1  \n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "agent_ids = [\"IT1\", \"IT2\", \"IT3\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\"]\n",
    "#agent = pop_agent[0]\n",
    "#print(agent)\n",
    "pbar = trange(max_steps, unit=\"step\")\n",
    "while np.less([agent.steps[-1] for agent in pop_agent], max_steps).all():\n",
    "    pop_episode_scores = []\n",
    "    for agent in pop_agent:  # Loop through population\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        scores = np.zeros(num_envs)\n",
    "        completed_episode_scores = []\n",
    "        steps = 0\n",
    "        processed_obs = {\n",
    "            agent_id: spaces.flatten(env.observation_space, obs[agent_id]) \n",
    "            for agent_id in agent_ids\n",
    "            }\n",
    "\n",
    "    for idx_step in range(training_steps // num_envs):\n",
    "        # Ensure obs is flattened before passing to agent\n",
    "        cont_actions, discrete_action = agent.get_action(obs=processed_obs, training=True, infos=info)\n",
    "        if agent.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "\n",
    "        # Act in environment\n",
    "        next_state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "        scores += np.sum(np.array(list(reward.values())).transpose(), axis=-1)\n",
    "        total_steps += num_envs\n",
    "        steps += num_envs\n",
    "        \"\"\"\n",
    "        # Save experiences to replay buffer\n",
    "\n",
    "        #erroring out must fix\n",
    "\n",
    "        memory.save_to_memory(\n",
    "            state,\n",
    "            cont_actions,\n",
    "            reward,\n",
    "            next_state,\n",
    "            termination,\n",
    "            is_vectorised=True,\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Learn according to learning frequency\n",
    "        # Handle learn steps > num_envs\n",
    "        if agent.learn_step > num_envs:\n",
    "            learn_step = agent.learn_step // num_envs\n",
    "            if (\n",
    "                idx_step % learn_step == 0\n",
    "                and len(memory) >= agent.batch_size\n",
    "                and memory.counter > learning_delay\n",
    "            ):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "        # Handle num_envs > learn step; learn multiple times per step in env\n",
    "        elif (\n",
    "            len(memory) >= agent.batch_size and memory.counter > learning_delay\n",
    "        ):\n",
    "            for _ in range(num_envs // agent.learn_step):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Calculate scores and reset noise for finished episodes\n",
    "        reset_noise_indices = []\n",
    "        term_array = np.array(list(termination.values())).transpose()\n",
    "        trunc_array = np.array(list(truncation.values())).transpose()\n",
    "        for idx, (d, t) in enumerate(zip(term_array, trunc_array)):\n",
    "            if np.any(d) or np.any(t):\n",
    "                completed_episode_scores.append(scores[idx])\n",
    "                agent.scores.append(scores[idx])\n",
    "                scores[idx] = 0\n",
    "                reset_noise_indices.append(idx)\n",
    "        agent.reset_action_noise(reset_noise_indices)\n",
    "\n",
    "    pbar.update(evo_steps // len(pop_agent))\n",
    "\n",
    "    agent.steps[-1] += steps\n",
    "    pop_episode_scores.append(completed_episode_scores)\n",
    "\n",
    "# Evaluate population\n",
    "fitnesses = [\n",
    "    agent.test(\n",
    "        env,\n",
    "        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "        max_steps=eval_steps,\n",
    "        loop=eval_loop,\n",
    "    )\n",
    "    for agent in pop_agent\n",
    "]\n",
    "mean_scores = [\n",
    "    (\n",
    "        np.mean(episode_scores)\n",
    "        if len(episode_scores) > 0\n",
    "        else \"0 completed episodes\"\n",
    "    )\n",
    "    for episode_scores in pop_episode_scores\n",
    "]\n",
    "\n",
    "print(f\"--- Global steps {total_steps} ---\")\n",
    "print(f\"Steps {[agent.steps[-1] for agent in pop_agent]}\")\n",
    "print(f\"Scores: {mean_scores}\")\n",
    "print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n",
    "print(\n",
    "    f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop_agent]}'\n",
    ")\n",
    "\n",
    "# Tournament selection and population mutation\n",
    "elite, pop = tournament.select(pop_agent)\n",
    "pop = mutations.mutation(pop)\n",
    "\n",
    "# Update step counter\n",
    "for agent in pop:\n",
    "    agent.steps.append(agent.steps[-1])\n",
    "\n",
    "pbar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prisoner': (0, 48, 24), 'guard': (0, 48, 24)}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import random\n",
    "from copy import copy\n",
    "prisoner_y = 0\n",
    "possible_agents = [\"prisoner\", \"guard\"]\n",
    "\n",
    "agents = copy(possible_agents)\n",
    "guard_x = 6\n",
    "guard_y = 6\n",
    "prisoner_x = 0\n",
    "escape_x = random.randint(2, 5)\n",
    "escape_y = random.randint(2, 5)\n",
    "pobs = {\n",
    "    a: (\n",
    "            prisoner_x + 7 * prisoner_y,\n",
    "            guard_x + 7 * guard_y,\n",
    "            escape_x + 7 * escape_y,\n",
    "            )\n",
    "            for a in agents\n",
    "}\n",
    "print(p)\"\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pop, fitnesses = train_multi_agent(\n",
    "    pop=pop_agent,\n",
    "    env=env,\n",
    "    algo=\"MATD3\",\n",
    "    env_name=\"cichyenv\",\n",
    "    memory=memory,\n",
    "    swap_channels=False,\n",
    "    max_steps=500000,\n",
    "    evo_steps=10000,\n",
    "    eval_steps=None,\n",
    "    eval_loop=1,\n",
    "    target=200.0,\n",
    "    tournament=tournament,\n",
    "    wb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CichyEnv(x_train, y_train1, y_train2)\n",
    "\n",
    "# Initial Hyperparameters\n",
    "INIT_HP = {\n",
    "    \"DOUBLE\": True,\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"POPULATION_SIZE\": 9,\n",
    "    \"O_U_NOISE\": 0.2,\n",
    "    \"EXPL_NOISE\": 0.1,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"LR\": 0.001,\n",
    "    \"LR_ACTOR\": 0.002,\n",
    "    \"LR_CRITIC\": 0.002,\n",
    "    \"TAU\": 0.5,\n",
    "    \"GAMMA\": 1.0,\n",
    "    \"LAMBDA\": 1.0,\n",
    "    \"REG\": 0.0625,\n",
    "    \"LEARN_STEP\": 2,\n",
    "    \"MEAN_NOISE\": 1,\n",
    "    \"THETA\": 1,\n",
    "    \"DT\": 1,\n",
    "    \"POLICY_FREQ\": 2,\n",
    "    \"AGENT_IDS\": [\"IT\",\"EVC\"],\n",
    "    \"MEMORY_SIZE\": 100000\n",
    "        \n",
    "}\n",
    "\n",
    "hp_config = HyperparameterConfig(\n",
    "    #lr=RLParameter(min=6.25e-5, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n",
    ")\n",
    "\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "\n",
    "# Create populations for each agent\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# Tournament selection\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,\n",
    "    elitism=True,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    eval_loop=1,\n",
    ")\n",
    "\n",
    "# Mutation settings\n",
    "mutations = Mutations(\n",
    "    no_mutation=0.4,\n",
    "    architecture=0.2,\n",
    "    new_layer_prob=0.2,\n",
    "    parameters=0.2,\n",
    "    activation=0,\n",
    "    rl_hp=0.2,\n",
    "    mutation_sd=0.1,\n",
    "    rand_seed=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code below trains skills\n",
    "\n",
    "note: skills are not full envs. They are classes that connect to an env and only consist of a single reward function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.wrappers.learning import Skill\n",
    "import os\n",
    "from agilerl.algorithms.ppo import PPO\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.wrappers.learning import Skill\n",
    "from agilerl.utils.algo_utils import obs_channels_to_first\n",
    "from agilerl.utils.utils import (\n",
    "   create_population,\n",
    "   make_skill_vect_envs,\n",
    "   make_vect_envs,\n",
    "   observation_space_channels_to_first\n",
    ")\n",
    "NET_CONFIG = {\n",
    "   \"encoder_config\": {\"hidden_size\": [64, 64]}  # Actor encoder hidden size\n",
    "}\n",
    "\n",
    "INIT_HP = {\n",
    "   \"ENV_NAME\": \"LunarLander-v2\",\n",
    "   \"ALGO\": \"PPO\",\n",
    "   \"POPULATION_SIZE\": 1,  # Population size\n",
    "   \"BATCH_SIZE\": 128,  # Batch size\n",
    "   \"LR\": 1e-3,  # Learning rate\n",
    "   \"LEARN_STEP\": 128,  # Learning frequency\n",
    "   \"GAMMA\": 0.99,  # Discount factor\n",
    "   \"GAE_LAMBDA\": 0.95,  # Lambda for general advantage estimation\n",
    "   \"ACTION_STD_INIT\": 0.6,  # Initial action standard deviation\n",
    "   \"CLIP_COEF\": 0.2,  # Surrogate clipping coefficient\n",
    "   \"ENT_COEF\": 0.01,  # Entropy coefficient\n",
    "   \"VF_COEF\": 0.5,  # Value function coefficient\n",
    "   \"MAX_GRAD_NORM\": 0.5,  # Maximum norm for gradient clipping\n",
    "   \"TARGET_KL\": None,  # Target KL divergence threshold\n",
    "   \"TARGET_SCORE\": 2000,\n",
    "   \"MAX_STEPS\": 1_000_000,\n",
    "   \"EVO_STEPS\": 10_000,\n",
    "   \"UPDATE_EPOCHS\": 4,  # Number of policy update epochs\n",
    "   # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "   \"CHANNELS_LAST\": False,\n",
    "   \"WANDB\": True,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Directory to save trained agents and skills\n",
    "save_dir = \"./models/PPO\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "skills = {\n",
    "   \"stabilize\": StabilizeSkill,\n",
    "   \"center\": CenterSkill,\n",
    "   \"landing\": LandingSkill,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for skill in skills.keys():\n",
    "   env = make_skill_vect_envs(\n",
    "         INIT_HP[\"ENV_NAME\"], skills[skill], num_envs=1\n",
    "   )  # Create environment\n",
    "\n",
    "   observation_space = env.single_observation_space\n",
    "   action_space = env.single_action_space\n",
    "   if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "         observation_space = observation_space_channels_to_first(observation_space)\n",
    "\n",
    "   pop = create_population(\n",
    "         algo=\"PPO\",  # Algorithm\n",
    "         observation_space=observation_space,  # Observation space\n",
    "         action_space=action_space,  # Action space\n",
    "         net_config=NET_CONFIG,  # Network configuration\n",
    "         INIT_HP=INIT_HP,  # Initial hyperparameters\n",
    "         population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "         device=device,\n",
    "   )\n",
    "\n",
    "   trained_pop, pop_fitnesses = train_on_policy(\n",
    "         env=env,  # Gym-style environment\n",
    "         env_name=f\"{INIT_HP['ENV_NAME']}-{skill}\",  # Environment name\n",
    "         algo=INIT_HP[\"ALGO\"],  # Algorithm\n",
    "         pop=pop,  # Population of agents\n",
    "         swap_channels=INIT_HP[\n",
    "            \"CHANNELS_LAST\"\n",
    "         ],  # Swap image channel from last to first\n",
    "         max_steps=INIT_HP[\"MAX_STEPS\"],  # Max number of training episodes\n",
    "         evo_steps=INIT_HP[\"EVO_STEPS\"],  # Evolution frequency\n",
    "         evo_loop=3,  # Number of evaluation episodes per agent\n",
    "         target=INIT_HP[\"TARGET_SCORE\"],  # Target score for early stopping\n",
    "         tournament=None,  # Tournament selection object\n",
    "         mutation=None,  # Mutations object\n",
    "         wb=INIT_HP[\"WANDB\"],  # Weights and Biases tracking\n",
    "   )\n",
    "\n",
    "   # Save the trained algorithm\n",
    "   filename = f\"PPO_trained_agent_{skill}.pt\"\n",
    "   save_path = os.path.join(save_dir, filename)\n",
    "   trained_pop[0].save_checkpoint(save_path)\n",
    "\n",
    "   env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is for the meta selector agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stabilize_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_stabilize.pt\"))\n",
    "center_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_center.pt\"))\n",
    "landing_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_landing.pt\"))\n",
    "\n",
    "trained_skills = {\n",
    "   0: {\"skill\": \"stabilize\", \"agent\": stabilize_agent, \"skill_duration\": 40},\n",
    "   1: {\"skill\": \"center\", \"agent\": center_agent, \"skill_duration\": 40},\n",
    "   2: {\"skill\": \"landing\", \"agent\": landing_agent, \"skill_duration\": 40},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vect_envs(INIT_HP[\"ENV_NAME\"], num_envs=1)  # Create environment\n",
    "\n",
    "observation_space = env.single_observation_space\n",
    "\n",
    "action_dim = len(\n",
    "   trained_skills\n",
    ")  # Selector will be trained to choose which trained skill to use\n",
    "\n",
    "action_space = spaces.Discrete(action_dim)\n",
    "\n",
    "if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "   observation_space = observation_space_channels_to_first(observation_space)\n",
    "\n",
    "pop = create_population(\n",
    "   algo=\"PPO\",  # Algorithm\n",
    "   observation_space=observation_space,  # Observation space\n",
    "   action_space=action_space,  # Action space\n",
    "   net_config=NET_CONFIG,  # Network configuration\n",
    "   INIT_HP=INIT_HP,  # Initial hyperparameters\n",
    "   population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "   device=device,\n",
    ")\n",
    "\n",
    "if INIT_HP[\"WANDB\"]:\n",
    "   wandb.init(\n",
    "         # set the wandb project where this run will be logged\n",
    "         project=\"EvoWrappers\",\n",
    "         name=\"{}-EvoHPO-{}-{}\".format(\n",
    "            INIT_HP[\"ENV_NAME\"],\n",
    "            INIT_HP[\"ALGO\"],\n",
    "            datetime.now().strftime(\"%m%d%Y%H%M%S\"),\n",
    "         ),\n",
    "         # track hyperparameters and run metadata\n",
    "         config={\n",
    "            \"algo\": f\"Evo HPO {INIT_HP['ALGO']}\",\n",
    "            \"env\": INIT_HP[\"ENV_NAME\"],\n",
    "            \"INIT_HP\": INIT_HP,\n",
    "         },\n",
    "   )\n",
    "\n",
    "bar_format = \"{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]\"\n",
    "pbar = trange(\n",
    "  INIT_HP[\"MAX_STEPS\"],\n",
    "  unit=\"step\",\n",
    "  bar_format=bar_format,\n",
    "  ascii=True)\n",
    "\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.less([agent.steps[-1] for agent in pop], INIT_HP[\"MAX_STEPS\"]).all():\n",
    "   for agent in pop:  # Loop through population\n",
    "         state = env.reset()[0]  # Reset environment at start of episode\n",
    "         score = 0\n",
    "\n",
    "         states = []\n",
    "         actions = []\n",
    "         log_probs = []\n",
    "         rewards = []\n",
    "         terminations = []\n",
    "         values = []\n",
    "\n",
    "         for idx_step in range(500):\n",
    "            # Get next action from agent\n",
    "            action, log_prob, _, value = agent.get_action(state)\n",
    "\n",
    "            # Internal loop to execute trained skill\n",
    "            skill_agent = trained_skills[action[0]][\"agent\"]\n",
    "            skill_duration = trained_skills[action[0]][\"skill_duration\"]\n",
    "            reward = 0\n",
    "            for skill_step in range(skill_duration):\n",
    "               # If landed, do nothing\n",
    "               if state[0][6] or state[0][7]:\n",
    "                     next_state, skill_reward, termination, truncation, _ = env.step(\n",
    "                        [0]\n",
    "                     )\n",
    "               else:\n",
    "                     skill_action, _, _, _ = skill_agent.get_action(state)\n",
    "                     next_state, skill_reward, termination, truncation, _ = env.step(\n",
    "                        skill_action\n",
    "                     )  # Act in environment\n",
    "               reward += skill_reward\n",
    "               if np.any(termination) or np.any(truncation):\n",
    "                     break\n",
    "               state = next_state\n",
    "            score += reward\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            terminations.append(termination)\n",
    "            values.append(value)\n",
    "\n",
    "         agent.scores.append(score)\n",
    "\n",
    "         # Learn according to agent's RL algorithm\n",
    "         agent.learn(\n",
    "            (\n",
    "               states,\n",
    "               actions,\n",
    "               log_probs,\n",
    "               rewards,\n",
    "               terminations,\n",
    "               values,\n",
    "               next_state,\n",
    "            )\n",
    "         )\n",
    "\n",
    "         agent.steps[-1] += idx_step + 1\n",
    "         total_steps += idx_step + 1\n",
    "\n",
    "   if (agent.steps[-1]) % INIT_HP[\"EVO_STEPS\"] == 0:\n",
    "      mean_scores = np.mean([agent.scores[-20:] for agent in pop], axis=1)\n",
    "      if INIT_HP[\"WANDB\"]:\n",
    "          wandb.log(\n",
    "              {\n",
    "                  \"global_step\": total_steps,\n",
    "                  \"train/mean_score\": np.mean(mean_scores),\n",
    "              }\n",
    "          )\n",
    "      print(\n",
    "          f\"\"\"\n",
    "          --- Global Steps {total_steps} ---\n",
    "          Score:\\t\\t{mean_scores}\n",
    "          \"\"\",\n",
    "          end=\"\\r\",\n",
    "      )\n",
    "\n",
    "if INIT_HP[\"WANDB\"]:\n",
    "   wandb.finish()\n",
    "env.close()\n",
    "\n",
    "# Save the trained selector\n",
    "filename = \"PPO_trained_agent_selector.pt\"\n",
    "save_path = os.path.join(save_dir, filename)\n",
    "pop[0].save_checkpoint(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
