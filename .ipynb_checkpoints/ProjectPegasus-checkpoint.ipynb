{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e175e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "import argparse\n",
    "from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "\"\"\"\n",
    "from speedster import optimize_model\n",
    "import tensorflow as tf\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "import logging\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "#from ray.rllib.utils.torch_utils import \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f708ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22ec654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CModel(torch.nn.Module,ivy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._avg_pool = torch.nn.AvgPool2d(4)\n",
    "        self._linear = torch.nn.Linear(3136, 1024)\n",
    "        self._relu = torch.nn.ReLU()\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        \n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x = self._avg_pool(x).mean(dim=-3).view(-1, 3136)\n",
    "        x = self._relu(self._linear(x))\n",
    "        return self._linears(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257c10b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class RNNModel with abstract method _forward",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11620\\447223560.py\"\u001b[1;36m, line \u001b[1;32m166\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    model = RNNModel()#note that this particular line compiles if we get rid of parameters. at initialization we will call this with parameters\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m\u001b[1;31m:\u001b[0m Can't instantiate abstract class RNNModel with abstract method _forward\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow.compat.v2 as tf\n",
    "#tf.enable_v2_behavior()\n",
    "num_inducing_points=1\n",
    "#this is the agent network that will be used \n",
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "input_shape = [0,0]\n",
    "from numba import jit\n",
    "#@accelerate_model()\n",
    "\n",
    "class RNNModel(TorchModelV2,nn.Module,ivy.Module):\n",
    "  \n",
    "  def __init__(self):#,obs_space, action_space, num_outputs, model_config, name): #add this in later\n",
    "    \n",
    "    #self.obs_size = _get_size(obs_space)\n",
    "    #self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "\n",
    "    \"\"\"\n",
    "    what do we want our VAE to do?\n",
    "\n",
    "    our vae should: \n",
    "    1. infer a hidden state and \n",
    "    2. output an action   `\n",
    "\n",
    "    \"\"\"\n",
    "    tfk = tf.keras\n",
    "    tfkl = tf.keras.layers\n",
    "    \n",
    "    tfpl = tfp.layers\n",
    "    tfd = tfp.distributions\n",
    "\n",
    "    tfd = tfp.distributions\n",
    "    encoded_size = 16\n",
    "    \"\"\"\n",
    "    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),#This scale variable cannot be fixed but instead must be allowed to vary \n",
    "                                reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    The prior should be some kind of composite or dict type distribution with multiple dirchlet distributions. \n",
    "\n",
    "\n",
    "    \n",
    "    #note: the tfd Independent is for assumptions which are independent and unaffected or related to other assumptions. These distribtutions may need to be hand tuned based on domain knowledge and data analysis\n",
    "    #second note: this will be the top level prior. we will have to figure out a way to ensure that this prior is only inherited from posteriors of above signals\n",
    "\n",
    "    #solution: create an algorithm that tunes parameters of distribution and have it default to hand-written prior parameters if upper level observation space is empty and epoch is 0. \n",
    "    \n",
    "    the last layer needs to be a discrete (bernoulli) distribution layer for the high and midlevel layers and a continuous distribution for the lowest layer \n",
    "\n",
    "    dist = tfd.Normal(loc=0.5, scale=0.25). Note: we may want to consider using the normal for all agents and just discretize for higher level agents. \n",
    "\n",
    "    this is going to be where we consider things like lateral disinhibition where the prior pertaining to the probability carrying out an action is going to decrease if it has already carried out an action under certain circumstances\n",
    "    \n",
    "    Note: we are going to replace almost all the code in the init with the compiled model laplace CModel we have above\n",
    "    \"\"\"\n",
    "    #encoded_shape = 2\n",
    "    num_schools=2\n",
    "    \"\"\"\n",
    "    prior  = tfd.JointDistributionSequential([\n",
    "        tfd.Normal(loc=0., scale=10., name=\"avg_effect\"),  # `mu` above\n",
    "        tfd.Dirichlet(2),\n",
    "        tfd.Normal(loc=5., scale=1., name=\"avg_stddev\"),  # `log(tau)` above\n",
    "        tfd.Independent(tfd.Normal(loc=tf.zeros(num_schools),\n",
    "                                  scale=tf.ones(num_schools),\n",
    "                                  name=\"school_effects_standard\"),  # `theta_prime` \n",
    "                        reinterpreted_batch_ndims=1)\n",
    "    ])\n",
    "    \"\"\"\n",
    "    #prior= tfd.Normal(loc=0., scale=10., name=\"avg_effect\")\n",
    "    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                        reinterpreted_batch_ndims=1)\n",
    "    tfpl = tfp.layers\n",
    "    encoder = tfk.Sequential([\n",
    "        tfkl.InputLayer(input_shape=input_shape),\n",
    "        #tfkl.Dense(8)\n",
    "        tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                      activation=None),\n",
    "\n",
    "        tfpl.MultivariateNormalTriL(\n",
    "                encoded_size,\n",
    "                activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "    ])\n",
    "    decoder = tfk.Sequential([\n",
    "        tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "        tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "        #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "    ])\n",
    "    negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "    \"\"\"\n",
    "    #working theory as of 11/26/2022\n",
    "    \n",
    "    in order to convert from posterior -> action potential -> prior we need information theory specifically \n",
    "\n",
    "    probability of excitatory action potential = -log2(p(w)) if P(w) = 0.5 probability of excitatory action potential is 1\n",
    "\n",
    "    furthermore a prediction of w being 0.5 also yields a prediction of not(w) also being 0.5 \n",
    "\n",
    "    this is where I suspect inhibitory potentials come in since not(w) is expressed as log2(p(w)) which of 0.5 is -1 \n",
    "\n",
    "    we will probably do this translation from the observation space and action space\n",
    "\n",
    "    each action will actually correspond to a series of a 100 steps\n",
    "\n",
    "    if the algorithm tries to send a signal of 0.2 bits to a particular neuronal agent we will send \n",
    "    20 excitatory action potentials and 80 0s over 100 steps  or to put it another way we will say that 0.2 \n",
    "\n",
    "    is just the probability of it sending a excitatory potential at any given time \n",
    "\n",
    "    we will need to do an if else thing\n",
    "\n",
    "    if neurons is top neuron: \n",
    "      prior = <hand coded prior>\n",
    "    else: \n",
    "      if obs == 1:\n",
    "        prob = obs**2 #where obs will be the observed frequency over n previously observed values\n",
    "      if obs == -1:\n",
    "        prob = obs**2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    uncomment out later\n",
    "\n",
    "    loss = lambda y, rv_y: rv_y.variational_loss(\n",
    "        y, kl_weight=np.array(batch_size, tf.float64) / x.shape[0])\n",
    "    #tf.keras.optimizers.Adam(1e-4) tf.optimizers.Adam(learning_rate=0.011)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.011), loss=loss)#tf.optimizers.Adam(learning_rate=0.01)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    we are choosing to use a variational autoencoder because there are two things that need to be done: make predictions and select action that minimizes free energy \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    decoder = tfk.Sequential([\n",
    "        tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "        #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits),\n",
    "        ])\n",
    "    vae = tfk.Model(inputs=encoder.inputs,\n",
    "                        outputs=decoder(encoder.outputs[0]))\n",
    "    \"\"\"\n",
    "    ivy.Module.__init__(self)\n",
    "    \n",
    "  #@override(ModelV2)\n",
    "    @jit(nopython=True)\n",
    "    def get_initial_state(self):\n",
    "        # Place hidden states on same device as model.\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "            ]\n",
    "    \n",
    "    #@override(ModelV2)\n",
    "    @jit(nopython=True)\n",
    "    def _forward(self, input_dict, hidden_state, seq_lens):# _forward\n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)#this may be the q-value \n",
    "        return q, [h]\n",
    "\n",
    "  \n",
    "    def _get_size(obs_space):\n",
    "        return get_preprocessor(obs_space)(obs_space).size\n",
    "\n",
    "ivy.set_framework('torch') \n",
    "model = RNNModel()#note that this particular line compiles if we get rid of parameters. at initialization we will call this with parameters \n",
    "#model = Laplace(model, 'regression', subset_of_weights='all', hessian_structure='full')\n",
    "\n",
    "\"\"\"\n",
    "optimized_model = optimize_model(\n",
    "    model, input_data=input_data, optimization_time=\"constrained\"\n",
    ")\n",
    "note: under current code minimization of kullbacker-leibler divergence is already part of the code in the form the KLD divergence regularizer which means the loss simply needs to be the \"suprise\" \n",
    "\n",
    "w\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4495a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "ivy.set_framework('torch') \n",
    "model = CModel()\n",
    "cmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "369a2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<laplace.lllaplace.FullLLLaplace object at 0x00000289263ABA90>\n"
     ]
    }
   ],
   "source": [
    "print(cmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put this code into \n",
    "\n",
    "model = optimize_model(\n",
    "  cmodel, input_data=input_data, device=\"gpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623846bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7accdaf6",
   "metadata": {},
   "source": [
    "As of january 13 2022 we have performed a sucessful partial execution test of the project pegasus agent \n",
    "network\n",
    "\n",
    "We still have to determine the following:\n",
    "1. how to create heterogenuous agents\n",
    "2. how to visualize using pysurfer\n",
    "3. figure out what if any alterations will be needed for the mixer network \n",
    "\n",
    "ANS for 3: we should perhaps think of the mixer network as maximizing the overall free energy and \n",
    "acting on the level of active inference instead of predictive coding. \n",
    "\n",
    "we are going to use laplace-torch for among other things model selection\n",
    "\n",
    "This means that after we finish pretraining our RL algorithm on normal conditions we will fine tune and select simulated models generated by our RL algorithm by how well they fit the frieburg data. \n",
    "\n",
    "we are actually only going to have 1 env. I dont think we need the brain env only the humanoid env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "692d335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch must be installed.\n",
    "#torch, nn = try_import_torch(error=True)\n",
    "\n",
    "#logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class QMixLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        target_model,\n",
    "        mixer,\n",
    "        target_mixer,\n",
    "        n_agents,\n",
    "        n_actions,\n",
    "        double_q=True,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.mixer = mixer\n",
    "        self.target_mixer = target_mixer\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        self.double_q = double_q\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rewards,\n",
    "        actions,\n",
    "        terminated,\n",
    "        mask,\n",
    "        obs,\n",
    "        next_obs,\n",
    "        action_mask,\n",
    "        next_action_mask,\n",
    "        state=None,\n",
    "        next_state=None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the loss.\n",
    "        Args:\n",
    "            rewards: Tensor of shape [B, T, n_agents]\n",
    "            actions: Tensor of shape [B, T, n_agents]\n",
    "            terminated: Tensor of shape [B, T, n_agents]\n",
    "            mask: Tensor of shape [B, T, n_agents]\n",
    "            obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            next_obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            next_action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            state: Tensor of shape [B, T, state_dim] (optional)\n",
    "            next_state: Tensor of shape [B, T, state_dim] (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert either none or both of state and next_state are given\n",
    "        if state is None and next_state is None:\n",
    "            state = obs  # default to state being all agents' observations\n",
    "            next_state = next_obs\n",
    "        elif (state is None) != (next_state is None):\n",
    "            raise ValueError(\n",
    "                \"Expected either neither or both of `state` and \"\n",
    "                \"`next_state` to be given. Got: \"\n",
    "                \"\\n`state` = {}\\n`next_state` = {}\".format(state, next_state)\n",
    "            )\n",
    "\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = _unroll_mac(self.model, obs)\n",
    "\n",
    "        # Pick the Q-Values for the actions taken -> [B * n_agents, T]\n",
    "        chosen_action_qvals = torch.gather(\n",
    "            mac_out, dim=3, index=actions.unsqueeze(3)\n",
    "        ).squeeze(3)\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = _unroll_mac(self.target_model, next_obs)\n",
    "\n",
    "        # Mask out unavailable actions for the t+1 step\n",
    "        ignore_action_tp1 = (next_action_mask == 0) & (mask == 1).unsqueeze(-1)\n",
    "        target_mac_out[ignore_action_tp1] = -np.inf\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.double_q:\n",
    "            # Double Q learning computes the target Q values by selecting the\n",
    "            # t+1 timestep action according to the \"policy\" neural network and\n",
    "            # then estimating the Q-value of that action with the \"target\"\n",
    "            # neural network\n",
    "            \"\"\"\n",
    "            note that the target neural network uses the RNN network defined above and that \n",
    "\n",
    "            policy neural network also appears to be the mixer network \n",
    "\n",
    "            double q-learning will find 2 q values. One will be used in our case to identify an action that minimizes free energy while another \n",
    "            will be used to evaluate that action \n",
    "            \"\"\"\n",
    "\n",
    "            # Compute the t+1 Q-values to be used in action selection\n",
    "            # using next_obs\n",
    "            mac_out_tp1 = _unroll_mac(self.model, next_obs)\n",
    "\n",
    "            # mask out unallowed actions\n",
    "            mac_out_tp1[ignore_action_tp1] = -np.inf\n",
    "\n",
    "            # obtain best actions at t+1 according to policy NN\n",
    "            cur_max_actions = mac_out_tp1.argmax(dim=3, keepdim=True)\n",
    "\n",
    "            # use the target network to estimate the Q-values of policy\n",
    "            # network's selected actions\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(\n",
    "                3\n",
    "            )\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "        assert (\n",
    "            target_max_qvals.min().item() != -np.inf\n",
    "        ), \"target_max_qvals contains a masked action; \\\n",
    "            there may be a state with no valid actions.\"\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state)\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, next_state)\n",
    "\n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n",
    "\n",
    "        # Td-error\n",
    "        td_error = chosen_action_qvals - targets.detach()\n",
    "\n",
    "        mask = mask.expand_as(td_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = td_error * mask\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        loss = (masked_td_error ** 2).sum() / mask.sum()\n",
    "        return loss, mask, masked_td_error, chosen_action_qvals, targets\n",
    "\n",
    "\n",
    "class QMixTorchPolicy(TorchPolicy):\n",
    "    \"\"\"QMix impl. Assumes homogeneous agents for now.\n",
    "    You must use MultiAgentEnv.with_agent_groups() to group agents\n",
    "    together for QMix. This creates the proper Tuple obs/action spaces and\n",
    "    populates the '_group_rewards' info field.\n",
    "    Action masking: to specify an action mask for individual agents, use a\n",
    "    dict space with an action_mask key, e.g. {\"obs\": ob, \"action_mask\": mask}.\n",
    "    The mask space must be `Box(0, 1, (n_actions,))`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        _validate(obs_space, action_space)\n",
    "        config = dict(ray.rllib.algorithms.qmix.qmix.DEFAULT_CONFIG, **config)\n",
    "        self.framework = \"torch\"\n",
    "\n",
    "        self.n_agents = len(obs_space.original_space.spaces)\n",
    "        config[\"model\"][\"n_agents\"] = self.n_agents\n",
    "        self.n_actions = action_space.spaces[0].n\n",
    "        self.h_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "        self.has_env_global_state = False\n",
    "        self.has_action_mask = False\n",
    "\n",
    "        agent_obs_space = obs_space.original_space.spaces[0]\n",
    "        if isinstance(agent_obs_space, gym.spaces.Dict):\n",
    "            space_keys = set(agent_obs_space.spaces.keys())\n",
    "            if \"obs\" not in space_keys:\n",
    "                raise ValueError(\"Dict obs space must have subspace labeled `obs`\")\n",
    "            self.obs_size = _get_size(agent_obs_space.spaces[\"obs\"])\n",
    "            if \"action_mask\" in space_keys:\n",
    "                mask_shape = tuple(agent_obs_space.spaces[\"action_mask\"].shape)\n",
    "                if mask_shape != (self.n_actions,):\n",
    "                    raise ValueError(\n",
    "                        \"Action mask shape must be {}, got {}\".format(\n",
    "                            (self.n_actions,), mask_shape\n",
    "                        )\n",
    "                    )\n",
    "                self.has_action_mask = True\n",
    "            if ENV_STATE in space_keys:\n",
    "                self.env_global_state_shape = _get_size(\n",
    "                    agent_obs_space.spaces[ENV_STATE]\n",
    "                )\n",
    "                self.has_env_global_state = True\n",
    "            else:\n",
    "                self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            # The real agent obs space is nested inside the dict\n",
    "            config[\"model\"][\"full_obs_space\"] = agent_obs_space\n",
    "            agent_obs_space = agent_obs_space.spaces[\"obs\"]\n",
    "        else:\n",
    "            self.obs_size = _get_size(agent_obs_space)\n",
    "            self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"model\",\n",
    "            default_model=cmodel#RNNModel,\n",
    "        )\n",
    "\n",
    "        super().__init__(obs_space, action_space, config, model=self.model)\n",
    "\n",
    "        self.target_model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"target_model\",\n",
    "            default_model=cmodel#CModel#RNNModel,#this target model is the agent model \n",
    "        ).to(self.device)\n",
    "\n",
    "        self.exploration = self._create_exploration()\n",
    "\n",
    "        # Setup the mixer network.\n",
    "        if config[\"mixer\"] is None:\n",
    "            self.mixer = None\n",
    "            self.target_mixer = None\n",
    "        elif config[\"mixer\"] == \"qmix\":\n",
    "            self.mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "            self.target_mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "        elif config[\"mixer\"] == \"vdn\":\n",
    "            self.mixer = VDNMixer().to(self.device)\n",
    "            self.target_mixer = VDNMixer().to(self.device)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown mixer type {}\".format(config[\"mixer\"]))\n",
    "\n",
    "        self.cur_epsilon = 1.0\n",
    "        self.update_target()  # initial sync\n",
    "\n",
    "        # Setup optimizer\n",
    "        self.params = list(self.model.parameters())\n",
    "        if self.mixer:\n",
    "            self.params += list(self.mixer.parameters())\n",
    "        self.loss = QMixLoss(\n",
    "            self.model,\n",
    "            self.target_model,\n",
    "            self.mixer,\n",
    "            self.target_mixer,\n",
    "            self.n_agents,\n",
    "            self.n_actions,\n",
    "            self.config[\"double_q\"],\n",
    "            self.config[\"gamma\"],\n",
    "        )\n",
    "        from torch.optim import RMSprop\n",
    "\n",
    "        self.rmsprop_optimizer = RMSprop(#it appears that this RMSprop is optimizing the mixer network and not the agent network\n",
    "            params=self.params,\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"optim_alpha\"],\n",
    "            eps=config[\"optim_eps\"],\n",
    "        )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions_from_input_dict(\n",
    "        self,\n",
    "        input_dict: Dict[str, TensorType],\n",
    "        explore: bool = None,\n",
    "        timestep: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:\n",
    "\n",
    "        obs_batch = input_dict[SampleBatch.OBS]\n",
    "        state_batches = []\n",
    "        i = 0\n",
    "        while f\"state_in_{i}\" in input_dict:\n",
    "            state_batches.append(input_dict[f\"state_in_{i}\"])\n",
    "            i += 1\n",
    "\n",
    "        explore = explore if explore is not None else self.config[\"explore\"]\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        # We need to ensure we do not use the env global state\n",
    "        # to compute actions\n",
    "\n",
    "        # Compute actions\n",
    "        with torch.no_grad():\n",
    "            q_values, hiddens = _mac(\n",
    "                self.model,\n",
    "                torch.as_tensor(obs_batch, dtype=torch.float, device=self.device),\n",
    "                [\n",
    "                    torch.as_tensor(np.array(s), dtype=torch.float, device=self.device)\n",
    "                    for s in state_batches\n",
    "                ],\n",
    "            )\n",
    "            avail = torch.as_tensor(action_mask, dtype=torch.float, device=self.device)\n",
    "            masked_q_values = q_values.clone()\n",
    "            masked_q_values[avail == 0.0] = -float(\"inf\")\n",
    "            masked_q_values_folded = torch.reshape(\n",
    "                masked_q_values, [-1] + list(masked_q_values.shape)[2:]\n",
    "            )\n",
    "            actions, _ = self.exploration.get_exploration_action(\n",
    "                action_distribution=TorchCategorical(masked_q_values_folded),\n",
    "                timestep=timestep,\n",
    "                explore=explore,\n",
    "            )\n",
    "            actions = (\n",
    "                torch.reshape(actions, list(masked_q_values.shape)[:-1]).cpu().numpy()\n",
    "            )\n",
    "            hiddens = [s.cpu().numpy() for s in hiddens]\n",
    "\n",
    "        return tuple(actions.transpose([1, 0])), hiddens, {}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions(self, *args, **kwargs):\n",
    "        return self.compute_actions_from_input_dict(*args, **kwargs)\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "    ):\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        return np.zeros(obs_batch.size()[0])\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        obs_batch, action_mask, env_global_state = self._unpack_observation(\n",
    "            samples[SampleBatch.CUR_OBS]\n",
    "        )\n",
    "        (\n",
    "            next_obs_batch,\n",
    "            next_action_mask,\n",
    "            next_env_global_state,\n",
    "        ) = self._unpack_observation(samples[SampleBatch.NEXT_OBS])\n",
    "        group_rewards = self._get_group_rewards(samples[SampleBatch.INFOS])\n",
    "\n",
    "        input_list = [\n",
    "            group_rewards,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            samples[SampleBatch.ACTIONS],\n",
    "            samples[SampleBatch.DONES],\n",
    "            obs_batch,\n",
    "            next_obs_batch,\n",
    "        ]\n",
    "        if self.has_env_global_state:\n",
    "            input_list.extend([env_global_state, next_env_global_state])\n",
    "\n",
    "        output_list, _, seq_lens = chop_into_sequences(\n",
    "            episode_ids=samples[SampleBatch.EPS_ID],\n",
    "            unroll_ids=samples[SampleBatch.UNROLL_ID],\n",
    "            agent_indices=samples[SampleBatch.AGENT_INDEX],\n",
    "            feature_columns=input_list,\n",
    "            state_columns=[],  # RNN states not used here\n",
    "            max_seq_len=self.config[\"model\"][\"max_seq_len\"],\n",
    "            dynamic_max=True,\n",
    "        )\n",
    "        # These will be padded to shape [B * T, ...]\n",
    "        if self.has_env_global_state:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                dones,\n",
    "                obs,\n",
    "                next_obs,\n",
    "                env_global_state,\n",
    "                next_env_global_state,\n",
    "            ) = output_list\n",
    "        else:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                dones,\n",
    "                obs,\n",
    "                next_obs,\n",
    "            ) = output_list\n",
    "        B, T = len(seq_lens), max(seq_lens)\n",
    "\n",
    "        def to_batches(arr, dtype):\n",
    "            new_shape = [B, T] + list(arr.shape[1:])\n",
    "            return torch.as_tensor(\n",
    "                np.reshape(arr, new_shape), dtype=dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        rewards = to_batches(rew, torch.float)\n",
    "        actions = to_batches(act, torch.long)\n",
    "        obs = to_batches(obs, torch.float).reshape([B, T, self.n_agents, self.obs_size])\n",
    "        action_mask = to_batches(action_mask, torch.float)\n",
    "        next_obs = to_batches(next_obs, torch.float).reshape(\n",
    "            [B, T, self.n_agents, self.obs_size]\n",
    "        )\n",
    "        next_action_mask = to_batches(next_action_mask, torch.float)\n",
    "        if self.has_env_global_state:\n",
    "            env_global_state = to_batches(env_global_state, torch.float)\n",
    "            next_env_global_state = to_batches(next_env_global_state, torch.float)\n",
    "\n",
    "        # TODO(ekl) this treats group termination as individual termination\n",
    "        terminated = (\n",
    "            to_batches(dones, torch.float).unsqueeze(2).expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Create mask for where index is < unpadded sequence length\n",
    "        filled = np.reshape(\n",
    "            np.tile(np.arange(T, dtype=np.float32), B), [B, T]\n",
    "        ) < np.expand_dims(seq_lens, 1)\n",
    "        mask = (\n",
    "            torch.as_tensor(filled, dtype=torch.float, device=self.device)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss_out, mask, masked_td_error, chosen_action_qvals, targets = self.loss(\n",
    "            rewards,\n",
    "            actions,\n",
    "            terminated,\n",
    "            mask,\n",
    "            obs,\n",
    "            next_obs,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            env_global_state,\n",
    "            next_env_global_state,\n",
    "        )\n",
    "\n",
    "        # Optimise\n",
    "        self.rmsprop_optimizer.zero_grad()\n",
    "        loss_out.backward()\n",
    "        grad_norm_info = apply_grad_clipping(self, self.rmsprop_optimizer, loss_out)\n",
    "        self.rmsprop_optimizer.step()\n",
    "\n",
    "        mask_elems = mask.sum().item()\n",
    "        stats = {\n",
    "            \"loss\": loss_out.item(),\n",
    "            \"td_error_abs\": masked_td_error.abs().sum().item() / mask_elems,\n",
    "            \"q_taken_mean\": (chosen_action_qvals * mask).sum().item() / mask_elems,\n",
    "            \"target_mean\": (targets * mask).sum().item() / mask_elems,\n",
    "        }\n",
    "        stats.update(grad_norm_info)\n",
    "\n",
    "        return {LEARNER_STATS_KEY: stats}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_initial_state(self):  # initial RNN state\n",
    "        return [\n",
    "            s.expand([self.n_agents, -1]).cpu().numpy()\n",
    "            for s in self.model.get_initial_state()\n",
    "        ]\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            \"model\": self._cpu_dict(self.model.state_dict()),\n",
    "            \"target_model\": self._cpu_dict(self.target_model.state_dict()),\n",
    "            \"mixer\": self._cpu_dict(self.mixer.state_dict()) if self.mixer else None,\n",
    "            \"target_mixer\": self._cpu_dict(self.target_mixer.state_dict())\n",
    "            if self.mixer\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(self._device_dict(weights[\"model\"]))\n",
    "        self.target_model.load_state_dict(self._device_dict(weights[\"target_model\"]))\n",
    "        if weights[\"mixer\"] is not None:\n",
    "            self.mixer.load_state_dict(self._device_dict(weights[\"mixer\"]))\n",
    "            self.target_mixer.load_state_dict(\n",
    "                self._device_dict(weights[\"target_mixer\"])\n",
    "            )\n",
    "\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_state(self):\n",
    "        state = self.get_weights()\n",
    "        state[\"cur_epsilon\"] = self.cur_epsilon\n",
    "        return state\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_state(self, state):\n",
    "        self.set_weights(state)\n",
    "        self.set_epsilon(state[\"cur_epsilon\"])\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        logger.debug(\"Updated target networks\")\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cur_epsilon = epsilon\n",
    "\n",
    "    def _get_group_rewards(self, info_batch):\n",
    "        group_rewards = np.array(\n",
    "            [info.get(GROUP_REWARDS, [0.0] * self.n_agents) for info in info_batch]\n",
    "        )\n",
    "        return group_rewards\n",
    "\n",
    "    def _device_dict(self, state_dict):\n",
    "        return {\n",
    "            k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _cpu_dict(state_dict):\n",
    "        return {k: v.cpu().detach().numpy() for k, v in state_dict.items()}\n",
    "\n",
    "    def _unpack_observation(self, obs_batch):\n",
    "        \"\"\"Unpacks the observation, action mask, and state (if present)\n",
    "        from agent grouping.\n",
    "        Returns:\n",
    "            obs (np.ndarray): obs tensor of shape [B, n_agents, obs_size]\n",
    "            mask (np.ndarray): action mask, if any\n",
    "            state (np.ndarray or None): state tensor of shape [B, state_size]\n",
    "                or None if it is not in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        unpacked = _unpack_obs(\n",
    "            np.array(obs_batch, dtype=np.float32),\n",
    "            self.observation_space.original_space,\n",
    "            tensorlib=np,\n",
    "        )\n",
    "\n",
    "        if isinstance(unpacked[0], dict):\n",
    "            assert \"obs\" in unpacked[0]\n",
    "            unpacked_obs = [np.concatenate(tree.flatten(u[\"obs\"]), 1) for u in unpacked]\n",
    "        else:\n",
    "            unpacked_obs = unpacked\n",
    "\n",
    "        obs = np.concatenate(unpacked_obs, axis=1).reshape(\n",
    "            [len(obs_batch), self.n_agents, self.obs_size]\n",
    "        )\n",
    "\n",
    "        if self.has_action_mask:\n",
    "            action_mask = np.concatenate(\n",
    "                [o[\"action_mask\"] for o in unpacked], axis=1\n",
    "            ).reshape([len(obs_batch), self.n_agents, self.n_actions])\n",
    "        else:\n",
    "            action_mask = np.ones(\n",
    "                [len(obs_batch), self.n_agents, self.n_actions], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.has_env_global_state:\n",
    "            state = np.concatenate(tree.flatten(unpacked[0][ENV_STATE]), 1)\n",
    "        else:\n",
    "            state = None\n",
    "        return obs, action_mask, state\n",
    "\n",
    "\n",
    "def _validate(obs_space, action_space):\n",
    "    if not hasattr(obs_space, \"original_space\") or not isinstance(\n",
    "        obs_space.original_space, gym.spaces.Tuple\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Obs space must be a Tuple, got {}. Use \".format(obs_space)\n",
    "            + \"MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space, gym.spaces.Tuple):\n",
    "        raise ValueError(\n",
    "            \"Action space must be a Tuple, got {}. \".format(action_space)\n",
    "            + \"Use MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space.spaces[0], gym.spaces.Discrete):\n",
    "        raise ValueError(\n",
    "            \"QMix requires a discrete action space, got {}\".format(\n",
    "                action_space.spaces[0]\n",
    "            )\n",
    "        )\n",
    "    if len({str(x) for x in obs_space.original_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: observations of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(obs_space.original_space.spaces)\n",
    "        )\n",
    "    if len({str(x) for x in action_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: action space of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(action_space.spaces)\n",
    "        )\n",
    "\n",
    "\n",
    "def _mac(model, obs, h):\n",
    "    \"\"\"Forward pass of the multi-agent controller.\n",
    "    Args:\n",
    "        model: TorchModelV2 class\n",
    "        obs: Tensor of shape [B, n_agents, obs_size]\n",
    "        h: List of tensors of shape [B, n_agents, h_size]\n",
    "    Returns:\n",
    "        q_vals: Tensor of shape [B, n_agents, n_actions]\n",
    "        h: Tensor of shape [B, n_agents, h_size]\n",
    "    \"\"\"\n",
    "    B, n_agents = obs.size(0), obs.size(1)\n",
    "    if not isinstance(obs, dict):\n",
    "        obs = {\"obs\": obs}\n",
    "    obs_agents_as_batches = {k: _drop_agent_dim(v) for k, v in obs.items()}\n",
    "    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n",
    "    q_flat, h_flat = model(obs_agents_as_batches, h_flat, None)\n",
    "    return q_flat.reshape([B, n_agents, -1]), [\n",
    "        s.reshape([B, n_agents, -1]) for s in h_flat\n",
    "    ]\n",
    "\n",
    "\n",
    "def _unroll_mac(model, obs_tensor):\n",
    "    \"\"\"Computes the estimated Q values for an entire trajectory batch\"\"\"\n",
    "    B = obs_tensor.size(0)\n",
    "    T = obs_tensor.size(1)\n",
    "    n_agents = obs_tensor.size(2)\n",
    "    \"\"\"\n",
    "    model = optimize_model(\n",
    "      cmodel, input_data=input_data, device=\"gpu\"\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    mac_out = []\n",
    "    h = [s.expand([B, n_agents, -1]) for s in model.get_initial_state()]\n",
    "    for t in range(T):\n",
    "        q, h = _mac(model, obs_tensor[:, t], h)\n",
    "        mac_out.append(q)\n",
    "    mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "    return mac_out\n",
    "\n",
    "\n",
    "def _drop_agent_dim(T):\n",
    "    shape = list(T.shape)\n",
    "    B, n_agents = shape[0], shape[1]\n",
    "    return T.reshape([B * n_agents] + shape[2:])\n",
    "\n",
    "\n",
    "def _add_agent_dim(T, n_agents):\n",
    "    shape = list(T.shape)\n",
    "    B = shape[0] // n_agents\n",
    "    assert shape[0] % n_agents == 0\n",
    "    return T.reshape([B, n_agents] + shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c75f7d8",
   "metadata": {},
   "source": [
    "Everything up to this point has sucessfulyl executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b95bc9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AgentID' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11620\\3676672226.py\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    class brain(MultiAgentEnv):\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11620\\3676672226.py\"\u001b[1;36m, line \u001b[1;32m119\u001b[1;36m, in \u001b[1;35mbrain\u001b[1;36m\u001b[0m\n\u001b[1;33m    groups: Dict[str, List[AgentID]],\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'AgentID' is not defined\n"
     ]
    }
   ],
   "source": [
    "class brain(MultiAgentEnv):    \n",
    "\n",
    "  def __init__(self, config):\n",
    "    self.num_agents = 3\n",
    "    \"\"\"\n",
    "    this is how the actions and actions space will work: neurons do either spatial or temporal summation depending on\n",
    "    whether they are getting repeated inputs from on other neuron or inputs from multiple neurons.\n",
    "\n",
    "    The rate of fire of the neuron for specific impulses to the external agent will determine how much to slow\n",
    "    things down in the acceleration game or how much to speed things up. \n",
    "\n",
    "    There will be a baseline probability of firing is 0.5 \n",
    "    and in order to collectively minimize Free energy the neuronal agents will have to increase or decrease \n",
    "    the probability of firing \n",
    "\n",
    "    so the action space cannot be encoded as just one discrete thing. It will have to be a dict composed of:\n",
    "    box(probability between -1 ans 1 of firing) and discrete(num_neuronal_agents)\n",
    "\n",
    "    \"\"\"\n",
    "    self._spaces_in_preferred_format = True\n",
    "    self.a = self.num_agents-1#Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
    "    #the two kind of actions any agent can do are action potentials and post-synaptic potentials\n",
    "    #in addition the agent neuron can direct their action to any one of the neurons that exist\n",
    "    self.action_space = gym.spaces.Dict(\n",
    "        {\n",
    "            \"agent0\": Dict(\n",
    "                {\n",
    "                    \"prob\": Discrete(100), \n",
    "                    \"pos\": Discrete(self.a),\n",
    "                    \"action_potential\": Discrete(1) #external agent neurons\n",
    "                },dtype=np.float32),\n",
    "            \"agent1\": Dict(\n",
    "                {\n",
    "                    \"prob\": Discrete(100),\n",
    "                    \"pos\": Discrete(self.a),\n",
    "                    \"action_potential\": Discrete(1) #excitatory neurons\n",
    "                } ,dtype=np.float32),\n",
    "            \"agent2\": Dict(\n",
    "                {\n",
    "                    \"prob\": Discrete(100),\n",
    "                    \"pos\": Discrete(self.a),\n",
    "                 \n",
    "                    \"action_potential\": Discrete(1) #inhibitory neurons\n",
    "                }, dtype=np.float32)\n",
    "        }\n",
    "    )\n",
    "    self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent0\": gym.spaces.Box(low=-1.0, high=1.0, shape=(10,)),#agent 0 will be our external agent/motor neurons\n",
    "                \"agent1\": Discrete(1), #excitatory neurons\n",
    "                \"agent2\": Discrete(1)#inhibitory neurons\n",
    "            }\n",
    "        )\n",
    "\n",
    "        \n",
    " #config.get(Dict({\"prob\": Discrete(200),\"pos\": Discrete(self.a),\"action_potential\": Discrete(2),\"ps_potential\":Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32))}))\n",
    "\n",
    "    #external agent action space needs to involve observation = accesible_state*d where d is the distortion variable \n",
    "\n",
    "    #the observation space is only what the neuronal agent recievs from the other neuronal agent\n",
    "    #self.observation_space = config.get(Discrete(self.a))\n",
    "    self.timestep_limit = config.get(\"ts\", 1000)\n",
    "\n",
    "    \"\"\" Create the A matrix  \"\"\"\n",
    "    \"\"\"\n",
    "    A = np.zeros( (n_states, n_observations))\n",
    "    np.fill_diagonal(A, 0.5) \n",
    "    log_likelihood = log_stable(A[observation_index,:])\n",
    "\n",
    "    log_prior = log_stable(prior)\n",
    "\n",
    "    qs = softmax(log_likelihood + log_prior)\n",
    "\n",
    "    qs_past = utils.onehot(4, n_states) # agent believes they were at location 4 -- i.e. (1,1) one timestep ag\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    A-matrix here is about the conditional distribution between observations and the hidden state. The hidden state being\n",
    "    the action corresponding to a change in the acceleration number needed for the acceleration number to become 0 \n",
    "\n",
    "    The b-matrix is probability of current state given past state and past action. \n",
    "\n",
    "    if state is synapse strength then what we need to do is say, synapse strength depends on past synapse strength \n",
    "    and whether or not the neuron fired a positive signal at t-1, an inhibitory signal or did not fire at all.\n",
    "\n",
    "    THe second factor which will go into state is the optimal acceleration number. \n",
    "\n",
    "    c-matrix for the external hidden state factor will be a uniform prior across all possible acceleration numbers. Furthermore the \n",
    "    d-matrix for the external state factor will be a uniform prior across all possible acceleration numbers. \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Resets the episode and returns the initial observation of the new one.\n",
    "    # Reset the episode len.\n",
    "    self.episode_len = 0\n",
    "    # Sample a random number from our observation space. in the line directly below is where we should do A \n",
    "    self.cur_obs = self.observation_space.sample()#qo_u_left = get_expected_observations(A, qs_u_left)\n",
    "    # Return initial observation.\n",
    "    return self.cur_obs\n",
    "    \"\"\"\n",
    "    self.dones = set()\n",
    "    return {i: self.observation_space[i].sample() for i in self.agents}\n",
    "\n",
    "  def step(self, action):        \n",
    "    obs, rew, done, info = {}, {}, {}, {}\n",
    "    for i, action in action_dict.items():\n",
    "      obs[i] = self.observation_space[i].sample()\n",
    "      rew[i] = 0.0\n",
    "      done[i] = False\n",
    "      info[i] = {}\n",
    "      done[\"__all__\"] = len(self.dones) == len(self.agents)\n",
    "    return obs, rew, done, info\n",
    "  #this ensures that the agents are not all homogenous \n",
    "  def with_agent_groups(\n",
    "        self,\n",
    "        groups: Dict[str, List[AgentID]],\n",
    "        obs_space: gym.Space = None,\n",
    "            act_space: gym.Space = None) -> \"MultiAgentEnv\":\n",
    "        \"\"\"Convenience method for grouping together agents in this env.\n",
    "\n",
    "        An agent group is a list of agent IDs that are mapped to a single\n",
    "        logical agent. All agents of the group must act at the same time in the\n",
    "        environment. The grouped agent exposes Tuple action and observation\n",
    "        spaces that are the concatenated action and obs spaces of the\n",
    "        individual agents.\n",
    "\n",
    "        The rewards of all the agents in a group are summed. The individual\n",
    "        agent rewards are available under the \"individual_rewards\" key of the\n",
    "        group info return.\n",
    "\n",
    "        Agent grouping is required to leverage algorithms such as Q-Mix.\n",
    "\n",
    "        Args:\n",
    "            groups: Mapping from group id to a list of the agent ids\n",
    "                of group members. If an agent id is not present in any group\n",
    "                value, it will be left ungrouped. The group id becomes a new agent ID\n",
    "                in the final environment.\n",
    "            obs_space: Optional observation space for the grouped\n",
    "                env. Must be a tuple space. If not provided, will infer this to be a\n",
    "                Tuple of n individual agents spaces (n=num agents in a group).\n",
    "            act_space: Optional action space for the grouped env.\n",
    "                Must be a tuple space. If not provided, will infer this to be a Tuple\n",
    "                of n individual agents spaces (n=num agents in a group).\n",
    "\n",
    "        Examples:\n",
    "            >>> from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "            >>> class MyMultiAgentEnv(MultiAgentEnv): # doctest: +SKIP\n",
    "            ...     # define your env here\n",
    "            ...     ... # doctest: +SKIP\n",
    "            >>> env = MyMultiAgentEnv(...) # doctest: +SKIP\n",
    "            >>> grouped_env = env.with_agent_groups(env, { # doctest: +SKIP\n",
    "            ...   \"group1\": [\"agent1\", \"agent2\", \"agent3\"], # doctest: +SKIP\n",
    "            ...   \"group2\": [\"agent4\", \"agent5\"], # doctest: +SKIP\n",
    "            ... }) # doctest: +SKIP\n",
    "        \"\"\"\n",
    "\n",
    "        from ray.rllib.env.wrappers.group_agents_wrapper import \\\n",
    "            GroupAgentsWrapper\n",
    "        return GroupAgentsWrapper(self, groups, obs_space, act_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10397ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"    \n",
    "    config = (\n",
    "            PPOConfig()\n",
    "            .environment(HierarchicalWindyMazeEnv)\n",
    "            .framework(args.framework)\n",
    "            .rollouts(num_rollout_workers=0)\n",
    "            .training(entropy_coeff=0.01)\n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        maze.observation_space,\n",
    "                        Discrete(4),\n",
    "                        PPOConfig.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        Tuple([maze.observation_space, Discrete(4)]),\n",
    "                        maze.action_space,\n",
    "                        PPOConfig.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn,\n",
    "            )\n",
    "            # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "            .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "            \n",
    "        )\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84978063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMixConfig(SimpleQConfig):\n",
    "    \"\"\"Defines a configuration class from which QMix can be built.\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> config = QMixConfig().training(gamma=0.9, lr=0.01, kl_coeff=0.3)\\\n",
    "        ...             .resources(num_gpus=0)\\\n",
    "        ...             .rollouts(num_workers=4)\n",
    "        >>> print(config.to_dict())\n",
    "        >>> # Build an Algorithm object from the config and run 1 training iteration.\n",
    "        >>> algo = config.build(env=TwoStepGame)\n",
    "        >>> algo.train()\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> from ray import tune\n",
    "        >>> config = QMixConfig()\n",
    "        >>> # Print out some default values.\n",
    "        >>> print(config.optim_alpha)\n",
    "        >>> # Update the config object.\n",
    "        >>> config.training(lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97)\n",
    "        >>> # Set the config object's env.\n",
    "        >>> config.environment(env=TwoStepGame)\n",
    "        >>> # Use to_dict() to get the old-style python config dict\n",
    "        >>> # when running with tune.\n",
    "        >>> tune.run(\n",
    "        ...     \"QMix\",\n",
    "        ...     stop={\"episode_reward_mean\": 200},\n",
    "        ...     config=config.to_dict(),\n",
    "        ... )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PPOConfig instance.\"\"\"\n",
    "        super().__init__(algo_class=QMix)\n",
    "\n",
    "        # fmt: off\n",
    "        # __sphinx_doc_begin__\n",
    "        # QMix specific settings:\n",
    "        self.mixer = \"qmix\"\n",
    "        self.mixing_embed_dim = 32\n",
    "        self.double_q = True\n",
    "        self.optim_alpha = 0.99\n",
    "        self.optim_eps = 0.00001\n",
    "        self.grad_clip = 10\n",
    "\n",
    "        # Override some of AlgorithmConfig's default values with QMix-specific values.\n",
    "        # .training()\n",
    "        self.lr = 0.0005\n",
    "        self.train_batch_size = 32\n",
    "        self.target_network_update_freq = 500\n",
    "        # Number of timesteps to collect from rollout workers before we start\n",
    "        # sampling from replay buffers for learning. Whether we count this in agent\n",
    "        # steps  or environment steps depends on config[\"multiagent\"][\"count_steps_by\"].\n",
    "        self.num_steps_sampled_before_learning_starts = 1000\n",
    "        self.replay_buffer_config = {\n",
    "            \"type\": \"ReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "            \"prioritized_replay\": DEPRECATED_VALUE,\n",
    "            # Size of the replay buffer in batches (not timesteps!).\n",
    "            \"capacity\": 1000,\n",
    "            \n",
    "            # Choosing `fragments` here makes it so that the buffer stores entire\n",
    "            # batches, instead of sequences, episodes or timesteps.\n",
    "            \"storage_unit\": \"fragments\",\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        }\n",
    "        self.model = {\n",
    "            \"lstm_cell_size\": 64,\n",
    "            \"max_seq_len\": 999999,\n",
    "        }\n",
    "\n",
    "        # .framework()\n",
    "        self.framework_str = \"torch\"\n",
    "\n",
    "        # .rollouts()\n",
    "        self.num_workers = 3\n",
    "        self.rollout_fragment_length = 4\n",
    "        self.batch_mode = \"complete_episodes\"\n",
    "\n",
    "        # .reporting()\n",
    "        self.min_time_s_per_iteration = 1\n",
    "        self.min_sample_timesteps_per_iteration = 1000\n",
    "\n",
    "        # .exploration()\n",
    "        self.exploration_config = {\n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\",\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.01,\n",
    "            # Timesteps over which to anneal epsilon.\n",
    "            \"epsilon_timesteps\": 40000,\n",
    "\n",
    "            # For soft_q, use:\n",
    "            # \"exploration_config\" = {\n",
    "            #   \"type\": \"SoftQ\"\n",
    "            #   \"temperature\": [float, e.g. 1.0]\n",
    "            # }\n",
    "        }\n",
    "\n",
    "        # .evaluation()\n",
    "        # Evaluate with epsilon=0 every `evaluation_interval` training iterations.\n",
    "        # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "        # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "        # metrics are already only reported for the lowest epsilon workers.\n",
    "        self.evaluation_interval = None\n",
    "        self.evaluation_duration = 10\n",
    "        self.evaluation_config = {\n",
    "            \"explore\": False,\n",
    "        }\n",
    "        # __sphinx_doc_end__\n",
    "        # fmt: on\n",
    "\n",
    "        self.worker_side_prioritization = DEPRECATED_VALUE\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        mixer: Optional[str] = None,\n",
    "        mixing_embed_dim: Optional[int] = None,\n",
    "        double_q: Optional[bool] = None,\n",
    "        target_network_update_freq: Optional[int] = None,\n",
    "        replay_buffer_config: Optional[dict] = None,\n",
    "        optim_alpha: Optional[float] = None,\n",
    "        optim_eps: Optional[float] = None,\n",
    "        grad_norm_clipping: Optional[float] = None,\n",
    "        grad_clip: Optional[float] = None,\n",
    "        **kwargs,\n",
    "    ) -> \"QMixConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            mixer: Mixing network. Either \"qmix\", \"vdn\", or None.\n",
    "            mixing_embed_dim: Size of the mixing network embedding.\n",
    "            double_q: Whether to use Double_Q learning.\n",
    "            target_network_update_freq: Update the target network every\n",
    "                `target_network_update_freq` sample steps.\n",
    "            replay_buffer_config:\n",
    "            optim_alpha: RMSProp alpha.\n",
    "            optim_eps: RMSProp epsilon.\n",
    "            grad_clip: If not None, clip gradients during optimization at\n",
    "                this value.\n",
    "            grad_norm_clipping: Depcrecated in favor of grad_clip\n",
    "        Returns:\n",
    "            This updated AlgorithmConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if grad_norm_clipping is not None:\n",
    "            deprecation_warning(\n",
    "                old=\"grad_norm_clipping\",\n",
    "                new=\"grad_clip\",\n",
    "                help=\"Parameter `grad_norm_clipping` has been \"\n",
    "                \"deprecated in favor of grad_clip in QMix. \"\n",
    "                \"This is now the same parameter as in other \"\n",
    "                \"algorithms. `grad_clip` will be overwritten by \"\n",
    "                \"`grad_norm_clipping={}`\".format(grad_norm_clipping),\n",
    "                error=False,\n",
    "            )\n",
    "            grad_clip = grad_norm_clipping\n",
    "\n",
    "        if mixer is not None:\n",
    "            self.mixer = mixer\n",
    "        if mixing_embed_dim is not None:\n",
    "            self.mixing_embed_dim = mixing_embed_dim\n",
    "        if double_q is not None:\n",
    "            self.double_q = double_q\n",
    "        if target_network_update_freq is not None:\n",
    "            self.target_network_update_freq = target_network_update_freq\n",
    "        if replay_buffer_config is not None:\n",
    "            self.replay_buffer_config = replay_buffer_config\n",
    "        if optim_alpha is not None:\n",
    "            self.optim_alpha = optim_alpha\n",
    "        if optim_eps is not None:\n",
    "            self.optim_eps = optim_eps\n",
    "        if grad_clip is not None:\n",
    "            self.grad_clip = grad_clip\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class QMix(SimpleQ):\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_config(cls) -> AlgorithmConfigDict:\n",
    "        return QMixConfig().to_dict()\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def validate_config(self, config: AlgorithmConfigDict) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate_config(config)\n",
    "\n",
    "        if config[\"framework\"] != \"torch\":\n",
    "            raise ValueError(\"Only `framework=torch` supported so far for QMix!\")\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def get_default_policy_class(self, config: AlgorithmConfigDict) -> Type[Policy]:\n",
    "        return QMixTorchPolicy\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def training_step(self) -> ResultDict:\n",
    "        \"\"\"QMIX training iteration function.\n",
    "        - Sample n MultiAgentBatches from n workers synchronously.\n",
    "        - Store new samples in the replay buffer.\n",
    "        - Sample one training MultiAgentBatch from the replay buffer.\n",
    "        - Learn on the training batch.\n",
    "        - Update the target network every `target_network_update_freq` sample steps.\n",
    "        - Return all collected training metrics for the iteration.\n",
    "        Returns:\n",
    "            The results dict from executing the training iteration.\n",
    "        \"\"\"\n",
    "        # Sample n batches from n workers.\n",
    "        new_sample_batches = synchronous_parallel_sample(\n",
    "            worker_set=self.workers, concat=False\n",
    "        )\n",
    "\n",
    "        for batch in new_sample_batches:\n",
    "            # Update counters.\n",
    "            self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n",
    "            self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n",
    "            # Store new samples in the replay buffer.\n",
    "            self.local_replay_buffer.add(batch)\n",
    "\n",
    "        # Update target network every `target_network_update_freq` sample steps.\n",
    "        cur_ts = self._counters[\n",
    "            NUM_AGENT_STEPS_SAMPLED if self._by_agent_steps else NUM_ENV_STEPS_SAMPLED\n",
    "        ]\n",
    "\n",
    "        train_results = {}\n",
    "\n",
    "        if cur_ts > self.config[\"num_steps_sampled_before_learning_starts\"]:\n",
    "            # Sample n batches from replay buffer until the total number of timesteps\n",
    "            # reaches `train_batch_size`.\n",
    "            train_batch = sample_min_n_steps_from_buffer(\n",
    "                replay_buffer=self.local_replay_buffer,\n",
    "                min_steps=self.config[\"train_batch_size\"],\n",
    "                count_by_agent_steps=self._by_agent_steps,\n",
    "            )\n",
    "\n",
    "            # Learn on the training batch.\n",
    "            # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
    "            # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
    "            if self.config.get(\"simple_optimizer\") is True:\n",
    "                train_results = train_one_step(self, train_batch)\n",
    "            else:\n",
    "                train_results = multi_gpu_train_one_step(self, train_batch)\n",
    "\n",
    "            # Update target network every `target_network_update_freq` sample steps.\n",
    "            last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
    "            if cur_ts - last_update >= self.config[\"target_network_update_freq\"]:\n",
    "                to_update = self.workers.local_worker().get_policies_to_train()\n",
    "                self.workers.local_worker().foreach_policy_to_train(\n",
    "                    lambda p, pid: pid in to_update and p.update_target()\n",
    "                )\n",
    "                self._counters[NUM_TARGET_UPDATES] += 1\n",
    "                self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
    "\n",
    "            update_priorities_in_replay_buffer(\n",
    "                self.local_replay_buffer, self.config, train_batch, train_results\n",
    "            )\n",
    "\n",
    "            # Update weights and global_vars - after learning on the local worker -\n",
    "            # on all remote workers.\n",
    "            global_vars = {\n",
    "                \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
    "            }\n",
    "            # Update remote workers' weights and global vars after learning on local\n",
    "            # worker.\n",
    "            with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
    "                self.workers.sync_weights(global_vars=global_vars)\n",
    "\n",
    "        # Return all collected metrics for the iteration.\n",
    "        return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed277ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba99ab67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11620\\1227002304.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    param_space=config.to_dict(),\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'config' is not defined\n"
     ]
    }
   ],
   "source": [
    "tune.Tuner(  \n",
    "    \"QMix\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "654144d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The subjects directory has to be specified using the subjects_dir parameter or the SUBJECTS_DIR environment variable.",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11620\\3846482204.py\"\u001b[0m, line \u001b[0;32m7\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    brain = Brain(sub, hemi, surf)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\surfer\\viz.py\"\u001b[0m, line \u001b[0;32m419\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    subjects_dir = _get_subjects_dir(subjects_dir=subjects_dir)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\surfer\\utils.py\"\u001b[1;36m, line \u001b[1;32m723\u001b[1;36m, in \u001b[1;35m_get_subjects_dir\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise ValueError('The subjects directory has to be specified '\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m\u001b[1;31m:\u001b[0m The subjects directory has to be specified using the subjects_dir parameter or the SUBJECTS_DIR environment variable.\n"
     ]
    }
   ],
   "source": [
    "from surfer import Brain\n",
    "\n",
    "sub = 'fsaverage'\n",
    "hemi = 'lh'\n",
    "surf = 'inflated'\n",
    "\n",
    "brain = Brain(sub, hemi, surf)\n",
    "\n",
    "brain.animate(['l', 'c'])\n",
    "\n",
    "# control number of steps\n",
    "brain.animate(['l', 'm'], n_steps=30)\n",
    "\n",
    "# any path you can think of\n",
    "brain.animate(['l', 'c', 'm', 'r', 'c', 'r', 'l'], n_steps=45)\n",
    "\n",
    "# full turns\n",
    "brain.animate([\"m\"] * 3)\n",
    "\n",
    "# movies\n",
    "brain.animate(['l', 'l'], n_steps=10, fname='simple_animation.avi')\n",
    "\n",
    "# however, rotating out of the axial plane is not allowed\n",
    "try:\n",
    "    brain.animate(['l', 'd'])\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0ffea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
