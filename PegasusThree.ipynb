{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b39134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\nebulgym\\training_learners\\model_engines\\ort_engine.py:14: UserWarning: No ONNXRuntime training library for pytorch has been detected. The ORT backend won't be used.\n",
      "  warnings.warn(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\nebulgym\\training_learners\\model_engines\\rammer_engine.py:16: UserWarning: No valid Rammer installation found. Using the Rammer backend will result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.6.0, llvm 15.0.1, commit f1c6fbbd, win, python 3.9.16\n",
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "#mo_gym is for creating enviroments with multiple reward functions simultaneously \n",
    "import mo_gymnasium as mo_gym#we are going to phase out gym in favor of mo_gym\n",
    "\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "#from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "import functools\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "if we do this carefully we can use taichi to carry out speedup\n",
    "\n",
    "The mixer and the bmodel would be ti.funcs\n",
    "\n",
    "qmixpolicy would also be a ti.func\n",
    "\n",
    "qmix would be the ti.kernel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import torch\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "#from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE, make_multi_agent\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer.abstract_infer import TracePosterior\n",
    "import nitime\n",
    "from deeptime.sindy import SINDy\n",
    "\n",
    "#non causal counterfactuals\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers # helper functions\n",
    "\n",
    "#data visualization\n",
    "import pygwalker as pyg\n",
    "\n",
    "\"\"\"\n",
    "from github3 import login, GitHub\n",
    "from getpass import getpass, getuser\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7207eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.ode import odeint\n",
    "#\n",
    "#from jax.random import PRNGKey\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.examples.datasets import LYNXHARE, load_dataset\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "from numpyro.infer import Predictive, SVI, Trace_ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f92365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "x = jnp.arange(10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3ba5510",
   "metadata": {},
   "source": [
    "For the ESM the observations are stuff from observation space, x is the reward\n",
    "\n",
    "The epistemic value has form expectation of entropy of p(outcomes given states)-Q(outcomes given policy) \n",
    "\n",
    "pragmatic value has lnP(outcomes|c)\n",
    "\n",
    "APE = expected entropy of probability theta given observations and design\n",
    "\n",
    "Theta is q as defined by the guide\n",
    "\n",
    "Note: replace as much of our pyro with numpyro as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b9b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create this like the outcome predictor. This is the expected free energy sub-module ESM\n",
    "#this will essentially calculate the q(x|y)\n",
    "#@profile\n",
    "class ESM(nn.Module,ivy.Module):\n",
    "     def __init__(self):     \n",
    "\n",
    "        super().__init__()\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior_entropy = dist.Bernoulli(prior_w_prob).entropy()\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        #model = tf.keras.Sequential([\n",
    "        u = tfkl.InputLayer(input_shape=input_shape),\n",
    "        \"\"\"\n",
    "        u = tf.keras.layers.LSTM(25,kernel_initializer='zeros',activation='tanh', dtype = x.dtype, use_bias=True)(u),\n",
    "        u = tfp.layers.VariationalGaussianProcess(\n",
    "                num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\n",
    "                inducing_index_points_initializer=tf.compat.v1.constant_initializer(\n",
    "                    np.linspace(0,x_range, num=1125,\n",
    "                                dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))), mean_fn=None,\n",
    "                jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)(u)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "        #in unconstrained thing replace astype with tf.dtype thing.    #tf.initializers.constant(-10.0)\n",
    "        #])\n",
    "        def compute_probability(self, y):\n",
    "            #fk.Model()\n",
    "            z = nn.functional.relu(self.h1(y))\n",
    "            z = nn.functional.relu(self.h2(z))\n",
    "            #o = tf.nn.relu(u)\n",
    "            return self.h3(z)\n",
    "        def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "            pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "            y = y_dict[\"y\"]\n",
    "\n",
    "            #bmodel = FullLaplace(bmodel,'regression',prior_precision=2)\n",
    "            dem_prob = self.compute_probability(y).squeeze()\n",
    "            pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462ecb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyro.contrib.oed.eig import marginal_eig\\ndef ES(design, observation_labels, target_labels):\\n    # This shape allows us to learn a different parameter for each candidate design l\\n    q_logit = pyro.param(\"q_logit\", torch.zeros(design.shape[-2:])).entropy()\\n    pyro.sample(\"y\", dist.Bernoulli(logits=q_logit).to_event(1))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pyro.contrib.oed.eig import marginal_eig\n",
    "def ES(design, observation_labels, target_labels):\n",
    "    # This shape allows us to learn a different parameter for each candidate design l\n",
    "    q_logit = pyro.param(\"q_logit\", torch.zeros(design.shape[-2:])).entropy()\n",
    "    pyro.sample(\"y\", dist.Bernoulli(logits=q_logit).to_event(1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6f3bfbf",
   "metadata": {},
   "source": [
    "we are going to use the network design algorithm that uses pysindy and its physics informed neural networks. If we can use this in \n",
    "conjunction with judea perls algorithm for verifying any causal thing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12660c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profile\n",
    "def memoize(fn=None, **kwargs):\n",
    "    if fn is None:\n",
    "        return lambda _fn: memoize(_fn, **kwargs)\n",
    "    return functools.lru_cache(**kwargs)(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42861e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profile\n",
    "class HashingMarginal(dist.Distribution):\n",
    "    \"\"\"\n",
    "    :param trace_dist: a TracePosterior instance representing a Monte Carlo posterior\n",
    "\n",
    "    Marginal histogram distribution.\n",
    "    Turns a TracePosterior object into a Distribution\n",
    "    over the return values of the TracePosterior's model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trace_dist, sites=None):\n",
    "        assert isinstance(\n",
    "            trace_dist, TracePosterior\n",
    "        ), \"trace_dist must be trace posterior distribution object\"\n",
    "\n",
    "        if sites is None:\n",
    "            sites = \"_RETURN\"\n",
    "\n",
    "        assert isinstance(sites, (str, list)), \"sites must be either '_RETURN' or list\"\n",
    "\n",
    "        self.sites = sites\n",
    "        super().__init__()\n",
    "        self.trace_dist = trace_dist\n",
    "\n",
    "    has_enumerate_support = True\n",
    "\n",
    "    @memoize(maxsize=10)\n",
    "    def _dist_and_values(self):\n",
    "        # XXX currently this whole object is very inefficient\n",
    "        values_map, logits = collections.OrderedDict(), collections.OrderedDict()\n",
    "        for tr, logit in zip(self.trace_dist.exec_traces, self.trace_dist.log_weights):\n",
    "            if isinstance(self.sites, str):\n",
    "                value = tr.nodes[self.sites][\"value\"]\n",
    "            else:\n",
    "                value = {site: tr.nodes[site][\"value\"] for site in self.sites}\n",
    "            if not torch.is_tensor(logit):\n",
    "                logit = torch.tensor(logit)\n",
    "\n",
    "            if torch.is_tensor(value):\n",
    "                value_hash = hash(value.cpu().contiguous().numpy().tobytes())\n",
    "            elif isinstance(value, dict):\n",
    "                value_hash = hash(self._dict_to_tuple(value))\n",
    "            else:\n",
    "                value_hash = hash(value)\n",
    "            if value_hash in logits:\n",
    "                # Value has already been seen.\n",
    "                logits[value_hash] = dist.util.logsumexp(\n",
    "                    torch.stack([logits[value_hash], logit]), dim=-1\n",
    "                )\n",
    "            else:\n",
    "                logits[value_hash] = logit\n",
    "                values_map[value_hash] = value\n",
    "\n",
    "        logits = torch.stack(list(logits.values())).contiguous().view(-1)\n",
    "        logits = logits - dist.util.logsumexp(logits, dim=-1)\n",
    "        d = dist.Categorical(logits=logits)\n",
    "        return d, values_map\n",
    "\n",
    "    def sample(self):\n",
    "        d, values_map = self._dist_and_values()\n",
    "        ix = d.sample()\n",
    "        return list(values_map.values())[ix]\n",
    "\n",
    "    def log_prob(self, val):\n",
    "        d, values_map = self._dist_and_values()\n",
    "        if torch.is_tensor(val):\n",
    "            value_hash = hash(val.cpu().contiguous().numpy().tobytes())\n",
    "        elif isinstance(val, dict):\n",
    "            value_hash = hash(self._dict_to_tuple(val))\n",
    "        else:\n",
    "            value_hash = hash(val)\n",
    "        return d.log_prob(torch.tensor([list(values_map.keys()).index(value_hash)]))\n",
    "\n",
    "    def enumerate_support(self):\n",
    "        d, values_map = self._dist_and_values()\n",
    "        return list(values_map.values())[:]\n",
    "\n",
    "    def _dict_to_tuple(self, d):\n",
    "        \"\"\"\n",
    "        Recursively converts a dictionary to a list of key-value tuples\n",
    "        Only intended for use as a helper function inside HashingMarginal!!\n",
    "        May break when keys cant be sorted, but that is not an expected use-case\n",
    "        \"\"\"\n",
    "        if isinstance(d, dict):\n",
    "            return tuple([(k, self._dict_to_tuple(d[k])) for k in sorted(d.keys())])\n",
    "        else:\n",
    "            return d\n",
    "\n",
    "    def _weighted_mean(self, value, dim=0):\n",
    "        weights = self._log_weights.reshape([-1] + (value.dim() - 1) * [1])\n",
    "        max_weight = weights.max(dim=dim)[0]\n",
    "        relative_probs = (weights - max_weight).exp()\n",
    "        return (value * relative_probs).sum(dim=dim) / relative_probs.sum(dim=dim)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        samples = torch.stack(list(self._dist_and_values()[1].values()))\n",
    "        return self._weighted_mean(samples)\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        samples = torch.stack(list(self._dist_and_values()[1].values()))\n",
    "        deviation_squared = torch.pow(samples - self.mean, 2)\n",
    "        return self._weighted_mean(deviation_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7613374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profile\n",
    "def Marginal(fn=None, **kwargs):\n",
    "    if fn is None:\n",
    "        return lambda _fn: Marginal(_fn, **kwargs)\n",
    "    return memoize(\n",
    "        lambda *args: HashingMarginal(BestFirstSearch(fn, **kwargs).run(*args))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bacb9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ti.data_oriented\n",
    "class Search(TracePosterior):\n",
    "    \"\"\"\n",
    "    Exact inference by enumerating over all possible executions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, max_tries=int(1e6), **kwargs):\n",
    "        self.model = model\n",
    "        self.max_tries = max_tries\n",
    "        super().__init__(**kwargs)\n",
    "    #@ti.kernel\n",
    "    def _traces(self, *args, **kwargs):\n",
    "        q = queue.Queue()\n",
    "        q.put(poutine.Trace())\n",
    "        p = poutine.trace(poutine.queue(self.model, queue=q, max_tries=self.max_tries))\n",
    "        while not q.empty():\n",
    "            tr = p.get_trace(*args, **kwargs)\n",
    "            yield tr, tr.log_prob_sum()\n",
    "#@ti.data_oriented\n",
    "class BestFirstSearch(TracePosterior):\n",
    "    \"\"\"\n",
    "    Inference by enumerating executions ordered by their probabilities.\n",
    "    Exact (and results equivalent to Search) if all executions are enumerated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, num_samples=None, **kwargs):\n",
    "        if num_samples is None:\n",
    "            num_samples = 100\n",
    "        self.num_samples = num_samples\n",
    "        self.model = model\n",
    "        super().__init__(**kwargs)\n",
    "    #@ti.kernel\n",
    "    def _traces(self, *args, **kwargs):\n",
    "        q = queue.PriorityQueue()\n",
    "        # add a little bit of noise to the priority to break ties...\n",
    "        q.put((torch.zeros(1).item() - torch.rand(1).item() * 1e-2, poutine.Trace()))\n",
    "        q_fn = pqueue(self.model, queue=q)\n",
    "        for i in range(self.num_samples):\n",
    "            if q.empty():\n",
    "                # num_samples was too large!\n",
    "                break\n",
    "            tr = poutine.trace(q_fn).get_trace(*args, **kwargs)  # XXX should block\n",
    "            yield tr, tr.log_prob_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67b43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pragmatic value function \n",
    "\n",
    "@Marginal\n",
    "def speaker(state):\n",
    "    alpha = 1.\n",
    "    with poutine.scale(scale=torch.tensor(alpha)):\n",
    "        utterance = utterance_prior()\n",
    "        pyro.sample(\"listener\", literal_listener(utterance), obs=state)\n",
    "    return utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb28d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dadaptation import DAdaptAdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a82ea695",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =  DAdaptAdaGrad#torch.optim.SGD\n",
    "scheduler = pyro.optim.ExponentialLR({'optimizer': optimizer, 'optim_args': {}, 'gamma': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0025cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@accelerate_model()\n",
    "class emodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "\n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        pragmatic value is probability of outcomes averaged over all policies or E(lnP*o|c) where c is prior\n",
    "        policies with outcomes close to the prior become more probable\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #our job here is to minimize the log probability of y given prior. \n",
    "        #input_dict[\"obs_flat\"].float()\n",
    "        #encoder = tfk.Sequential([\n",
    "        \"\"\"\n",
    "        i = tfkl.InputLayer(input_shape=input_shape)\n",
    "        x = tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "               activation=None)(i)\n",
    "        z = tfp.layers.VariationalGaussianProcess(\n",
    "                num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\n",
    "                inducing_index_points_initializer=tf.compat.v1.constant_initializer(\n",
    "                    np.linspace(0,x_range, num=1125,\n",
    "                                dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))), mean_fn=None,\n",
    "                jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)(x)\n",
    "\n",
    "        #in unconstrained thing replace astype with tf.dtype thing.    #tf.initializers.constant(-10.0)\n",
    "\n",
    "        #negloglik = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "\n",
    "        eig = marginal_eig(model,\n",
    "                           candidate_designs,       # design, or in this case, tensor of possible designs\n",
    "                           \"y\",                     # site label of observations, could be a list\n",
    "                           \"theta\",                 # site label of 'targets' (latent variables), could also be list\n",
    "                           num_samples=100,         # number of samples to draw per step in the expectation\n",
    "                           num_steps=num_steps,     # number of gradient steps\n",
    "                           guide=marginal_guide,    # guide q(y)\n",
    "                           optim=optimizer,         # optimizer with learning rate decay\n",
    "                           final_num_samples=10000  # at the last step, we draw more samples\n",
    "                                                    # for a more accurate EIG estimate\n",
    "                          )\n",
    "\n",
    "        expected free energy is minimized when when observations are slected that cause a large change in beliefs \n",
    "        \n",
    "        Expected free energy will consist of 3 components\n",
    "        \n",
    "        ESM -> q(y|d)\n",
    "        |\n",
    "        emodels -> initialization pragmatic value neural network. \n",
    "        This is going to be neural network for \n",
    "        log(p(y|prior)) +epistemic value  emodels + (epistemic value = ESM - expected ambiguity)\n",
    "        |\n",
    "        EFE -> expected free energy/temperature   \n",
    "        \n",
    "        pragmatic value will be calculated using a variational gaussian process regression tensorflow neural network \n",
    "        \n",
    "        we are going to explore The Rational Speech Act framework and its relation to calculating the pragmatic value\n",
    "        \n",
    "        We will actually calculate both epistemic value and define the neural network for pragmatic value in emodels\n",
    "        \n",
    "        but inputs, parameterization and outputs of pragmatic value network will happen on EFE\n",
    "        \n",
    "        Next objective: determine the probaility of hidden state given observation\n",
    "        \n",
    "        we should use a neural network to calculate the probability of reward given action \n",
    "        \n",
    "        if there is 0 correlation, that is to say reward does not change given a certain action then p(s|y) = 0\n",
    "        \n",
    "        pragmatic value observations expecteation of log probability of outcomes given prior \n",
    "        \n",
    "        theta = s=w=x is the hidden latent variable state. Here it is going to be the reward\n",
    "        \n",
    "        design is going to be action, outcome is observation, state is the reward\n",
    "        \n",
    "        also -H(P(s)) = E(lnP(s))\n",
    "        \n",
    "        in EFE we are going to use a relaxed bernoulli with temperature for distribution of entropy of p(o|s)\n",
    " \n",
    "        average predictive entropy = H(posteririor pdf) = entropy of entire epsitemic value \n",
    "        \n",
    "        for our purposes p(theta) = q(theta|design)\n",
    "        \n",
    "        pragmatic value is -E(ln(p(y|prior)) which could be converted to prior entropy \n",
    "        \n",
    "        we will implement enhanced expected free energy later to include learning. \n",
    "        \n",
    "        we for now can set the pragmatic value to 0. this is used to set objectives \n",
    "        \n",
    "        NOte: to clear up confusion theta can refer to states or to beliefs about parameters in model \n",
    "        \n",
    "        later for enhanced expected free energy we will have to include theta side by side with states outcomes,\n",
    "        designs and prior \n",
    "        \n",
    "        Reinforcement learning use value functions whereas active inference uses priors to set objectives\n",
    "        \n",
    "        the pragmatic value will be computed in emodels and will be the basis for the first q value because \n",
    "        \n",
    "        the pragmatic value deals with exploitation and selection based on realizing the objective set by prior\n",
    "        \n",
    "        note: observation noise is simply a counterpart of prior precision that allows us to adjust how much weight\n",
    "        to put on observations as updates to the prior. in short we can make updates small by making the weight of \n",
    "        prior precision extremely large or making observation noise large. conversely \n",
    "        we can make the updates large by making prior precision extremely small or by making the obs noise small \n",
    "        \n",
    "        we should note that prior precision acts as a gain control devide for amplifying signals. This may also be related\n",
    "        to attention which is related to consciousness ans learning. \n",
    "        \n",
    "        optimization of precisions is the statstical-mechanical basis of signal prioritisation in general\n",
    "        \n",
    "        to do this we need to be able to predict precisons so that we can weight the expected precision of error signals\n",
    "        over the precision of outgoing predictions\n",
    "        \n",
    "        optimization beyond action and perception, the adjustment of precision to match the ampltitude of incoming\n",
    "        prediction errors is the basis of consciousness in fristons work \n",
    "        \n",
    "        the next phase of pegasus will be the following: for learning we need to do the inverse of inference \n",
    "        \n",
    "        which involves optimizing beliefs about relationships between variables in the generative model. This may only\n",
    "        occur after states have been inferred \n",
    "        \n",
    "        to proceed with the next phase we will have to dispense with the mixer network \n",
    "        \n",
    "        we should use the memory example rather then the election example as our primary reference mainly due to the fact\n",
    "        that we are not using two latent variables \n",
    "        \n",
    "        we dont need tow wory about sample size or optimizer since here we are optimizing over psuedo observations\n",
    "        \n",
    "        at some point tasha would like to use this project to model the brain of a dog as well as a human\n",
    "        \n",
    "        tasha would like visualization for simulated nueurons \n",
    "        \n",
    "        \n",
    "        our final two options are:\n",
    "        \n",
    "        set explore setting to false. Do expected free energy optimization on emodels and jettison efe\n",
    "        \n",
    "        or\n",
    "        \n",
    "        keep efe. calculate only pragmatic value on emodels and do full expected free energy calculation on EFE\n",
    "        \n",
    "        or \n",
    "        \n",
    "        keep efe. do full calculations of epistemic and pragmatic in emodels.  \n",
    "          \n",
    "        On may 15th 2023 let us try to cut out efe and do the full Expected free energy calculation in emodel \n",
    "        \"\"\"\n",
    "            \n",
    "        guide = ESM()# marginal_guide = posterior predictive entropy d can also be expressed as pi \n",
    "        pyro.clear_param_store()\n",
    "        # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "        # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "        # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)] where w = x  \n",
    "        with pyro.plate_stack(\"plate\", l.shape[:-1]):\n",
    "            theta = pyro.sample(\"theta\", dist.Dirichlet(1))#Normal(0, 0.8))# in our case this will represent the reward\n",
    "            # Share theta across the number of rounds of the experiment\n",
    "            # This represents repeatedly testing the same participant\n",
    "            theta = theta.unsqueeze(-1)\n",
    "            # This define a *logistic regression* model for y\n",
    "            #logit_p = sensitivity * (theta - l)\n",
    "            # The event shape represents responses from the same participant\n",
    "            y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n",
    "            return y\n",
    "            #ape = average posterior entropy\n",
    "        #we dont need tow wory about sample size or optimizer since here we are optimizing over psuedo observations\n",
    "        ape = posterior_eig(model, allocation, \"y\", \"theta\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "        \n",
    "        eigs[strategy] = prior_entropy - ape\n",
    "        print(eigs[strategy].item())\n",
    "\n",
    "        if eigs[strategy] > best_eig:\n",
    "            best_strategy, best_eig = strategy, eigs[strategy]\n",
    "\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        def utterance_prior():\n",
    "            ix = pyro.sample(\"utt\", dist.Categorical(probs=torch.ones(3) / 3))\n",
    "            return [\"none\",\"some\",\"all\"][ix]\n",
    "        @Marginal\n",
    "        def speaker():\n",
    "            alpha = 1.\n",
    "            with poutine.scale(scale=torch.tensor(alpha)):\n",
    "                state = utterance_prior()\n",
    "                pyro.sample(\"POMDP\",state , obs=input_dict[\"obs_flat\"])\n",
    "            return utterance\n",
    "        guide = ESM()\n",
    "\n",
    "        # Step 3: learn the posterior using all data seen so far\n",
    "        \n",
    "\n",
    "        conditioned_model = pyro.condition(model, {\"y\": input_dict[\"obs_flat\"]})\n",
    "        svi = SVI(conditioned_model,\n",
    "                  guide,\n",
    "                  scheduler,#Adam({\"lr\": .005}),\n",
    "                  loss=Trace_ELBO(),\n",
    "                  num_samples=1)\n",
    "        #num_iters = 2000\n",
    "        #for i in range(num_iters):\n",
    "        elbo = svi.step(ls)\n",
    "        \n",
    "        history.append((pyro.param(\"posterior_mean\").detach().clone().numpy(),\n",
    "                        pyro.param(\"posterior_sd\").detach().clone().numpy()))\n",
    "        current_model = make_model(pyro.param(\"posterior_mean\").detach().clone(),\n",
    "                                   pyro.param(\"posterior_sd\").detach().clone())\n",
    "        print(\"Estimate of \\u03b8: {:.3f} \\u00b1 {:.3f}\\n\".format(*history[-1]))     \n",
    "                                \n",
    "                                \n",
    "        utterance = speaker()\n",
    "        q = elbo - utterance \n",
    "        #####################################################################################\n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        \"\"\"\n",
    "        \n",
    "        model = gpflow.models.VGP((x, h_in),kernel=gpflow.kernels.SquaredExponential(),\n",
    "        likelihood=gpflow.likelihoods.Bernoulli())\n",
    "        opt = gpflow.optimizers.Scipy()\n",
    "        opt.minimize(model.training_loss, model.trainable_variables)\n",
    "        model = gpflow.models.SVGP(\n",
    "        kernel=gpflow.kernels.SquaredExponential(),\n",
    "        likelihood=gpflow.likelihoods.Bernoulli(),\n",
    "        inducing_variable=np.linspace(0.0, 1.0, 4)[:, None],)\n",
    "        opt = gpflow.optimizers.Scipy()\n",
    "        opt.minimize(model.training_loss_closure((X, Y)), model.trainable_variables)\n",
    "        gpflow.utilities.print_summary(model, \"notebook\")\n",
    "\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        #q = self.fc2(h)\n",
    "        #model = tfk.Model(inputs=i(input_dict[\"obs_flat\"]),outputs=z(h))\n",
    "        #i = tfkl.InputLayer(input_shape=input_shape)\n",
    "        #x = tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),activation=None)(i)\n",
    "        \"\"\"\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),reinterpreted_batch_ndims=1)\n",
    "        x  = tf.keras.layers.Dense(50,kernel_initializer='ones', use_bias=False,kernel_regularizer=tfpl.KLDivergenceRegularizer(prior))(h)\n",
    "        #x = tfpl.MultivariateNormalTriL(d)\n",
    "        z = tfp.layers.VariationalGaussianProcess(\n",
    "                num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\n",
    "                inducing_index_points_initializer=tf.compat.v1.constant_initializer(\n",
    "                    np.linspace(0,x_range, num=1125,\n",
    "                                dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))), mean_fn=None,\n",
    "                jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)(x)\n",
    "\n",
    "        model = tfk.Model(inputs=x,outputs=q)#decoder(input_dict[\"obs_flat\"][0]))\n",
    "        \n",
    "        yhat = model(x_tst)#Note that this is a distribution not a tensor\n",
    "        num_samples = 13 \n",
    "        for i in range(num_samples):\n",
    "            sample_ = yhat.sample().numpy()\n",
    "            sample_[..., 0].T\n",
    "            e = sample_[..., 0].T\n",
    "            r.append(e)\n",
    "  \n",
    "        rfinal = (r[0]+r[1]+r[2]+r[3]+r[4]+r[5]+r[6]+r[7]+r[8]+r[9]+r[10]+r[11]+r[12])/13\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        #\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))\n",
    "        \n",
    "        h = self.rnn(x,h_in)\n",
    "        q = self.fc2(h)\n",
    "        \"\"\"\n",
    "        #negloglik = lambda x, rv_x: -rv_x.log_prob(h)#tf.keras.losses.KLDivergence()\n",
    "        #model.add_loss(negloglik)\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n",
    "ivy.set_framework('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6293520",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "#@ti.func\n",
    "@accelerate_model()\n",
    "class bmodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        #under advice from Tasha and Dimitrov and given what we know about\n",
    "        #long term dependencce int he brain we are using LSTMs for neuronal agents\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        \n",
    "        \"\"\"\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(priora, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))#nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        #\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))\n",
    "        \n",
    "        h = self.rnn(x,h_in)\n",
    "        q = self.fc2(h)\n",
    "\n",
    "        \"\"\"\n",
    "        vae = tfk.Model(inputs=encoder(input_dict[\"obs_flat\"]),\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        \"\"\"\n",
    "\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf67fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "y = np.ones(30)\n",
    "y = np.expand_dims(y,1)\n",
    "x = tf.keras.layers.Dense(1)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91f46fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\"\"\"\n",
    "next objective for april 8th 2023: we are going to have to modify the mixer network \n",
    "\n",
    "The mixer network is going to be a bayesian tensorflow network with a -ELBO builtin loss and a kullbacker leibeler loss\n",
    "that will be seen in the QMIX loss function\n",
    "\n",
    "We will have a configurable parameter that depending on whether the policy is high or low will have either -elbo or \n",
    "kullbacker leibler as builtin losses with a generic bayesian neural network . \n",
    "\n",
    "finally we will have emodel which will have a neural network guide\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#@ti.func\n",
    "#@ti.data_oriented\n",
    "class QMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape, mixing_embed_dim):\n",
    "        super(QMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.embed_dim = mixing_embed_dim\n",
    "        self.state_dim = int(np.prod(state_shape))\n",
    "\n",
    "        self.hyper_w_1 = nn.Linear(self.state_dim, self.embed_dim * self.n_agents)#tfkl.Embedding(self.state_dim,  self.embed_dim * self.n_agents)#\n",
    "        self.hyper_w_final = nn.Linear(self.state_dim, self.embed_dim)#tfkl.Embedding(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # State dependent bias for hidden layer\n",
    "        self.hyper_b_1 = nn.Linear(self.state_dim, self.embed_dim)#tfkl.Embedding(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # V(s) instead of a bias for the last layers\n",
    "        self.V = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.embed_dim),#tfkl.Embedding(self.state_dim, self.embed_dim)\n",
    "            nn.ReLU(),\n",
    "            # tf.nn.relu\n",
    "            nn.Linear(self.embed_dim, 1),#tfkl.Embedding(self.embed_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, agent_qs, states):\n",
    "        \"\"\"Forward pass for the mixer.\n",
    "        Args:\n",
    "            agent_qs: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            states: Tensor of shape [B, T, state_dim]\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        nn.Sequential(\n",
    "            torch.abs(self.hyper_w_1(states))\n",
    "            self.hyper_b_1(states)\n",
    "            nn.functional.elu(torch.bmm(agent_qs, w1) + b1)\n",
    "            torch.abs(self.hyper_w_final(states))\n",
    "            self.V\n",
    "            \n",
    "        )\n",
    "        tfkl.InputLayer(self.state_dim,self.embed_dim*self.n_agents)\n",
    "        \n",
    "        self.hyper_w_1 = tf.keras.activations.linear(self.state_dim, self.embed_dim * self.n_agents)\n",
    "        \n",
    "        tfkl.Embedding(1000, 64)#, input_length=10)\n",
    "        \n",
    "        x.numpy()\n",
    "        \n",
    "        #batch matrix multiplication\n",
    "        tf.matmul()\n",
    "        \n",
    "        model = keras.Model(inputs=[states, agent_qs, tags_input], outputs=output\n",
    "        \n",
    "        \n",
    "        #################################################################################################\n",
    "        w1 = tf.math.abs(tfkl.Embedding(self.state_dim,  self.embed_dim * self.n_agents)(states))\n",
    "        b1 = tfkl.Embedding(self.state_dim, self.embed_dim)(states)\n",
    "        w1 = tf.reshape(w1, [-1, self.n_agents, self.embed_dim])\n",
    "        b1 = tf.reshape(b1,[-1, 1, self.embed_dim])\n",
    "        hidden = tf.nn.elu(tf.matmul(agent_qs, w1)+ b1)\n",
    "        w_final = tf.math.abs(tfkl.Embedding(self.state_dim, self.embed_dim)(states))\n",
    "        w_final = tf.reshape(-1, self.embed_dim, 1)\n",
    "        v =  tfkl.Embedding(self.V(states),-1, 1, 1)\n",
    "        y = tf.matmul(hidden, w_final) + v\n",
    "        q_tot = tf.reshape(y,[bs, -1, 1])\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        bs = agent_qs.size(0)\n",
    "        states = states.reshape(-1, self.state_dim)#tf.keras.layers.Reshape((3, 4), input_shape=(12,))\n",
    "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
    "        # First layer\n",
    "        \n",
    "        w1 = torch.abs(self.hyper_w_1(states))\n",
    "        \n",
    "        b1 = self.hyper_b_1(states)\n",
    "        w1 = w1.view(-1, self.n_agents, self.embed_dim)\n",
    "        b1 = b1.view(-1, 1, self.embed_dim)\n",
    "        hidden = nn.functional.elu(torch.bmm(agent_qs, w1) + b1)#tf.nn.elu(tf.matmul(agent_qs, w1)+ b1)\n",
    "        # Second layer #tf.nn.elu()\n",
    "        w_final = torch.abs(self.hyper_w_final(states))\n",
    "        w_final = w_final.view(-1, self.embed_dim, 1)\n",
    "        # State-dependent bias\n",
    "        v = self.V(states).view(-1, 1, 1)\n",
    "        # Compute final output\n",
    "        y = torch.bmm(hidden, w_final) + v#tf.matmul(hidden, w_final)\n",
    "        \n",
    "        # Reshape and return\n",
    "        q_tot = y.view(bs, -1, 1)\n",
    "        return q_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b17e52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ti.data_oriented\n",
    "class QMixLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        target_model,\n",
    "        mixer,\n",
    "        target_mixer,\n",
    "        n_agents,\n",
    "        n_actions,\n",
    "        double_q=True,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.mixer = mixer\n",
    "        self.target_mixer = target_mixer\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        self.double_q = double_q\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rewards,\n",
    "        actions,\n",
    "        terminated,\n",
    "        mask,\n",
    "        obs,\n",
    "        next_obs,\n",
    "        action_mask,\n",
    "        next_action_mask,\n",
    "        state=None,\n",
    "        next_state=None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the loss.\n",
    "        Args:\n",
    "            rewards: Tensor of shape [B, T, n_agents]\n",
    "            actions: Tensor of shape [B, T, n_agents]\n",
    "            terminated: Tensor of shape [B, T, n_agents]\n",
    "            mask: Tensor of shape [B, T, n_agents]\n",
    "            obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            next_obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            next_action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            state: Tensor of shape [B, T, state_dim] (optional)\n",
    "            next_state: Tensor of shape [B, T, state_dim] (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert either none or both of state and next_state are given\n",
    "        if state is None and next_state is None:\n",
    "            state = obs  # default to state being all agents' observations\n",
    "            next_state = next_obs\n",
    "        elif (state is None) != (next_state is None):\n",
    "            raise ValueError(\n",
    "                \"Expected either neither or both of `state` and \"\n",
    "                \"`next_state` to be given. Got: \"\n",
    "                \"\\n`state` = {}\\n`next_state` = {}\".format(state, next_state)\n",
    "            )\n",
    "\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = _unroll_mac(self.model, obs)\n",
    "\n",
    "        # Pick the Q-Values for the actions taken -> [B * n_agents, T]\n",
    "        chosen_action_qvals = torch.gather(\n",
    "            mac_out, dim=3, index=actions.unsqueeze(3)\n",
    "        ).squeeze(3)\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = _unroll_mac(self.target_model, next_obs)\n",
    "\n",
    "        # Mask out unavailable actions for the t+1 step\n",
    "        ignore_action_tp1 = (next_action_mask == 0) & (mask == 1).unsqueeze(-1)\n",
    "        target_mac_out[ignore_action_tp1] = -np.inf\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.double_q:\n",
    "            # Double Q learning computes the target Q values by selecting the\n",
    "            # t+1 timestep action according to the \"policy\" neural network and\n",
    "            # then estimating the Q-value of that action with the \"target\"\n",
    "            # neural network\n",
    "            \n",
    "            #target neural network does expected free energy while policy\n",
    "            #neural network will be variational free energy\n",
    "\n",
    "            # Compute the t+1 Q-values to be used in action selection\n",
    "            # using next_obs\n",
    "            mac_out_tp1 = _unroll_mac(self.model, next_obs)\n",
    "\n",
    "            # mask out unallowed actions\n",
    "            mac_out_tp1[ignore_action_tp1] = -np.inf\n",
    "\n",
    "            # obtain best actions at t+1 according to policy NN\n",
    "            cur_max_actions = mac_out_tp1.argmax(dim=3, keepdim=True)\n",
    "\n",
    "            # use the target network to estimate the Q-values of policy\n",
    "            # network's selected actions\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(\n",
    "                3\n",
    "            )\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "        assert (\n",
    "            target_max_qvals.min().item() != -np.inf\n",
    "        ), \"target_max_qvals contains a masked action; \\\n",
    "            there may be a state with no valid actions.\"\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state)\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, next_state)\n",
    "\n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n",
    "        \"\"\"\n",
    "        \n",
    "        guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        elbo = elbo_(model, guide)\n",
    "        \n",
    "        el = elbo(data)\n",
    "        \n",
    "        #rewards = {self.agent_1:kullbacker, self.agent_2: el}\n",
    "        #loss = lambda y, rv_y: rv_y.variational_loss(y, kl_weight=np.array(batch_size, x.dtype) / x.shape[0])\n",
    "                \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Td-error\n",
    "        #we need to replace this with a variational free energy error\n",
    "        td_error = chosen_action_qvals - targets.detach()\n",
    "        te_error= tf.keras.losses.KLDivergence(chosen_action_qvals - targets.detach()).numpy()\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)#this is ELBO \n",
    "        \n",
    "        #we are going to ahve a kldivergence loss and a -ELBO regularizer for the mixer network \n",
    "        \n",
    "        mask = mask.expand_as(te_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = te_error * mask\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        \n",
    "        #guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        #elbo_ = pyro.infer.Trace_ELBO(num_particles=1)\n",
    "\n",
    "        # Fix the model/guide pair\n",
    "        #elbo = elbo_(model, guide)        \n",
    "        \n",
    "        #data = obs + preds \n",
    "        #loss = -ln()\n",
    "        #loss = lambda y, rv_y: -rv_y.log_prob(y)\n",
    "        loss = masked_error#(masked_td_error**2).sum() / mask.sum()\n",
    "        return loss, mask, masked_td_error, chosen_action_qvals, targets\n",
    "\n",
    "    \n",
    "#this part just above is what we need to revise\n",
    "    \n",
    "#@ti.func\n",
    "#@ti.data_oriented\n",
    "class QMixTorchPolicy(TorchPolicy):\n",
    "    \"\"\"QMix impl. Assumes homogeneous agents for now.\n",
    "    You must use MultiAgentEnv.with_agent_groups() to group agents\n",
    "    together for QMix. This creates the proper Tuple obs/action spaces and\n",
    "    populates the '_group_rewards' info field.\n",
    "    Action masking: to specify an action mask for individual agents, use a\n",
    "    dict space with an action_mask key, e.g. {\"obs\": ob, \"action_mask\": mask}.\n",
    "    The mask space must be `Box(0, 1, (n_actions,))`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        # We want to error out on instantiation and not on import, because tune\n",
    "        # imports all RLlib algorithms when registering them\n",
    "        # TODO (Artur): Find a way to only import algorithms when needed\n",
    "        if not torch:\n",
    "            raise ImportError(\"Could not import PyTorch, which QMix requires.\")\n",
    "\n",
    "        _validate(obs_space, action_space)\n",
    "        config = dict(ray.rllib.algorithms.qmix.qmix.DEFAULT_CONFIG, **config)\n",
    "        self.framework = \"torch\"\n",
    "\n",
    "        self.n_agents = 3000#len(obs_space.original_space.spaces)\n",
    "        config[\"model\"][\"n_agents\"] = self.n_agents\n",
    "        self.n_actions = action_space.spaces[0].n\n",
    "        self.h_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "        self.has_env_global_state = False\n",
    "        self.has_action_mask = False\n",
    "\n",
    "        agent_obs_space = obs_space.original_space.spaces[0]\n",
    "        if isinstance(agent_obs_space, gym.spaces.Dict):\n",
    "            space_keys = set(agent_obs_space.spaces.keys())\n",
    "            if \"obs\" not in space_keys:\n",
    "                raise ValueError(\"Dict obs space must have subspace labeled `obs`\")\n",
    "            self.obs_size = _get_size(agent_obs_space.spaces[\"obs\"])\n",
    "            if \"action_mask\" in space_keys:\n",
    "                mask_shape = tuple(agent_obs_space.spaces[\"action_mask\"].shape)\n",
    "                if mask_shape != (self.n_actions,):\n",
    "                    raise ValueError(\n",
    "                        \"Action mask shape must be {}, got {}\".format(\n",
    "                            (self.n_actions,), mask_shape\n",
    "                        )\n",
    "                    )\n",
    "                self.has_action_mask = True\n",
    "            if ENV_STATE in space_keys:\n",
    "                self.env_global_state_shape = _get_size(\n",
    "                    agent_obs_space.spaces[ENV_STATE]\n",
    "                )\n",
    "                self.has_env_global_state = True\n",
    "            else:\n",
    "                self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            # The real agent obs space is nested inside the dict\n",
    "            config[\"model\"][\"full_obs_space\"] = agent_obs_space\n",
    "            agent_obs_space = agent_obs_space.spaces[\"obs\"]\n",
    "        else:\n",
    "            self.obs_size = _get_size(agent_obs_space)\n",
    "            self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "        #model = bmodel()#CModel()#CModel()\n",
    "        #bmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#\n",
    "        ivy.set_framework('torch')\n",
    "        #model = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')\n",
    "        bmodel = bmodel()\n",
    "        emodel = emodel()\n",
    "        bmodel = FullLaplace(bmodel,'regression',prior_precision=0.00000000000000000000001)#0.00000000000000000000001)#10000000000000000000000000)\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"model\",\n",
    "            default_model=emodel#a#RNNModel#bmodel()#RNNModel,\n",
    "        )\n",
    "\n",
    "        super().__init__(obs_space, action_space, config, model=self.model)\n",
    "\n",
    "        self.target_model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"target_model\",\n",
    "            default_model=bmodel#bmodel()#RNNModel\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.exploration = self._create_exploration()\n",
    "        \n",
    "        # Setup the mixer network.\n",
    "        if config[\"mixer\"] is None:\n",
    "            self.mixer = None\n",
    "            self.target_mixer = None\n",
    "        elif config[\"mixer\"] == \"qmix\":\n",
    "            self.mixer = FullLaplace(QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ),prior_precision=1).to(self.device)\n",
    "            self.target_mixer = FullLaplace(QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ),prior_precision=1).to(self.device)\n",
    "\n",
    "        self.cur_epsilon = 1.0\n",
    "        self.update_target()  # initial sync\n",
    "\n",
    "        # Setup optimizer\n",
    "        self.params = list(self.model.parameters())\n",
    "        if self.mixer:\n",
    "            self.params += list(self.mixer.parameters())\n",
    "        self.loss = QMixLoss(\n",
    "            self.model,\n",
    "            self.target_model,\n",
    "            self.mixer,\n",
    "            self.target_mixer,\n",
    "            self.n_agents,\n",
    "            self.n_actions,\n",
    "            self.config[\"double_q\"],\n",
    "            self.config[\"gamma\"],\n",
    "        )\n",
    "        from torch.optim import RMSprop\n",
    "        \n",
    "        self.rmsprop_optimizer = RMSprop(\n",
    "            params=self.params,\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"optim_alpha\"],\n",
    "            eps=config[\"optim_eps\"],\n",
    "        )#replace with dadptation \n",
    "        self.rs_prop_optimizer = DAdaptAdaGrad(\n",
    "            params=self.params,\n",
    "            alpha=config(\"optim_alpha\"),\n",
    "            eps = config[\"optim_eps\"],\n",
    "        \n",
    "        )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions_from_input_dict(\n",
    "        self,\n",
    "        input_dict: Dict[str, TensorType],\n",
    "        explore: bool = None,\n",
    "        timestep: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:\n",
    "\n",
    "        obs_batch = input_dict[SampleBatch.OBS]\n",
    "        state_batches = []\n",
    "        i = 0\n",
    "        while f\"state_in_{i}\" in input_dict:\n",
    "            state_batches.append(input_dict[f\"state_in_{i}\"])\n",
    "            i += 1\n",
    "\n",
    "        explore = explore if explore is not None else self.config[\"explore\"]\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        # We need to ensure we do not use the env global state\n",
    "        # to compute actions\n",
    "\n",
    "        # Compute actions\n",
    "        with torch.no_grad():\n",
    "            q_values, hiddens = _mac(\n",
    "                self.model,\n",
    "                torch.as_tensor(obs_batch, dtype=torch.float, device=self.device),\n",
    "                [\n",
    "                    torch.as_tensor(np.array(s), dtype=torch.float, device=self.device)\n",
    "                    for s in state_batches\n",
    "                ],\n",
    "            )\n",
    "            avail = torch.as_tensor(action_mask, dtype=torch.float, device=self.device)\n",
    "            masked_q_values = q_values.clone()\n",
    "            masked_q_values[avail == 0.0] = -float(\"inf\")\n",
    "            masked_q_values_folded = torch.reshape(\n",
    "                masked_q_values, [-1] + list(masked_q_values.shape)[2:]\n",
    "            )\n",
    "            actions, _ = self.exploration.get_exploration_action(\n",
    "                action_distribution=TorchCategorical(masked_q_values_folded),\n",
    "                timestep=timestep,\n",
    "                explore=explore,\n",
    "            )\n",
    "            actions = (\n",
    "                torch.reshape(actions, list(masked_q_values.shape)[:-1]).cpu().numpy()\n",
    "            )\n",
    "            hiddens = [s.cpu().numpy() for s in hiddens]\n",
    "\n",
    "        return tuple(actions.transpose([1, 0])), hiddens, {}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions(self, *args, **kwargs):\n",
    "        return self.compute_actions_from_input_dict(*args, **kwargs)\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "    ):\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        return np.zeros(obs_batch.size()[0])\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        obs_batch, action_mask, env_global_state = self._unpack_observation(\n",
    "            samples[SampleBatch.CUR_OBS]\n",
    "        )\n",
    "        (\n",
    "            next_obs_batch,\n",
    "            next_action_mask,\n",
    "            next_env_global_state,\n",
    "        ) = self._unpack_observation(samples[SampleBatch.NEXT_OBS])\n",
    "        group_rewards = self._get_group_rewards(samples[SampleBatch.INFOS])\n",
    "\n",
    "        input_list = [\n",
    "            group_rewards,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            samples[SampleBatch.ACTIONS],\n",
    "            samples[SampleBatch.TERMINATEDS],\n",
    "            obs_batch,\n",
    "            next_obs_batch,\n",
    "        ]\n",
    "        if self.has_env_global_state:\n",
    "            input_list.extend([env_global_state, next_env_global_state])\n",
    "\n",
    "        output_list, _, seq_lens = chop_into_sequences(\n",
    "            episode_ids=samples[SampleBatch.EPS_ID],\n",
    "            unroll_ids=samples[SampleBatch.UNROLL_ID],\n",
    "            agent_indices=samples[SampleBatch.AGENT_INDEX],\n",
    "            feature_columns=input_list,\n",
    "            state_columns=[],  # RNN states not used here\n",
    "            max_seq_len=self.config[\"model\"][\"max_seq_len\"],\n",
    "            dynamic_max=True,\n",
    "        )\n",
    "        # These will be padded to shape [B * T, ...]\n",
    "        if self.has_env_global_state:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "                env_global_state,\n",
    "                next_env_global_state,\n",
    "            ) = output_list\n",
    "        else:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "            ) = output_list\n",
    "        B, T = len(seq_lens), max(seq_lens)\n",
    "\n",
    "        def to_batches(arr, dtype):\n",
    "            new_shape = [B, T] + list(arr.shape[1:])\n",
    "            return torch.as_tensor(\n",
    "                np.reshape(arr, new_shape), dtype=dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        rewards = to_batches(rew, torch.float)\n",
    "        actions = to_batches(act, torch.long)\n",
    "        obs = to_batches(obs, torch.float).reshape([B, T, self.n_agents, self.obs_size])\n",
    "        action_mask = to_batches(action_mask, torch.float)\n",
    "        next_obs = to_batches(next_obs, torch.float).reshape(\n",
    "            [B, T, self.n_agents, self.obs_size]\n",
    "        )\n",
    "        next_action_mask = to_batches(next_action_mask, torch.float)\n",
    "        if self.has_env_global_state:\n",
    "            env_global_state = to_batches(env_global_state, torch.float)\n",
    "            next_env_global_state = to_batches(next_env_global_state, torch.float)\n",
    "\n",
    "        # TODO(ekl) this treats group termination as individual termination\n",
    "        terminated = (\n",
    "            to_batches(terminateds, torch.float)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Create mask for where index is < unpadded sequence length\n",
    "        filled = np.reshape(\n",
    "            np.tile(np.arange(T, dtype=np.float32), B), [B, T]\n",
    "        ) < np.expand_dims(seq_lens, 1)\n",
    "        mask = (\n",
    "            torch.as_tensor(filled, dtype=torch.float, device=self.device)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss_out, mask, masked_td_error, chosen_action_qvals, targets = self.loss(\n",
    "            rewards,\n",
    "            actions,\n",
    "            terminated,\n",
    "            mask,\n",
    "            obs,\n",
    "            next_obs,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            env_global_state,\n",
    "            next_env_global_state,\n",
    "        )\n",
    "\n",
    "        # Optimise\n",
    "        self.rsprop_optimizer.zero_grad()\n",
    "\n",
    "        loss_out.backward()\n",
    "        grad_norm_info = apply_grad_clipping(self, self.rsprop_optimizer, loss_out)\n",
    "        self.rsprop_optimizer.step()\n",
    "\n",
    "        mask_elems = mask.sum().item()\n",
    "        stats = {\n",
    "            \"loss\": loss_out.item(),\n",
    "            \"td_error_abs\": masked_td_error.abs().sum().item() / mask_elems,\n",
    "            \"q_taken_mean\": (chosen_action_qvals * mask).sum().item() / mask_elems,\n",
    "            \"target_mean\": (targets * mask).sum().item() / mask_elems,\n",
    "        }\n",
    "        stats.update(grad_norm_info)\n",
    "\n",
    "        return {LEARNER_STATS_KEY: stats}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_initial_state(self):  # initial RNN state\n",
    "        return [\n",
    "            s.expand([self.n_agents, -1]).cpu().numpy()\n",
    "            for s in self.model.get_initial_state()\n",
    "        ]\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            \"model\": self._cpu_dict(self.model.state_dict()),\n",
    "            \"target_model\": self._cpu_dict(self.target_model.state_dict()),\n",
    "            \"mixer\": self._cpu_dict(self.mixer.state_dict()) if self.mixer else None,\n",
    "            \"target_mixer\": self._cpu_dict(self.target_mixer.state_dict())\n",
    "            if self.mixer\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(self._device_dict(weights[\"model\"]))\n",
    "        self.target_model.load_state_dict(self._device_dict(weights[\"target_model\"]))\n",
    "        if weights[\"mixer\"] is not None:\n",
    "            self.mixer.load_state_dict(self._device_dict(weights[\"mixer\"]))\n",
    "            self.target_mixer.load_state_dict(\n",
    "                self._device_dict(weights[\"target_mixer\"])\n",
    "            )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_state(self):\n",
    "        state = self.get_weights()\n",
    "        state[\"cur_epsilon\"] = self.cur_epsilon\n",
    "        return state\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_state(self, state):\n",
    "        self.set_weights(state)\n",
    "        self.set_epsilon(state[\"cur_epsilon\"])\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        logger.debug(\"Updated target networks\")\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cur_epsilon = epsilon\n",
    "\n",
    "    def _get_group_rewards(self, info_batch):\n",
    "        group_rewards = np.array(\n",
    "            [info.get(GROUP_REWARDS, [0.0] * self.n_agents) for info in info_batch]\n",
    "        )\n",
    "        return group_rewards\n",
    "\n",
    "    def _device_dict(self, state_dict):\n",
    "        return {\n",
    "            k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _cpu_dict(state_dict):\n",
    "        return {k: v.cpu().detach().numpy() for k, v in state_dict.items()}\n",
    "\n",
    "    def _unpack_observation(self, obs_batch):\n",
    "        \"\"\"Unpacks the observation, action mask, and state (if present)\n",
    "        from agent grouping.\n",
    "        Returns:\n",
    "            obs (np.ndarray): obs tensor of shape [B, n_agents, obs_size]\n",
    "            mask (np.ndarray): action mask, if any\n",
    "            state (np.ndarray or None): state tensor of shape [B, state_size]\n",
    "                or None if it is not in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        unpacked = _unpack_obs(\n",
    "            np.array(obs_batch, dtype=np.float32),\n",
    "            self.observation_space.original_space,\n",
    "            tensorlib=np,\n",
    "        )\n",
    "\n",
    "        if isinstance(unpacked[0], dict):\n",
    "            assert \"obs\" in unpacked[0]\n",
    "            unpacked_obs = [np.concatenate(tree.flatten(u[\"obs\"]), 1) for u in unpacked]\n",
    "        else:\n",
    "            unpacked_obs = unpacked\n",
    "\n",
    "        obs = np.concatenate(unpacked_obs, axis=1).reshape(\n",
    "            [len(obs_batch), self.n_agents, self.obs_size]\n",
    "        )\n",
    "\n",
    "        if self.has_action_mask:\n",
    "            action_mask = np.concatenate(\n",
    "                [o[\"action_mask\"] for o in unpacked], axis=1\n",
    "            ).reshape([len(obs_batch), self.n_agents, self.n_actions])\n",
    "        else:\n",
    "            action_mask = np.ones(\n",
    "                [len(obs_batch), self.n_agents, self.n_actions], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.has_env_global_state:\n",
    "            state = np.concatenate(tree.flatten(unpacked[0][ENV_STATE]), 1)\n",
    "        else:\n",
    "            state = None\n",
    "        return obs, action_mask, state\n",
    "\n",
    "#@ti.func\n",
    "def _validate(obs_space, action_space):\n",
    "    if not hasattr(obs_space, \"original_space\") or not isinstance(\n",
    "        obs_space.original_space, gym.spaces.Tuple\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Obs space must be a Tuple, got {}. Use \".format(obs_space)\n",
    "            + \"MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space, gym.spaces.Tuple):\n",
    "        raise ValueError(\n",
    "            \"Action space must be a Tuple, got {}. \".format(action_space)\n",
    "            + \"Use MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space.spaces[0], gym.spaces.Discrete):\n",
    "        raise ValueError(\n",
    "            \"QMix requires a discrete action space, got {}\".format(\n",
    "                action_space.spaces[0]\n",
    "            )\n",
    "        )\n",
    "    if len({str(x) for x in obs_space.original_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: observations of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(obs_space.original_space.spaces)\n",
    "        )\n",
    "    if len({str(x) for x in action_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: action space of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(action_space.spaces)\n",
    "        )\n",
    "\n",
    "#@ti.func\n",
    "def _mac(model, obs, h):\n",
    "    \"\"\"Forward pass of the multi-agent controller.\n",
    "    Args:\n",
    "        model: TorchModelV2 class\n",
    "        obs: Tensor of shape [B, n_agents, obs_size]\n",
    "        h: List of tensors of shape [B, n_agents, h_size]\n",
    "    Returns:\n",
    "        q_vals: Tensor of shape [B, n_agents, n_actions]\n",
    "        h: Tensor of shape [B, n_agents, h_size]\n",
    "    \"\"\"\n",
    "    B, n_agents = obs.size(0), obs.size(1)\n",
    "    if not isinstance(obs, dict):\n",
    "        obs = {\"obs\": obs}\n",
    "    obs_agents_as_batches = {k: _drop_agent_dim(v) for k, v in obs.items()}\n",
    "    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n",
    "    q_flat, h_flat = model(obs_agents_as_batches, h_flat, None)\n",
    "    return q_flat.reshape([B, n_agents, -1]), [\n",
    "        s.reshape([B, n_agents, -1]) for s in h_flat\n",
    "    ]\n",
    "#@ti.func\n",
    "def _unroll_mac(model, obs_tensor):\n",
    "    \"\"\"Computes the estimated Q values for an entire trajectory batch\"\"\"\n",
    "    B = obs_tensor.size(0)\n",
    "    T = obs_tensor.size(1)\n",
    "    n_agents = obs_tensor.size(2)\n",
    "\n",
    "    mac_out = []\n",
    "    h = [s.expand([B, n_agents, -1]) for s in model.get_initial_state()]\n",
    "    for t in range(T):\n",
    "        q, h = _mac(model, obs_tensor[:, t], h)\n",
    "        mac_out.append(q)\n",
    "    mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "    return mac_out\n",
    "#@ti.func\n",
    "def _drop_agent_dim(T):\n",
    "    shape = list(T.shape)\n",
    "    B, n_agents = shape[0], shape[1]\n",
    "    return T.reshape([B * n_agents] + shape[2:])\n",
    "#@ti.func\n",
    "def _add_agent_dim(T, n_agents):\n",
    "    shape = list(T.shape)\n",
    "    B = shape[0] // n_agents\n",
    "    assert shape[0] % n_agents == 0\n",
    "    return T.reshape([B, n_agents] + shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bf7dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ti.data_oriented\n",
    "class TwoStepGame(MultiAgentEnv):\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(2)\n",
    "        self.state = None\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        #self.agent_3=2\n",
    "        self._skip_env_checking = True\n",
    "        gpus_available = ray.get_gpu_ids()\n",
    "        # MADDPG emits action logits instead of actual discrete actions\n",
    "        self.actions_are_logits = env_config.get(\"actions_are_logits\", False)\n",
    "        self.one_hot_state_encoding = env_config.get(\"one_hot_state_encoding\", False)\n",
    "        self.with_state = env_config.get(\"separate_state_space\", False)\n",
    "        self._agent_ids = {0, 1}\n",
    "        if not self.one_hot_state_encoding:\n",
    "            self.observation_space = Tuple([Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )])\n",
    "            #self.observation_space = self.observation_space)\n",
    "            self.with_state = False\n",
    "        else:\n",
    "            # Each agent gets the full state (one-hot encoding of which of the\n",
    "            # three states are active) as input with the receiving agent's\n",
    "            # ID (1 or 2) concatenated onto the end.\n",
    "            if self.with_state:\n",
    "                self.observation_space = Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.observation_space = MultiDiscrete([2, 2, 2, 3])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = np.array([1, 0, 0])\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "\n",
    "        state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "        \n",
    "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
    "        obs = self._obs()\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        truncateds = {\"__all__\": False}\n",
    "        infos = {\n",
    "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
    "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "            #self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "        }\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "\n",
    "    def _obs(self):\n",
    "        if self.with_state:\n",
    "            return {\n",
    "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
    "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
    "                #self.agent_3: {\"obs\": self.agent_3_obs(), ENV_STATE: self.state},\n",
    "            }\n",
    "        else:\n",
    "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
    "\n",
    "    def agent_1_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [1]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0]\n",
    "\n",
    "    def agent_2_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [2]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0] + 3\n",
    "\n",
    "\n",
    "        #if self.render_mode == \"rgb_array\":\n",
    "            #return self._render_frame()\n",
    "\n",
    "class TwoStepGameWithGroupedAgents(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        env = TwoStepGame(env_config)\n",
    "        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\n",
    "        tuple_act_space = Tuple([env.action_space, env.action_space])\n",
    "\n",
    "        self.env = env.with_agent_groups(\n",
    "            groups={\"agents\": [0, 1]},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self._agent_ids = {\"agents\"}\n",
    "        self._skip_env_checking = True\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        return self.env.reset(seed=seed, options=options)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1558a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.0 (SDL 2.28.0, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete, Tuple\n",
    "import logging\n",
    "import random\n",
    "\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "note: QMIX is ONLY Discrete. \n",
    "\n",
    "\"\"\"\n",
    "class Pegasus(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(2)\n",
    "        self.state = None\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        self._skip_env_checking = True\n",
    "        gpus_available = ray.get_gpu_ids()\n",
    "        # MADDPG emits action logits instead of actual discrete actions\n",
    "        self.actions_are_logits = env_config.get(\"actions_are_logits\", False)\n",
    "        self.one_hot_state_encoding = env_config.get(\"one_hot_state_encoding\", False)\n",
    "        self.with_state = env_config.get(\"separate_state_space\", False)\n",
    "        self._agent_ids = {0, 1}\n",
    "        if not self.one_hot_state_encoding:\n",
    "            self.observation_space = Tuple([Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )])\n",
    "            #self.observation_space = self.observation_space)\n",
    "            self.with_state = False\n",
    "        else:\n",
    "            # Each agent gets the full state (one-hot encoding of which of the\n",
    "            # three states are active) as input with the receiving agent's\n",
    "            # ID (1 or 2) concatenated onto the end.\n",
    "            if self.with_state:\n",
    "                self.observation_space = Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.observation_space = MultiDiscrete([2, 2, 2, 3])\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = np.array([1, 0, 0])\n",
    "        return self._obs(), {}\n",
    "    def step(self, action_dict):\n",
    "\n",
    "        state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "        \n",
    "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
    "        obs = self._obs()\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        truncateds = {\"__all__\": False}\n",
    "        infos = {\n",
    "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
    "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "            #self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "        }\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "    def _obs(self):\n",
    "        if self.with_state:\n",
    "            return {\n",
    "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
    "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
    "                #self.agent_3: {\"obs\": self.agent_3_obs(), ENV_STATE: self.state},\n",
    "            }\n",
    "        else:\n",
    "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
    "\n",
    "    def agent_1_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [1]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0]\n",
    "\n",
    "    def agent_2_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [2]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0] + 3\n",
    "    \n",
    "class HierarchicalGroupedPegasus(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.flat_env = Pegasus(env_config)\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = np.array([1, 0, 0])\n",
    "        return self._obs(), {}\n",
    "    def _high_level_step(self, action):\n",
    "        self.current_goal = action\n",
    "        obs = {self.low_level_agent_id: [self.cur_obs, self.current_goal]}\n",
    "        rew = {self.low_level_agent_id: 0}\n",
    "        done = truncated = {\"__all__\": False}\n",
    "        return obs, rew, done, truncated, {}\n",
    "    def _low_level_step(self, action):\n",
    "        logger.debug(\"Low level agent step {}\".format(action))\n",
    "        self.steps_remaining_at_level -= 1\n",
    "        cur_pos = tuple(self.cur_obs[0])\n",
    "        goal_pos = self.flat_env._get_new_pos(cur_pos, self.current_goal)\n",
    "\n",
    "        # Step in the actual env\n",
    "        f_obs, f_rew, f_terminated, f_truncated, info = self.flat_env.step(action)\n",
    "        new_pos = tuple(f_obs[0])\n",
    "        self.cur_obs = f_obs\n",
    "        # Calculate low-level agent observation and reward\n",
    "        obs = {self.low_level_agent_id: [f_obs, self.current_goal]}\n",
    "        if new_pos != cur_pos:\n",
    "            if new_pos == goal_pos:\n",
    "                rew = {self.low_level_agent_id: 1}\n",
    "            else:\n",
    "                rew = {self.low_level_agent_id: -1}\n",
    "        else:\n",
    "            rew = {self.low_level_agent_id: 0}\n",
    "\n",
    "        # Handle env termination & transitions back to higher level.\n",
    "        terminated = {\"__all__\": False}\n",
    "        truncated = {\"__all__\": False}\n",
    "        if f_terminated or f_truncated:\n",
    "            terminated[\"__all__\"] = f_terminated\n",
    "            truncated[\"__all__\"] = f_truncated\n",
    "            logger.debug(\"high level final reward {}\".format(f_rew))\n",
    "            rew[\"high_level_agent\"] = f_rew\n",
    "            obs[\"high_level_agent\"] = f_obs\n",
    "        elif self.steps_remaining_at_level == 0:\n",
    "            terminated[self.low_level_agent_id] = True\n",
    "            truncated[self.low_level_agent_id] = False\n",
    "            rew[\"high_level_agent\"] = 0\n",
    "            obs[\"high_level_agent\"] = f_obs\n",
    "\n",
    "        return obs, rew, terminated, truncated, {self.low_level_agent_id: info}\n",
    "\n",
    "# Agent has to traverse the maze from the starting position S -> F\n",
    "# Observation space [x_pos, y_pos, wind_direction]\n",
    "# Action space: stay still OR move in current wind direction\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddd80bc2",
   "metadata": {},
   "source": [
    "init stage\n",
    "\n",
    "define agents: done\n",
    "\n",
    "define observation space and mapping: done\n",
    "\n",
    "define action space: done\n",
    "\n",
    "_get obs stage\n",
    "\n",
    "convert states to observations: consult with tasha \n",
    "\n",
    "step stage(action)\n",
    "convert action to direction : not done\n",
    "condition for termination: not done\n",
    "inverse pendulum will fall over \n",
    "\n",
    "render stage\n",
    "not done\n",
    "\n",
    "we are going to convert action to direction by saying\n",
    "\n",
    "self._action_to_direction = if action>0: direction= pos+action\n",
    "\n",
    "if action< 0: direction=pos-action \n",
    "\n",
    "termination will occur when timestep ==10000 or under a fail \n",
    "condition\n",
    "\n",
    "render():\n",
    "    create paddle\n",
    "    create pendulum\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e37b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ti.data_oriented\n",
    "class PEnv(MultiAgentEnv):\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        #maps env states to observations. we may want to have observations that are correlated to but not the same as env st\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=np.float32),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=np.float32),\n",
    "            }\n",
    "        )\n",
    "        self.observation_space = Box(-inf, inf, (1,), np.float32)\n",
    "        self.action_space = Box(-3.0, 3.0, (1,), np.float32)\n",
    "        \n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "        \n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    #this converts enviroments state to observation \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    #We can also implement a similar method for the auxiliary information that is returned by step and reset\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tomorrow our entire job will be getting the step function running \n",
    "    \n",
    "    \"\"\"\n",
    "    def step(self, action): \n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0.0\n",
    "        \n",
    "        \n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        \"\"\"\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        # An episode is done if the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        terminated = 0\n",
    "        reward = 1 if terminated else 0  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "    \"\"\"\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "        ):\n",
    "        super().reset(seed=seed)\n",
    "        \"\"\"\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        self.low, self.high = utils.maybe_parse_reset_bounds(\n",
    "            options, -0.05, 0.05  # default low\n",
    "        )  # default high\n",
    "        self.state = self.np_random.uniform(\n",
    "            low=self.low, high=self.high, size=(4, self.num_envs)\n",
    "        ).astype(np.float32)\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return self.state.T, {}\n",
    "        \"\"\"\n",
    "    def render():\n",
    "\n",
    "        print(\"taichi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a006bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, make_multi_agent\n",
    "MultiAgentCartPole = make_multi_agent(\"PEnv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6ce6e57",
   "metadata": {},
   "source": [
    "for internal visualization we are going to use pathpy4 together with nichord or nipy ecosystem\n",
    "\n",
    "for external visualization it is going to be taichi\n",
    "\n",
    "for causal inference and analysis it is going to be nemtropy\n",
    "amd other librareis that allow for two one sided t tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d42933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "ti.init(arch=ti.gpu)\n",
    "n = 128\n",
    "val = ti.field(ti.i32, shape=n)\n",
    "@ti.kernel\n",
    "def fill():\n",
    "    #ti.loop_config(parallelize=8, block_dim=16)\n",
    "    # If the kernel is run on the CPU backend, 8 threads will be used to run it\n",
    "    # If the kernel is run on the CUDA backend, each block will have 16 threads.\n",
    "    ti.loop_config(parallelize=8,block_dim=128)\n",
    "    for i in range(n):\n",
    "        val[i] = i\n",
    "fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fcf435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "#@ti.data_oriented\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a2495e2",
   "metadata": {},
   "source": [
    "1.finish modifying mixer network - status: basically complete\n",
    "2. rengineer the env -status: ongoing\n",
    "3. visualization and analysis - status: ongoing\n",
    "4. expected free energy module Status: basically complete\n",
    "\n",
    "note: there are actually two mixers: a regular mixer network and a target mixer network. self.target_mixer and self.mixer\n",
    "\n",
    "we should probably use umato to preprocess the inputs going into the neural network for the pragmatic value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec7ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class EFE(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \n",
    "    In our case we will be this is where we will create the pragmatic value and together with the epistemic value\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a EFE Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        #pragmatic value function \n",
    "\n",
    "        @Marginal\n",
    "        def speaker(state):\n",
    "            alpha = 1.\n",
    "            with poutine.scale(scale=torch.tensor(alpha)):\n",
    "                utterance = utterance_prior()\n",
    "                pyro.sample(\"listener\", literal_listener(utterance), obs=state)\n",
    "            return utterance\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        # Step 3: learn the posterior using all data seen so far\n",
    "        guide = ESM()\n",
    "        model = self.model\n",
    "        conditioned_model = pyro.condition(model, {\"y\": action_distribution.inputs})\n",
    "        svi = SVI(conditioned_model,\n",
    "                  guide,\n",
    "                  Adam({\"lr\": .005}),\n",
    "                  loss=Trace_ELBO(),\n",
    "                  num_samples=100)\n",
    "        #num_iters = 2000\n",
    "        #for i in range(num_iters):\n",
    "        elbo = svi.step(ls)\n",
    "        \n",
    "        history.append((pyro.param(\"posterior_mean\").detach().clone().numpy(),\n",
    "                        pyro.param(\"posterior_sd\").detach().clone().numpy()))\n",
    "        current_model = make_model(pyro.param(\"posterior_mean\").detach().clone(),\n",
    "                                   pyro.param(\"posterior_sd\").detach().clone())\n",
    "        print(\"Estimate of \\u03b8: {:.3f} \\u00b1 {:.3f}\\n\".format(*history[-1]))\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        for pragmatic value we can use soem of the math for Rational Speech Act framework\n",
    "        \n",
    "        note: \"allocation\" is used together with alpha to create a distribution which is the variable y. \n",
    "        \n",
    "        eig = marginal_eig(model,\n",
    "                           candidate_designs,       # design, or in this case, tensor of possible designs\n",
    "                           \"y\",                     # site label of observations, could be a list\n",
    "                           \"theta\",                 # site label of 'targets' (latent variables), could also be list\n",
    "                           num_samples=100,         # number of samples to draw per step in the expectation\n",
    "                           num_steps=num_steps,     # number of gradient steps\n",
    "                           guide=marginal_guide,    # guide q(y)\n",
    "                           optim=optimizer,         # optimizer with learning rate decay\n",
    "                           final_num_samples=10000  # at the last step, we draw more samples\n",
    "                                                    # for a more accurate EIG estimate\n",
    "                          )\n",
    "                          \n",
    "        we should use pragmatic for emodels and complete either full or last half or first half of epistemic for efe \n",
    "        \"\"\"\n",
    "\n",
    "        #self.model  is the first agent model we use to get the first q value used for model selection\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1313612e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b63b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our custom replacement for softq in explore config\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class SoftQ(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a SoftQ Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c7a52a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig, NotProvided\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "\"\"\"\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    "    SAMPLE_TIMER,\n",
    ")\n",
    "\"\"\"\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "#@ti.data_oriented\n",
    "class QMixConfig(SimpleQConfig):\n",
    "    \"\"\"Defines a configuration class from which QMix can be built.\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> config = QMixConfig()  # doctest: +SKIP\n",
    "        >>> config = config.training(gamma=0.9, lr=0.01, kl_coeff=0.3)  # doctest: +SKIP\n",
    "        >>> config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "        >>> config = config.rollouts(num_rollout_workers=4)  # doctest: +SKIP\n",
    "        >>> print(config.to_dict())  # doctest: +SKIP\n",
    "        >>> # Build an Algorithm object from the config and run 1 training iteration.\n",
    "        >>> algo = config.build(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> algo.train()  # doctest: +SKIP\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> from ray import air\n",
    "        >>> from ray import tune\n",
    "        >>> config = QMixConfig()\n",
    "        >>> # Print out some default values.\n",
    "        >>> print(config.optim_alpha)  # doctest: +SKIP\n",
    "        >>> # Update the config object.\n",
    "        >>> config.training(  # doctest: +SKIP\n",
    "        ...     lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97\n",
    "        ... )\n",
    "        >>> # Set the config object's env.\n",
    "        >>> config.environment(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> # Use to_dict() to get the old-style python config dict\n",
    "        >>> # when running with tune.\n",
    "        >>> tune.Tuner(  # doctest: +SKIP\n",
    "        ...     \"QMix\",\n",
    "        ...     run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "        ...     param_space=config.to_dict(),\n",
    "        ... ).fit()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PPOConfig instance.\"\"\"\n",
    "        super().__init__(algo_class=QMix)\n",
    "\n",
    "        # fmt: off\n",
    "        # __sphinx_doc_begin__\n",
    "        # QMix specific settings:\n",
    "        self.mixer = \"qmix\"\n",
    "        self.mixing_embed_dim = 32\n",
    "        self.double_q = True\n",
    "        self.optim_alpha = 0.99\n",
    "        self.optim_eps = 0.00001\n",
    "        self.grad_clip = 10\n",
    "        #self.render_mode = 'rgb_array'\n",
    "        # QMix-torch overrides the TorchPolicy's learn_on_batch w/o specifying a\n",
    "        # alternative `learn_on_loaded_batch` alternative for the GPU.\n",
    "        # TODO: This hack will be resolved once we move all algorithms to the new\n",
    "        #  RLModule/Learner APIs.\n",
    "        self.simple_optimizer = True\n",
    "\n",
    "        # Override some of AlgorithmConfig's default values with QMix-specific values.\n",
    "        # .training()\n",
    "        self.lr = 0.0005\n",
    "        self.train_batch_size = 32\n",
    "        self.target_network_update_freq = 500\n",
    "        self.num_steps_sampled_before_learning_starts = 1000\n",
    "        self.replay_buffer_config = {\n",
    "            \"type\": \"MultiAgentReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "            \"prioritized_replay\": 'MultiAgentPrioritizedReplayBuffer',\n",
    "            # Size of the replay buffer in batches (not timesteps!).\n",
    "            \"capacity\": 1000,\n",
    "            # Choosing `fragments` here makes it so that the buffer stores entire\n",
    "            # batches, instead of sequences, episodes or timesteps.\n",
    "            \"storage_unit\": \"fragments\",\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "            #\"no_local_replay_buffer\": True,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.model = {\n",
    "            \"lstm_cell_size\": 64,\n",
    "            \"max_seq_len\": 999999,\n",
    "        }\n",
    "        \"\"\"\n",
    "        # .framework()\n",
    "        self.framework_str = \"torch\"\n",
    "\n",
    "        # .rollouts()\n",
    "        self.rollout_fragment_length = 4\n",
    "        self.batch_mode = \"complete_episodes\"\n",
    "\n",
    "        # .reporting()\n",
    "        self.min_time_s_per_iteration = 1\n",
    "        self.min_sample_timesteps_per_iteration = 1000\n",
    "\n",
    "        # .exploration()\n",
    "        \"\"\"\n",
    "        self.exploration_config = {\n",
    "           \n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\", #replace this with SoftQ\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.01,\n",
    "            # Timesteps over which to anneal epsilon.\n",
    "            \"epsilon_timesteps\": 40000,\n",
    "            \n",
    "            \"type\": \"EFE\",\n",
    "            \"temperature\": 1.0\n",
    "            # For soft_q, use:\n",
    "            # \"exploration_config\" = {\n",
    "            #   \"type\": \"SoftQ\"\n",
    "            #   \"temperature\": [float, e.g. 1.0]\n",
    "            # }\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        # .evaluation()\n",
    "        # Evaluate with epsilon=0 every `evaluation_interval` training iterations.\n",
    "        # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "        self.evaluation(\n",
    "            evaluation_config=AlgorithmConfig.overrides(explore=False)#False\n",
    "        )\n",
    "        # __sphinx_doc_end__\n",
    "        # fmt: on\n",
    "\n",
    "        self.worker_side_prioritization = DEPRECATED_VALUE\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        mixer: Optional[str] = NotProvided,\n",
    "        mixing_embed_dim: Optional[int] = NotProvided,\n",
    "        double_q: Optional[bool] = NotProvided,\n",
    "        target_network_update_freq: Optional[int] = NotProvided,\n",
    "        replay_buffer_config: Optional[dict] = NotProvided,\n",
    "        optim_alpha: Optional[float] = NotProvided,\n",
    "        optim_eps: Optional[float] = NotProvided,\n",
    "        grad_clip: Optional[float] = NotProvided,\n",
    "        # Deprecated args.\n",
    "        grad_norm_clipping=DEPRECATED_VALUE,\n",
    "        **kwargs,\n",
    "    ) -> \"QMixConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            mixer: Mixing network. Either \"qmix\", \"vdn\", or None.\n",
    "            mixing_embed_dim: Size of the mixing network embedding.\n",
    "            double_q: Whether to use Double_Q learning.\n",
    "            target_network_update_freq: Update the target network every\n",
    "                `target_network_update_freq` sample steps.\n",
    "            replay_buffer_config:\n",
    "            optim_alpha: RMSProp alpha.\n",
    "            optim_eps: RMSProp epsilon.\n",
    "            grad_clip: If not None, clip gradients during optimization at\n",
    "                this value.\n",
    "            grad_norm_clipping: Depcrecated in favor of grad_clip\n",
    "        Returns:\n",
    "            This updated AlgorithmConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if grad_norm_clipping != DEPRECATED_VALUE:\n",
    "            deprecation_warning(\n",
    "                old=\"grad_norm_clipping\",\n",
    "                new=\"grad_clip\",\n",
    "                help=\"Parameter `grad_norm_clipping` has been \"\n",
    "                \"deprecated in favor of grad_clip in QMix. \"\n",
    "                \"This is now the same parameter as in other \"\n",
    "                \"algorithms. `grad_clip` will be overwritten by \"\n",
    "                \"`grad_norm_clipping={}`\".format(grad_norm_clipping),\n",
    "                error=True,\n",
    "            )\n",
    "            grad_clip = grad_norm_clipping\n",
    "\n",
    "        if mixer is not NotProvided:\n",
    "            self.mixer = mixer\n",
    "        if mixing_embed_dim is not NotProvided:\n",
    "            self.mixing_embed_dim = mixing_embed_dim\n",
    "        if double_q is not NotProvided:\n",
    "            self.double_q = double_q\n",
    "        if target_network_update_freq is not NotProvided:\n",
    "            self.target_network_update_freq = target_network_update_freq\n",
    "        if replay_buffer_config is not NotProvided:\n",
    "            self.replay_buffer_config = replay_buffer_config\n",
    "        if optim_alpha is not NotProvided:\n",
    "            self.optim_alpha = optim_alpha\n",
    "        if optim_eps is not NotProvided:\n",
    "            self.optim_eps = optim_eps\n",
    "        if grad_clip is not NotProvided:\n",
    "            self.grad_clip = grad_clip\n",
    "\n",
    "        return self\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def validate(self) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate()\n",
    "\n",
    "        if self.framework_str != \"torch\":\n",
    "            raise ValueError(\n",
    "                \"Only `config.framework('torch')` supported so far for QMix!\"\n",
    "            )\n",
    "\n",
    "class QMix(SimpleQ):\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_config(cls) -> AlgorithmConfig:\n",
    "        return QMixConfig()\n",
    "\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_policy_class(\n",
    "        cls, config: AlgorithmConfig\n",
    "    ) -> Optional[Type[Policy]]:\n",
    "        return QMixTorchPolicy\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def training_step(self) -> ResultDict:\n",
    "        \"\"\"QMIX training iteration function.\n",
    "        - Sample n MultiAgentBatches from n workers synchronously.\n",
    "        - Store new samples in the replay buffer.\n",
    "        - Sample one training MultiAgentBatch from the replay buffer.\n",
    "        - Learn on the training batch.\n",
    "        - Update the target network every `target_network_update_freq` sample steps.\n",
    "        - Return all collected training metrics for the iteration.\n",
    "        Returns:\n",
    "            The results dict from executing the training iteration.\n",
    "        \"\"\"\n",
    "        # Sample n batches from n workers.\n",
    "        with self._timers[SAMPLE_TIMER]:\n",
    "            new_sample_batches = synchronous_parallel_sample(\n",
    "                worker_set=self.workers, concat=False\n",
    "            )\n",
    "\n",
    "        for batch in new_sample_batches:\n",
    "            # Update counters.\n",
    "            self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n",
    "            self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n",
    "            # Store new samples in the replay buffer.\n",
    "            self.local_replay_buffer.add(batch)\n",
    "\n",
    "        # Update target network every `target_network_update_freq` sample steps.\n",
    "        cur_ts = self._counters[\n",
    "            NUM_AGENT_STEPS_SAMPLED\n",
    "            if self.config.count_steps_by == \"agent_steps\"\n",
    "            else NUM_ENV_STEPS_SAMPLED\n",
    "        ]\n",
    "\n",
    "        train_results = {}\n",
    "\n",
    "        if cur_ts > self.config.num_steps_sampled_before_learning_starts:\n",
    "            # Sample n batches from replay buffer until the total number of timesteps\n",
    "            # reaches `train_batch_size`.\n",
    "            train_batch = sample_min_n_steps_from_buffer(\n",
    "                replay_buffer=self.local_replay_buffer,\n",
    "                min_steps=self.config.train_batch_size,\n",
    "                count_by_agent_steps=self.config.count_steps_by == \"agent_steps\",\n",
    "            )\n",
    "\n",
    "            # Learn on the training batch.\n",
    "            # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
    "            # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
    "            if self.config.get(\"simple_optimizer\") is True:\n",
    "                train_results = train_one_step(self, train_batch)\n",
    "            else:\n",
    "                train_results = multi_gpu_train_one_step(self, train_batch)\n",
    "\n",
    "            # Update target network every `target_network_update_freq` sample steps.\n",
    "            last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
    "            if cur_ts - last_update >= self.config.target_network_update_freq:\n",
    "                to_update = self.workers.local_worker().get_policies_to_train()\n",
    "                self.workers.local_worker().foreach_policy_to_train(\n",
    "                    lambda p, pid: pid in to_update and p.update_target()\n",
    "                )\n",
    "                self._counters[NUM_TARGET_UPDATES] += 1\n",
    "                self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
    "\n",
    "            update_priorities_in_replay_buffer(\n",
    "                self.local_replay_buffer, self.config, train_batch, train_results\n",
    "            )\n",
    "\n",
    "            # Update weights and global_vars - after learning on the local worker -\n",
    "            # on all remote workers.\n",
    "            global_vars = {\n",
    "                \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
    "            }\n",
    "            # Update remote workers' weights and global vars after learning on local\n",
    "            # worker.\n",
    "            with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
    "                self.workers.sync_weights(global_vars=global_vars)\n",
    "\n",
    "        # Return all collected metrics for the iteration.\n",
    "        return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "90a9b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "#b = np.ones(3000).tolist()\n",
    "#obs_space= Tuple({MultiDiscrete([])})\n",
    "\n",
    "observation_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]),#sensing whether it got inhibitory, excitatory or none\n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]),\n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "action_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([1,3])#MultiDiscrete([1,3]),\n",
    "                #ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,1]),#3,1\n",
    "                #ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "obs_space = Tuple([#\n",
    "    Dict({\n",
    "        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    \n",
    "    }\n",
    "    )\n",
    "]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad806712",
   "metadata": {},
   "source": [
    "plan for visualization and causal inference\n",
    "\n",
    "we are going to sample the replay buffer and then feed it into the pysindy based algorithm and use that for network reconstruction. This should cover internal visualization \n",
    "\n",
    "for causal inference we explore using either tigramite or transfer entropy. \n",
    "\n",
    "However our current primary method we are going to explore is using BICM library with what we can determine from the \n",
    "comparative advantage paper to for causal and statisical validation \n",
    "\n",
    "paper for internal visualization: reconstructing network dynamics of coupled discrete chaotic units of data\n",
    "\n",
    "paper for causal inference on this: Inferring comparative advantage via entropy maximization \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "704112c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.replay_buffers.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer import MultiAgentPrioritizedReplayBuffer\n",
    "from ray.rllib.utils.replay_buffers import ReplayBuffer, StorageUnit \n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50991d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 12:05:32,297\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-09 12:07:26</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:52.98        </td></tr>\n",
       "<tr><td>Memory:      </td><td>34.5/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 7.0/16 CPUs, 0/1 GPUs, 0.0/20.28 GiB heap, 0.0/10.14 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_94ba4_00000</td><td>RUNNING </td><td>127.0.0.1:35768</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         92.9217</td><td style=\"text-align: right;\">69672</td><td style=\"text-align: right;\">  5.5119</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=35768)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(QMix pid=35768)\u001b[0m 2023-07-09 12:05:42,611\tWARNING algorithm_config.py:596 -- Cannot create QMixConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(QMix pid=35768)\u001b[0m 2023-07-09 12:05:42,851\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=30180)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=35784)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=10932)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=37076)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=37292)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=28216)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=37076)\u001b[0m 2023-07-09 12:05:52,411\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30180)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py:528: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30180)\u001b[0m   k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35784)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py:528: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35784)\u001b[0m   k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10932)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py:528: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10932)\u001b[0m   k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=37076)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py:528: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=37076)\u001b[0m   k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=37292)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py:528: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=37292)\u001b[0m   k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28216)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py:528: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28216)\u001b[0m   k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                            </th><th>counters                                                                                                                                                                                       </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                               </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                    </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </th><th>timers                                                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_94ba4_00000</td><td style=\"text-align: right;\">                  69672</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.0821349166688465, &#x27;StateBufferConnector_ms&#x27;: 0.019821713841150678, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.26024158984895734}</td><td>{&#x27;num_env_steps_sampled&#x27;: 69672, &#x27;num_env_steps_trained&#x27;: 91584, &#x27;num_agent_steps_sampled&#x27;: 69672, &#x27;num_agent_steps_trained&#x27;: 91584, &#x27;last_target_update_ts&#x27;: 69552, &#x27;num_target_updates&#x27;: 137}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">               5.5119</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 504</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;loss&#x27;: 0.006332256831228733, &#x27;td_error_abs&#x27;: 0.059429142624139786, &#x27;q_taken_mean&#x27;: 0.017120197415351868, &#x27;target_mean&#x27;: 0.07654934376478195, &#x27;grad_gnorm&#x27;: 0.7803376317024231}}}, &#x27;num_env_steps_sampled&#x27;: 69672, &#x27;num_env_steps_trained&#x27;: 91584, &#x27;num_agent_steps_sampled&#x27;: 69672, &#x27;num_agent_steps_trained&#x27;: 91584, &#x27;last_target_update_ts&#x27;: 69552, &#x27;num_target_updates&#x27;: 137}</td><td style=\"text-align: right;\">                    69672</td><td style=\"text-align: right;\">                    91584</td><td style=\"text-align: right;\">                  69672</td><td style=\"text-align: right;\">                             1008</td><td style=\"text-align: right;\">                  91584</td><td style=\"text-align: right;\">                             1344</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    6</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         1344</td><td>{&#x27;cpu_util_percent&#x27;: 3.5, &#x27;ram_util_percent&#x27;: 54.2, &#x27;gpu_util_percent0&#x27;: 0.075, &#x27;vram_util_percent0&#x27;: 0.37060546875}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 1.1158510595632787, &#x27;mean_inference_ms&#x27;: 0.8137555342597363, &#x27;mean_action_processing_ms&#x27;: 0.17214864351270445, &#x27;mean_env_wait_ms&#x27;: 0.0658195004191785, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 8.0, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 5.511904761904762, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 504, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 8.0, 8.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 8.0, 8.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 8.0, 8.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 8.0, 8.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 8.0, 8.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 8.0, 8.0, 8.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 8.0, 1.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 1.1158510595632787, &#x27;mean_inference_ms&#x27;: 0.8137555342597363, &#x27;mean_action_processing_ms&#x27;: 0.17214864351270445, &#x27;mean_env_wait_ms&#x27;: 0.0658195004191785, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.0821349166688465, &#x27;StateBufferConnector_ms&#x27;: 0.019821713841150678, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.26024158984895734}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 28.909, &#x27;learn_time_ms&#x27;: 4.999, &#x27;learn_throughput&#x27;: 6401.074, &#x27;synch_weights_time_ms&#x27;: 6.28}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "from gymnasium.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "import logging\n",
    "import os\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "#from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "from gymnasium.spaces import Dict, Discrete, MultiDiscrete, Tuple,Box\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"QMIX\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--mixer\",\n",
    "    type=str,\n",
    "    default=\"qmix\",\n",
    "    choices=[\"qmix\", \"vdn\", \"none\"],\n",
    "    help=\"The mixer model to use.\",\n",
    ")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=16)\n",
    "#parser.add_argument(\"--num-gpus\", type=int, default=1)\n",
    "parser.add_argument(\"--num-gpus\", type=float, default=0)#0.25)\n",
    "\n",
    "parser.add_argument(\"--num-workers\", type=int, default=6)\n",
    "parser.add_argument(\"--num-gpus-per-worker\", type=float, default=0.0)\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=70000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=8.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "#parser.add_argument(\"--num-workers\", type=int, default=6)\n",
    "parser.add_argument(\"--num_envs_per_worker\", type=int,default=1)\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ray.init(num_cpus=args.num_cpus or None, local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0,1],\n",
    "    }\n",
    "    obs_space = Tuple(\n",
    "        [\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    act_space = Tuple(\n",
    "        [\n",
    "            TwoStepGame.action_space,\n",
    "            TwoStepGame.action_space,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=obs_space, act_space=act_space\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "        #if agent_id.startswith(\"low_level_\"):\n",
    "        if agent_id.startswith(\"group_1\"):\n",
    "            return \"low_level_policy\"\n",
    "        else:\n",
    "            return \"high_level_policy\"\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(TwoStepGame)\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "    if args.run == \"QMIX\":\n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "            .training(mixer=args.mixer, train_batch_size=32)\n",
    "            .rollouts(num_rollout_workers=args.num_workers, rollout_fragment_length=4)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "                }\n",
    "            )\n",
    "            .environment(\n",
    "                env=\"grouped_twostep\",\n",
    "                env_config={\n",
    "                    \"separate_state_space\": True,\n",
    "                    \"one_hot_state_encoding\": True,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr='training_iteration',\n",
    "        metric='episode_reward_mean',#'loss',\n",
    "        mode='min',\n",
    "        perturbation_interval=1,\n",
    "        hyperparam_mutations={\n",
    "            #\"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"alpha\": tune.uniform(0.0, 1.0),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()\n",
    "    \"\"\"\n",
    "    if args.run == \"QMIX\":\n",
    "\n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "            .environment(env_config={\"actions_are_logits\": True})#line below was causing problems\n",
    "            .training(mixer=args.mixer, train_batch_size=32)#.training(num_steps_sampled_before_learning_starts=100)\n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        observation_space,\n",
    "                        action_space,\n",
    "                        config.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        #Tuple([observation_space,Box(-inf, inf, (1,), float64)]),#,Box()\n",
    "                        Tuple([observation_space,obs_space]),\n",
    "                        Tuple([action_space,obs_space]),#action_space,\n",
    "                        config.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn#policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"pol2\"\n",
    "            )\n",
    "            .rollouts(num_rollout_workers=args.num_workers, num_envs_per_worker=args.num_envs_per_worker)#.rollouts(num_rollout_workers=args.num_workers, num_envs_per_worker=args.num_envs_per_worker,)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            .environment(\n",
    "                env=\"grouped_twostep\",\n",
    "                env_config={\n",
    "                    \"separate_state_space\": True,\n",
    "                    \"one_hot_state_encoding\": True,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=4,\n",
    "            scheduler=pbt_scheduler,\n",
    "            #reuse_actors=True,\n",
    "        ),\n",
    "        \n",
    "        \n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aa0b7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76c7fcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 15:46:11,396\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8267 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-04 15:46:25</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:11.07        </td></tr>\n",
       "<tr><td>Memory:      </td><td>44.2/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/12.99 GiB heap, 0.0/6.49 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_9484b_00000</td><td style=\"text-align: right;\">           1</td><td>C:\\Users\\subar\\ray_results\\QMIX\\QMIX_grouped_twostep_9484b_00000_0_2023-07-04_15-46-14\\error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_9484b_00000</td><td>ERROR   </td><td>127.0.0.1:47488</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=47488)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(QMix pid=47488)\u001b[0m 2023-07-04 15:46:24,903\tWARNING algorithm_config.py:596 -- Cannot create QMixConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(QMix pid=47488)\u001b[0m 2023-07-04 15:46:25,286\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(QMix pid=47488)\u001b[0m 2023-07-04 15:46:25,289\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-07-04 15:46:25,340\tERROR trial_runner.py:1062 -- Trial QMIX_grouped_twostep_9484b_00000: Error processing event.\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::QMix.train()\u001b[39m (pid=47488, ip=127.0.0.1, repr=QMix)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 368, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 365, in train\n",
      "    result = self.step()\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 782, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 2713, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix.py\", line 259, in training_step\n",
      "    new_sample_batches = synchronous_parallel_sample(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 82, in synchronous_parallel_sample\n",
      "    sample_batches = [worker_set.local_worker().sample()]\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 914, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 323, in run\n",
      "    outputs = self.step()\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 349, in step\n",
      "    active_envs, to_eval, outputs = self._process_observations(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 616, in _process_observations\n",
      "    processed = policy.agent_connectors(acd_list)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\pipeline.py\", line 41, in __call__\n",
      "    ret = c(ret)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in __call__\n",
      "    return [self.transform(d) for d in acd_list]\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in <listcomp>\n",
      "    return [self.transform(d) for d in acd_list]\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\obs_preproc.py\", line 58, in transform\n",
      "    d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 246, in transform\n",
      "    self.check_shape(observation)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 69, in check_shape\n",
      "    observation = convert_element_to_space_type(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\spaces\\space_utils.py\", line 378, in convert_element_to_space_type\n",
      "    return tree.map_structure(map_, element, sampled_element, check_types=False)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tree\\__init__.py\", line 428, in map_structure\n",
      "    assert_same_structure(structures[0], other, check_types=check_types)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tree\\__init__.py\", line 284, in assert_same_structure\n",
      "    raise type(e)(\"%s\\n\"\n",
      "ValueError: The two structures don't have the same nested structure.\n",
      "\n",
      "First structure: type=list str=[0, 3]\n",
      "\n",
      "Second structure: type=tuple str=(3,)\n",
      "\n",
      "More specifically: The two structures don't have the same number of elements. First structure: type=list str=[0, 3]. Second structure: type=tuple str=(3,)\n",
      "Entire first structure:\n",
      "[., .]\n",
      "Entire second structure:\n",
      "(.,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_9484b_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 15:46:26,074\tERROR tune.py:794 -- Trials did not complete: [QMIX_grouped_twostep_9484b_00000]\n",
      "2023-07-04 15:46:26,075\tINFO tune.py:798 -- Total run time: 11.78 seconds (11.07 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "from gymnasium.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"QMIX\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"--mixer\",\n",
    "    type=str,\n",
    "    default=\"qmix\",\n",
    "    choices=[\"qmix\", \"vdn\", \"none\"],\n",
    "    help=\"The mixer model to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=70000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=8.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ray.init(num_cpus=args.num_cpus or None, local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0, 1],\n",
    "    }\n",
    "    \n",
    "    obs_space = Tuple(\n",
    "        [\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    act_space = Tuple(\n",
    "        [\n",
    "            TwoStepGame.action_space,\n",
    "            TwoStepGame.action_space,\n",
    "        ]\n",
    "    )\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=obs_space, act_space=act_space\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(\"grouped_twostep\")\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "\n",
    "    if args.run == \"QMIX\":\n",
    "        observation_space = Tuple([Discrete(6)])\n",
    "        act_space = Tuple([TwoStepGame.action_space]) \n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "            .environment(env_config={\"actions_are_logits\": True})\n",
    "            .training(num_steps_sampled_before_learning_starts=100)\n",
    "\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a161f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1edb3ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 11:18:39,415\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-06 11:23:20</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:38.58        </td></tr>\n",
       "<tr><td>Memory:      </td><td>32.8/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/19.96 GiB heap, 0.0/9.98 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_89a90_00000</td><td>TERMINATED</td><td>127.0.0.1:49584</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         265.672</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=49584)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(QMix pid=49584)\u001b[0m 2023-07-06 11:18:52,753\tWARNING algorithm_config.py:596 -- Cannot create QMixConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(QMix pid=49584)\u001b[0m 2023-07-06 11:18:53,079\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(QMix pid=49584)\u001b[0m 2023-07-06 11:18:53,082\tWARNING env.py:53 -- Skipping env checking for this experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                             </th><th>counters                                                                                                                                                                                         </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                    </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                                             </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                     </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </th><th>timers                                                                                                                    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_89a90_00000</td><td style=\"text-align: right;\">                  70000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.09924001693725586, &#x27;StateBufferConnector_ms&#x27;: 0.018151473999023438, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.25716204643249513}</td><td>{&#x27;num_env_steps_sampled&#x27;: 70000, &#x27;num_env_steps_trained&#x27;: 552000, &#x27;num_agent_steps_sampled&#x27;: 70000, &#x27;num_agent_steps_trained&#x27;: 552000, &#x27;last_target_update_ts&#x27;: 69504, &#x27;num_target_updates&#x27;: 138}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 500</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;loss&#x27;: 7.590557652292773e-05, &#x27;td_error_abs&#x27;: 0.008675613440573215, &#x27;q_taken_mean&#x27;: 0.010148399509489536, &#x27;target_mean&#x27;: 0.0014727864181622863, &#x27;grad_gnorm&#x27;: 0.0645175576210022}}}, &#x27;num_env_steps_sampled&#x27;: 70000, &#x27;num_env_steps_trained&#x27;: 552000, &#x27;num_agent_steps_sampled&#x27;: 70000, &#x27;num_agent_steps_trained&#x27;: 552000, &#x27;last_target_update_ts&#x27;: 69504, &#x27;num_target_updates&#x27;: 138}</td><td style=\"text-align: right;\">                    70000</td><td style=\"text-align: right;\">                   552000</td><td style=\"text-align: right;\">                  70000</td><td style=\"text-align: right;\">                             1000</td><td style=\"text-align: right;\">                 552000</td><td style=\"text-align: right;\">                             8000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         8000</td><td>{&#x27;cpu_util_percent&#x27;: 7.3, &#x27;ram_util_percent&#x27;: 51.79999999999999, &#x27;gpu_util_percent0&#x27;: 0.32499999999999996, &#x27;vram_util_percent0&#x27;: 0.1719970703125}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.9526090978481107, &#x27;mean_inference_ms&#x27;: 0.7371998581194408, &#x27;mean_action_processing_ms&#x27;: 0.15473369189622452, &#x27;mean_env_wait_ms&#x27;: 0.06172967667910093, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 500, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.9526090978481107, &#x27;mean_inference_ms&#x27;: 0.7371998581194408, &#x27;mean_action_processing_ms&#x27;: 0.15473369189622452, &#x27;mean_env_wait_ms&#x27;: 0.06172967667910093, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.09924001693725586, &#x27;StateBufferConnector_ms&#x27;: 0.018151473999023438, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.25716204643249513}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 16.485, &#x27;learn_time_ms&#x27;: 6.483, &#x27;learn_throughput&#x27;: 4936.345, &#x27;synch_weights_time_ms&#x27;: 0.0}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 11:23:21,620\tINFO tune.py:798 -- Total run time: 279.24 seconds (278.57 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from gymnasium.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"QMIX\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"--mixer\",\n",
    "    type=str,\n",
    "    default=\"qmix\",\n",
    "    choices=[\"qmix\", \"vdn\", \"none\"],\n",
    "    help=\"The mixer model to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=70000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=8.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ray.init(num_cpus=args.num_cpus or None, local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0,1],\n",
    "    }\n",
    "    def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "        #if agent_id.startswith(\"low_level_\"):\n",
    "        if agent_id.startswith(0):\n",
    "            return \"low_level_policy\"\n",
    "        else:\n",
    "            return \"high_level_policy\"\n",
    "    obs_space = Tuple(\n",
    "        [\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    act_space = Tuple(\n",
    "        [\n",
    "            TwoStepGame.action_space,\n",
    "            TwoStepGame.action_space,\n",
    "        ]\n",
    "    )\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=obs_space, act_space=act_space\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(TwoStepGame)\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "\n",
    "    if args.run == \"MADDPG\":\n",
    "        obs_space = Discrete(6)\n",
    "        act_space = TwoStepGame.action_space\n",
    "        (\n",
    "            config.framework(\"tf\")\n",
    "            .environment(env_config={\"actions_are_logits\": True})\n",
    "            .training(num_steps_sampled_before_learning_starts=100)\n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"pol1\": PolicySpec(\n",
    "                        observation_space=obs_space,\n",
    "                        action_space=act_space,\n",
    "                        config=config.overrides(agent_id=0),\n",
    "                    ),\n",
    "                    \"pol2\": PolicySpec(\n",
    "                        observation_space=obs_space,\n",
    "                        action_space=act_space,\n",
    "                        config=config.overrides(agent_id=1),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"pol2\"\n",
    "                if agent_id\n",
    "                else \"pol1\",\n",
    "            )\n",
    "        )\n",
    "    elif args.run == \"QMIX\":\n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "            .training(mixer=args.mixer, train_batch_size=32)\n",
    "            \n",
    "            .rollouts(num_rollout_workers=0, rollout_fragment_length=4)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "                }\n",
    "            )\n",
    "            .environment(\n",
    "                env=\"grouped_twostep\",\n",
    "                env_config={\n",
    "                    \"separate_state_space\": True,\n",
    "                    \"one_hot_state_encoding\": True,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f9aeb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dd55869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj,\n",
    "    num_dense_logs=1\n",
    "):\n",
    "    dense_logs = {}\n",
    "    for idx in range(num_dense_logs):\n",
    "        # Set initial states\n",
    "        agent_states = {}\n",
    "        for agent_idx in range(3000):\n",
    "            agent_states[str(agent_idx)] = trainer.get_policy(\"high_level_policy\").get_initial_state()\n",
    "        planner_states = trainer.get_policy(\"group_1\").get_initial_state()   \n",
    "        \"\"\"\n",
    "        # Get weights of the default local policy\n",
    "        algo.get_policy().get_weights()\n",
    "\n",
    "        # Same as above\n",
    "        algo.workers.local_worker().policy_map[\"default_policy\"].get_weights()\n",
    "\n",
    "        # Get list of weights of each worker, including remote replicas\n",
    "        algo.workers.foreach_worker(lambda worker: worker.get_policy().get_weights())\n",
    "\n",
    "        # Same as above, but with index.\n",
    "        algo.workers.foreach_worker_with_id(\n",
    "            lambda _id, worker: worker.get_policy().get_weights()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # Play out the episode\n",
    "        obs = env_obj.reset(force_dense_logging=True)\n",
    "        for t in range(env_obj.episode_length):\n",
    "            actions = {}\n",
    "            for agent_idx in range(3000):\n",
    "                # Use the trainer object directly to sample actions for each agent\n",
    "                actions[str(agent_idx)] = trainer.compute_action(\n",
    "                    obs[str(agent_idx)], \n",
    "                    agent_states[str(agent_idx)], \n",
    "                    policy_id=\"high_level_policy\",\n",
    "                    full_fetch=False\n",
    "                )\n",
    "\n",
    "            # Action sampling for the planner\n",
    "            actions[\"group_1\"] = trainer.compute_action(\n",
    "                obs['group_1'], \n",
    "                planner_states, \n",
    "                policy_id='low_level_policy',\n",
    "                full_fetch=False\n",
    "            )\n",
    "\n",
    "            obs, rew, done, info = env_obj.step(actions)        \n",
    "            if done['__all__']:\n",
    "                break\n",
    "        dense_logs[idx] = env_obj.dense_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c3ee398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 00:26:56,927\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('default_policy/fc_1/kernel',\n",
       "               array([[-0.19699307, -0.77786523,  0.2695395 , ..., -0.20872183,\n",
       "                        0.60184073,  0.15955698],\n",
       "                      [ 0.44580653,  0.36319947,  0.24732333, ..., -0.72212946,\n",
       "                        0.67725825, -0.02158662],\n",
       "                      [ 0.6292287 , -0.01198673,  0.16471617, ...,  0.64464194,\n",
       "                       -0.17498262, -0.68713987],\n",
       "                      [ 0.60541016, -0.51270676,  0.9159958 , ..., -0.13928802,\n",
       "                        0.3853442 ,  0.7084592 ]], dtype=float32)),\n",
       "              ('default_policy/fc_1/bias',\n",
       "               array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0.], dtype=float32)),\n",
       "              ('default_policy/fc_out/kernel',\n",
       "               array([[ 0.03922524,  0.03836435, -0.00514024, ..., -0.04851685,\n",
       "                        0.14497057,  0.10757451],\n",
       "                      [-0.10650252, -0.06632864,  0.04139837, ..., -0.03809531,\n",
       "                       -0.01414561, -0.02309426],\n",
       "                      [-0.02233423, -0.14214991,  0.02278218, ...,  0.12107196,\n",
       "                       -0.02628977,  0.0032232 ],\n",
       "                      ...,\n",
       "                      [ 0.07388712, -0.00324146, -0.01082274, ...,  0.00936439,\n",
       "                       -0.00222038, -0.10229807],\n",
       "                      [-0.00511517,  0.02765071, -0.01285565, ...,  0.03621878,\n",
       "                       -0.07885719, -0.03271953],\n",
       "                      [ 0.04351667,  0.01092449, -0.02077376, ..., -0.0460316 ,\n",
       "                        0.10149302, -0.00460022]], dtype=float32)),\n",
       "              ('default_policy/fc_out/bias',\n",
       "               array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0.], dtype=float32)),\n",
       "              ('default_policy/value_out/kernel',\n",
       "               array([[ 2.29483383e-04],\n",
       "                      [ 1.81467651e-04],\n",
       "                      [ 5.95634738e-05],\n",
       "                      [ 2.49129371e-04],\n",
       "                      [-2.10656348e-04],\n",
       "                      [-2.57418840e-04],\n",
       "                      [-4.93082189e-05],\n",
       "                      [-8.90319468e-04],\n",
       "                      [-4.87201061e-04],\n",
       "                      [ 1.35575177e-03],\n",
       "                      [-4.31874447e-04],\n",
       "                      [-2.04829150e-04],\n",
       "                      [-3.65853222e-04],\n",
       "                      [-6.06877657e-05],\n",
       "                      [-7.37474824e-04],\n",
       "                      [ 3.51861148e-04],\n",
       "                      [-3.18320992e-04],\n",
       "                      [ 7.86881137e-05],\n",
       "                      [ 1.26615632e-03],\n",
       "                      [ 8.42617941e-04],\n",
       "                      [ 1.02934035e-04],\n",
       "                      [-1.14364224e-03],\n",
       "                      [-3.73175542e-04],\n",
       "                      [ 6.03981724e-04],\n",
       "                      [ 3.27801012e-04],\n",
       "                      [-2.75313534e-04],\n",
       "                      [ 1.01901358e-04],\n",
       "                      [ 9.69528744e-04],\n",
       "                      [-6.35454257e-04],\n",
       "                      [ 3.25645806e-06],\n",
       "                      [ 3.67825298e-04],\n",
       "                      [-3.71955510e-04],\n",
       "                      [ 3.32591328e-04],\n",
       "                      [-5.23996598e-04],\n",
       "                      [ 3.90363479e-04],\n",
       "                      [ 1.46761921e-03],\n",
       "                      [-6.71468442e-05],\n",
       "                      [ 2.02738793e-05],\n",
       "                      [-2.26144926e-04],\n",
       "                      [-1.81146839e-04],\n",
       "                      [-6.60711783e-04],\n",
       "                      [ 2.41824062e-04],\n",
       "                      [-3.12035263e-04],\n",
       "                      [ 3.06930830e-04],\n",
       "                      [-1.01325847e-03],\n",
       "                      [ 1.75960991e-03],\n",
       "                      [ 3.51845316e-04],\n",
       "                      [ 6.92504807e-04],\n",
       "                      [-4.85106779e-04],\n",
       "                      [-7.26447433e-06],\n",
       "                      [ 2.82334600e-04],\n",
       "                      [ 4.88382007e-04],\n",
       "                      [ 3.20408522e-04],\n",
       "                      [ 3.54548596e-04],\n",
       "                      [ 7.88620033e-04],\n",
       "                      [ 1.55744739e-04],\n",
       "                      [ 6.69769594e-04],\n",
       "                      [ 4.15476643e-05],\n",
       "                      [-2.08165700e-04],\n",
       "                      [ 3.05350768e-05],\n",
       "                      [ 2.95372796e-04],\n",
       "                      [-1.39073248e-03],\n",
       "                      [ 4.21964971e-04],\n",
       "                      [ 7.89025580e-05],\n",
       "                      [-2.22546791e-04],\n",
       "                      [-3.38643236e-04],\n",
       "                      [-7.95287022e-04],\n",
       "                      [-1.14450068e-03],\n",
       "                      [ 2.70743738e-04],\n",
       "                      [-4.78937116e-04],\n",
       "                      [ 4.03124985e-04],\n",
       "                      [ 7.87872006e-04],\n",
       "                      [ 6.80729572e-04],\n",
       "                      [ 3.45469860e-04],\n",
       "                      [ 1.04401610e-03],\n",
       "                      [-5.01988223e-04],\n",
       "                      [ 1.24254276e-03],\n",
       "                      [-2.84664711e-04],\n",
       "                      [-1.38172670e-03],\n",
       "                      [ 5.88180614e-04],\n",
       "                      [-2.26605766e-06],\n",
       "                      [-3.51427239e-04],\n",
       "                      [-1.54628811e-04],\n",
       "                      [-2.77785060e-04],\n",
       "                      [ 1.86626203e-04],\n",
       "                      [-1.08880973e-04],\n",
       "                      [-1.99639035e-04],\n",
       "                      [ 2.45547213e-04],\n",
       "                      [-1.26770150e-03],\n",
       "                      [-2.22998162e-04],\n",
       "                      [ 2.31892744e-04],\n",
       "                      [-1.36432049e-04],\n",
       "                      [-2.83772097e-04],\n",
       "                      [-1.25557068e-03],\n",
       "                      [-1.17143709e-03],\n",
       "                      [ 3.37967969e-04],\n",
       "                      [-1.25809456e-04],\n",
       "                      [ 6.92184258e-04],\n",
       "                      [ 7.84358126e-05],\n",
       "                      [-8.35529529e-04],\n",
       "                      [ 5.11487888e-04],\n",
       "                      [ 6.86301355e-05],\n",
       "                      [ 5.04931377e-04],\n",
       "                      [ 2.57813808e-04],\n",
       "                      [ 4.95487533e-04],\n",
       "                      [-9.71616246e-05],\n",
       "                      [ 5.37744490e-04],\n",
       "                      [ 1.26745703e-03],\n",
       "                      [ 7.44395715e-04],\n",
       "                      [-6.92271977e-04],\n",
       "                      [ 1.37051757e-04],\n",
       "                      [ 6.67128537e-04],\n",
       "                      [ 7.86160017e-05],\n",
       "                      [-9.89840133e-04],\n",
       "                      [ 4.10123503e-05],\n",
       "                      [ 8.62201035e-04],\n",
       "                      [-4.38523217e-04],\n",
       "                      [ 7.32168555e-04],\n",
       "                      [ 4.97939298e-04],\n",
       "                      [-7.52553809e-04],\n",
       "                      [ 3.50611546e-04],\n",
       "                      [ 1.91928393e-07],\n",
       "                      [ 6.55982294e-04],\n",
       "                      [ 9.23254120e-04],\n",
       "                      [ 4.73079344e-05],\n",
       "                      [ 4.60792333e-04],\n",
       "                      [ 2.45989591e-04],\n",
       "                      [-3.64412233e-04],\n",
       "                      [ 4.41908516e-04],\n",
       "                      [ 6.17693968e-06],\n",
       "                      [ 2.44693074e-04],\n",
       "                      [ 2.43396153e-06],\n",
       "                      [ 8.57427716e-04],\n",
       "                      [-4.99949499e-04],\n",
       "                      [ 4.62827797e-04],\n",
       "                      [ 3.63423547e-04],\n",
       "                      [-8.12681741e-04],\n",
       "                      [ 3.01076972e-04],\n",
       "                      [-3.75938573e-04],\n",
       "                      [ 1.64509227e-04],\n",
       "                      [ 2.87837000e-04],\n",
       "                      [-1.08826825e-04],\n",
       "                      [ 4.09080851e-04],\n",
       "                      [-5.51498262e-04],\n",
       "                      [-1.62261247e-04],\n",
       "                      [ 2.38162451e-04],\n",
       "                      [-5.52619342e-04],\n",
       "                      [-3.41905165e-04],\n",
       "                      [ 1.20155700e-03],\n",
       "                      [ 9.58829623e-05],\n",
       "                      [-2.28930759e-04],\n",
       "                      [-1.28381012e-04],\n",
       "                      [-7.35053036e-04],\n",
       "                      [ 3.23260378e-04],\n",
       "                      [-8.68554271e-05],\n",
       "                      [ 3.78620665e-04],\n",
       "                      [-5.52401645e-04],\n",
       "                      [-3.43717344e-04],\n",
       "                      [-8.17156397e-04],\n",
       "                      [-5.02746785e-04],\n",
       "                      [-4.52905893e-04],\n",
       "                      [ 9.01025080e-04],\n",
       "                      [ 4.79325274e-04],\n",
       "                      [-1.28869712e-03],\n",
       "                      [ 1.92288528e-04],\n",
       "                      [ 1.01072236e-03],\n",
       "                      [ 5.44369570e-04],\n",
       "                      [ 7.07878149e-04],\n",
       "                      [-1.71253120e-03],\n",
       "                      [-7.18879048e-04],\n",
       "                      [-5.60910383e-04],\n",
       "                      [ 1.02377388e-04],\n",
       "                      [ 3.42166163e-06],\n",
       "                      [ 5.03742602e-04],\n",
       "                      [ 2.78517458e-04],\n",
       "                      [ 1.22258067e-03],\n",
       "                      [-3.51493305e-04],\n",
       "                      [-4.42726712e-04],\n",
       "                      [ 3.30799550e-04],\n",
       "                      [-8.61804292e-04],\n",
       "                      [ 6.47319976e-05],\n",
       "                      [ 1.02084909e-04],\n",
       "                      [ 1.96816283e-04],\n",
       "                      [ 3.99097044e-04],\n",
       "                      [-1.70748658e-03],\n",
       "                      [-4.82915348e-04],\n",
       "                      [-1.08158949e-03],\n",
       "                      [-1.06768182e-03],\n",
       "                      [-1.43591606e-03],\n",
       "                      [ 9.16925957e-04],\n",
       "                      [-2.96481623e-04],\n",
       "                      [ 4.74355184e-04],\n",
       "                      [-1.00669997e-04],\n",
       "                      [-2.44660565e-04],\n",
       "                      [ 3.71853472e-04],\n",
       "                      [ 2.25465687e-04],\n",
       "                      [-8.85143992e-04],\n",
       "                      [-6.05241861e-04],\n",
       "                      [ 1.30020591e-04],\n",
       "                      [ 5.50225202e-04],\n",
       "                      [ 5.95918827e-05],\n",
       "                      [ 3.27172456e-04],\n",
       "                      [-1.72715416e-04],\n",
       "                      [ 9.15546494e-04],\n",
       "                      [-2.64571019e-04],\n",
       "                      [ 8.50921322e-04],\n",
       "                      [ 4.98650013e-04],\n",
       "                      [ 5.80737076e-04],\n",
       "                      [ 9.21689498e-04],\n",
       "                      [ 1.98798880e-04],\n",
       "                      [-5.05061995e-04],\n",
       "                      [-2.04910029e-04],\n",
       "                      [-4.16377094e-04],\n",
       "                      [ 3.32770898e-04],\n",
       "                      [ 4.02952195e-04],\n",
       "                      [ 8.93766060e-04],\n",
       "                      [-6.67610671e-04],\n",
       "                      [ 8.35919054e-05],\n",
       "                      [ 1.26604096e-03],\n",
       "                      [-6.94317569e-04],\n",
       "                      [-8.54341488e-05],\n",
       "                      [ 6.54354226e-04],\n",
       "                      [ 8.92051030e-04],\n",
       "                      [ 2.74393911e-04],\n",
       "                      [-4.19216114e-04],\n",
       "                      [-6.12792326e-04],\n",
       "                      [ 9.05971450e-04],\n",
       "                      [ 1.30788167e-03],\n",
       "                      [ 5.76694147e-04],\n",
       "                      [-3.36036959e-04],\n",
       "                      [-6.25534522e-05],\n",
       "                      [-9.54696661e-05],\n",
       "                      [-6.23277389e-04],\n",
       "                      [-4.91921091e-04],\n",
       "                      [-2.41638380e-04],\n",
       "                      [ 4.26500803e-04],\n",
       "                      [ 1.27221097e-03],\n",
       "                      [ 3.57860379e-04],\n",
       "                      [-9.05500900e-04],\n",
       "                      [-1.28618383e-03],\n",
       "                      [ 5.13863750e-04],\n",
       "                      [-8.97462014e-04],\n",
       "                      [ 1.70629169e-03],\n",
       "                      [-2.67940341e-04],\n",
       "                      [ 3.32545926e-04],\n",
       "                      [-4.45028156e-04],\n",
       "                      [-8.91813659e-04],\n",
       "                      [-1.79692666e-04],\n",
       "                      [ 2.82670837e-04],\n",
       "                      [ 1.51129701e-04],\n",
       "                      [ 2.80009379e-04],\n",
       "                      [-2.73374608e-04],\n",
       "                      [ 8.12591607e-05],\n",
       "                      [ 2.65803741e-04],\n",
       "                      [ 6.45304855e-04],\n",
       "                      [ 3.15245066e-04]], dtype=float32)),\n",
       "              ('default_policy/value_out/bias', array([0.], dtype=float32)),\n",
       "              ('default_policy/hidden_0/kernel',\n",
       "               array([[ 0.09100769, -0.10515917,  0.10405912, ..., -0.05200218,\n",
       "                       -0.09838269, -0.07679588],\n",
       "                      [ 0.02476216, -0.09851432, -0.05488923, ..., -0.0240394 ,\n",
       "                        0.10770162,  0.10354861],\n",
       "                      [-0.09984153, -0.02008787,  0.0842151 , ...,  0.08949324,\n",
       "                        0.03319319, -0.09530924],\n",
       "                      ...,\n",
       "                      [ 0.03889432, -0.06871571, -0.01516245, ..., -0.02087148,\n",
       "                        0.1030215 , -0.05404216],\n",
       "                      [-0.06704789, -0.06812704,  0.02664147, ...,  0.0543934 ,\n",
       "                        0.00546229, -0.02096003],\n",
       "                      [-0.09844208, -0.05151992,  0.10597137, ...,  0.09242528,\n",
       "                       -0.09527612,  0.02164388]], dtype=float32)),\n",
       "              ('default_policy/hidden_0/bias',\n",
       "               array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0.], dtype=float32)),\n",
       "              ('default_policy/dense/kernel',\n",
       "               array([[-0.07999421, -0.10304717],\n",
       "                      [-0.02595793, -0.0932667 ],\n",
       "                      [ 0.09277157, -0.05761632],\n",
       "                      [-0.08037728, -0.06175566],\n",
       "                      [-0.06916196,  0.11973536],\n",
       "                      [ 0.06938022,  0.14807114],\n",
       "                      [ 0.04340506, -0.06334617],\n",
       "                      [ 0.00719644, -0.1264092 ],\n",
       "                      [ 0.02029985,  0.03480828],\n",
       "                      [ 0.13929722,  0.0264468 ],\n",
       "                      [-0.11662637, -0.08307832],\n",
       "                      [-0.06771021, -0.02907269],\n",
       "                      [ 0.02375886, -0.04827484],\n",
       "                      [-0.01546207, -0.08837859],\n",
       "                      [-0.07734589,  0.01013049],\n",
       "                      [-0.07854758, -0.0667306 ],\n",
       "                      [-0.11651686,  0.10341653],\n",
       "                      [ 0.13254401,  0.13083634],\n",
       "                      [ 0.06696293,  0.00295877],\n",
       "                      [ 0.08996572,  0.02670775],\n",
       "                      [ 0.12332356,  0.14533082],\n",
       "                      [ 0.0801066 ,  0.04525432],\n",
       "                      [-0.04100627, -0.14479959],\n",
       "                      [-0.07040728,  0.01098605],\n",
       "                      [-0.03397687,  0.09380484],\n",
       "                      [-0.10445079, -0.14029063],\n",
       "                      [ 0.07093288,  0.0446279 ],\n",
       "                      [ 0.13414627,  0.1112099 ],\n",
       "                      [ 0.03687057, -0.1223463 ],\n",
       "                      [ 0.09767848,  0.07789746],\n",
       "                      [ 0.04222755,  0.06081085],\n",
       "                      [ 0.08433057, -0.11063802],\n",
       "                      [ 0.09863585,  0.0880682 ],\n",
       "                      [-0.0423438 , -0.08815829],\n",
       "                      [-0.05868682,  0.05598041],\n",
       "                      [-0.12187015,  0.04020126],\n",
       "                      [-0.13359739,  0.1444861 ],\n",
       "                      [ 0.09714012,  0.04182717],\n",
       "                      [-0.08436941,  0.05298269],\n",
       "                      [ 0.05400367, -0.00953156],\n",
       "                      [ 0.13793436, -0.10010628],\n",
       "                      [ 0.03190403,  0.01255575],\n",
       "                      [ 0.09557648, -0.03126885],\n",
       "                      [-0.04503141, -0.10408906],\n",
       "                      [ 0.02107748,  0.00783627],\n",
       "                      [ 0.01308368,  0.14423451],\n",
       "                      [ 0.02479032,  0.0931053 ],\n",
       "                      [-0.04910233, -0.05461624],\n",
       "                      [-0.08743912,  0.09735204],\n",
       "                      [-0.09359716,  0.09392901],\n",
       "                      [-0.10292861,  0.10867882],\n",
       "                      [ 0.03065509,  0.11554861],\n",
       "                      [ 0.05914868, -0.12473235],\n",
       "                      [ 0.15095592,  0.06055903],\n",
       "                      [-0.01950659, -0.0056752 ],\n",
       "                      [ 0.02189276,  0.07039656],\n",
       "                      [ 0.1177862 ,  0.10613877],\n",
       "                      [ 0.07197618, -0.09521962],\n",
       "                      [ 0.06193426, -0.14078394],\n",
       "                      [-0.10654788, -0.03557693],\n",
       "                      [ 0.00788464,  0.11117303],\n",
       "                      [-0.03961767,  0.05729964],\n",
       "                      [-0.10036213, -0.10443155],\n",
       "                      [ 0.13316926,  0.02286662],\n",
       "                      [-0.14846292,  0.05089636],\n",
       "                      [-0.02418818,  0.01795287],\n",
       "                      [-0.07656171, -0.03967766],\n",
       "                      [-0.1220006 , -0.02214159],\n",
       "                      [ 0.04711047,  0.09441951],\n",
       "                      [-0.00393769,  0.13051835],\n",
       "                      [-0.07725078, -0.14027117],\n",
       "                      [-0.02187589, -0.03204365],\n",
       "                      [-0.05396375,  0.13989007],\n",
       "                      [ 0.01836242,  0.04365215],\n",
       "                      [-0.00317614, -0.14056411],\n",
       "                      [-0.0660383 ,  0.06862386],\n",
       "                      [-0.08187925, -0.09218206],\n",
       "                      [-0.08942484,  0.05179657],\n",
       "                      [-0.05903892, -0.0202543 ],\n",
       "                      [ 0.10235921,  0.05561922],\n",
       "                      [ 0.15191966,  0.12994224],\n",
       "                      [ 0.06776652, -0.13205634],\n",
       "                      [ 0.05049674, -0.12485132],\n",
       "                      [-0.13949801,  0.05451207],\n",
       "                      [-0.03505766, -0.13138324],\n",
       "                      [-0.12027121, -0.0487362 ],\n",
       "                      [-0.03191926, -0.09255582],\n",
       "                      [ 0.07925235,  0.10320947],\n",
       "                      [ 0.12019512,  0.06454752],\n",
       "                      [ 0.07103065,  0.11003613],\n",
       "                      [ 0.09009881, -0.04978991],\n",
       "                      [ 0.07224552,  0.04109001],\n",
       "                      [ 0.10947531, -0.1285484 ],\n",
       "                      [ 0.01360756,  0.02733733],\n",
       "                      [ 0.14186198,  0.08715701],\n",
       "                      [ 0.10640216, -0.00510885],\n",
       "                      [-0.13160211,  0.01801597],\n",
       "                      [-0.05731887,  0.0651381 ],\n",
       "                      [ 0.14357439,  0.1416204 ],\n",
       "                      [-0.09162151,  0.12985894],\n",
       "                      [-0.07588086,  0.07587941],\n",
       "                      [ 0.03389822,  0.09615298],\n",
       "                      [-0.11662361, -0.13303412],\n",
       "                      [ 0.06418808,  0.14444506],\n",
       "                      [-0.09567156, -0.13199766],\n",
       "                      [-0.08557146,  0.06197818],\n",
       "                      [-0.06895057,  0.01000986],\n",
       "                      [-0.08380614, -0.10523417],\n",
       "                      [ 0.1295766 , -0.08200029],\n",
       "                      [ 0.10937208, -0.14563431],\n",
       "                      [-0.0931761 , -0.07682382],\n",
       "                      [ 0.04910131,  0.06286092],\n",
       "                      [-0.1311684 ,  0.08554728],\n",
       "                      [-0.07321164,  0.03864163],\n",
       "                      [ 0.03242749, -0.05843501],\n",
       "                      [-0.01462902,  0.09092459],\n",
       "                      [ 0.12930858, -0.00863841],\n",
       "                      [ 0.01407081, -0.1123777 ],\n",
       "                      [-0.02611452, -0.1144569 ],\n",
       "                      [-0.01852331, -0.13476905],\n",
       "                      [-0.05165739, -0.05963542],\n",
       "                      [-0.07359959, -0.02887192],\n",
       "                      [ 0.04855956, -0.02049896],\n",
       "                      [-0.12359877,  0.08459505],\n",
       "                      [-0.01243894,  0.11909097],\n",
       "                      [-0.04292284,  0.08701119],\n",
       "                      [-0.08748261, -0.00815812],\n",
       "                      [-0.01288916, -0.01730679],\n",
       "                      [-0.03279576,  0.07659313],\n",
       "                      [-0.0098429 ,  0.06139004],\n",
       "                      [ 0.10885757,  0.01692368],\n",
       "                      [ 0.12026531,  0.07715309],\n",
       "                      [ 0.07681964, -0.02939002],\n",
       "                      [ 0.13057569,  0.13669309],\n",
       "                      [ 0.09007692,  0.01049328],\n",
       "                      [ 0.0878955 , -0.11290541],\n",
       "                      [ 0.06924883,  0.14792687],\n",
       "                      [ 0.02575861, -0.13293065],\n",
       "                      [-0.12385699,  0.06395939],\n",
       "                      [-0.04756367, -0.06944534],\n",
       "                      [-0.06621452,  0.07032827],\n",
       "                      [ 0.06391624,  0.05941655],\n",
       "                      [-0.05280758, -0.02703549],\n",
       "                      [-0.11856966,  0.09269267],\n",
       "                      [-0.04653011, -0.13405147],\n",
       "                      [-0.03651876, -0.0781873 ],\n",
       "                      [-0.03121257, -0.12844405],\n",
       "                      [ 0.01317425, -0.10929978],\n",
       "                      [-0.13497774, -0.0425045 ],\n",
       "                      [ 0.04047489,  0.01908414],\n",
       "                      [-0.02569579,  0.03404787],\n",
       "                      [ 0.13799426,  0.13833722],\n",
       "                      [-0.09630252, -0.13395628],\n",
       "                      [ 0.02518716,  0.08152062],\n",
       "                      [ 0.06195283,  0.11776489],\n",
       "                      [-0.06870941,  0.04133365],\n",
       "                      [-0.01477878, -0.0538017 ],\n",
       "                      [-0.1297532 ,  0.12810636],\n",
       "                      [-0.10479754, -0.12638226],\n",
       "                      [ 0.02233189,  0.02888246],\n",
       "                      [ 0.05030295, -0.09775839],\n",
       "                      [ 0.13954863,  0.1431326 ],\n",
       "                      [ 0.05809058,  0.05300334],\n",
       "                      [ 0.14832258, -0.01496959],\n",
       "                      [ 0.14274734,  0.1169056 ],\n",
       "                      [-0.0935728 , -0.14072914],\n",
       "                      [-0.13708697, -0.10464258],\n",
       "                      [-0.05740272,  0.0373133 ],\n",
       "                      [ 0.14870772, -0.06023472],\n",
       "                      [ 0.05048214,  0.07801238],\n",
       "                      [ 0.11007676,  0.00729436],\n",
       "                      [-0.1004964 , -0.00397609],\n",
       "                      [-0.05970457, -0.0426283 ],\n",
       "                      [-0.14443982,  0.04749994],\n",
       "                      [-0.04491605,  0.1363149 ],\n",
       "                      [ 0.01773255, -0.00051822],\n",
       "                      [ 0.11944696,  0.14745855],\n",
       "                      [-0.0028452 , -0.05758633],\n",
       "                      [ 0.11868167, -0.01939689],\n",
       "                      [ 0.12525073, -0.12805766],\n",
       "                      [ 0.08467078, -0.11679791],\n",
       "                      [ 0.01637001, -0.01639521],\n",
       "                      [-0.09713597, -0.08638015],\n",
       "                      [ 0.13544136,  0.06457151],\n",
       "                      [ 0.06813993,  0.05545183],\n",
       "                      [ 0.14428589,  0.10472777],\n",
       "                      [ 0.02726279, -0.07476215],\n",
       "                      [-0.03376253,  0.14529738],\n",
       "                      [-0.0869285 ,  0.10735589],\n",
       "                      [-0.08188362, -0.07667068],\n",
       "                      [-0.11086112,  0.09190267],\n",
       "                      [-0.01342268, -0.01274812],\n",
       "                      [ 0.01877426,  0.02581312],\n",
       "                      [ 0.05546008, -0.03569044],\n",
       "                      [ 0.09557703, -0.09733005],\n",
       "                      [-0.12326834, -0.09439632],\n",
       "                      [ 0.01389581, -0.05902438],\n",
       "                      [-0.010298  ,  0.02315189],\n",
       "                      [ 0.06569991, -0.02565145],\n",
       "                      [ 0.08656292, -0.00713219],\n",
       "                      [-0.11576562, -0.11487968],\n",
       "                      [ 0.02840194,  0.02751015],\n",
       "                      [ 0.03652564, -0.05435628],\n",
       "                      [-0.10078883, -0.11761384],\n",
       "                      [ 0.12583414,  0.08762284],\n",
       "                      [ 0.10150063,  0.14528999],\n",
       "                      [-0.11556994,  0.11643404],\n",
       "                      [-0.10439774,  0.00615549],\n",
       "                      [ 0.09780103, -0.10031461],\n",
       "                      [ 0.13602719,  0.05666399],\n",
       "                      [-0.05445793, -0.10328983],\n",
       "                      [ 0.13964656, -0.07533058],\n",
       "                      [-0.03100547,  0.1483613 ],\n",
       "                      [-0.02150637,  0.10816109],\n",
       "                      [-0.02195057,  0.15175304],\n",
       "                      [ 0.08356349, -0.0236491 ],\n",
       "                      [ 0.06971276, -0.05507275],\n",
       "                      [ 0.04544318, -0.04894893],\n",
       "                      [ 0.15029842, -0.06210398],\n",
       "                      [-0.14796929, -0.11464186],\n",
       "                      [ 0.11864278,  0.09583688],\n",
       "                      [-0.06476571,  0.10067493],\n",
       "                      [ 0.01003763, -0.02971965],\n",
       "                      [-0.10944408,  0.08539203],\n",
       "                      [ 0.07258141, -0.11156513],\n",
       "                      [-0.10084173, -0.03671339],\n",
       "                      [-0.13645521,  0.08450252],\n",
       "                      [ 0.05801365,  0.15063182],\n",
       "                      [-0.1114979 ,  0.00990009],\n",
       "                      [ 0.09932479, -0.09379026],\n",
       "                      [-0.12210175,  0.0875718 ],\n",
       "                      [-0.053688  ,  0.14499924],\n",
       "                      [-0.12773277,  0.05495651],\n",
       "                      [ 0.00615059,  0.03205696],\n",
       "                      [-0.07419427, -0.07014233],\n",
       "                      [ 0.01628551, -0.00812881],\n",
       "                      [-0.01257586, -0.09954326],\n",
       "                      [ 0.10242274, -0.0168491 ],\n",
       "                      [ 0.13457969, -0.12345356],\n",
       "                      [ 0.04022576, -0.07599827],\n",
       "                      [ 0.00388934, -0.12760875],\n",
       "                      [ 0.09723087, -0.03663598],\n",
       "                      [ 0.0005645 ,  0.02815136],\n",
       "                      [-0.06595394, -0.01336817],\n",
       "                      [-0.02960131,  0.1207833 ],\n",
       "                      [ 0.00846376, -0.07660414],\n",
       "                      [ 0.1032899 , -0.06655622],\n",
       "                      [-0.07103744,  0.01383387],\n",
       "                      [-0.03181379, -0.02649119],\n",
       "                      [ 0.04827747,  0.02978757],\n",
       "                      [-0.10179669,  0.11614385],\n",
       "                      [-0.04926376,  0.14458132],\n",
       "                      [ 0.13330415, -0.11447057],\n",
       "                      [-0.07360733,  0.07883997],\n",
       "                      [ 0.06250253,  0.02259062],\n",
       "                      [ 0.01630089, -0.06473444]], dtype=float32)),\n",
       "              ('default_policy/dense/bias', array([0., 0.], dtype=float32)),\n",
       "              ('default_policy/dense_1/kernel',\n",
       "               array([[ 0.02247178, -0.08193387, -0.05046736, ..., -0.10518514,\n",
       "                        0.08369999,  0.09280785],\n",
       "                      [ 0.07968172,  0.02348319,  0.05746009, ..., -0.06849356,\n",
       "                       -0.011453  , -0.05825612],\n",
       "                      [ 0.00086281,  0.05266971, -0.026503  , ...,  0.05402761,\n",
       "                       -0.06491633,  0.05926961],\n",
       "                      ...,\n",
       "                      [ 0.03827959,  0.10300506,  0.05338263, ...,  0.02165236,\n",
       "                        0.01281729,  0.10229214],\n",
       "                      [-0.05953335,  0.05996756,  0.01588078, ..., -0.06371541,\n",
       "                        0.00135753,  0.02145394],\n",
       "                      [ 0.05005907,  0.05212095,  0.03533062, ..., -0.09385474,\n",
       "                        0.09509353, -0.07506184]], dtype=float32)),\n",
       "              ('default_policy/dense_1/bias',\n",
       "               array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0.], dtype=float32)),\n",
       "              ('default_policy/dense_2/kernel',\n",
       "               array([[-0.00256203],\n",
       "                      [ 0.1323549 ],\n",
       "                      [ 0.0891941 ],\n",
       "                      [ 0.07434091],\n",
       "                      [-0.01794869],\n",
       "                      [ 0.00273718],\n",
       "                      [-0.08783227],\n",
       "                      [-0.06181876],\n",
       "                      [ 0.07248966],\n",
       "                      [ 0.15112938],\n",
       "                      [ 0.11699454],\n",
       "                      [-0.10668181],\n",
       "                      [-0.02483794],\n",
       "                      [ 0.01520695],\n",
       "                      [-0.08326297],\n",
       "                      [-0.07413553],\n",
       "                      [ 0.06381384],\n",
       "                      [ 0.07535358],\n",
       "                      [ 0.0280474 ],\n",
       "                      [-0.0141806 ],\n",
       "                      [-0.12619153],\n",
       "                      [ 0.01466988],\n",
       "                      [ 0.08904415],\n",
       "                      [ 0.02102078],\n",
       "                      [ 0.01875646],\n",
       "                      [-0.10481606],\n",
       "                      [-0.03573908],\n",
       "                      [ 0.09214774],\n",
       "                      [ 0.08646086],\n",
       "                      [-0.01752411],\n",
       "                      [-0.02055523],\n",
       "                      [-0.04015878],\n",
       "                      [ 0.0060809 ],\n",
       "                      [ 0.10895483],\n",
       "                      [-0.07784689],\n",
       "                      [-0.05617825],\n",
       "                      [-0.14268577],\n",
       "                      [ 0.03677477],\n",
       "                      [-0.02900362],\n",
       "                      [ 0.07329376],\n",
       "                      [-0.10710315],\n",
       "                      [ 0.09797095],\n",
       "                      [ 0.12784456],\n",
       "                      [ 0.04664455],\n",
       "                      [ 0.07220249],\n",
       "                      [ 0.13008524],\n",
       "                      [ 0.12664281],\n",
       "                      [ 0.12684308],\n",
       "                      [-0.10058036],\n",
       "                      [-0.06106431],\n",
       "                      [ 0.05171017],\n",
       "                      [-0.01754829],\n",
       "                      [ 0.09665348],\n",
       "                      [-0.13041271],\n",
       "                      [-0.08924084],\n",
       "                      [ 0.02120508],\n",
       "                      [ 0.06398956],\n",
       "                      [ 0.14316435],\n",
       "                      [-0.13004649],\n",
       "                      [ 0.02531247],\n",
       "                      [ 0.10235827],\n",
       "                      [ 0.04715598],\n",
       "                      [ 0.14781941],\n",
       "                      [-0.07271606],\n",
       "                      [-0.06388979],\n",
       "                      [-0.05172059],\n",
       "                      [-0.02117689],\n",
       "                      [ 0.06027824],\n",
       "                      [-0.0541702 ],\n",
       "                      [-0.01157369],\n",
       "                      [-0.00798251],\n",
       "                      [-0.12198796],\n",
       "                      [ 0.07307635],\n",
       "                      [-0.12422697],\n",
       "                      [ 0.08638006],\n",
       "                      [ 0.0978768 ],\n",
       "                      [ 0.02607286],\n",
       "                      [-0.14177103],\n",
       "                      [ 0.08235665],\n",
       "                      [ 0.07370442],\n",
       "                      [ 0.06465968],\n",
       "                      [ 0.00538969],\n",
       "                      [ 0.08383027],\n",
       "                      [-0.01944938],\n",
       "                      [ 0.02557069],\n",
       "                      [ 0.0087074 ],\n",
       "                      [ 0.11571376],\n",
       "                      [-0.03428312],\n",
       "                      [ 0.10725789],\n",
       "                      [ 0.02445868],\n",
       "                      [-0.13855857],\n",
       "                      [ 0.04401699],\n",
       "                      [ 0.01111078],\n",
       "                      [-0.0395001 ],\n",
       "                      [-0.01294772],\n",
       "                      [ 0.11819316],\n",
       "                      [-0.05467616],\n",
       "                      [-0.13189757],\n",
       "                      [-0.14289622],\n",
       "                      [-0.13592492],\n",
       "                      [-0.0454865 ],\n",
       "                      [-0.13123415],\n",
       "                      [ 0.09844555],\n",
       "                      [ 0.10862075],\n",
       "                      [ 0.06253597],\n",
       "                      [ 0.08996575],\n",
       "                      [ 0.14991729],\n",
       "                      [ 0.02788197],\n",
       "                      [-0.07358956],\n",
       "                      [ 0.05166209],\n",
       "                      [-0.11641212],\n",
       "                      [ 0.06762728],\n",
       "                      [-0.02714187],\n",
       "                      [ 0.06087811],\n",
       "                      [-0.02521098],\n",
       "                      [ 0.1171235 ],\n",
       "                      [-0.13432954],\n",
       "                      [ 0.05652049],\n",
       "                      [-0.06353092],\n",
       "                      [ 0.07900646],\n",
       "                      [-0.02669679],\n",
       "                      [ 0.03682992],\n",
       "                      [ 0.09341653],\n",
       "                      [ 0.13058643],\n",
       "                      [-0.03733304],\n",
       "                      [-0.05648512],\n",
       "                      [-0.07300309],\n",
       "                      [-0.04313581],\n",
       "                      [-0.11949271],\n",
       "                      [-0.13057888],\n",
       "                      [-0.00672822],\n",
       "                      [-0.10006212],\n",
       "                      [-0.10557538],\n",
       "                      [ 0.05671969],\n",
       "                      [-0.04394472],\n",
       "                      [-0.03500551],\n",
       "                      [-0.00289021],\n",
       "                      [ 0.11170314],\n",
       "                      [-0.04900552],\n",
       "                      [ 0.06187642],\n",
       "                      [-0.04011116],\n",
       "                      [-0.02146684],\n",
       "                      [ 0.12935309],\n",
       "                      [-0.09848547],\n",
       "                      [ 0.04009801],\n",
       "                      [ 0.0138759 ],\n",
       "                      [ 0.08726127],\n",
       "                      [ 0.03161883],\n",
       "                      [ 0.09923376],\n",
       "                      [ 0.00072013],\n",
       "                      [ 0.09140994],\n",
       "                      [-0.13970987],\n",
       "                      [ 0.01300007],\n",
       "                      [-0.06990825],\n",
       "                      [ 0.09837507],\n",
       "                      [ 0.14046322],\n",
       "                      [-0.10122126],\n",
       "                      [-0.08126278],\n",
       "                      [-0.07426012],\n",
       "                      [ 0.09743701],\n",
       "                      [ 0.00926729],\n",
       "                      [-0.0527491 ],\n",
       "                      [ 0.04197022],\n",
       "                      [-0.14890131],\n",
       "                      [ 0.02243857],\n",
       "                      [ 0.06816435],\n",
       "                      [ 0.05572408],\n",
       "                      [ 0.09472409],\n",
       "                      [-0.11429776],\n",
       "                      [-0.09781666],\n",
       "                      [-0.14946873],\n",
       "                      [ 0.01669899],\n",
       "                      [ 0.08269648],\n",
       "                      [-0.03796097],\n",
       "                      [ 0.12168349],\n",
       "                      [-0.0309724 ],\n",
       "                      [ 0.04645839],\n",
       "                      [-0.00918382],\n",
       "                      [-0.06100566],\n",
       "                      [-0.08892646],\n",
       "                      [-0.12552539],\n",
       "                      [-0.08283007],\n",
       "                      [ 0.09185755],\n",
       "                      [ 0.13664939],\n",
       "                      [ 0.01021977],\n",
       "                      [ 0.00228462],\n",
       "                      [-0.01991594],\n",
       "                      [ 0.02825911],\n",
       "                      [-0.0702002 ],\n",
       "                      [-0.13491023],\n",
       "                      [ 0.02521695],\n",
       "                      [-0.1360671 ],\n",
       "                      [ 0.14711119],\n",
       "                      [-0.11497564],\n",
       "                      [ 0.09474653],\n",
       "                      [-0.02479295],\n",
       "                      [-0.00384386],\n",
       "                      [ 0.12838255],\n",
       "                      [ 0.04543911],\n",
       "                      [ 0.0822154 ],\n",
       "                      [-0.05232287],\n",
       "                      [ 0.04893865],\n",
       "                      [-0.07844204],\n",
       "                      [ 0.05346897],\n",
       "                      [-0.1351488 ],\n",
       "                      [ 0.00525527],\n",
       "                      [-0.12339734],\n",
       "                      [-0.05226801],\n",
       "                      [-0.04466903],\n",
       "                      [-0.05515116],\n",
       "                      [ 0.02478382],\n",
       "                      [ 0.07509066],\n",
       "                      [-0.10802852],\n",
       "                      [-0.04432412],\n",
       "                      [ 0.04644389],\n",
       "                      [-0.03402487],\n",
       "                      [ 0.05452287],\n",
       "                      [ 0.00686857],\n",
       "                      [ 0.04726425],\n",
       "                      [-0.0553572 ],\n",
       "                      [-0.03697283],\n",
       "                      [-0.10044546],\n",
       "                      [ 0.06693541],\n",
       "                      [ 0.14890029],\n",
       "                      [-0.04074215],\n",
       "                      [ 0.11055617],\n",
       "                      [-0.09629411],\n",
       "                      [-0.02080993],\n",
       "                      [ 0.05929068],\n",
       "                      [ 0.03112547],\n",
       "                      [-0.04968788],\n",
       "                      [ 0.10259114],\n",
       "                      [ 0.04434991],\n",
       "                      [ 0.07036872],\n",
       "                      [ 0.08971231],\n",
       "                      [-0.10267818],\n",
       "                      [ 0.09060478],\n",
       "                      [ 0.03836694],\n",
       "                      [-0.04062667],\n",
       "                      [ 0.10091989],\n",
       "                      [ 0.04778525],\n",
       "                      [ 0.01362039],\n",
       "                      [-0.00056049],\n",
       "                      [ 0.05410816],\n",
       "                      [ 0.02872126],\n",
       "                      [-0.04712924],\n",
       "                      [ 0.04368684],\n",
       "                      [ 0.05363294],\n",
       "                      [-0.08225708],\n",
       "                      [-0.0515364 ],\n",
       "                      [ 0.09113723],\n",
       "                      [-0.15267633],\n",
       "                      [-0.12496962],\n",
       "                      [-0.09285957],\n",
       "                      [-0.11277637],\n",
       "                      [ 0.00187883]], dtype=float32)),\n",
       "              ('default_policy/dense_2/bias', array([0.], dtype=float32))])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "algo = DQNConfig().environment(env=\"CartPole-v1\").build()\n",
    "print(algo)\n",
    "# Get weights of the default local policy\n",
    "algo.get_policy().get_weights()\n",
    "\n",
    "# Same as above\n",
    "algo.workers.local_worker().policy_map[\"default_policy\"].get_weights()\n",
    "\n",
    "# Get list of weights of each worker, including remote replicas\n",
    "algo.workers.foreach_worker(lambda worker: worker.get_policy().get_weights())\n",
    "\n",
    "# Same as above, but with index.\n",
    "algo.workers.foreach_worker_with_id(\n",
    "    lambda _id, worker: worker.get_policy().get_weights()\n",
    ")\n",
    "\n",
    "#env_obj.env.n_agents"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34ef9c33",
   "metadata": {},
   "source": [
    "#episode reward mean is 4.64 and policy loss is 1.6 under prior precision 20\n",
    "#policy loss increased to 1.65 and epsidoe reward mean increased to 4.79 under prior precison 0.0000000001\n",
    "#under prior precision 2 episdoe reward mean is 5.05 and loss is 1.7. second trial of same prior precison yielded 5.15 reward mean and 1.8 loss \n",
    "#trial 3 under prior precision 2 loss is 1.73 and episdoe reward mean is 5.04. trial 4 loss is 1.65 and epsidoe reward mean and reward mean 4.79\n",
    "#trial 5 is 1.43 policy loss and reward of 4.18 for prior precision 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80c1aaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 16:00:06,065\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-02 16:00:47</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:37.66        </td></tr>\n",
       "<tr><td>Memory:      </td><td>44.0/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/16.58 GiB heap, 0.0/8.29 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>AIRQMix_317a6_00000</td><td style=\"text-align: right;\">           1</td><td>C:\\Users\\subar\\ray_results\\AIRQMix_2023-07-02_16-00-09\\AIRQMix_317a6_00000_0_2023-07-02_16-00-09\\error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>AIRQMix_317a6_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=19064)\u001b[0m \n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:22,145\tWARNING algorithm_config.py:596 -- Cannot create QMixConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:22,622\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m Stack (most recent call first):\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 1173 in create_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 565 in module_from_spec\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 666 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pyarrow\\__init__.py\", line 65 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\"Windows fatal exception: code 0x, line c0000139850\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m  in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\air\\util\\data_batch_conversion.py\", line 16 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\data\\dataset.py\", line 32 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\data\\__init__.py\", line 3 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 972 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\air\\config.py\", line 18 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\air\\__init__.py\", line 2 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1058 in _handle_fromlist\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\test_utils.py\", line 27 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\__init__.py\", line 29 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 972 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\env\\base_env.py\", line 6 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\env\\__init__.py\", line 1 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 972 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\__init__.py\", line 7 in <module>\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 850 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 680 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 986 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 972 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 228 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 972 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1007 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 625 in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 522 in load_actor_class\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\worker.py\", line 772 in main_loop\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 226 in <module>\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m   fn()\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=32272)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=31648)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=5968)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=27856)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=23764)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=47712)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=52808)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=32540)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\flax\\struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "\u001b[2m\u001b[36m(pid=52052)\u001b[0m   jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m 2023-07-02 16:00:46,651\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m 2023-07-02 16:00:46,663\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=31648, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001BBB629F820>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31648)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m 2023-07-02 16:00:46,674\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5968, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000141367A9B20>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5968)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m 2023-07-02 16:00:46,712\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=27856, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000285B67A8B20>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27856)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m 2023-07-02 16:00:46,788\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=32272, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C89D61AC70>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32272)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m 2023-07-02 16:00:46,843\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=23764, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002459D61BC40>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23764)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m 2023-07-02 16:00:46,877\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=32540, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000018A1D61ACA0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32540)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m 2023-07-02 16:00:46,838\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=28596, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023C21DF9CA0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28596)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,986\tERROR actor_manager.py:496 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=31648, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001BBB629F820>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,987\tERROR actor_manager.py:496 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=32540, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000018A1D61ACA0>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,988\tERROR actor_manager.py:496 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=32272, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C89D61AC70>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,989\tERROR actor_manager.py:496 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5968, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000141367A9B20>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,990\tERROR actor_manager.py:496 -- Ray error, taking actor 5 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=47712, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002521D61BBB0>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,991\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=27856, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000285B67A8B20>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,992\tERROR actor_manager.py:496 -- Ray error, taking actor 7 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=23764, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002459D61BC40>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,993\tERROR actor_manager.py:496 -- Ray error, taking actor 8 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=28596, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023C21DF9CA0>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,994\tERROR actor_manager.py:496 -- Ray error, taking actor 9 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=52808, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001971D61E8E0>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:46,994\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=52052, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001A19D61BA90>)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m 2023-07-02 16:00:47,009\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::AIRRLTrainer.__init__()\u001b[39m (pid=19064, ip=127.0.0.1, repr=AIRQMix)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\train\\rl\\rl_trainer.py\", line 209, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     super(AIRRLTrainer, self).__init__(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 445, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 169, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 571, in setup\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 192, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise e.args[0].args[2]\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 850, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 902, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(AIRQMix pid=19064)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m 2023-07-02 16:00:46,914\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=47712, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002521D61BBB0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47712)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m 2023-07-02 16:00:46,942\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=52808, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001971D61E8E0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52808)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m 2023-07-02 16:00:46,973\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=52052, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001A19D61BA90>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52052)\u001b[0m ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-07-02 16:00:47,061\tERROR trial_runner.py:1062 -- Trial AIRQMix_317a6_00000: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\execution\\ray_trial_executor.py\", line 1276, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\worker.py\", line 2382, in get\n",
      "    raise value\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1166, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1072, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 805, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 972, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 621, in ray._raylet.store_task_errors\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::AIRRLTrainer.__init__()\u001b[39m (pid=19064, ip=127.0.0.1, repr=AIRQMix)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\train\\rl\\rl_trainer.py\", line 209, in __init__\n",
      "    super(AIRRLTrainer, self).__init__(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 445, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 169, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 571, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 192, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "  File \"python\\ray\\_raylet.pyx\", line 850, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 902, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n",
      "    self._build_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>AIRQMix_317a6_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 16:00:47,140\tERROR tune.py:794 -- Trials did not complete: [AIRQMix_317a6_00000]\n",
      "2023-07-02 16:00:47,141\tINFO tune.py:798 -- Total run time: 37.72 seconds (37.66 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m~\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\train\\base_trainer.py:368\u001b[1;36m in \u001b[1;35mfit\u001b[1;36m\n\u001b[1;33m    raise result.error\u001b[1;36m\n",
      "\u001b[1;31mTuneError\u001b[0m\u001b[1;31m:\u001b[0m Failure # 1 (occurred at 2023-07-02_16-00-47)\nTraceback (most recent call last):\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\execution\\ray_trial_executor.py\", line 1276, in get_next_executor_event\n    future_result = ray.get(ready_future)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\worker.py\", line 2382, in get\n    raise value\n  File \"python\\ray\\_raylet.pyx\", line 1166, in ray._raylet.task_execution_handler\n  File \"python\\ray\\_raylet.pyx\", line 1072, in ray._raylet.execute_task_with_cancellation_handler\n  File \"python\\ray\\_raylet.pyx\", line 805, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 972, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 621, in ray._raylet.store_task_errors\nray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::AIRRLTrainer.__init__()\u001b[39m (pid=19064, ip=127.0.0.1, repr=AIRQMix)\n  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\train\\rl\\rl_trainer.py\", line 209, in __init__\n    super(AIRRLTrainer, self).__init__(\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 445, in __init__\n    super().__init__(\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 169, in __init__\n    self.setup(copy.deepcopy(self.config))\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 571, in setup\n    self.workers = WorkerSet(\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 192, in __init__\n    raise e.args[0].args[2]\n  File \"python\\ray\\_raylet.pyx\", line 850, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 902, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 737, in __init__\n    self._build_policy_map(policy_dict=self.policy_dict)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1984, in _build_policy_map\n    new_policy = create_policy_for_framework(\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 139, in create_policy_for_framework\n    return policy_class(observation_space, action_space, merged_config)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 176, in __init__\n    _validate(obs_space, action_space)\n  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 582, in _validate\n    raise ValueError(\nValueError: Obs space must be a Tuple, got Tuple(Dict('obs': MultiDiscrete([2 2 2 3]), 'state': MultiDiscrete([2 2 2]))). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n\n\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[0;32mIn[40], line 171\u001b[0m\n    trainer = RLTrainer(\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\train\\base_trainer.py:370\u001b[1;36m in \u001b[1;35mfit\u001b[1;36m\n\u001b[1;33m    raise TrainingFailedError from e\u001b[1;36m\n",
      "\u001b[1;31mTrainingFailedError\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.train.rl import RLTrainer\n",
    "from gymnasium.spaces import Dict, Discrete, MultiDiscrete, Tuple,Box\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"PG\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=16)\n",
    "#parser.add_argument(\"--num-gpus\", type=int, default=1)\n",
    "parser.add_argument(\"--num-gpus\", type=float, default=0)#0.25)\n",
    "\n",
    "parser.add_argument(\"--num-workers\", type=int, default=6)\n",
    "parser.add_argument(\"--num-gpus-per-worker\", type=float, default=0.0)\n",
    "#parser.add_argument(\"render_mode\", type=int, default=1)\n",
    "parser.add_argument(\n",
    "    \"--mixer\",\n",
    "    type=str,\n",
    "    default=\"qmix\",\n",
    "    choices=[\"qmix\", \"vdn\", \"none\"],\n",
    "    help=\"The mixer model to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=70000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=9.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "            #if agent_id.startswith(\"low_level_\"):\n",
    "                #return \"low_level_policy\"\n",
    "            #else:\n",
    "                #return \"high_level_policy\"\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ray.init(num_cpus=16, num_gpus=1,local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0],\n",
    "        \"group_2\": [0,1],#2\n",
    "        #\"group_3\": [0]\n",
    "    }\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    from ray.tune import register_env\n",
    "    from ray.rllib.algorithms.dqn import DQN \n",
    "    YourExternalEnv = ... \n",
    "    register_env(\"my_env\", \n",
    "        lambda config: YourExternalEnv(config))\n",
    "    trainer = DQN(env=\"my_env\") \n",
    "    while True: \n",
    "        print(trainer.train()) \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(TwoStepGame)#\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=args.num_gpus,num_gpus_per_worker=args.num_gpus_per_worker,)\n",
    "        #.reset_config(reuse_actors=True)\n",
    "    )\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=observation_space, act_space=action_space\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "        #if agent_id.startswith(\"low_level_\"):\n",
    "        if agent_id.startswith(\"group_1\"):\n",
    "            return \"low_level_policy\"\n",
    "        else:\n",
    "            return \"high_level_policy\"\n",
    "\n",
    "    if args.run == \"QMIX\":\n",
    "        \n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "\n",
    "            .training(mixer=args.mixer, train_batch_size=32,replay_buffer_config={\"type\": \"MultiAgentReplayBuffer\",\"underlying_replay_buffer_config\":{\"type\":MultiAgentPrioritizedReplayBuffer}})\n",
    "            .resources(\n",
    "                # How many GPUs does the local worker (driver) need? For most algos,\n",
    "                # this is where the learning updates happen.\n",
    "                # Set this to > 1 for multi-GPU learning.\n",
    "                num_gpus=args.num_gpus,\n",
    "                # How many GPUs does each RolloutWorker (`num_workers`) need?\n",
    "                num_gpus_per_worker=args.num_gpus_per_worker,#0.25,\n",
    "            )           \n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        observation_space,\n",
    "                        action_space,\n",
    "                        config.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        #Tuple([observation_space,Box(-inf, inf, (1,), float64)]),#,Box()\n",
    "                        Tuple([observation_space,obs_space]),\n",
    "                        Tuple([action_space,obs_space]),#action_space,\n",
    "                        config.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn#lambda agent_id, episode, worker, **kwargs: \"pol2\"\n",
    "                #policy_mapping_fn=(lambda agent_id, episode, worker, **kw: (\"pol1\" if agent_id == \"agent1\" else \"pol2\")\n",
    "    #)#policy_mapping_fn,\n",
    "            )\n",
    "            .rollouts(num_rollout_workers=args.num_workers, num_envs_per_worker=args.num_envs_per_worker,)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "                }\n",
    "            )\n",
    "            .environment(\n",
    "                env=\"grouped_twostep\",\n",
    "                env_config={\n",
    "                    \"separate_state_space\": True,\n",
    "                    \"one_hot_state_encoding\": True,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr='training_iteration',\n",
    "        metric='episode_reward_mean',#'loss',\n",
    "        mode='min',\n",
    "        perturbation_interval=1,\n",
    "        hyperparam_mutations={\n",
    "            #\"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"alpha\": tune.uniform(0.0, 1.0),\n",
    "        }\n",
    "    )\n",
    "    trainer = RLTrainer(\n",
    "        \"QMIX\",#args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),#RunConfig(stop={\"training_iteration\": 5}),\n",
    "        scaling_config=ScalingConfig(num_workers=10,use_gpu=True),\n",
    "        #algorithm=\"QMIX\",\n",
    "        config=config.to_dict()\n",
    "        \n",
    "    ).fit()\n",
    "    traine = RLTrainer(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),#RunConfig(stop={\"training_iteration\": 5}),\n",
    "        scaling_config=ScalingConfig(num_workers=10,use_gpu=True),\n",
    "        #algorithm=\"QMIX\",\n",
    "        config=config.to_dict()\n",
    "        \n",
    "    )\n",
    "    #con = config.to_dict()\n",
    "    dense_logs = {}\n",
    "    num_dense_logs=1\n",
    "    args.run.workers.foreach_worker(lambda worker: worker.get_policy().get_weights())\n",
    "    #import rllib\n",
    "    #conf = config.build()\n",
    "    #env_obj = env\n",
    "    #env = config#TwoStepGame(config)\n",
    "    #env_obj = con.build()#RLlibEnvWrapper({\"env_config_dict\": con}, verbose=True)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    algo.get_policy(\"high_level_policy\").get_weights()\n",
    "    #QMixConfig()\n",
    "    # Same as above\n",
    "    \n",
    "    # Same as above\n",
    "    algo.workers.local_worker().policy_map[\"high_level_policy\"].get_weights()\n",
    "\n",
    "    # Get list of weights of each worker, including remote replicas\n",
    "    algo.workers.foreach_worker(lambda worker: worker.get_policy().get_weights())\n",
    "\n",
    "    # Same as above, but with index.\n",
    "    algo.workers.foreach_worker_with_id(\n",
    "        lambda _id, worker: worker.get_policy().get_weights()\n",
    "    )\n",
    "    \n",
    "    algo.workers.local_worker().policy_map[\"low_level_policy\"].get_weights()\n",
    "\n",
    "    # Get list of weights of each worker, including remote replicas\n",
    "    algo.workers.foreach_worker(lambda worker: worker.get_policy().get_weights())\n",
    "\n",
    "    # Same as above, but with index.\n",
    "    algo.workers.foreach_worker_with_id(\n",
    "        lambda _id, worker: worker.get_policy().get_weights()\n",
    "    )\n",
    "    \n",
    "    # Get weights of the default local policy\n",
    "    algo.get_policy().get_weights()\n",
    "\n",
    "    # Same as above\n",
    "    algo.workers.local_worker().policy_map[\"default_policy\"].get_weights()\n",
    "\n",
    "    # Get list of weights of each worker, including remote replicas\n",
    "    algo.workers.foreach_worker(lambda worker: worker.get_policy().get_weights())\n",
    "\n",
    "    # Same as above, but with index.\n",
    "    algo.workers.foreach_worker_with_id(\n",
    "        lambda _id, worker: worker.get_policy().get_weights()\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    alg = args.run.build()\n",
    "    dense_logs = generate_rollout_from_current_trainer_policy(\n",
    "    traine, \n",
    "    env_obj=alg,\n",
    "    num_dense_logs=2)\n",
    "\n",
    "    #result = trainer.fit().compute_actions_from_input_dict(_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "160cc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc492abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 15:37:20,057\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.pg.pg.PG'>\n",
      "<ray.rllib.algorithms.pg.pg.PGConfig object at 0x000001984BA86D60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 15:37:25,038\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "2023-07-02 15:37:29,196\tWARNING worker.py:846 -- `ray.get_gpu_ids()` will always return the empty list when called from the driver. This is because Ray does not manage GPU allocations to the driver process.\n",
      "2023-07-02 15:37:29,198\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-07-02 15:37:29,202\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property worker_index not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'one_hot.0._hidden_layers.0._model.0.weight': array([[ 0.05115514, -0.19302343,  0.8061472 , -0.41678998, -0.11491208,\n",
       "           0.35118827],\n",
       "         [ 0.4665562 ,  0.00163631,  0.30534065,  0.11895028, -0.4912515 ,\n",
       "           0.6584927 ],\n",
       "         [ 0.60043055, -0.01314684,  0.46437773,  0.4914663 , -0.4243539 ,\n",
       "           0.0452592 ],\n",
       "         ...,\n",
       "         [-0.19543499,  0.71752524,  0.25108176, -0.35281   ,  0.501591  ,\n",
       "          -0.08861205],\n",
       "         [ 0.46107793, -0.730963  ,  0.11883523, -0.06139861,  0.44397885,\n",
       "          -0.19517028],\n",
       "         [ 0.11252671, -0.42771512,  0.40449834, -0.5652666 , -0.35822108,\n",
       "          -0.43923792]], dtype=float32),\n",
       "  'one_hot.0._hidden_layers.0._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.], dtype=float32),\n",
       "  'one_hot.0._hidden_layers.1._model.0.weight': array([[-0.04992994,  0.02127983, -0.01890656, ..., -0.00550072,\n",
       "          -0.03838048, -0.0215592 ],\n",
       "         [-0.01938509, -0.06183284,  0.05517624, ...,  0.06542704,\n",
       "           0.15950604, -0.00767697],\n",
       "         [-0.04981415,  0.00948162,  0.03768278, ...,  0.0842066 ,\n",
       "           0.07852344, -0.10329466],\n",
       "         ...,\n",
       "         [-0.018205  ,  0.04932391,  0.01706438, ..., -0.03674204,\n",
       "           0.15746154, -0.04167246],\n",
       "         [-0.06029639,  0.00838946, -0.07955825, ...,  0.08304399,\n",
       "           0.0541203 , -0.11378158],\n",
       "         [-0.08103073, -0.04448324, -0.02201165, ...,  0.05476978,\n",
       "           0.06229812, -0.14481919]], dtype=float32),\n",
       "  'one_hot.0._hidden_layers.1._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.], dtype=float32),\n",
       "  'one_hot.0._value_branch_separate.0._model.0.weight': array([[-0.2950255 ,  0.15830374, -0.264353  ,  0.13442013, -0.8055879 ,\n",
       "          -0.38855746],\n",
       "         [-0.8155204 , -0.29412085, -0.03583084,  0.22346012, -0.09256516,\n",
       "           0.4343188 ],\n",
       "         [-0.85017514,  0.14439245, -0.04136632,  0.47409207, -0.07254725,\n",
       "           0.15689325],\n",
       "         ...,\n",
       "         [-0.06383611, -0.5061859 , -0.67969495,  0.19320945, -0.48982403,\n",
       "           0.02139984],\n",
       "         [-0.01842592,  0.00704473,  0.109889  ,  0.08473387,  0.6025418 ,\n",
       "           0.7856835 ],\n",
       "         [ 0.8059029 , -0.22727615,  0.1754056 , -0.2563134 , -0.31053364,\n",
       "          -0.32553226]], dtype=float32),\n",
       "  'one_hot.0._value_branch_separate.0._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.], dtype=float32),\n",
       "  'one_hot.0._value_branch_separate.1._model.0.weight': array([[ 0.06352809, -0.1086102 ,  0.00350296, ...,  0.04634377,\n",
       "           0.01853706,  0.05271982],\n",
       "         [-0.08281002,  0.10553642, -0.1085325 , ...,  0.04488949,\n",
       "           0.0524741 , -0.06140584],\n",
       "         [-0.0096757 ,  0.02166214, -0.06009958, ..., -0.08213239,\n",
       "          -0.00568958, -0.01203149],\n",
       "         ...,\n",
       "         [ 0.16542189, -0.15956396,  0.00534589, ..., -0.00566625,\n",
       "          -0.04973083, -0.03704785],\n",
       "         [-0.00624697,  0.06103949,  0.03708639, ..., -0.05503263,\n",
       "           0.01674161, -0.05666687],\n",
       "         [ 0.01035409, -0.00861324, -0.08093411, ...,  0.0363842 ,\n",
       "           0.15225793,  0.01505549]], dtype=float32),\n",
       "  'one_hot.0._value_branch_separate.1._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.], dtype=float32),\n",
       "  'one_hot.0._value_branch._model.0.weight': array([[-4.8895807e-05, -3.5177488e-04,  6.6772358e-05,  6.9807406e-04,\n",
       "          -2.0498730e-04,  1.1727393e-03, -2.8358866e-04, -1.1103851e-03,\n",
       "           7.7981479e-04,  3.8788246e-05, -5.5800978e-04,  1.7869119e-04,\n",
       "          -6.2718216e-05, -4.3515119e-04,  7.0857327e-04, -9.4988703e-05,\n",
       "          -5.6204030e-05, -5.7894440e-04,  7.3808391e-05, -1.2849123e-04,\n",
       "          -7.9848367e-04,  2.2528680e-04, -2.3213675e-04,  3.2272056e-04,\n",
       "           3.6536856e-04, -9.1502676e-04,  2.5292538e-04, -4.4132443e-04,\n",
       "           6.8541260e-05,  4.4855729e-04,  2.0162755e-05,  1.7902981e-04,\n",
       "           1.7937744e-04,  2.2937379e-04, -5.3307996e-04, -8.8238268e-04,\n",
       "           2.9423594e-04, -1.5202482e-04, -7.4732519e-04, -9.7990478e-04,\n",
       "          -9.5060433e-04, -1.4633780e-04, -1.0733397e-04,  5.1599316e-04,\n",
       "          -3.0881100e-04,  3.0688886e-04, -4.7459800e-04, -1.5576175e-05,\n",
       "           1.4853697e-04,  5.3190242e-04, -8.5038511e-05, -7.0194941e-04,\n",
       "          -4.3341139e-04, -2.3433106e-04,  1.7611534e-04,  4.8595262e-04,\n",
       "          -3.2667950e-04,  2.8800985e-04,  2.3391622e-04, -5.4971984e-04,\n",
       "           4.8979407e-04, -1.0403803e-03, -1.7906763e-04, -3.9120912e-04,\n",
       "          -1.0257512e-04,  1.9654997e-04,  4.2229259e-04,  1.2363575e-03,\n",
       "           5.6144997e-04,  5.1508118e-06, -3.3563771e-04, -6.9267832e-04,\n",
       "          -3.0761477e-04,  7.5331351e-05, -5.1085593e-04,  8.5989496e-04,\n",
       "          -6.8994093e-04,  1.3297761e-03,  6.8416185e-04,  5.0119311e-04,\n",
       "          -1.5746801e-03,  2.5936219e-04,  7.6259236e-04,  5.6243793e-04,\n",
       "           7.0340355e-04, -1.4746623e-04,  5.7827064e-04, -7.6710852e-04,\n",
       "          -4.4217111e-05, -5.7236780e-04, -6.6321605e-04,  5.8319274e-04,\n",
       "           4.6256490e-04, -1.0396748e-04,  7.3290156e-04,  2.9225700e-04,\n",
       "           9.6017422e-05, -8.7109132e-04,  1.2767226e-04, -2.1903732e-04,\n",
       "          -6.7555474e-04, -5.3696736e-04, -6.7794457e-04, -3.9112638e-04,\n",
       "          -3.5556882e-06, -3.4458391e-04,  6.5321027e-04,  9.3922683e-04,\n",
       "           3.3246551e-04, -2.6023903e-04,  6.9208466e-04,  1.6835495e-04,\n",
       "          -1.1831333e-04,  3.6888878e-04,  5.7086139e-04,  4.1522965e-04,\n",
       "          -2.1956554e-04, -2.5101830e-04,  3.1407081e-04,  8.0189195e-05,\n",
       "           1.2651664e-03,  8.5430460e-05, -1.0873284e-03,  1.0642706e-03,\n",
       "           1.7069537e-03, -5.3020002e-04, -4.2265448e-05, -3.9926218e-04,\n",
       "          -8.7483386e-05,  4.0387598e-04,  6.1612925e-04, -1.5011661e-03,\n",
       "           3.7412954e-04, -4.2003277e-04, -3.3292660e-04, -5.6247314e-04,\n",
       "           5.0877611e-04, -1.6908964e-05, -1.4774996e-03,  3.2284061e-04,\n",
       "          -1.3848393e-04, -2.7019749e-04,  4.9446576e-04, -7.6212035e-04,\n",
       "           3.4723352e-04, -4.3124944e-04, -1.5509578e-03,  8.6716365e-04,\n",
       "           1.6073546e-04,  1.2387490e-04, -1.4151197e-03, -3.6091730e-04,\n",
       "           9.4258873e-04, -5.6361430e-04, -2.5329235e-04,  1.3062309e-03,\n",
       "           7.4799667e-04, -7.0666072e-05,  8.7788055e-04,  7.7103730e-04,\n",
       "          -1.2431302e-03, -7.6671416e-04, -4.2871799e-04, -5.4116237e-05,\n",
       "           8.0984581e-04,  5.6255673e-04,  1.2398389e-04,  8.2245970e-05,\n",
       "           3.0740721e-05,  7.7140523e-04, -2.5705072e-05, -4.2467832e-04,\n",
       "          -6.7989237e-04,  1.2954276e-04,  1.2500708e-03,  1.3175635e-03,\n",
       "          -4.9515078e-05, -4.2710346e-04,  1.7314612e-04, -1.3391415e-05,\n",
       "           5.3201329e-05, -3.6215427e-04, -6.9992838e-04, -2.0052015e-04,\n",
       "           4.1877897e-04,  4.7224047e-04,  6.9961854e-04,  4.4166121e-05,\n",
       "          -5.6253897e-04,  3.5394498e-04,  4.2009252e-04,  1.0720023e-03,\n",
       "          -7.7976077e-04,  2.8536431e-04,  7.8748293e-05,  4.9252488e-04,\n",
       "          -8.3946582e-04,  1.5829813e-04, -1.2823947e-03,  2.0770379e-04,\n",
       "           7.9943164e-04, -9.6225389e-04, -6.7069934e-04, -2.1291400e-04,\n",
       "          -2.2650737e-04,  1.0601695e-03, -1.3008231e-04,  1.2201788e-03,\n",
       "          -1.3779942e-03, -1.7311668e-05,  6.2523806e-04,  1.7535958e-03,\n",
       "           2.0619784e-04, -2.5347120e-04,  4.3765284e-04,  9.0947648e-04,\n",
       "           1.3476653e-03,  4.3877248e-05,  3.2279000e-04, -5.3308211e-04,\n",
       "          -3.4336941e-04,  2.0059739e-04,  5.5082724e-04,  5.1273103e-04,\n",
       "          -6.4167904e-04, -8.8488823e-04, -1.0851255e-03,  3.3379247e-04,\n",
       "          -3.7922637e-04, -3.2772182e-04,  5.7886058e-04, -2.8088866e-04,\n",
       "           1.9192092e-04, -2.0405765e-04, -4.5929174e-04, -7.8517338e-04,\n",
       "          -8.7487925e-04,  5.0274224e-04, -1.6548247e-04, -7.1482657e-04,\n",
       "          -8.2138262e-04,  4.6433113e-05, -1.4729332e-04, -4.5189276e-04,\n",
       "          -1.4492904e-04,  1.4346390e-04, -4.6579659e-04,  4.5030800e-04,\n",
       "          -1.0551599e-04, -1.4040222e-04,  1.7355483e-03, -6.8241864e-04,\n",
       "           5.4505584e-04, -5.5636634e-04,  9.6021371e-04,  2.2205168e-04]],\n",
       "        dtype=float32),\n",
       "  'one_hot.0._value_branch._model.0.bias': array([0.], dtype=float32),\n",
       "  'one_hot_0._hidden_layers.0._model.0.weight': array([[ 0.05115514, -0.19302343,  0.8061472 , -0.41678998, -0.11491208,\n",
       "           0.35118827],\n",
       "         [ 0.4665562 ,  0.00163631,  0.30534065,  0.11895028, -0.4912515 ,\n",
       "           0.6584927 ],\n",
       "         [ 0.60043055, -0.01314684,  0.46437773,  0.4914663 , -0.4243539 ,\n",
       "           0.0452592 ],\n",
       "         ...,\n",
       "         [-0.19543499,  0.71752524,  0.25108176, -0.35281   ,  0.501591  ,\n",
       "          -0.08861205],\n",
       "         [ 0.46107793, -0.730963  ,  0.11883523, -0.06139861,  0.44397885,\n",
       "          -0.19517028],\n",
       "         [ 0.11252671, -0.42771512,  0.40449834, -0.5652666 , -0.35822108,\n",
       "          -0.43923792]], dtype=float32),\n",
       "  'one_hot_0._hidden_layers.0._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.], dtype=float32),\n",
       "  'one_hot_0._hidden_layers.1._model.0.weight': array([[-0.04992994,  0.02127983, -0.01890656, ..., -0.00550072,\n",
       "          -0.03838048, -0.0215592 ],\n",
       "         [-0.01938509, -0.06183284,  0.05517624, ...,  0.06542704,\n",
       "           0.15950604, -0.00767697],\n",
       "         [-0.04981415,  0.00948162,  0.03768278, ...,  0.0842066 ,\n",
       "           0.07852344, -0.10329466],\n",
       "         ...,\n",
       "         [-0.018205  ,  0.04932391,  0.01706438, ..., -0.03674204,\n",
       "           0.15746154, -0.04167246],\n",
       "         [-0.06029639,  0.00838946, -0.07955825, ...,  0.08304399,\n",
       "           0.0541203 , -0.11378158],\n",
       "         [-0.08103073, -0.04448324, -0.02201165, ...,  0.05476978,\n",
       "           0.06229812, -0.14481919]], dtype=float32),\n",
       "  'one_hot_0._hidden_layers.1._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.], dtype=float32),\n",
       "  'one_hot_0._value_branch_separate.0._model.0.weight': array([[-0.2950255 ,  0.15830374, -0.264353  ,  0.13442013, -0.8055879 ,\n",
       "          -0.38855746],\n",
       "         [-0.8155204 , -0.29412085, -0.03583084,  0.22346012, -0.09256516,\n",
       "           0.4343188 ],\n",
       "         [-0.85017514,  0.14439245, -0.04136632,  0.47409207, -0.07254725,\n",
       "           0.15689325],\n",
       "         ...,\n",
       "         [-0.06383611, -0.5061859 , -0.67969495,  0.19320945, -0.48982403,\n",
       "           0.02139984],\n",
       "         [-0.01842592,  0.00704473,  0.109889  ,  0.08473387,  0.6025418 ,\n",
       "           0.7856835 ],\n",
       "         [ 0.8059029 , -0.22727615,  0.1754056 , -0.2563134 , -0.31053364,\n",
       "          -0.32553226]], dtype=float32),\n",
       "  'one_hot_0._value_branch_separate.0._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.], dtype=float32),\n",
       "  'one_hot_0._value_branch_separate.1._model.0.weight': array([[ 0.06352809, -0.1086102 ,  0.00350296, ...,  0.04634377,\n",
       "           0.01853706,  0.05271982],\n",
       "         [-0.08281002,  0.10553642, -0.1085325 , ...,  0.04488949,\n",
       "           0.0524741 , -0.06140584],\n",
       "         [-0.0096757 ,  0.02166214, -0.06009958, ..., -0.08213239,\n",
       "          -0.00568958, -0.01203149],\n",
       "         ...,\n",
       "         [ 0.16542189, -0.15956396,  0.00534589, ..., -0.00566625,\n",
       "          -0.04973083, -0.03704785],\n",
       "         [-0.00624697,  0.06103949,  0.03708639, ..., -0.05503263,\n",
       "           0.01674161, -0.05666687],\n",
       "         [ 0.01035409, -0.00861324, -0.08093411, ...,  0.0363842 ,\n",
       "           0.15225793,  0.01505549]], dtype=float32),\n",
       "  'one_hot_0._value_branch_separate.1._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.], dtype=float32),\n",
       "  'one_hot_0._value_branch._model.0.weight': array([[-4.8895807e-05, -3.5177488e-04,  6.6772358e-05,  6.9807406e-04,\n",
       "          -2.0498730e-04,  1.1727393e-03, -2.8358866e-04, -1.1103851e-03,\n",
       "           7.7981479e-04,  3.8788246e-05, -5.5800978e-04,  1.7869119e-04,\n",
       "          -6.2718216e-05, -4.3515119e-04,  7.0857327e-04, -9.4988703e-05,\n",
       "          -5.6204030e-05, -5.7894440e-04,  7.3808391e-05, -1.2849123e-04,\n",
       "          -7.9848367e-04,  2.2528680e-04, -2.3213675e-04,  3.2272056e-04,\n",
       "           3.6536856e-04, -9.1502676e-04,  2.5292538e-04, -4.4132443e-04,\n",
       "           6.8541260e-05,  4.4855729e-04,  2.0162755e-05,  1.7902981e-04,\n",
       "           1.7937744e-04,  2.2937379e-04, -5.3307996e-04, -8.8238268e-04,\n",
       "           2.9423594e-04, -1.5202482e-04, -7.4732519e-04, -9.7990478e-04,\n",
       "          -9.5060433e-04, -1.4633780e-04, -1.0733397e-04,  5.1599316e-04,\n",
       "          -3.0881100e-04,  3.0688886e-04, -4.7459800e-04, -1.5576175e-05,\n",
       "           1.4853697e-04,  5.3190242e-04, -8.5038511e-05, -7.0194941e-04,\n",
       "          -4.3341139e-04, -2.3433106e-04,  1.7611534e-04,  4.8595262e-04,\n",
       "          -3.2667950e-04,  2.8800985e-04,  2.3391622e-04, -5.4971984e-04,\n",
       "           4.8979407e-04, -1.0403803e-03, -1.7906763e-04, -3.9120912e-04,\n",
       "          -1.0257512e-04,  1.9654997e-04,  4.2229259e-04,  1.2363575e-03,\n",
       "           5.6144997e-04,  5.1508118e-06, -3.3563771e-04, -6.9267832e-04,\n",
       "          -3.0761477e-04,  7.5331351e-05, -5.1085593e-04,  8.5989496e-04,\n",
       "          -6.8994093e-04,  1.3297761e-03,  6.8416185e-04,  5.0119311e-04,\n",
       "          -1.5746801e-03,  2.5936219e-04,  7.6259236e-04,  5.6243793e-04,\n",
       "           7.0340355e-04, -1.4746623e-04,  5.7827064e-04, -7.6710852e-04,\n",
       "          -4.4217111e-05, -5.7236780e-04, -6.6321605e-04,  5.8319274e-04,\n",
       "           4.6256490e-04, -1.0396748e-04,  7.3290156e-04,  2.9225700e-04,\n",
       "           9.6017422e-05, -8.7109132e-04,  1.2767226e-04, -2.1903732e-04,\n",
       "          -6.7555474e-04, -5.3696736e-04, -6.7794457e-04, -3.9112638e-04,\n",
       "          -3.5556882e-06, -3.4458391e-04,  6.5321027e-04,  9.3922683e-04,\n",
       "           3.3246551e-04, -2.6023903e-04,  6.9208466e-04,  1.6835495e-04,\n",
       "          -1.1831333e-04,  3.6888878e-04,  5.7086139e-04,  4.1522965e-04,\n",
       "          -2.1956554e-04, -2.5101830e-04,  3.1407081e-04,  8.0189195e-05,\n",
       "           1.2651664e-03,  8.5430460e-05, -1.0873284e-03,  1.0642706e-03,\n",
       "           1.7069537e-03, -5.3020002e-04, -4.2265448e-05, -3.9926218e-04,\n",
       "          -8.7483386e-05,  4.0387598e-04,  6.1612925e-04, -1.5011661e-03,\n",
       "           3.7412954e-04, -4.2003277e-04, -3.3292660e-04, -5.6247314e-04,\n",
       "           5.0877611e-04, -1.6908964e-05, -1.4774996e-03,  3.2284061e-04,\n",
       "          -1.3848393e-04, -2.7019749e-04,  4.9446576e-04, -7.6212035e-04,\n",
       "           3.4723352e-04, -4.3124944e-04, -1.5509578e-03,  8.6716365e-04,\n",
       "           1.6073546e-04,  1.2387490e-04, -1.4151197e-03, -3.6091730e-04,\n",
       "           9.4258873e-04, -5.6361430e-04, -2.5329235e-04,  1.3062309e-03,\n",
       "           7.4799667e-04, -7.0666072e-05,  8.7788055e-04,  7.7103730e-04,\n",
       "          -1.2431302e-03, -7.6671416e-04, -4.2871799e-04, -5.4116237e-05,\n",
       "           8.0984581e-04,  5.6255673e-04,  1.2398389e-04,  8.2245970e-05,\n",
       "           3.0740721e-05,  7.7140523e-04, -2.5705072e-05, -4.2467832e-04,\n",
       "          -6.7989237e-04,  1.2954276e-04,  1.2500708e-03,  1.3175635e-03,\n",
       "          -4.9515078e-05, -4.2710346e-04,  1.7314612e-04, -1.3391415e-05,\n",
       "           5.3201329e-05, -3.6215427e-04, -6.9992838e-04, -2.0052015e-04,\n",
       "           4.1877897e-04,  4.7224047e-04,  6.9961854e-04,  4.4166121e-05,\n",
       "          -5.6253897e-04,  3.5394498e-04,  4.2009252e-04,  1.0720023e-03,\n",
       "          -7.7976077e-04,  2.8536431e-04,  7.8748293e-05,  4.9252488e-04,\n",
       "          -8.3946582e-04,  1.5829813e-04, -1.2823947e-03,  2.0770379e-04,\n",
       "           7.9943164e-04, -9.6225389e-04, -6.7069934e-04, -2.1291400e-04,\n",
       "          -2.2650737e-04,  1.0601695e-03, -1.3008231e-04,  1.2201788e-03,\n",
       "          -1.3779942e-03, -1.7311668e-05,  6.2523806e-04,  1.7535958e-03,\n",
       "           2.0619784e-04, -2.5347120e-04,  4.3765284e-04,  9.0947648e-04,\n",
       "           1.3476653e-03,  4.3877248e-05,  3.2279000e-04, -5.3308211e-04,\n",
       "          -3.4336941e-04,  2.0059739e-04,  5.5082724e-04,  5.1273103e-04,\n",
       "          -6.4167904e-04, -8.8488823e-04, -1.0851255e-03,  3.3379247e-04,\n",
       "          -3.7922637e-04, -3.2772182e-04,  5.7886058e-04, -2.8088866e-04,\n",
       "           1.9192092e-04, -2.0405765e-04, -4.5929174e-04, -7.8517338e-04,\n",
       "          -8.7487925e-04,  5.0274224e-04, -1.6548247e-04, -7.1482657e-04,\n",
       "          -8.2138262e-04,  4.6433113e-05, -1.4729332e-04, -4.5189276e-04,\n",
       "          -1.4492904e-04,  1.4346390e-04, -4.6579659e-04,  4.5030800e-04,\n",
       "          -1.0551599e-04, -1.4040222e-04,  1.7355483e-03, -6.8241864e-04,\n",
       "           5.4505584e-04, -5.5636634e-04,  9.6021371e-04,  2.2205168e-04]],\n",
       "        dtype=float32),\n",
       "  'one_hot_0._value_branch._model.0.bias': array([0.], dtype=float32),\n",
       "  'post_fc_stack._value_branch._model.0.weight': array([[-4.74917091e-04,  2.45429663e-04,  6.58076839e-04,\n",
       "          -3.84462212e-04, -1.38583139e-03,  1.45492711e-04,\n",
       "           2.38819761e-04, -7.15325936e-04, -1.48464786e-03,\n",
       "           7.18407100e-04, -1.52128967e-04,  6.56397897e-04,\n",
       "          -5.35050116e-04,  3.07142502e-04, -9.48831905e-04,\n",
       "          -5.42290691e-05, -1.58172916e-04,  1.58298353e-03,\n",
       "          -2.53039296e-04,  5.38477849e-04, -2.08892088e-04,\n",
       "           9.42798040e-04,  2.23015391e-04, -1.40134047e-03,\n",
       "           3.04600835e-04, -2.21026115e-04,  8.66343908e-05,\n",
       "          -1.26009554e-05, -3.20591527e-04,  5.34953550e-04,\n",
       "          -3.56625853e-04,  9.21183499e-04, -9.52548871e-04,\n",
       "           3.63401690e-04,  3.69564194e-04,  5.60439192e-04,\n",
       "          -2.39639863e-04,  2.29559431e-04,  5.93385019e-04,\n",
       "           1.02174439e-04, -1.20469159e-03,  1.58292823e-04,\n",
       "          -7.12092500e-04,  6.02756278e-04,  1.31519977e-03,\n",
       "           1.11242454e-03, -5.60169574e-04,  7.40160816e-04,\n",
       "           5.05191674e-05,  5.88328170e-04, -2.48599419e-04,\n",
       "           1.31514808e-03, -5.22540999e-04,  2.82699126e-04,\n",
       "          -1.96583787e-05,  4.39170050e-04, -4.83523560e-04,\n",
       "          -4.90982144e-04, -1.14798371e-03,  2.91168562e-05,\n",
       "          -3.58299760e-04, -7.52336564e-05,  1.60869455e-03,\n",
       "           1.75552432e-05, -1.85619807e-04, -5.26630087e-04,\n",
       "          -9.12294898e-04, -8.61904002e-04, -7.86348712e-04,\n",
       "           8.24184099e-04,  9.84069309e-04, -9.42189072e-04,\n",
       "          -8.12640472e-04,  1.45227590e-04,  3.29326082e-04,\n",
       "          -4.40489443e-04,  5.37166379e-05,  3.56404576e-04,\n",
       "           6.43717416e-04, -1.85492070e-04,  3.37822567e-05,\n",
       "           5.94635610e-04,  1.83758224e-04, -1.19397453e-04,\n",
       "           8.12789018e-04, -2.54367391e-04,  3.15542304e-04,\n",
       "           6.86495923e-06, -9.06102359e-04, -8.42716603e-04,\n",
       "           9.11518175e-04, -3.11963871e-04,  1.00910990e-03,\n",
       "           1.54132431e-04,  5.49758784e-04, -2.25314347e-04,\n",
       "           7.21372271e-05,  5.26168988e-05, -1.42781777e-04,\n",
       "          -2.00425624e-04, -5.52680169e-04,  1.15063926e-03,\n",
       "          -1.08120992e-04, -1.69099003e-04,  3.96115589e-04,\n",
       "          -9.51649781e-06, -8.96097627e-04, -9.59831523e-04,\n",
       "          -1.25809602e-04, -1.16819958e-03, -1.22987817e-03,\n",
       "          -7.17141898e-04, -2.28336488e-04,  4.74431145e-04,\n",
       "          -3.00696323e-04,  7.69174716e-04, -9.89200125e-05,\n",
       "           9.32810362e-04, -4.23977152e-04, -4.52474414e-05,\n",
       "           1.44486010e-04,  2.82708876e-04, -5.63738613e-05,\n",
       "          -2.47323740e-04,  1.30897781e-04, -8.70422635e-04,\n",
       "           2.74439808e-04,  5.11887076e-04, -6.59864512e-04,\n",
       "           1.27381377e-03, -8.53561622e-04,  5.44225448e-04,\n",
       "           1.55030328e-04, -5.56911109e-04, -2.35300075e-04,\n",
       "           8.99320803e-05, -4.45080863e-04, -2.36849868e-04,\n",
       "           3.03245906e-04,  4.78998700e-04,  8.60072250e-05,\n",
       "          -6.04499131e-04,  4.86088771e-04, -7.88011821e-04,\n",
       "          -9.29118018e-04,  5.50206867e-04, -1.12721336e-03,\n",
       "          -4.15643095e-04,  1.31567824e-03, -2.01694769e-04,\n",
       "          -1.90674837e-04,  6.57396507e-04,  8.98434198e-04,\n",
       "          -1.65004691e-04,  5.94672165e-04, -3.52984731e-04,\n",
       "           9.75354284e-04,  5.90112177e-04,  1.24551391e-03,\n",
       "           5.41300687e-04, -1.56982476e-03, -5.30645077e-04,\n",
       "           4.44743288e-04,  2.67911037e-05, -5.28845179e-04,\n",
       "          -2.86436873e-04, -4.23244164e-05, -6.14503369e-05,\n",
       "          -1.38056403e-05, -4.37455055e-05, -2.89574848e-04,\n",
       "           2.42843846e-04,  6.67240412e-04,  2.61529058e-04,\n",
       "          -8.00199632e-04, -4.77081368e-04, -3.52911418e-04,\n",
       "          -5.96401922e-04, -2.13536186e-04, -1.79597162e-04,\n",
       "           1.15607679e-03,  1.94158129e-04, -1.27913896e-04,\n",
       "           2.71511934e-04,  7.43929762e-04, -2.90737429e-04,\n",
       "           5.36291162e-04,  3.53910640e-04,  1.56206297e-04,\n",
       "          -2.24617805e-04, -5.27758151e-04, -3.52512230e-04,\n",
       "           9.08939750e-04, -2.52484140e-04,  1.00334233e-03,\n",
       "           6.40609302e-04,  1.27469818e-03,  4.96802037e-04,\n",
       "           3.40923580e-04, -1.20776436e-04, -5.41001937e-05,\n",
       "          -2.38046123e-04, -7.81661714e-04,  6.61181111e-05,\n",
       "           7.88981652e-06,  5.27878525e-04,  1.70023748e-04,\n",
       "          -3.21566069e-04, -1.00272882e-03,  3.55117460e-04,\n",
       "          -1.08638150e-03, -6.37965801e-04, -1.16060616e-03,\n",
       "           4.31965484e-04, -5.43359783e-04,  4.75188572e-04,\n",
       "           2.83451867e-04,  8.82642111e-04, -5.78823790e-04,\n",
       "          -4.38023882e-04, -1.76708074e-03, -1.35918352e-04,\n",
       "           8.60400032e-04, -7.70131592e-04,  8.79921077e-04,\n",
       "           1.57100498e-04, -8.26929347e-04,  2.69448239e-04,\n",
       "          -3.67296831e-04,  7.47036131e-04, -1.30789567e-05,\n",
       "          -4.62567768e-05, -5.16461849e-04,  3.55786469e-04,\n",
       "          -8.65458060e-05,  1.10085402e-03, -4.84039541e-04,\n",
       "           7.67171950e-05, -1.01538659e-04,  5.10893355e-04,\n",
       "           7.72197498e-04, -3.98869568e-04, -1.86052697e-04,\n",
       "          -1.05357356e-03, -1.80174102e-04,  3.36345729e-05,\n",
       "           1.92178515e-04, -3.16971506e-04, -7.50248219e-05,\n",
       "           1.53383851e-04,  3.96867195e-04, -7.65991630e-04,\n",
       "           5.53738908e-04, -6.72944472e-04,  1.21249082e-04,\n",
       "          -6.81555015e-04]], dtype=float32),\n",
       "  'post_fc_stack._value_branch._model.0.bias': array([0.], dtype=float32),\n",
       "  'logits_layer._model.0.weight': array([[ 1.68930099e-04,  4.94521577e-04,  2.15890817e-04,\n",
       "          -3.50600021e-04,  5.19997702e-05,  4.64175100e-05,\n",
       "          -4.51380940e-04, -2.61658744e-04, -5.00734022e-04,\n",
       "          -5.15011721e-04,  3.54273507e-04, -1.04932569e-03,\n",
       "           2.73017533e-04, -7.41327181e-04, -7.64164259e-04,\n",
       "           8.94198383e-05,  1.50267719e-04, -7.05460901e-04,\n",
       "          -8.64414789e-04,  7.31603650e-04, -8.19610083e-04,\n",
       "          -8.07304052e-04,  1.14563161e-04,  3.82945407e-04,\n",
       "          -7.55569199e-05,  7.37501134e-04, -9.87816486e-04,\n",
       "          -7.71485677e-04, -4.40621297e-05,  9.35271441e-04,\n",
       "          -1.36899525e-05,  3.01128428e-04, -1.18086173e-03,\n",
       "           7.85027165e-04, -3.84627143e-04,  3.98792094e-04,\n",
       "          -3.97341530e-04, -2.72608071e-04, -4.83252603e-04,\n",
       "          -1.01947912e-03,  2.95277219e-04, -3.67111730e-04,\n",
       "           4.19759745e-05,  5.11478675e-06,  3.13606666e-04,\n",
       "          -1.06681965e-03, -3.22204258e-04, -9.63484345e-05,\n",
       "           3.55919270e-04, -1.54075940e-04,  8.61634471e-05,\n",
       "           6.49377587e-04, -4.76668967e-04, -2.68335541e-04,\n",
       "          -6.51753158e-04,  1.56629278e-04,  1.97713947e-04,\n",
       "          -4.32654546e-04, -5.10684855e-04, -4.10948793e-04,\n",
       "           7.10720196e-04,  6.81342324e-04,  4.00977151e-04,\n",
       "          -3.25476925e-04,  3.60905542e-04, -8.30612029e-04,\n",
       "          -1.25206943e-05,  6.61302358e-04, -5.32248814e-04,\n",
       "          -1.40303164e-04,  7.86091026e-04, -4.34858557e-05,\n",
       "           1.40821014e-03,  1.69129096e-04,  3.32457566e-04,\n",
       "           2.29338926e-04, -1.57404720e-04,  2.89193587e-04,\n",
       "          -4.41691285e-04,  6.55504060e-04, -2.21750481e-04,\n",
       "           4.19024320e-04,  2.34576699e-04, -5.78442763e-04,\n",
       "           5.66381030e-04,  3.79346857e-05, -2.90745375e-04,\n",
       "           8.85120069e-04,  4.15576273e-04,  7.67709862e-04,\n",
       "           9.73818009e-04,  4.32739325e-04, -7.67629826e-04,\n",
       "           1.93540633e-04,  1.23334292e-03, -9.78976139e-04,\n",
       "           8.73775629e-04, -6.09747542e-04, -6.52171962e-04,\n",
       "          -8.72961886e-04,  3.02055589e-04, -1.04229094e-03,\n",
       "          -3.12886987e-05, -2.70734134e-04, -5.79341104e-05,\n",
       "          -5.50414261e-04, -2.39162007e-04,  1.09969034e-04,\n",
       "          -3.50932940e-04, -8.63543319e-05, -8.82091525e-04,\n",
       "           5.20039059e-04,  1.41151110e-03,  3.07720766e-05,\n",
       "           1.85829689e-04,  4.20897035e-04,  3.74613068e-04,\n",
       "           8.48571362e-04, -1.61730466e-04, -4.87560377e-04,\n",
       "          -2.74261954e-04, -1.37163035e-03, -1.49440859e-03,\n",
       "           7.82168820e-04, -1.14271774e-04,  9.21999279e-04,\n",
       "          -6.00147643e-04, -1.83722295e-04,  1.06045674e-03,\n",
       "          -2.25233060e-04, -1.88399653e-03, -3.04680434e-04,\n",
       "           1.96792360e-04,  1.98845417e-04,  1.18886214e-03,\n",
       "           5.02744690e-04, -5.15480991e-04,  4.13141010e-04,\n",
       "           7.25765654e-04,  6.61399215e-04,  2.36626820e-05,\n",
       "          -6.44604326e-04,  1.57009374e-04, -6.16355392e-04,\n",
       "           5.95699705e-04,  5.77232917e-04, -6.29386283e-04,\n",
       "           9.01831605e-04,  2.32926368e-06, -5.42408845e-04,\n",
       "           9.21252577e-05,  6.15078665e-04,  4.65633260e-04,\n",
       "           7.03909027e-05, -6.74142866e-05,  6.67194196e-04,\n",
       "          -7.17295916e-04, -9.44086176e-04,  2.52697700e-05,\n",
       "          -1.48848281e-03,  6.07773909e-05, -5.14683838e-04,\n",
       "           7.32353423e-04, -5.89474861e-04, -5.81959903e-04,\n",
       "           5.49447665e-04, -2.40470661e-04,  6.15974190e-04,\n",
       "          -1.42712600e-03, -1.42825511e-03,  1.11193222e-03,\n",
       "           3.66227410e-04,  1.03935390e-03,  3.60121543e-04,\n",
       "           2.86880968e-04, -3.74358293e-04, -4.30021668e-04,\n",
       "           1.65831385e-04, -3.41163599e-04,  4.08309250e-04,\n",
       "          -4.14210081e-04, -4.86768520e-04,  1.50399224e-03,\n",
       "           1.95909335e-04, -4.12836263e-04, -3.53320793e-04,\n",
       "          -5.64259652e-04,  1.22778234e-03,  8.95352059e-05,\n",
       "           7.59332543e-05,  1.48933948e-04, -5.31089463e-05,\n",
       "           5.90904747e-05, -4.75323730e-04, -5.29579236e-04,\n",
       "          -8.78618215e-04, -3.94938310e-04,  9.76789011e-07,\n",
       "          -1.98043720e-03,  3.80238489e-05,  3.41057050e-04,\n",
       "          -7.44043908e-04, -1.34293936e-04, -1.23044723e-04,\n",
       "           2.89405696e-04,  4.94094973e-04,  5.27560769e-04,\n",
       "           9.28448746e-04,  3.91880574e-04, -9.89610213e-04,\n",
       "          -9.95223993e-04, -7.20702519e-04,  1.60844880e-04,\n",
       "           1.07258651e-03,  2.90824857e-04, -4.71608742e-04,\n",
       "           6.32334559e-05, -1.22829064e-04, -5.56522748e-04,\n",
       "           9.45878346e-05,  2.27608543e-04, -1.36125751e-03,\n",
       "           2.28108882e-04, -4.36627655e-04, -1.77017297e-04,\n",
       "           3.10358628e-05,  4.98762645e-04,  2.32943334e-04,\n",
       "           5.95625752e-05,  5.70137869e-04,  9.49381443e-04,\n",
       "          -7.50561157e-05, -5.13120845e-04,  1.78756454e-04,\n",
       "          -3.60624326e-05, -1.57165501e-04,  9.25660250e-04,\n",
       "           1.67168095e-04, -6.94969145e-04, -4.13695321e-04,\n",
       "          -8.38643929e-04,  1.94007487e-04,  6.39588223e-04,\n",
       "          -1.04805704e-05, -1.53025810e-03, -1.34509907e-03,\n",
       "           6.23969536e-04, -1.33448584e-05, -4.10740540e-05,\n",
       "          -1.42693127e-04, -5.04119787e-04,  7.22067198e-04,\n",
       "           3.43900087e-04,  1.71408115e-04, -7.45597295e-04,\n",
       "           1.45374259e-04],\n",
       "         [-8.73390236e-04, -4.57841496e-04,  2.17633715e-04,\n",
       "          -2.00356633e-04, -1.07580388e-03, -9.61271726e-05,\n",
       "          -4.76831628e-04,  4.32325818e-04, -3.77687655e-04,\n",
       "          -3.61940358e-04,  1.79310809e-05,  6.58778183e-04,\n",
       "          -1.85895740e-04, -5.28153032e-04, -5.03488758e-04,\n",
       "          -4.33838140e-05,  3.59332007e-05,  4.42106189e-04,\n",
       "           6.62618491e-04,  9.51671740e-04, -5.69640484e-04,\n",
       "          -1.50751660e-03,  8.60451546e-05,  5.21377719e-04,\n",
       "          -1.85766839e-04, -2.69084499e-04, -8.55792387e-05,\n",
       "          -9.42626575e-05,  3.58771576e-05,  5.21966547e-04,\n",
       "           5.57053834e-04,  7.59572722e-04,  1.85726487e-04,\n",
       "          -4.20766853e-04,  2.66718474e-04, -3.74525931e-04,\n",
       "           4.70120751e-04, -7.24055135e-05,  4.21529228e-04,\n",
       "           4.88280843e-04, -6.43265957e-05, -5.51987869e-05,\n",
       "           1.81816606e-04,  4.60832234e-04, -4.34447284e-04,\n",
       "           5.44458977e-04, -6.16089092e-04, -4.66614933e-04,\n",
       "          -2.32008402e-04,  4.21754550e-04,  7.43256824e-04,\n",
       "           6.35102566e-04, -5.74867416e-04,  7.46949110e-04,\n",
       "           7.04541802e-04, -1.76197267e-04, -7.79024092e-04,\n",
       "           5.98400038e-05, -7.06022955e-04,  1.73391207e-04,\n",
       "          -4.08705993e-04,  2.21958369e-04, -7.02746212e-04,\n",
       "          -1.44928868e-04,  1.03717123e-03, -2.21489725e-04,\n",
       "          -8.20719462e-04,  2.44511961e-04, -4.72442553e-05,\n",
       "           6.73063507e-04, -5.68272430e-04, -2.65307463e-04,\n",
       "           1.90404695e-04, -2.10546321e-04,  7.41685741e-04,\n",
       "           1.31764240e-03,  6.87479987e-05, -1.35191181e-03,\n",
       "           6.72863272e-04,  6.23059459e-04,  1.10718980e-03,\n",
       "           2.20013317e-04, -1.12110632e-03, -8.12777784e-04,\n",
       "           1.73858381e-04, -5.92763361e-04,  4.78718895e-04,\n",
       "           9.12121759e-05,  1.08665554e-03,  2.83487025e-04,\n",
       "          -4.55546979e-04,  9.16976482e-04,  3.18549137e-04,\n",
       "          -5.35249303e-04, -1.08941691e-04, -4.62548254e-04,\n",
       "           3.69146379e-04, -3.09081428e-04,  2.91665609e-04,\n",
       "           1.04581658e-03,  4.03879589e-04, -1.10606205e-04,\n",
       "          -3.19946819e-04,  8.76744452e-05,  2.15148801e-04,\n",
       "          -6.47125242e-04, -1.10299361e-03, -5.09988284e-04,\n",
       "           3.04383255e-04,  2.23713723e-04, -8.96677549e-04,\n",
       "           7.81747920e-04,  1.36827570e-04, -7.28177038e-05,\n",
       "           3.36906523e-04, -3.21963715e-04,  4.26092040e-04,\n",
       "           5.10502025e-04,  4.22784826e-04, -9.84565937e-04,\n",
       "           7.64522003e-04,  1.41085344e-04,  2.79290864e-04,\n",
       "          -1.19493436e-03, -1.00726774e-03,  3.98435659e-04,\n",
       "           8.15232925e-05, -1.39057334e-03,  7.99745554e-04,\n",
       "           4.36059607e-04,  3.05395602e-04,  6.07188616e-04,\n",
       "           2.10060636e-04, -5.14155312e-04,  7.10568915e-04,\n",
       "          -1.22524827e-04, -2.19989124e-05,  1.25489163e-03,\n",
       "          -5.15674590e-04,  1.09262008e-03, -8.49894190e-04,\n",
       "          -1.00448879e-03,  1.57685392e-03,  7.56055349e-04,\n",
       "          -3.43721738e-04,  1.02061580e-03, -2.84182257e-04,\n",
       "          -1.60765630e-04,  1.58351241e-03,  1.13042151e-04,\n",
       "           4.14698443e-04,  9.27380519e-04, -1.53146416e-03,\n",
       "          -8.11480568e-04,  3.21200147e-04,  5.55660299e-05,\n",
       "          -6.23833796e-04, -6.86034851e-04,  1.34081260e-04,\n",
       "           8.61031294e-04, -1.49306399e-03, -8.29115568e-04,\n",
       "          -1.60290481e-04, -5.18179091e-04, -2.83402100e-04,\n",
       "          -4.73117543e-04,  8.81348737e-04, -2.47199321e-04,\n",
       "           4.03105863e-04,  9.15252196e-04, -4.38510790e-04,\n",
       "           3.56741657e-04, -4.74011526e-04,  1.17383155e-04,\n",
       "          -3.32766882e-04, -1.55433023e-04, -1.10338570e-03,\n",
       "           1.12646070e-04, -2.38342574e-04, -2.74450547e-04,\n",
       "          -4.66994388e-05,  1.65491877e-03, -1.02948688e-03,\n",
       "           1.06939864e-04, -7.61684496e-04, -1.59056493e-04,\n",
       "          -1.75044435e-04, -4.37858456e-04,  2.29378900e-04,\n",
       "          -4.26622952e-04, -3.57642013e-04,  5.92166383e-04,\n",
       "           6.11787982e-05,  5.12184524e-05, -2.05498392e-04,\n",
       "           1.23722202e-04, -8.06344324e-04, -1.13479956e-03,\n",
       "          -2.37537708e-04, -6.06364338e-05, -1.92608830e-04,\n",
       "          -3.02262546e-04,  3.07526614e-04,  1.38128053e-05,\n",
       "           1.05382770e-03, -6.14101416e-04, -1.27121122e-04,\n",
       "          -4.80242685e-04, -1.12514815e-03,  2.40456084e-05,\n",
       "           4.20507946e-04,  1.96382194e-03, -3.06765927e-04,\n",
       "           9.34465672e-04,  6.32469426e-04, -3.12634482e-04,\n",
       "          -4.72267362e-04, -5.66940929e-04,  2.35686839e-05,\n",
       "          -1.08270724e-04,  1.06159982e-03, -1.39025331e-03,\n",
       "          -2.78943975e-04, -7.15389557e-04, -2.57649572e-05,\n",
       "          -1.06986286e-03,  2.29161567e-04, -6.73146977e-04,\n",
       "           9.30424430e-04,  6.46118540e-04,  2.91015633e-04,\n",
       "          -2.03334348e-04, -4.20364348e-04,  2.63844180e-04,\n",
       "          -5.41390000e-05, -3.70383437e-04, -1.44858423e-04,\n",
       "          -3.06435977e-04,  1.08505737e-04,  6.48614019e-04,\n",
       "           6.60373305e-04,  1.04479643e-03, -7.39721814e-04,\n",
       "           1.08801702e-03,  4.53581335e-04,  5.51208272e-04,\n",
       "           7.04412378e-05, -3.35668650e-04,  2.77448155e-04,\n",
       "          -3.08702700e-04, -6.42271305e-04,  6.97798794e-04,\n",
       "          -5.36140637e-04, -6.74040639e-05, -4.06978688e-05,\n",
       "          -5.33308601e-04]], dtype=float32),\n",
       "  'logits_layer._model.0.bias': array([0., 0.], dtype=float32),\n",
       "  'value_layer._model.0.weight': array([[ 6.69857080e-04,  4.54119960e-04, -6.27516303e-04,\n",
       "          -9.76577983e-04,  2.99471256e-04,  3.07808405e-05,\n",
       "          -4.28462168e-04, -4.64486162e-04,  2.85178772e-04,\n",
       "           2.98577361e-04, -2.46756099e-04, -5.62664529e-04,\n",
       "           3.06056638e-04,  6.16361911e-04,  9.41679231e-04,\n",
       "           7.26446393e-04, -4.61763702e-04, -2.93884368e-04,\n",
       "          -7.32968620e-04,  3.05080357e-05, -1.41527504e-04,\n",
       "          -2.88205192e-04,  3.04371424e-05, -1.01867400e-03,\n",
       "          -2.56278909e-05,  2.92578421e-04,  6.85755571e-04,\n",
       "          -1.04390507e-04,  4.82697476e-04,  6.94335613e-05,\n",
       "           2.28154677e-04, -5.85209091e-05, -7.20167183e-04,\n",
       "          -6.76856493e-04,  8.28915800e-05, -1.08447636e-03,\n",
       "          -1.94126769e-05,  7.45870231e-04,  4.97606175e-04,\n",
       "           1.69195671e-04,  5.19267749e-04,  1.09558715e-03,\n",
       "           7.93222454e-04, -8.25950119e-04, -1.16108300e-03,\n",
       "          -2.41048241e-04,  2.46638083e-04,  9.12250689e-05,\n",
       "           1.08228916e-04, -6.04295637e-04,  5.44786861e-04,\n",
       "          -2.26129796e-05,  2.02015828e-04,  6.09644921e-05,\n",
       "          -1.16552785e-03, -1.35665364e-03,  6.81242163e-05,\n",
       "          -7.50108331e-04,  1.11127424e-03, -4.46375954e-04,\n",
       "           5.06642566e-04,  5.19758207e-04,  2.86561233e-04,\n",
       "          -5.68372270e-05, -4.39650525e-04, -8.77428756e-05,\n",
       "           7.93520376e-05, -7.82518648e-04, -2.57184205e-04,\n",
       "          -1.21571145e-04, -5.02697600e-04, -8.40509092e-05,\n",
       "          -1.41369482e-03, -3.23544606e-04,  2.87186966e-04,\n",
       "          -3.69792571e-04,  7.06941704e-04, -1.04881939e-04,\n",
       "           2.40036083e-04,  5.37852466e-04,  4.83627118e-05,\n",
       "           7.22880999e-04,  6.15324650e-04, -3.40025523e-04,\n",
       "          -4.26751532e-04, -4.86996636e-04,  2.89776159e-04,\n",
       "          -1.01343181e-03,  4.75266097e-05,  8.73716373e-04,\n",
       "          -3.35600635e-04, -4.32387693e-04,  5.22468414e-04,\n",
       "          -6.95253329e-05,  3.62019433e-04, -8.21549387e-04,\n",
       "          -9.70690162e-04, -5.79396583e-05, -3.30547482e-04,\n",
       "           9.39412799e-04, -1.38884655e-03,  1.48451375e-03,\n",
       "          -6.87058433e-04,  1.76718796e-03, -4.19704040e-04,\n",
       "           1.29144394e-03, -2.83369940e-04, -4.10072913e-04,\n",
       "          -1.16873230e-03, -3.38987593e-04,  4.59224859e-04,\n",
       "          -1.08411412e-04,  4.86700555e-05,  9.58583958e-04,\n",
       "          -8.68247356e-04,  4.79597365e-04,  8.81602318e-05,\n",
       "          -9.51919865e-05, -4.62094118e-04, -5.90286334e-04,\n",
       "           1.22057670e-03, -3.13444107e-05, -4.44398087e-04,\n",
       "           8.65814392e-04, -2.79728789e-04,  2.71156950e-05,\n",
       "           2.48047785e-04, -1.67694094e-03,  7.31851964e-04,\n",
       "          -7.40853546e-04,  8.80231441e-04,  5.48464363e-04,\n",
       "           8.37873013e-05, -4.01616038e-04,  1.40973119e-04,\n",
       "          -2.76169361e-04,  5.51136909e-04, -4.13737172e-04,\n",
       "          -3.88014771e-04,  1.52745005e-03, -5.89396921e-04,\n",
       "          -1.10033015e-03, -6.38381171e-04,  1.55850430e-04,\n",
       "           6.95598952e-04,  9.89345717e-04, -1.87942956e-03,\n",
       "          -1.57261486e-04,  5.93255609e-05, -1.91454659e-04,\n",
       "          -5.13779931e-04,  5.14994084e-04,  3.34481767e-04,\n",
       "          -2.03851581e-04,  7.10249587e-04, -4.59113682e-04,\n",
       "           5.24375879e-04, -4.68812999e-04, -4.51786182e-04,\n",
       "           5.58848435e-04, -7.12269466e-05,  7.34899193e-04,\n",
       "           3.37529927e-04, -3.17419093e-04, -5.58294472e-04,\n",
       "          -9.65366024e-04, -1.73140579e-04, -8.77262035e-04,\n",
       "           2.00682087e-04, -4.37431067e-04, -1.49095184e-04,\n",
       "           6.93441485e-04,  9.31159884e-04,  1.68618033e-04,\n",
       "           8.20587811e-05, -9.72848735e-04,  7.99505855e-04,\n",
       "          -8.43159272e-04,  2.35627449e-05,  1.44331309e-04,\n",
       "          -2.40652880e-04,  4.75649955e-04, -4.22176032e-04,\n",
       "          -1.59022689e-04,  6.18782302e-04,  7.91811442e-04,\n",
       "          -2.37278902e-04,  6.93724549e-04,  4.17554431e-04,\n",
       "          -1.72743792e-04,  7.18657742e-04, -2.83673598e-05,\n",
       "           5.96457743e-04,  1.11313816e-03,  6.26819790e-04,\n",
       "           1.68695522e-04, -1.10086624e-03,  7.42208271e-04,\n",
       "           1.37488503e-04,  6.52293238e-05,  3.03232257e-04,\n",
       "          -9.66424646e-04,  9.89920576e-04,  3.23618820e-04,\n",
       "           1.20018004e-03, -9.16697492e-04, -3.03502427e-04,\n",
       "           3.18552018e-04, -3.99431534e-04,  1.39083003e-03,\n",
       "          -8.21972790e-04, -2.06876328e-04,  8.55272054e-04,\n",
       "          -8.61079432e-04,  9.97071184e-05,  3.62919294e-04,\n",
       "           2.91601056e-04,  1.96402776e-04, -2.96336919e-04,\n",
       "           6.81802514e-04, -9.07840222e-05, -4.93630418e-04,\n",
       "           7.82190647e-04,  3.64882319e-04,  1.02798245e-03,\n",
       "           1.19580387e-03, -8.94290861e-04,  3.93788912e-04,\n",
       "          -3.18477396e-05,  2.52926489e-04, -7.99464702e-04,\n",
       "           3.16299192e-05,  2.24890464e-04,  3.81013582e-04,\n",
       "          -5.34667284e-04, -1.44129110e-04, -5.71570534e-04,\n",
       "          -9.35567004e-06, -5.58123051e-04,  4.90489547e-05,\n",
       "           2.21113703e-04, -3.93901370e-04, -5.82776207e-04,\n",
       "          -1.09065976e-03, -6.89407520e-04,  4.34085319e-04,\n",
       "           1.90790641e-04, -7.59627728e-04,  9.05858760e-05,\n",
       "          -1.44438061e-04, -4.64237120e-04, -3.94243310e-04,\n",
       "           4.15581424e-04,  3.17974831e-04,  4.41945216e-04,\n",
       "           2.51963240e-04]], dtype=float32),\n",
       "  'value_layer._model.0.bias': array([0.], dtype=float32)}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = get_trainable_cls(args.run)#.build()##args.run#TwoStepGame(config)\n",
    "print(algo)\n",
    "print(config)\n",
    "#alg = config.build()\n",
    "#algo.get_policy('low_level')#.get_weights()\n",
    "print(alg)\n",
    "#print(traine.policy('low_level_policy'))\n",
    "#alg.workers.foreach_worker(lambda worker: worker.get_policy('low_level_policy').get_weights())\n",
    "# Same as above, but with index.\n",
    "\n",
    "alg.workers.foreach_worker_with_id(\n",
    "    lambda _id, worker: worker.get_policy().get_weights()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ed1870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8010945916175842, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.24, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 1.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 1.0, 7.0, 0.0, 7.0, 0.0, 8.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 0.0, 7.0, 8.0, 0.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 0.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8273373788862085, 'mean_inference_ms': 1.398365295941556, 'mean_action_processing_ms': 0.17054401226897734, 'mean_env_wait_ms': 0.05016042225396453, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.015010356903076172, 'ViewRequirementAgentConnector_ms': 0.13837265968322754}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.24, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 1.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 1.0, 7.0, 0.0, 7.0, 0.0, 8.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 0.0, 7.0, 8.0, 0.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 0.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8273373788862085, 'mean_inference_ms': 1.398365295941556, 'mean_action_processing_ms': 0.17054401226897734, 'mean_env_wait_ms': 0.05016042225396453, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.015010356903076172, 'ViewRequirementAgentConnector_ms': 0.13837265968322754}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 505.727, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.0, 'learn_throughput': 16666.881, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': 'e20ad0ebaa4a4e32a1303f0cdbeb9a11', 'date': '2023-07-02_00-08-52', 'timestamp': 1688281732, 'time_this_iter_s': 0.5077288150787354, 'time_total_s': 0.5077288150787354, 'pid': 23708, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x0000029BF40259D0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x0000029BBDC141F0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.5077288150787354, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 30.89317011833191, 'perf': {'cpu_util_percent': 6.116216216216216, 'ram_util_percent': 55.27297297297297, 'gpu_util_percent0': 0.0008108108108108108, 'vram_util_percent0': 0.193115234375}}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[41], line 4\u001b[1;36m\n\u001b[1;33m    obs, info = env.reset()\u001b[1;36m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'env' is not defined\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(alg.train()) \n",
    "    #env = TwoStepGame(config)\n",
    "    obs, info = env.reset()\n",
    "    print(obs)\n",
    "    print(info)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    action = algo.compute_single_action(#trainer.compute_action\n",
    "                obs[1],  \n",
    "                policy_id='low_level_policy',\n",
    "                full_fetch=False\n",
    "            )#algo.compute_single_action(3)\n",
    "    print(action)\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(0)\n",
    "\n",
    "    #Policy.compute_actions_from_input_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dd8f2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.pg.pg.PG'>\n",
      "PG\n",
      "[{'one_hot.0._hidden_layers.0._model.0.weight': array([[-0.2417314 ,  0.13188548, -0.11307124,  0.65049994, -0.5958364 ,\n",
      "        -0.36498737],\n",
      "       [-0.15053855,  0.34130767,  0.6101184 , -0.6719392 , -0.09989117,\n",
      "        -0.16468833],\n",
      "       [ 0.06607527,  0.53549373, -0.138774  , -0.08091102, -0.8257553 ,\n",
      "         0.03469821],\n",
      "       ...,\n",
      "       [-0.6290907 ,  0.07368284, -0.02462804, -0.65478146,  0.03681446,\n",
      "         0.4100185 ],\n",
      "       [ 0.48683763, -0.47035825,  0.5672494 , -0.0302962 , -0.46104297,\n",
      "         0.0806331 ],\n",
      "       [-0.4187051 ,  0.23055151, -0.47441444, -0.10279853, -0.35409957,\n",
      "         0.64070976]], dtype=float32), 'one_hot.0._hidden_layers.0._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), 'one_hot.0._hidden_layers.1._model.0.weight': array([[ 0.07616611,  0.1024904 , -0.06008341, ...,  0.0040775 ,\n",
      "         0.12163325,  0.05829847],\n",
      "       [-0.01296282, -0.00270244,  0.00387737, ..., -0.06672076,\n",
      "         0.06744676,  0.07621893],\n",
      "       [-0.02341603,  0.11321501, -0.04295593, ..., -0.0303192 ,\n",
      "         0.02299083, -0.04502003],\n",
      "       ...,\n",
      "       [-0.04480674, -0.02516159,  0.03547735, ..., -0.01441081,\n",
      "        -0.00146841,  0.09396315],\n",
      "       [ 0.00219834, -0.10252253,  0.05682303, ..., -0.02911886,\n",
      "        -0.09650017,  0.09447143],\n",
      "       [-0.13330932, -0.07217666, -0.05993138, ..., -0.06128887,\n",
      "         0.01014278, -0.02129514]], dtype=float32), 'one_hot.0._hidden_layers.1._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), 'one_hot.0._value_branch_separate.0._model.0.weight': array([[-0.3994743 , -0.3685434 , -0.08734183, -0.2058262 , -0.75568   ,\n",
      "         0.28905138],\n",
      "       [ 0.570233  ,  0.04255618,  0.30823562, -0.16011126, -0.42573687,\n",
      "         0.6092016 ],\n",
      "       [ 0.00708198, -0.5127578 , -0.21936549, -0.26955682, -0.26836357,\n",
      "        -0.7377182 ],\n",
      "       ...,\n",
      "       [-0.36714807,  0.7765461 , -0.23143956,  0.24141257,  0.25400913,\n",
      "         0.29293942],\n",
      "       [ 0.43768945,  0.3656941 ,  0.27996746, -0.6414771 , -0.1994951 ,\n",
      "         0.3808187 ],\n",
      "       [-0.4419332 , -0.09776116,  0.83805263,  0.07660171, -0.24555539,\n",
      "         0.16321845]], dtype=float32), 'one_hot.0._value_branch_separate.0._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), 'one_hot.0._value_branch_separate.1._model.0.weight': array([[-0.02261898, -0.06969503, -0.07503725, ..., -0.06970434,\n",
      "         0.06038707,  0.03687484],\n",
      "       [-0.01661276, -0.00141688, -0.03554655, ..., -0.02411317,\n",
      "        -0.08742684,  0.04906141],\n",
      "       [ 0.02217244,  0.02354937,  0.01553392, ..., -0.03095901,\n",
      "        -0.05497946, -0.00090166],\n",
      "       ...,\n",
      "       [-0.0289262 , -0.0204082 , -0.05778252, ..., -0.00356669,\n",
      "        -0.02078001,  0.00415433],\n",
      "       [ 0.00943259,  0.06832833, -0.14522651, ...,  0.01869841,\n",
      "        -0.04701215, -0.01346818],\n",
      "       [-0.0062555 , -0.08043516,  0.09847534, ...,  0.05544772,\n",
      "        -0.05562781,  0.04447228]], dtype=float32), 'one_hot.0._value_branch_separate.1._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), 'one_hot.0._value_branch._model.0.weight': array([[ 1.44789158e-03, -4.60472860e-04, -1.78967824e-03,\n",
      "         1.49359938e-03,  6.29010610e-05,  4.93665691e-04,\n",
      "         6.39036065e-04, -6.39967155e-04, -4.35310911e-04,\n",
      "        -2.94278871e-04, -1.79906841e-04, -9.02768516e-04,\n",
      "        -6.05150242e-04,  5.20734699e-04, -7.07093131e-05,\n",
      "         1.65562413e-03, -2.95827747e-04,  1.15256826e-03,\n",
      "        -1.49163068e-04,  5.03940209e-05,  1.44236872e-03,\n",
      "        -1.13173807e-03, -7.01883808e-04, -5.86123264e-04,\n",
      "        -1.99097383e-04,  1.42476102e-03,  2.89046184e-05,\n",
      "         1.07019744e-03, -8.04771087e-04, -1.96193796e-04,\n",
      "         2.24390737e-04,  6.46036351e-05, -7.74710614e-04,\n",
      "         4.15340546e-05, -6.20397506e-04,  2.82897417e-06,\n",
      "        -6.30141411e-04,  5.54942701e-04, -1.08095363e-03,\n",
      "         6.92484842e-04,  4.65836260e-04,  3.54423566e-04,\n",
      "        -5.66640847e-05, -1.02253514e-03,  4.65251564e-04,\n",
      "         7.64911179e-04,  2.24318108e-04,  2.43787625e-04,\n",
      "        -9.04668355e-04,  4.45685815e-04, -1.03732731e-04,\n",
      "        -3.50273222e-05, -5.61272085e-04,  1.29285036e-04,\n",
      "        -5.15945547e-04, -8.06008524e-04, -2.84696522e-04,\n",
      "        -7.02485733e-04,  7.69779144e-04, -7.30974076e-04,\n",
      "         1.37831536e-04, -4.57492948e-04, -8.02536961e-04,\n",
      "         4.95896093e-04,  6.80889701e-04, -2.34385261e-05,\n",
      "         1.40249752e-03,  8.75152298e-04,  6.12656819e-04,\n",
      "         5.17824548e-04, -2.52761907e-04, -4.95244225e-04,\n",
      "         1.28454703e-04,  7.82147283e-04,  3.53706819e-05,\n",
      "        -5.20087560e-05,  3.72083130e-04,  3.75241274e-04,\n",
      "        -6.74220093e-04, -3.64972220e-04,  1.31654233e-04,\n",
      "         3.34834156e-04, -1.86219404e-04, -1.15169110e-04,\n",
      "        -9.23311280e-04, -6.75658695e-04,  5.99680701e-04,\n",
      "        -1.28326923e-04, -9.82320751e-04,  4.02687321e-04,\n",
      "         7.90988212e-04,  6.50356596e-05,  5.33930834e-05,\n",
      "         6.47505105e-04, -2.71751342e-04, -6.22041698e-04,\n",
      "         6.82536163e-04, -4.16340743e-04, -3.26838723e-04,\n",
      "        -2.17237219e-04, -1.21446990e-03,  1.16015447e-03,\n",
      "         2.56171275e-04, -1.47389888e-04,  7.18358322e-04,\n",
      "        -6.11065829e-04, -9.15724886e-05, -3.33851582e-04,\n",
      "         9.64251522e-05, -3.13549099e-04,  1.10746230e-04,\n",
      "        -5.06112759e-04, -4.74189699e-04, -1.50011270e-04,\n",
      "         2.55909399e-04,  5.24878851e-04, -9.94214788e-04,\n",
      "         5.55948471e-04,  2.93555349e-04, -2.84813781e-04,\n",
      "        -1.51314700e-04, -1.94396576e-04, -1.15935632e-03,\n",
      "         9.06516332e-04, -4.42022720e-04,  1.54777044e-05,\n",
      "        -3.96008603e-04, -4.11499204e-04, -6.13326440e-04,\n",
      "         1.34467700e-04,  2.43154835e-04,  1.24704005e-04,\n",
      "        -1.14727827e-05, -6.46212546e-04, -1.55567657e-04,\n",
      "        -6.11860363e-04, -6.52068062e-04, -1.86815960e-04,\n",
      "         9.02735104e-04, -7.32153538e-04, -1.50143547e-04,\n",
      "         2.95315999e-06,  2.75822182e-04,  6.71973103e-04,\n",
      "         5.49451623e-04,  2.79754982e-04,  8.38673441e-05,\n",
      "         8.08888755e-04, -1.05928732e-04,  6.29751419e-04,\n",
      "         1.10257079e-03, -1.18932687e-03, -4.49575658e-04,\n",
      "        -9.50330519e-04,  9.24972759e-04,  8.22714792e-05,\n",
      "        -3.30646028e-04,  4.71935346e-04,  8.65110953e-04,\n",
      "        -6.69013942e-04, -1.08825057e-04,  7.86365767e-04,\n",
      "        -5.83874644e-04, -1.24256578e-04, -7.14604917e-04,\n",
      "        -1.04248233e-03,  1.86072721e-04,  1.82118732e-03,\n",
      "         5.47991483e-04,  7.30134489e-04,  3.35820194e-04,\n",
      "         2.65294133e-04,  2.61183362e-04,  4.46309656e-04,\n",
      "         3.67202418e-04,  5.61988971e-04,  4.76858753e-04,\n",
      "        -1.46573904e-04, -2.80069595e-04, -1.17131346e-03,\n",
      "        -5.54096012e-04, -2.14667380e-04,  1.23123551e-04,\n",
      "         4.40375530e-04,  1.19117287e-03, -5.03178511e-04,\n",
      "         6.22892810e-04,  2.24622767e-04, -5.45371848e-04,\n",
      "        -2.02848372e-04, -7.76080124e-04, -1.73600783e-04,\n",
      "         7.11349654e-04, -5.11132821e-04,  5.86411043e-04,\n",
      "         5.63511217e-04, -3.09133204e-04,  1.66972139e-04,\n",
      "        -5.16314933e-04,  1.13040360e-03, -3.93826427e-04,\n",
      "        -6.86562853e-04, -3.57956946e-04, -2.20152084e-04,\n",
      "         6.81494246e-04, -9.30526294e-04, -4.47451777e-04,\n",
      "         1.95213564e-04, -1.57271817e-04,  4.78109840e-04,\n",
      "        -6.17863669e-04,  9.68552951e-04, -1.61083066e-04,\n",
      "        -1.01674348e-03,  1.13798873e-04, -6.34346856e-04,\n",
      "        -4.99672642e-05,  1.32665236e-03,  4.36862028e-05,\n",
      "        -7.07049563e-04, -4.70387575e-04, -9.37513600e-04,\n",
      "         6.67707427e-05, -1.64782527e-04,  1.02593529e-03,\n",
      "        -2.88183044e-04,  5.31310870e-05,  3.45024237e-05,\n",
      "         9.54681484e-04, -1.21735335e-04, -3.75715405e-04,\n",
      "        -3.57182755e-04, -6.49543712e-04, -7.80146802e-04,\n",
      "         7.84777876e-05,  3.06717324e-04, -6.21275685e-04,\n",
      "         2.00861701e-04, -4.91020503e-04,  3.79441160e-04,\n",
      "         2.06981233e-04,  3.36064608e-04, -5.59964392e-04,\n",
      "        -6.42217230e-04, -6.97428186e-04, -7.72140629e-04,\n",
      "        -7.69243692e-04, -1.90681050e-04,  5.39439323e-04,\n",
      "         3.18040780e-04, -1.47552148e-03, -3.11680080e-04,\n",
      "        -1.01090685e-04,  6.39028673e-04, -4.06420993e-04,\n",
      "        -2.13442938e-04]], dtype=float32), 'one_hot.0._value_branch._model.0.bias': array([0.], dtype=float32), 'one_hot_0._hidden_layers.0._model.0.weight': array([[-0.2417314 ,  0.13188548, -0.11307124,  0.65049994, -0.5958364 ,\n",
      "        -0.36498737],\n",
      "       [-0.15053855,  0.34130767,  0.6101184 , -0.6719392 , -0.09989117,\n",
      "        -0.16468833],\n",
      "       [ 0.06607527,  0.53549373, -0.138774  , -0.08091102, -0.8257553 ,\n",
      "         0.03469821],\n",
      "       ...,\n",
      "       [-0.6290907 ,  0.07368284, -0.02462804, -0.65478146,  0.03681446,\n",
      "         0.4100185 ],\n",
      "       [ 0.48683763, -0.47035825,  0.5672494 , -0.0302962 , -0.46104297,\n",
      "         0.0806331 ],\n",
      "       [-0.4187051 ,  0.23055151, -0.47441444, -0.10279853, -0.35409957,\n",
      "         0.64070976]], dtype=float32), 'one_hot_0._hidden_layers.0._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), 'one_hot_0._hidden_layers.1._model.0.weight': array([[ 0.07616611,  0.1024904 , -0.06008341, ...,  0.0040775 ,\n",
      "         0.12163325,  0.05829847],\n",
      "       [-0.01296282, -0.00270244,  0.00387737, ..., -0.06672076,\n",
      "         0.06744676,  0.07621893],\n",
      "       [-0.02341603,  0.11321501, -0.04295593, ..., -0.0303192 ,\n",
      "         0.02299083, -0.04502003],\n",
      "       ...,\n",
      "       [-0.04480674, -0.02516159,  0.03547735, ..., -0.01441081,\n",
      "        -0.00146841,  0.09396315],\n",
      "       [ 0.00219834, -0.10252253,  0.05682303, ..., -0.02911886,\n",
      "        -0.09650017,  0.09447143],\n",
      "       [-0.13330932, -0.07217666, -0.05993138, ..., -0.06128887,\n",
      "         0.01014278, -0.02129514]], dtype=float32), 'one_hot_0._hidden_layers.1._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), 'one_hot_0._value_branch_separate.0._model.0.weight': array([[-0.3994743 , -0.3685434 , -0.08734183, -0.2058262 , -0.75568   ,\n",
      "         0.28905138],\n",
      "       [ 0.570233  ,  0.04255618,  0.30823562, -0.16011126, -0.42573687,\n",
      "         0.6092016 ],\n",
      "       [ 0.00708198, -0.5127578 , -0.21936549, -0.26955682, -0.26836357,\n",
      "        -0.7377182 ],\n",
      "       ...,\n",
      "       [-0.36714807,  0.7765461 , -0.23143956,  0.24141257,  0.25400913,\n",
      "         0.29293942],\n",
      "       [ 0.43768945,  0.3656941 ,  0.27996746, -0.6414771 , -0.1994951 ,\n",
      "         0.3808187 ],\n",
      "       [-0.4419332 , -0.09776116,  0.83805263,  0.07660171, -0.24555539,\n",
      "         0.16321845]], dtype=float32), 'one_hot_0._value_branch_separate.0._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), 'one_hot_0._value_branch_separate.1._model.0.weight': array([[-0.02261898, -0.06969503, -0.07503725, ..., -0.06970434,\n",
      "         0.06038707,  0.03687484],\n",
      "       [-0.01661276, -0.00141688, -0.03554655, ..., -0.02411317,\n",
      "        -0.08742684,  0.04906141],\n",
      "       [ 0.02217244,  0.02354937,  0.01553392, ..., -0.03095901,\n",
      "        -0.05497946, -0.00090166],\n",
      "       ...,\n",
      "       [-0.0289262 , -0.0204082 , -0.05778252, ..., -0.00356669,\n",
      "        -0.02078001,  0.00415433],\n",
      "       [ 0.00943259,  0.06832833, -0.14522651, ...,  0.01869841,\n",
      "        -0.04701215, -0.01346818],\n",
      "       [-0.0062555 , -0.08043516,  0.09847534, ...,  0.05544772,\n",
      "        -0.05562781,  0.04447228]], dtype=float32), 'one_hot_0._value_branch_separate.1._model.0.bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), 'one_hot_0._value_branch._model.0.weight': array([[ 1.44789158e-03, -4.60472860e-04, -1.78967824e-03,\n",
      "         1.49359938e-03,  6.29010610e-05,  4.93665691e-04,\n",
      "         6.39036065e-04, -6.39967155e-04, -4.35310911e-04,\n",
      "        -2.94278871e-04, -1.79906841e-04, -9.02768516e-04,\n",
      "        -6.05150242e-04,  5.20734699e-04, -7.07093131e-05,\n",
      "         1.65562413e-03, -2.95827747e-04,  1.15256826e-03,\n",
      "        -1.49163068e-04,  5.03940209e-05,  1.44236872e-03,\n",
      "        -1.13173807e-03, -7.01883808e-04, -5.86123264e-04,\n",
      "        -1.99097383e-04,  1.42476102e-03,  2.89046184e-05,\n",
      "         1.07019744e-03, -8.04771087e-04, -1.96193796e-04,\n",
      "         2.24390737e-04,  6.46036351e-05, -7.74710614e-04,\n",
      "         4.15340546e-05, -6.20397506e-04,  2.82897417e-06,\n",
      "        -6.30141411e-04,  5.54942701e-04, -1.08095363e-03,\n",
      "         6.92484842e-04,  4.65836260e-04,  3.54423566e-04,\n",
      "        -5.66640847e-05, -1.02253514e-03,  4.65251564e-04,\n",
      "         7.64911179e-04,  2.24318108e-04,  2.43787625e-04,\n",
      "        -9.04668355e-04,  4.45685815e-04, -1.03732731e-04,\n",
      "        -3.50273222e-05, -5.61272085e-04,  1.29285036e-04,\n",
      "        -5.15945547e-04, -8.06008524e-04, -2.84696522e-04,\n",
      "        -7.02485733e-04,  7.69779144e-04, -7.30974076e-04,\n",
      "         1.37831536e-04, -4.57492948e-04, -8.02536961e-04,\n",
      "         4.95896093e-04,  6.80889701e-04, -2.34385261e-05,\n",
      "         1.40249752e-03,  8.75152298e-04,  6.12656819e-04,\n",
      "         5.17824548e-04, -2.52761907e-04, -4.95244225e-04,\n",
      "         1.28454703e-04,  7.82147283e-04,  3.53706819e-05,\n",
      "        -5.20087560e-05,  3.72083130e-04,  3.75241274e-04,\n",
      "        -6.74220093e-04, -3.64972220e-04,  1.31654233e-04,\n",
      "         3.34834156e-04, -1.86219404e-04, -1.15169110e-04,\n",
      "        -9.23311280e-04, -6.75658695e-04,  5.99680701e-04,\n",
      "        -1.28326923e-04, -9.82320751e-04,  4.02687321e-04,\n",
      "         7.90988212e-04,  6.50356596e-05,  5.33930834e-05,\n",
      "         6.47505105e-04, -2.71751342e-04, -6.22041698e-04,\n",
      "         6.82536163e-04, -4.16340743e-04, -3.26838723e-04,\n",
      "        -2.17237219e-04, -1.21446990e-03,  1.16015447e-03,\n",
      "         2.56171275e-04, -1.47389888e-04,  7.18358322e-04,\n",
      "        -6.11065829e-04, -9.15724886e-05, -3.33851582e-04,\n",
      "         9.64251522e-05, -3.13549099e-04,  1.10746230e-04,\n",
      "        -5.06112759e-04, -4.74189699e-04, -1.50011270e-04,\n",
      "         2.55909399e-04,  5.24878851e-04, -9.94214788e-04,\n",
      "         5.55948471e-04,  2.93555349e-04, -2.84813781e-04,\n",
      "        -1.51314700e-04, -1.94396576e-04, -1.15935632e-03,\n",
      "         9.06516332e-04, -4.42022720e-04,  1.54777044e-05,\n",
      "        -3.96008603e-04, -4.11499204e-04, -6.13326440e-04,\n",
      "         1.34467700e-04,  2.43154835e-04,  1.24704005e-04,\n",
      "        -1.14727827e-05, -6.46212546e-04, -1.55567657e-04,\n",
      "        -6.11860363e-04, -6.52068062e-04, -1.86815960e-04,\n",
      "         9.02735104e-04, -7.32153538e-04, -1.50143547e-04,\n",
      "         2.95315999e-06,  2.75822182e-04,  6.71973103e-04,\n",
      "         5.49451623e-04,  2.79754982e-04,  8.38673441e-05,\n",
      "         8.08888755e-04, -1.05928732e-04,  6.29751419e-04,\n",
      "         1.10257079e-03, -1.18932687e-03, -4.49575658e-04,\n",
      "        -9.50330519e-04,  9.24972759e-04,  8.22714792e-05,\n",
      "        -3.30646028e-04,  4.71935346e-04,  8.65110953e-04,\n",
      "        -6.69013942e-04, -1.08825057e-04,  7.86365767e-04,\n",
      "        -5.83874644e-04, -1.24256578e-04, -7.14604917e-04,\n",
      "        -1.04248233e-03,  1.86072721e-04,  1.82118732e-03,\n",
      "         5.47991483e-04,  7.30134489e-04,  3.35820194e-04,\n",
      "         2.65294133e-04,  2.61183362e-04,  4.46309656e-04,\n",
      "         3.67202418e-04,  5.61988971e-04,  4.76858753e-04,\n",
      "        -1.46573904e-04, -2.80069595e-04, -1.17131346e-03,\n",
      "        -5.54096012e-04, -2.14667380e-04,  1.23123551e-04,\n",
      "         4.40375530e-04,  1.19117287e-03, -5.03178511e-04,\n",
      "         6.22892810e-04,  2.24622767e-04, -5.45371848e-04,\n",
      "        -2.02848372e-04, -7.76080124e-04, -1.73600783e-04,\n",
      "         7.11349654e-04, -5.11132821e-04,  5.86411043e-04,\n",
      "         5.63511217e-04, -3.09133204e-04,  1.66972139e-04,\n",
      "        -5.16314933e-04,  1.13040360e-03, -3.93826427e-04,\n",
      "        -6.86562853e-04, -3.57956946e-04, -2.20152084e-04,\n",
      "         6.81494246e-04, -9.30526294e-04, -4.47451777e-04,\n",
      "         1.95213564e-04, -1.57271817e-04,  4.78109840e-04,\n",
      "        -6.17863669e-04,  9.68552951e-04, -1.61083066e-04,\n",
      "        -1.01674348e-03,  1.13798873e-04, -6.34346856e-04,\n",
      "        -4.99672642e-05,  1.32665236e-03,  4.36862028e-05,\n",
      "        -7.07049563e-04, -4.70387575e-04, -9.37513600e-04,\n",
      "         6.67707427e-05, -1.64782527e-04,  1.02593529e-03,\n",
      "        -2.88183044e-04,  5.31310870e-05,  3.45024237e-05,\n",
      "         9.54681484e-04, -1.21735335e-04, -3.75715405e-04,\n",
      "        -3.57182755e-04, -6.49543712e-04, -7.80146802e-04,\n",
      "         7.84777876e-05,  3.06717324e-04, -6.21275685e-04,\n",
      "         2.00861701e-04, -4.91020503e-04,  3.79441160e-04,\n",
      "         2.06981233e-04,  3.36064608e-04, -5.59964392e-04,\n",
      "        -6.42217230e-04, -6.97428186e-04, -7.72140629e-04,\n",
      "        -7.69243692e-04, -1.90681050e-04,  5.39439323e-04,\n",
      "         3.18040780e-04, -1.47552148e-03, -3.11680080e-04,\n",
      "        -1.01090685e-04,  6.39028673e-04, -4.06420993e-04,\n",
      "        -2.13442938e-04]], dtype=float32), 'one_hot_0._value_branch._model.0.bias': array([0.], dtype=float32), 'post_fc_stack._value_branch._model.0.weight': array([[ 2.26961303e-04,  1.79289666e-04, -1.43940953e-04,\n",
      "         7.00922392e-04,  3.69001704e-04, -2.20407863e-04,\n",
      "        -2.93138379e-04,  1.73530585e-04, -1.15160865e-03,\n",
      "        -2.11574472e-04,  1.27871172e-03, -2.00404495e-04,\n",
      "         2.31390324e-04,  2.43281378e-04, -3.97291908e-04,\n",
      "         6.98922202e-04, -1.50094065e-03,  5.96526661e-04,\n",
      "         5.20228059e-04,  1.90424653e-05, -4.20709315e-04,\n",
      "         5.91185119e-04, -1.47750150e-04,  1.04207858e-04,\n",
      "        -1.07475743e-03,  3.51095834e-04,  2.13456384e-04,\n",
      "         3.61887418e-04,  9.43771563e-04,  7.75686407e-04,\n",
      "         5.48337412e-04,  7.38777337e-04, -7.64216355e-04,\n",
      "         1.10188592e-03,  3.52211791e-04, -2.38886059e-04,\n",
      "         1.76869638e-04, -5.44525508e-04, -8.12137267e-04,\n",
      "        -6.12789372e-05, -5.11877995e-04,  4.20221098e-04,\n",
      "         2.16389191e-04, -3.48945643e-04,  2.45636329e-06,\n",
      "         6.53710304e-05,  5.03951684e-04, -7.05657236e-04,\n",
      "        -2.78988009e-04,  1.13005633e-04, -6.95851923e-04,\n",
      "        -2.38170614e-03,  1.01706467e-03, -1.34869642e-03,\n",
      "         7.95803790e-05,  1.23053812e-03,  3.76767101e-04,\n",
      "         6.72272989e-04, -1.93533606e-06,  1.44412438e-03,\n",
      "        -5.82810026e-04, -3.47824331e-04,  4.10332694e-04,\n",
      "         9.29610047e-04, -1.23963968e-04, -6.71907794e-04,\n",
      "         2.10191778e-04, -3.50753748e-04, -4.31635650e-04,\n",
      "        -3.01217224e-04, -3.11622425e-04, -1.19040655e-04,\n",
      "        -4.34054673e-04, -1.11264107e-03, -1.57380709e-03,\n",
      "        -1.09433837e-03,  6.12993434e-04, -7.03449361e-04,\n",
      "         3.15445155e-04,  2.50945828e-04,  4.09197441e-04,\n",
      "        -4.28346859e-04,  1.42526245e-04,  3.17510247e-04,\n",
      "        -5.67618466e-04, -4.23723861e-04,  6.90608635e-04,\n",
      "         1.42794641e-04, -1.67836479e-05, -7.89791229e-04,\n",
      "        -1.32672096e-04, -8.07935605e-04,  9.54357791e-04,\n",
      "        -3.41284904e-04, -9.69848261e-05, -8.00241542e-04,\n",
      "        -9.54374205e-04, -7.90078950e-04, -6.15941884e-04,\n",
      "        -1.25111849e-03, -6.47401903e-05,  2.29844038e-04,\n",
      "         3.34648677e-04,  3.08534421e-04,  8.03223229e-04,\n",
      "         1.07221422e-03,  4.87616722e-04, -2.92399141e-04,\n",
      "        -1.66483558e-04,  4.67568665e-04,  7.48579158e-04,\n",
      "        -4.34001995e-04,  6.02475211e-06, -7.25850521e-04,\n",
      "         2.20948605e-05,  7.01853947e-04,  1.07368629e-03,\n",
      "         5.45149727e-04, -5.10691199e-04,  3.41826846e-04,\n",
      "        -3.12673248e-04,  3.04170768e-04, -1.56253554e-05,\n",
      "         4.21157514e-04,  8.43589442e-06,  6.59738289e-05,\n",
      "        -1.07621646e-03,  3.06267844e-04,  2.86205963e-04,\n",
      "        -7.89293554e-04,  3.49943002e-04, -1.71971144e-04,\n",
      "        -5.00325055e-04, -5.65462688e-04, -4.67654201e-04,\n",
      "        -3.86491534e-04,  9.67757311e-04,  1.05675899e-04,\n",
      "         2.93134421e-04, -7.07890547e-04, -9.45244450e-04,\n",
      "         9.75893854e-05, -2.91651322e-05, -8.47331947e-04,\n",
      "         4.48848878e-04,  7.38896953e-04, -1.31213746e-04,\n",
      "        -1.93607018e-04,  1.58443770e-04, -5.76152001e-04,\n",
      "         6.16739853e-04,  3.14542063e-04, -1.78338014e-04,\n",
      "         9.85654915e-05,  9.83048812e-04, -8.01168208e-04,\n",
      "         1.56256297e-04, -6.81552861e-04,  4.55461472e-04,\n",
      "         4.49154380e-04,  1.62785326e-03,  5.11122926e-04,\n",
      "        -5.48027339e-04, -1.94964363e-04,  6.45889522e-05,\n",
      "         5.21072245e-04, -4.17924690e-04, -1.18221884e-04,\n",
      "         9.91296838e-04,  3.07824921e-05, -2.27013836e-04,\n",
      "        -7.88610487e-04,  3.93585156e-04,  3.18044069e-04,\n",
      "         8.73315381e-04,  1.47883853e-04,  1.10898365e-03,\n",
      "         2.88870709e-04,  1.60549869e-04,  3.34444368e-04,\n",
      "         1.36567745e-04, -9.93786845e-04,  1.27919644e-04,\n",
      "         8.95366538e-05, -2.93658977e-05,  2.57400097e-04,\n",
      "         7.32634624e-04,  4.13898641e-04, -1.28790161e-05,\n",
      "         3.45281616e-04,  1.35260657e-03, -4.65030113e-04,\n",
      "        -1.29462429e-03, -4.82620148e-04,  2.60070839e-04,\n",
      "         1.16598509e-04,  3.23869433e-04, -1.23308098e-04,\n",
      "         8.34198552e-04, -4.34456888e-04,  3.98908393e-04,\n",
      "         3.06140282e-04, -2.16737855e-04, -1.18071260e-03,\n",
      "         3.88959015e-04,  2.04002805e-04, -5.41914662e-04,\n",
      "         4.39839903e-04,  5.24442818e-04,  4.13097558e-04,\n",
      "         4.77997732e-04,  8.22265501e-05,  5.67217103e-05,\n",
      "        -1.91174549e-04,  1.29962317e-03,  5.16824948e-05,\n",
      "         3.42379819e-04, -6.23445725e-04, -4.56290378e-04,\n",
      "        -8.83841221e-06, -1.11630448e-04, -5.52673300e-04,\n",
      "        -1.82892778e-04, -9.46722517e-04, -7.90526683e-04,\n",
      "         1.47844374e-03, -3.90014349e-04, -8.49171542e-04,\n",
      "         7.41527401e-05,  6.07196416e-04,  4.83667187e-04,\n",
      "         1.96876426e-04, -7.92455918e-04, -3.89173190e-04,\n",
      "        -1.51387931e-05, -7.02132820e-04,  5.56688639e-04,\n",
      "         3.23704706e-04,  4.98706591e-04, -9.87967505e-05,\n",
      "        -3.83946521e-04, -9.82282290e-05,  9.50758986e-04,\n",
      "        -4.93806438e-05,  1.01711729e-03, -8.22678208e-04,\n",
      "         5.04919153e-04, -5.08572208e-04, -3.40256171e-04,\n",
      "         5.65396331e-04,  1.15162169e-03,  2.12100364e-04,\n",
      "         5.02093229e-04, -1.01190817e-03, -1.39460759e-03,\n",
      "         8.38314299e-04]], dtype=float32), 'post_fc_stack._value_branch._model.0.bias': array([0.], dtype=float32), 'logits_layer._model.0.weight': array([[ 6.14841003e-04, -6.85737061e-04, -1.09142542e-03,\n",
      "         4.47479280e-04,  3.79984936e-04,  9.52627277e-04,\n",
      "        -2.25901822e-04, -4.93449508e-04, -7.69790029e-04,\n",
      "         7.08917214e-05,  4.95744345e-04, -2.89088290e-04,\n",
      "         2.90547177e-04, -1.22976140e-04, -4.20682918e-05,\n",
      "        -7.09356973e-04,  5.28773307e-05,  6.89860899e-04,\n",
      "        -2.64011353e-04, -1.36840579e-04,  3.63972009e-04,\n",
      "         2.25002615e-04,  7.57108093e-04,  4.11143032e-04,\n",
      "        -4.20448399e-04, -2.74564700e-05,  6.12652046e-04,\n",
      "         2.10833401e-04, -7.59243034e-04,  7.03688536e-04,\n",
      "        -1.47153551e-04,  4.57928458e-04,  1.45917453e-04,\n",
      "        -1.19114551e-03, -4.59564500e-04,  1.46067922e-03,\n",
      "         3.10357573e-04,  2.53944454e-04,  8.60037340e-04,\n",
      "         3.60538921e-04,  6.91414578e-04, -6.11648837e-04,\n",
      "         5.77555096e-04, -2.77130923e-04,  9.13870404e-04,\n",
      "         1.17817149e-03, -6.54688396e-04, -3.42523039e-04,\n",
      "         1.17932737e-04, -2.35857136e-04, -3.25516739e-04,\n",
      "         4.36136688e-05,  1.20325909e-04, -2.90994649e-04,\n",
      "        -1.55932363e-03, -2.82623514e-04,  2.12362083e-05,\n",
      "        -5.28288889e-04,  7.93765648e-04, -4.92159452e-04,\n",
      "         5.54045022e-04,  3.72611219e-04,  4.11998335e-04,\n",
      "         6.45213120e-04,  2.49991805e-04, -3.53435142e-04,\n",
      "        -4.47856524e-04, -9.01621243e-04,  1.54995170e-04,\n",
      "         3.23096465e-04, -7.03733356e-04, -1.04560098e-03,\n",
      "        -7.80075032e-04,  3.24491615e-04,  2.12993873e-05,\n",
      "        -5.16158587e-04,  6.36460492e-04,  6.52377435e-04,\n",
      "         1.22316327e-04,  4.65775491e-04,  6.36598386e-04,\n",
      "         6.56812044e-05, -6.03214081e-04, -9.75803472e-04,\n",
      "        -4.89821774e-04,  2.53853505e-04,  4.52093191e-05,\n",
      "        -5.88155468e-04, -8.77930666e-04, -2.14933054e-04,\n",
      "         7.07559404e-04,  1.46199949e-04, -6.87729160e-04,\n",
      "         7.01640383e-04, -6.99235097e-05, -5.69639204e-04,\n",
      "         2.10810613e-04,  3.27938469e-04,  1.11826594e-04,\n",
      "         5.31815342e-04,  2.71086930e-04, -7.75519351e-04,\n",
      "        -1.07196101e-03, -5.93070348e-04, -1.65320234e-04,\n",
      "         2.33613246e-04, -6.58825855e-04, -1.81309922e-04,\n",
      "        -5.86172508e-04,  6.52411079e-04,  1.16835552e-04,\n",
      "         5.24934658e-05, -5.53065387e-04,  7.26545986e-04,\n",
      "        -2.05757431e-04, -6.92109985e-04,  1.27131978e-04,\n",
      "         7.80443544e-04, -1.11524935e-03,  1.13843568e-03,\n",
      "         1.05226645e-03, -5.60017943e-04, -5.91234595e-04,\n",
      "        -7.21819466e-04,  1.20729872e-03,  9.13605210e-04,\n",
      "        -8.93287011e-04,  3.03075358e-04, -6.28431386e-04,\n",
      "         4.72697604e-04, -9.01580555e-04, -1.33171525e-05,\n",
      "        -6.22888911e-04,  7.50307925e-04,  7.61277683e-04,\n",
      "         5.95167221e-04,  7.95840751e-04, -3.47189081e-04,\n",
      "         3.53102922e-04, -4.54428955e-04, -5.30245961e-05,\n",
      "        -5.75550133e-04, -1.27637017e-04,  2.83099682e-04,\n",
      "        -7.06952005e-06, -4.54999681e-04, -4.46226622e-04,\n",
      "         3.01342836e-04, -3.59675731e-04, -3.80924728e-04,\n",
      "        -2.76444131e-04,  1.01513159e-03,  1.36424496e-04,\n",
      "         3.02152766e-04, -4.91563871e-04, -3.67598142e-04,\n",
      "         1.09499833e-03, -2.13166481e-04, -3.07357317e-04,\n",
      "        -7.10937253e-04, -5.25069598e-04,  6.08543691e-04,\n",
      "        -1.14343493e-05, -5.53299265e-04, -6.61726372e-05,\n",
      "         1.05331339e-04,  4.85151977e-05,  1.17735681e-03,\n",
      "        -2.26596472e-04,  3.74024676e-04, -1.60296651e-04,\n",
      "        -3.64805775e-04, -2.40413312e-04,  8.33927246e-04,\n",
      "        -2.71814904e-04, -5.26196673e-04,  2.63414928e-04,\n",
      "         6.55950635e-06,  1.61580265e-05,  2.37777247e-04,\n",
      "         5.26800053e-04, -5.40658191e-04,  6.94947841e-04,\n",
      "         6.05777197e-04, -1.58394466e-03, -6.95448325e-05,\n",
      "        -3.53701180e-04, -5.28610079e-04, -1.53322937e-03,\n",
      "         1.37958396e-03, -5.88343173e-05, -7.94764026e-04,\n",
      "         3.64310727e-05, -4.38111252e-04,  1.36098151e-05,\n",
      "        -3.65598644e-05,  8.45544913e-04, -2.11870021e-04,\n",
      "        -3.07453942e-04, -1.77337264e-04,  2.66007846e-04,\n",
      "        -4.22366109e-04, -6.20591105e-04,  9.00833984e-04,\n",
      "         3.97241354e-04,  9.29370290e-05, -6.50560251e-04,\n",
      "         6.74183771e-04, -1.20311289e-03, -9.01027583e-04,\n",
      "         3.92769871e-04,  8.74596299e-05, -1.02193514e-03,\n",
      "         7.72987260e-04,  9.29480520e-05, -1.55934202e-03,\n",
      "        -1.71381267e-04,  1.13856536e-03, -2.44449417e-04,\n",
      "        -6.59093726e-04, -1.98012960e-04,  1.06191030e-03,\n",
      "         1.30192295e-03,  8.65930633e-04, -5.17237640e-04,\n",
      "        -4.87861515e-04,  1.01366802e-03, -2.33496437e-04,\n",
      "        -6.45185704e-04, -4.44643025e-04,  4.40347998e-04,\n",
      "         4.26684448e-04,  1.03190017e-03, -1.29251694e-03,\n",
      "        -1.64521349e-04, -5.50225261e-04,  1.03383744e-03,\n",
      "         1.52123321e-04, -1.89012935e-04,  9.15178447e-04,\n",
      "        -2.82416469e-04, -1.13100301e-04,  7.69797072e-04,\n",
      "         9.29868082e-04, -3.44873433e-06, -3.25867761e-04,\n",
      "        -4.31656517e-04,  2.00904920e-04,  3.64599371e-04,\n",
      "         1.48882566e-03,  1.46028877e-03,  4.88645921e-04,\n",
      "         2.98098574e-04,  7.38806033e-04, -3.28644965e-04,\n",
      "         1.04048441e-03],\n",
      "       [-1.04139815e-03, -1.02978083e-03, -7.46142294e-04,\n",
      "         6.01820648e-04, -1.09627389e-03, -6.77573262e-04,\n",
      "        -5.83639894e-06, -4.98946116e-04,  5.15723135e-04,\n",
      "         4.05454513e-04,  1.27264138e-04,  4.55999543e-04,\n",
      "        -5.65805240e-04, -7.18801311e-05,  1.41275654e-04,\n",
      "        -5.77351428e-04,  4.41407494e-04, -2.29793543e-04,\n",
      "        -1.49817686e-04, -3.19076818e-04, -1.33713137e-03,\n",
      "        -4.13449219e-04,  4.43667232e-04,  1.61429889e-05,\n",
      "         3.84183455e-04,  1.00374265e-04,  2.29336729e-04,\n",
      "        -6.48331014e-04, -1.87118974e-04,  9.64892155e-04,\n",
      "        -3.80917714e-04, -9.09355913e-06,  1.62880824e-04,\n",
      "        -3.87831351e-05, -1.00288009e-04, -9.22078441e-04,\n",
      "         2.08015117e-04, -8.58130632e-04,  3.83921928e-04,\n",
      "        -1.83181211e-04, -5.69395896e-04, -4.10533812e-06,\n",
      "         2.50071444e-05,  1.44582460e-04, -5.39498978e-05,\n",
      "         3.45067790e-04, -8.73268058e-04,  2.59910128e-04,\n",
      "         8.74989433e-04, -4.35364927e-04,  4.01363868e-05,\n",
      "        -2.83151981e-04, -4.95931541e-04,  4.49201296e-04,\n",
      "         3.69487941e-04, -5.59532491e-04, -3.06992268e-04,\n",
      "        -3.03870038e-04,  6.17743295e-04, -1.02423051e-04,\n",
      "        -3.90179950e-04,  3.42438230e-04,  1.13244023e-04,\n",
      "         1.46929908e-03, -2.88892275e-04, -7.01059704e-04,\n",
      "         6.70167428e-05,  9.14590317e-04,  2.10768223e-04,\n",
      "         9.45457068e-05,  4.18777898e-04, -3.49223637e-06,\n",
      "        -3.37056583e-04, -1.08181918e-03, -1.36007775e-05,\n",
      "        -1.57584233e-04, -1.12761423e-04, -3.95886163e-04,\n",
      "         8.81957749e-05, -5.50775847e-04,  4.52453924e-06,\n",
      "        -3.22995475e-04, -1.40777603e-03,  5.79828571e-04,\n",
      "         3.43869091e-04, -5.59519322e-05, -2.84385984e-04,\n",
      "        -2.24743373e-04, -2.26218472e-04, -1.20594021e-04,\n",
      "        -9.45100153e-04,  3.25281580e-04, -1.05531409e-03,\n",
      "         5.89480391e-04,  5.57180843e-04, -1.20491977e-03,\n",
      "        -1.22592086e-04, -2.02635012e-04, -2.36729902e-04,\n",
      "         8.00352427e-05,  1.11414574e-03, -1.97221350e-04,\n",
      "        -3.44559347e-04,  3.76924436e-04,  9.11050010e-04,\n",
      "        -5.92283148e-04, -1.07131863e-03, -7.45477737e-05,\n",
      "        -1.30604533e-03, -1.56216585e-04, -2.51681660e-04,\n",
      "        -3.67316970e-04, -2.92216457e-04, -6.03971057e-05,\n",
      "         6.36968180e-05,  3.76995391e-04, -9.55275900e-04,\n",
      "        -5.97445003e-04, -1.11431036e-04, -1.01953582e-03,\n",
      "        -3.25326313e-04, -1.60208321e-04,  1.37051116e-04,\n",
      "        -7.86334203e-05,  1.61003484e-03, -7.21100776e-04,\n",
      "        -5.48264943e-04,  1.75020774e-04, -6.78107026e-04,\n",
      "        -9.96159157e-04,  8.39055792e-05,  1.05172585e-04,\n",
      "        -9.76160052e-04,  4.31167369e-04, -3.87726352e-04,\n",
      "         2.32164966e-04,  1.76394042e-05,  1.11815883e-04,\n",
      "         2.69024604e-05,  4.75212379e-04, -2.93358491e-04,\n",
      "        -3.41359904e-04,  5.43419854e-04,  1.99525584e-05,\n",
      "        -3.82162456e-04, -3.42885149e-04, -4.48808511e-04,\n",
      "        -1.03475970e-04,  3.49204347e-04,  2.08905782e-04,\n",
      "         2.31505212e-04, -3.17398924e-04,  2.20857590e-04,\n",
      "        -6.83802646e-05,  8.64041271e-04, -4.35728492e-04,\n",
      "        -7.19412172e-04,  1.50482298e-03,  4.41767566e-04,\n",
      "         6.63664978e-05, -7.68419413e-04,  7.93202664e-04,\n",
      "        -5.92895085e-04, -1.11627567e-03,  1.87564059e-03,\n",
      "        -5.60537912e-04, -1.25309991e-04,  6.73171831e-04,\n",
      "        -8.00610520e-04,  1.71613251e-03,  1.72644306e-03,\n",
      "        -9.27851306e-06,  5.60574932e-04, -1.05065026e-03,\n",
      "        -9.01709791e-05, -2.35292900e-04,  8.49729287e-04,\n",
      "        -7.61447474e-04,  1.07109128e-03, -5.93064469e-04,\n",
      "         1.37143128e-04, -5.19293069e-04, -1.01826049e-03,\n",
      "         5.51035977e-04,  1.28847512e-03,  1.42942683e-03,\n",
      "        -2.14128551e-04, -9.87303792e-04, -3.80379701e-04,\n",
      "        -2.32034610e-04,  6.39595266e-04, -7.25284917e-04,\n",
      "         7.74777902e-04,  1.77640541e-04,  1.06246989e-04,\n",
      "        -2.17339519e-04,  9.50610847e-04,  3.13321478e-04,\n",
      "        -2.40527716e-05, -2.91080214e-04,  1.66398517e-04,\n",
      "         1.30918692e-03, -7.35343376e-04, -4.89477941e-04,\n",
      "         6.62076287e-04,  1.33799640e-05, -3.88755958e-04,\n",
      "        -7.51060084e-04, -2.85869319e-04,  7.90786289e-05,\n",
      "        -8.53598467e-05,  5.89106290e-04,  7.16634851e-04,\n",
      "         5.70485252e-04, -6.76061551e-04, -5.84178953e-04,\n",
      "         5.72427176e-04,  7.13119924e-04, -2.45247124e-04,\n",
      "        -1.03299820e-03, -9.49940004e-04, -8.74176563e-04,\n",
      "        -2.79082597e-04, -1.21163588e-03, -8.90008931e-04,\n",
      "        -1.98924070e-04, -6.33468153e-04, -1.86899662e-04,\n",
      "         2.31940168e-04,  1.01597354e-04, -1.50356995e-04,\n",
      "         8.70886870e-05, -5.40983501e-06, -9.11749026e-04,\n",
      "        -5.48260869e-04, -9.75180999e-04,  1.46274280e-04,\n",
      "         3.42191372e-04, -5.95918624e-04,  4.72217907e-05,\n",
      "         7.44318881e-04,  9.65364045e-04,  1.92945052e-04,\n",
      "        -1.13336754e-03,  1.15874119e-03, -1.58386087e-04,\n",
      "        -1.07984734e-03, -9.83125414e-04,  4.23827383e-04,\n",
      "        -3.82320068e-05,  1.17866057e-05, -9.56429052e-04,\n",
      "        -4.31730004e-04,  7.15375354e-04, -8.68513744e-05,\n",
      "        -5.60931046e-04]], dtype=float32), 'logits_layer._model.0.bias': array([0., 0.], dtype=float32), 'value_layer._model.0.weight': array([[-1.93382148e-04, -3.98484815e-04,  3.53251453e-05,\n",
      "         1.11256109e-03,  1.91342391e-04,  3.66031018e-04,\n",
      "        -9.05179593e-04, -4.78931906e-05, -6.29706017e-04,\n",
      "        -1.36142003e-03,  7.39957322e-04, -1.71796404e-04,\n",
      "        -6.93830603e-04, -1.22618192e-04,  1.09204522e-03,\n",
      "        -3.03699460e-04,  8.08595680e-04,  8.53384088e-04,\n",
      "         1.03895080e-04,  2.60585948e-04,  1.00466801e-04,\n",
      "         6.22981403e-04, -6.27084170e-04, -7.06621911e-04,\n",
      "         1.17526230e-04, -2.56829546e-04,  6.34250580e-04,\n",
      "        -7.97900793e-05, -4.33539717e-05,  5.01067785e-04,\n",
      "         7.12937966e-04, -4.14883252e-05, -1.01009745e-03,\n",
      "        -4.34612477e-04,  5.68443676e-04, -1.18572381e-03,\n",
      "         2.24078889e-04, -4.57012706e-04,  1.35831215e-04,\n",
      "        -1.23455992e-03, -1.19702076e-03, -7.43597921e-05,\n",
      "         5.25102951e-05,  1.03622675e-03, -8.73507292e-04,\n",
      "        -7.06871273e-04,  1.30979141e-04, -4.81307099e-04,\n",
      "        -4.50217020e-04,  6.22340536e-04, -7.41041848e-04,\n",
      "         3.28262940e-05, -3.82379018e-04, -2.79313972e-04,\n",
      "        -3.78586759e-04,  7.72033643e-04,  1.91414336e-04,\n",
      "         5.96702041e-04, -3.07073613e-04, -2.89740536e-04,\n",
      "         2.01172603e-04,  5.20172995e-04, -3.32788099e-04,\n",
      "        -1.86503038e-03,  1.33124948e-03,  7.62372103e-04,\n",
      "         8.04663519e-04,  2.71201017e-04, -3.47979105e-04,\n",
      "        -1.37905625e-03,  1.01885271e-04,  1.48399922e-04,\n",
      "         1.10381888e-03, -3.29060131e-04, -5.53800492e-04,\n",
      "        -1.02405925e-03, -6.80799771e-04,  1.44066173e-04,\n",
      "         5.48074313e-05,  5.62139721e-05, -4.71464991e-05,\n",
      "         1.35241746e-04,  5.19889698e-04, -4.88509424e-04,\n",
      "         1.63292643e-04,  1.21179421e-03, -6.50175862e-05,\n",
      "        -6.88907821e-05, -1.81630137e-04, -7.96654145e-04,\n",
      "        -2.71414261e-04, -6.55217387e-04,  2.38448818e-04,\n",
      "        -9.95603157e-04, -3.16680118e-04, -6.02590408e-05,\n",
      "         2.17460591e-04, -4.53672605e-04,  3.83902778e-04,\n",
      "        -5.62414643e-04,  6.23163825e-04,  5.66520030e-04,\n",
      "         4.01249454e-05,  6.44587795e-04,  1.09572930e-03,\n",
      "         9.23229323e-04, -7.40512391e-04,  4.89683531e-04,\n",
      "         1.16704854e-04,  1.52092322e-03,  4.88295685e-04,\n",
      "        -8.49718752e-04, -4.71516396e-04,  8.80610896e-04,\n",
      "         7.26420491e-04,  7.88461257e-05,  6.53155905e-04,\n",
      "        -4.11860616e-04, -4.61620744e-04,  1.10578618e-03,\n",
      "        -4.85230528e-04,  3.72660928e-04, -1.62386161e-04,\n",
      "        -4.91017476e-04,  1.30557502e-03, -1.44572055e-04,\n",
      "        -5.41896734e-04, -1.06683452e-04,  1.59031188e-03,\n",
      "        -5.10067330e-04, -9.71986330e-04, -4.29849752e-04,\n",
      "        -5.18834451e-04, -6.56984164e-04,  4.28404921e-04,\n",
      "        -5.71488519e-04, -3.74001655e-04,  2.84698558e-06,\n",
      "         9.40829050e-05, -2.19035806e-04,  4.71655803e-05,\n",
      "         8.96311481e-04,  5.01705799e-04, -7.04809092e-04,\n",
      "        -5.78364648e-04, -1.00812945e-03,  4.29198029e-04,\n",
      "        -9.66157764e-04, -1.78964066e-04, -4.81411727e-04,\n",
      "         4.43764555e-04, -4.68290498e-04, -5.83896654e-05,\n",
      "         5.10511280e-04, -3.43101798e-04,  1.18084194e-04,\n",
      "         1.38740244e-04, -7.03228696e-04,  3.34237004e-04,\n",
      "         3.09937517e-04, -8.53860343e-04, -1.82030708e-05,\n",
      "         7.03035737e-04, -3.79498852e-05, -8.53359583e-04,\n",
      "         7.30503452e-05,  1.68680470e-03, -1.77299389e-05,\n",
      "        -4.15058486e-04, -1.75055131e-04,  1.22862100e-03,\n",
      "        -8.16347951e-04,  6.43878244e-04, -9.97525523e-04,\n",
      "         1.45647180e-04, -8.70498596e-04, -2.28086632e-04,\n",
      "         6.32467505e-04,  1.10971858e-03,  1.86394900e-04,\n",
      "         6.61123122e-05, -8.18010769e-04, -3.58499237e-04,\n",
      "         1.14197121e-03, -7.76792003e-04,  1.12301949e-03,\n",
      "        -1.87612895e-04,  6.80311641e-04, -1.54177775e-03,\n",
      "         2.85718095e-04,  7.91972678e-04, -4.39044234e-04,\n",
      "         3.20065410e-05,  6.44477783e-04, -5.26152668e-04,\n",
      "        -3.77518154e-05, -6.06917783e-05, -4.32979956e-04,\n",
      "         1.24205646e-04, -6.22117717e-04, -4.52268723e-04,\n",
      "         2.18935922e-04, -1.01904944e-03,  7.42665376e-04,\n",
      "         7.68744270e-04,  3.80797632e-04,  4.64770201e-05,\n",
      "         9.23318323e-04,  1.36980540e-04, -4.99212823e-04,\n",
      "        -3.85481944e-05,  3.48714122e-04, -4.48854553e-05,\n",
      "        -4.19455348e-04, -6.47790148e-04,  6.01191714e-04,\n",
      "        -4.86512086e-04,  8.77060738e-05, -1.14359288e-03,\n",
      "        -7.10865424e-04,  3.12256045e-04,  6.11849653e-04,\n",
      "        -1.66166996e-04,  2.98862960e-05,  2.23685929e-04,\n",
      "         1.06132496e-03,  4.37223607e-05, -4.22442274e-04,\n",
      "        -2.57865438e-04,  4.45816753e-04,  6.36177836e-04,\n",
      "        -4.65410965e-04, -8.30460995e-05,  1.04372062e-04,\n",
      "        -3.09778698e-04,  4.42007586e-04, -2.13279010e-04,\n",
      "         2.34374224e-04, -2.24696574e-04, -1.43357669e-04,\n",
      "         4.88726422e-04, -1.86115096e-04, -4.23325750e-04,\n",
      "         1.22802870e-04,  8.17807624e-04, -5.81301691e-04,\n",
      "        -5.22930990e-04,  4.84085089e-04,  2.51473102e-04,\n",
      "         1.53870278e-04,  7.47477286e-04, -4.17082570e-04,\n",
      "        -5.30017656e-04, -1.42065308e-03, -9.16333884e-05,\n",
      "        -5.54596598e-04]], dtype=float32), 'value_layer._model.0.bias': array([0.], dtype=float32)}]\n",
      "(1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nobs = env.reset()\\nThen, we call step to advance the state and advance time by one tick.\\n\\nactions = sample_random_actions(env, obs)\\nobs, rew, done, info = env.step(actions)\\n\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = get_trainable_cls(args.run)#.build()##args.run#TwoStepGame(config)\n",
    "print(algo)\n",
    "alg = config.build()\n",
    "#b = traine.get_policy('high_level')#.get_weights()\n",
    "print(alg)\n",
    "#print(b)\n",
    "c = alg.workers.foreach_worker(lambda worker: worker.get_policy().get_weights())\n",
    "#print(trainer.get_policy(\"low_level\"))\n",
    "print(c)\n",
    "c = np.array(c)\n",
    "print(c.shape)\n",
    "\"\"\"\n",
    "obs = env.reset()\n",
    "Then, we call step to advance the state and advance time by one tick.\n",
    "\n",
    "actions = sample_random_actions(env, obs)\n",
    "obs, rew, done, info = env.step(actions)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c8c04c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.645327866077423, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.78, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 8.0, 8.0, 8.0, 7.0, 7.0, 8.0, 1.0, 0.0, 8.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 0.0, 0.0, 7.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 8.0, 1.0, 8.0, 1.0, 7.0, 1.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9361867287858803, 'mean_inference_ms': 1.575353726818786, 'mean_action_processing_ms': 0.189784747451099, 'mean_env_wait_ms': 0.04055725401313743, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.020198583602905273, 'ViewRequirementAgentConnector_ms': 0.16250477896796334}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.78, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 8.0, 8.0, 8.0, 7.0, 7.0, 8.0, 1.0, 0.0, 8.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 0.0, 0.0, 7.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 8.0, 1.0, 8.0, 1.0, 7.0, 1.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9361867287858803, 'mean_inference_ms': 1.575353726818786, 'mean_action_processing_ms': 0.189784747451099, 'mean_env_wait_ms': 0.04055725401313743, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.020198583602905273, 'ViewRequirementAgentConnector_ms': 0.16250477896796334}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 581.905, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 26.76, 'learn_throughput': 7473.88, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '54ffabb45bf941adb06d79ed055ce3a6', 'date': '2023-06-19_02-02-58', 'timestamp': 1687165378, 'time_this_iter_s': 0.5829181671142578, 'time_total_s': 0.5829181671142578, 'pid': 24488, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002501AFA5B50>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002501A78ACA0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.5829181671142578, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 0.019076108932495117, 'perf': {'cpu_util_percent': 11.1, 'ram_util_percent': 49.2, 'gpu_util_percent0': 0.12, 'vram_util_percent0': 0.2242431640625}}\n",
      "{0: 0, 1: 3}\n",
      "{}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"PolicyID 'low_level_policy' not found in PolicyMap of the Trainer's local worker!\"",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[0;32mIn[98], line 17\u001b[0m\n    action = algo.compute_single_action(#trainer.compute_action\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:1514\u001b[1;36m in \u001b[1;35mcompute_single_action\u001b[1;36m\n\u001b[1;33m    raise KeyError(\u001b[1;36m\n",
      "\u001b[1;31mKeyError\u001b[0m\u001b[1;31m:\u001b[0m \"PolicyID 'low_level_policy' not found in PolicyMap of the Trainer's local worker!\"\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\"\"\"\n",
    "as of 6/19/2023 this is no longer the best\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(algo)\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train()) \n",
    "    env = TwoStepGame(config)\n",
    "    obs, info = env.reset()\n",
    "    print(obs)\n",
    "    print(info)\n",
    "    \"\"\"\n",
    "    action = algo.compute_single_action(#trainer.compute_action\n",
    "                obs[1],  \n",
    "                policy_id='low_level_policy',\n",
    "                full_fetch=False\n",
    "            )#algo.compute_single_action(3)\n",
    "    print(action)\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(0)\n",
    "#ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "92bdf000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG\n"
     ]
    }
   ],
   "source": [
    "env_obj = config.build()\n",
    "print(env_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbe55032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN\n"
     ]
    }
   ],
   "source": [
    "algo = DQNConfig().environment(env=\"CartPole-v1\").build()\n",
    "print(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9987bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d99b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=47408)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47408)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47408)\u001b[0m 2023-06-13 20:04:17,929\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47408)\u001b[0m 2023-06-13 20:04:18.415676: W tensorflow/c/c_api.cc:291] Operation '{name:'default_policy_wk1/lr/Assign' id:242 op device:{requested: '', assigned: ''} def:{{{node default_policy_wk1/lr/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](default_policy_wk1/lr, default_policy_wk1/lr/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47408)\u001b[0m 2023-06-13 20:04:18.492587: W tensorflow/c/c_api.cc:291] Operation '{name:'default_policy_wk1/default_policy_wk1/value_out/bias/Adam_1/Assign' id:1073 op device:{requested: '', assigned: ''} def:{{{node default_policy_wk1/default_policy_wk1/value_out/bias/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](default_policy_wk1/default_policy_wk1/value_out/bias/Adam_1, default_policy_wk1/default_policy_wk1/value_out/bias/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-06-13 20:04:19,484\tINFO trainable.py:172 -- Trainable.setup took 12.213 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PPOConfig' object has no attribute 'n_agents'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_38800\\39813507.py\"\u001b[0m, line \u001b[0;32m24\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    dense_logs = generate_rollout_from_current_trainer_policy(\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_38800\\70617460.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[1;36m, in \u001b[1;35mgenerate_rollout_from_current_trainer_policy\u001b[1;36m\u001b[0m\n\u001b[1;33m    for agent_idx in range(env_obj.n_agents):\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m\u001b[1;31m:\u001b[0m 'PPOConfig' object has no attribute 'n_agents'\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.train.rl import RLTrainer\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .rollouts(num_rollout_workers=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"CartPole-v1\")\n",
    "    .build()\n",
    ")\n",
    "#config=PPOConfig()\n",
    "trainer = RLTrainer(\n",
    "        'PPO',\n",
    "        #run_config=air.RunConfig(stop=stop, verbose=2),#RunConfig(stop={\"training_iteration\": 5}),\n",
    "        #scaling_config=ScalingConfig(use_gpu=True),\n",
    "        #algorithm=\"QMIX\",\n",
    "        config=config.to_dict()\n",
    "        \n",
    "        \n",
    "\n",
    "    )#.fit()\n",
    "\n",
    "dense_logs = generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj=config,\n",
    "    num_dense_logs=2)\n",
    "\"\"\"\n",
    "dense_logs = {}\n",
    "# Note: worker 0 is reserved for the trainer actor\n",
    "for worker in range((trainer_config[\"num_workers\"] > 0), trainer_config[\"num_workers\"] + 1):\n",
    "    for env_id in range(trainer_config[\"num_envs_per_worker\"]):\n",
    "        dense_logs[\"worker={};env_id={}\".format(worker, env_id)] = \\\n",
    "        trainer.workers.foreach_worker(lambda w: w.async_env)[worker].envs[env_id].env.previous_episode_dense_log\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#offline learning\n",
    "\n",
    "import ray\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.train.rl import RLTrainer\n",
    "from ray.rllib.algorithms.bc.bc import BC\n",
    "\"\"\"\n",
    "dataset = ray.data.read_json(\n",
    "    \"/tmp/data-dir\", parallelism=2, ray_remote_args={\"num_cpus\": 1}\n",
    ")\n",
    "\n",
    "trainer = RLTrainer(\n",
    "    run_config=RunConfig(stop={\"training_iteration\": 5}),\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    datasets={\"train\": dataset},\n",
    "    algorithm=BCTrainer,\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"framework\": \"tf\",\n",
    "        \"evaluation_num_workers\": 1,\n",
    "        \"evaluation_interval\": 1,\n",
    "        \"evaluation_config\": {\"input\": \"sampler\"},\n",
    "    },\n",
    ")\n",
    "result = trainer.fit()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b1faa08",
   "metadata": {},
   "source": [
    "we are going to have to explore using dense logs for rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf6481fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 15:26:30,079\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-06-12 15:26:53,821\tINFO trainable.py:172 -- Trainable.setup took 30.595 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RolloutWorker' object has no attribute 'previous_episode_dense_log'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_37892\\941653332.py\"\u001b[0m, line \u001b[0;32m10\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    print(algo.workers.foreach_worker_with_id(\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\"\u001b[0m, line \u001b[0;32m734\u001b[0m, in \u001b[0;35mforeach_worker_with_id\u001b[0m\n    local_result = [func(0, self.local_worker())]\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_37892\\941653332.py\"\u001b[1;36m, line \u001b[1;32m11\u001b[1;36m, in \u001b[1;35m<lambda>\u001b[1;36m\u001b[0m\n\u001b[1;33m    lambda _id, worker: worker.previous_episode_dense_log()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m\u001b[1;31m:\u001b[0m 'RolloutWorker' object has no attribute 'previous_episode_dense_log'\n"
     ]
    }
   ],
   "source": [
    "#replay buffer reference\n",
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(1):\n",
    "    algo.train()\n",
    "    #buffer = ReplayBuffer(capacity=2, storage_unit=StorageUnit.FRAGMENTS)\n",
    "    buffer = MultiAgentPrioritizedReplayBuffer(capacity=2,storage_unit='timesteps')#MultiAgentPrioritizedReplayBuffer((capacity=2)\n",
    "    \n",
    "    print(algo.workers.foreach_worker_with_id(\n",
    "        lambda _id, worker: worker.previous_episode_dense_log()\n",
    "    ))\n",
    "    \n",
    "    #sample_batch = buffer.sample(1)\n",
    "    #dummy_batch = SampleBatch({\"a\": [1], \"b\": [2]})\n",
    "    #buffer.add(dummy_batch)\n",
    "    print(\"                sample                \")\n",
    "    print()\n",
    "    #print(sample_batch.MultiAgentBatch(\"low_level_policy\"))\n",
    "    #print(sample_min_n_steps_from_buffer(buffer,2,True))\n",
    "    \n",
    "    \"\"\"\n",
    "    new_obs, rewards, dones, infos = env.step()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #print(buffer.sample(2))\n",
    "    #print(ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer(MultiAgentPrioritizedReplayBuffer,5,True))\n",
    "    #ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer(\n",
    "    \n",
    "    #obs, reward, terminated, truncated, info = TwoStepGame.step()\n",
    "    # Note that in order for RLlib to find out about start and end of an episode,\n",
    "    # \"t\" and \"terminateds\" have to properly mark an episode's trajectory\n",
    "\n",
    "    print(\"####################################\")\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9403a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.logging')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pyqode')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\logger\\tensorboardx.py:35: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   VALID_NP_HPARAMS = (np.bool8, np.float32, np.float64, np.int32, np.int64)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\google\\rpc\\__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   pkg_resources.declare_namespace(__name__)\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\pkg_resources\\__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(pid=34380)\u001b[0m   declare_namespace(parent)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34380)\u001b[0m 2023-06-19 00:37:28,406\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34380)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34380)\u001b[0m   if not isinstance(terminated, (bool, np.bool8)):\n",
      "2023-06-19 00:37:30,064\tINFO trainable.py:172 -- Trainable.setup took 10.691 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.010001243546951649\n",
      "  StateBufferConnector_ms: 0.004837679308514262\n",
      "  ViewRequirementAgentConnector_ms: 0.11020801788152651\n",
      "counters:\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "custom_metrics: {}\n",
      "date: 2023-06-19_00-37-36\n",
      "done: false\n",
      "episode_len_mean: 23.052325581395348\n",
      "episode_media: {}\n",
      "episode_reward_max: 117.0\n",
      "episode_reward_mean: 23.052325581395348\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 172\n",
      "episodes_total: 172\n",
      "experiment_id: 69869d85f33f4ff1842be047ebdeed7b\n",
      "hostname: Gipsy\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6664571762084961\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027433570474386215\n",
      "        model: {}\n",
      "        policy_loss: -0.03886064887046814\n",
      "        total_loss: 8.95967960357666\n",
      "        vf_explained_var: 0.005734757520258427\n",
      "        vf_loss: 8.99305248260498\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 465.5\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 4000\n",
      "num_agent_steps_trained: 4000\n",
      "num_env_steps_sampled: 4000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 4000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 8.9\n",
      "  gpu_util_percent0: 0.02875\n",
      "  ram_util_percent: 49.574999999999996\n",
      "  vram_util_percent0: 0.198974609375\n",
      "pid: 24488\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.14114141523823145\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.05680053003488018\n",
      "  mean_inference_ms: 0.6091856890933212\n",
      "  mean_raw_obs_processing_ms: 0.2979101821262518\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.010001243546951649\n",
      "    StateBufferConnector_ms: 0.004837679308514262\n",
      "    ViewRequirementAgentConnector_ms: 0.11020801788152651\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 23.052325581395348\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 117.0\n",
      "  episode_reward_mean: 23.052325581395348\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 172\n",
      "  hist_stats:\n",
      "    episode_lengths: [36, 14, 11, 13, 11, 11, 17, 29, 27, 38, 26, 11, 19, 15, 14,\n",
      "      20, 32, 22, 11, 10, 14, 15, 15, 17, 12, 11, 12, 27, 27, 41, 36, 39, 24, 18,\n",
      "      16, 13, 44, 46, 22, 11, 18, 26, 15, 17, 23, 11, 26, 20, 20, 14, 19, 14, 13,\n",
      "      16, 30, 22, 21, 24, 32, 24, 32, 26, 12, 20, 58, 26, 32, 23, 13, 18, 19, 27,\n",
      "      37, 19, 29, 13, 19, 28, 41, 11, 30, 20, 15, 20, 51, 17, 11, 10, 22, 20, 12,\n",
      "      21, 47, 20, 17, 42, 13, 28, 35, 24, 25, 12, 30, 16, 13, 36, 33, 28, 14, 11,\n",
      "      18, 22, 38, 10, 24, 16, 22, 14, 19, 75, 31, 14, 15, 16, 31, 17, 21, 27, 26,\n",
      "      18, 11, 18, 12, 44, 13, 75, 18, 14, 54, 16, 10, 19, 42, 67, 16, 14, 14, 17,\n",
      "      49, 117, 15, 20, 12, 14, 11, 10, 9, 20, 19, 14, 13, 12, 18, 32, 21, 40, 17,\n",
      "      16, 34, 18, 20, 18]\n",
      "    episode_reward: [36.0, 14.0, 11.0, 13.0, 11.0, 11.0, 17.0, 29.0, 27.0, 38.0, 26.0,\n",
      "      11.0, 19.0, 15.0, 14.0, 20.0, 32.0, 22.0, 11.0, 10.0, 14.0, 15.0, 15.0, 17.0,\n",
      "      12.0, 11.0, 12.0, 27.0, 27.0, 41.0, 36.0, 39.0, 24.0, 18.0, 16.0, 13.0, 44.0,\n",
      "      46.0, 22.0, 11.0, 18.0, 26.0, 15.0, 17.0, 23.0, 11.0, 26.0, 20.0, 20.0, 14.0,\n",
      "      19.0, 14.0, 13.0, 16.0, 30.0, 22.0, 21.0, 24.0, 32.0, 24.0, 32.0, 26.0, 12.0,\n",
      "      20.0, 58.0, 26.0, 32.0, 23.0, 13.0, 18.0, 19.0, 27.0, 37.0, 19.0, 29.0, 13.0,\n",
      "      19.0, 28.0, 41.0, 11.0, 30.0, 20.0, 15.0, 20.0, 51.0, 17.0, 11.0, 10.0, 22.0,\n",
      "      20.0, 12.0, 21.0, 47.0, 20.0, 17.0, 42.0, 13.0, 28.0, 35.0, 24.0, 25.0, 12.0,\n",
      "      30.0, 16.0, 13.0, 36.0, 33.0, 28.0, 14.0, 11.0, 18.0, 22.0, 38.0, 10.0, 24.0,\n",
      "      16.0, 22.0, 14.0, 19.0, 75.0, 31.0, 14.0, 15.0, 16.0, 31.0, 17.0, 21.0, 27.0,\n",
      "      26.0, 18.0, 11.0, 18.0, 12.0, 44.0, 13.0, 75.0, 18.0, 14.0, 54.0, 16.0, 10.0,\n",
      "      19.0, 42.0, 67.0, 16.0, 14.0, 14.0, 17.0, 49.0, 117.0, 15.0, 20.0, 12.0, 14.0,\n",
      "      11.0, 10.0, 9.0, 20.0, 19.0, 14.0, 13.0, 12.0, 18.0, 32.0, 21.0, 40.0, 17.0,\n",
      "      16.0, 34.0, 18.0, 20.0, 18.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14114141523823145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05680053003488018\n",
      "    mean_inference_ms: 0.6091856890933212\n",
      "    mean_raw_obs_processing_ms: 0.2979101821262518\n",
      "time_since_restore: 6.9019927978515625\n",
      "time_this_iter_s: 6.9019927978515625\n",
      "time_total_s: 6.9019927978515625\n",
      "timers:\n",
      "  learn_throughput: 1679.124\n",
      "  learn_time_ms: 2382.195\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  synch_weights_time_ms: 3.038\n",
      "  training_iteration_time_ms: 6893.407\n",
      "timestamp: 1687160256\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "warmup_time: 10.693188428878784\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'PPO' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[58], line 19\u001b[1;36m\n\u001b[1;33m    for worker in range((algo[\"num_workers\"] > 0), algo[\"num_workers\"] + 1):\u001b[1;36m\n",
      "\u001b[1;31mTypeError\u001b[0m\u001b[1;31m:\u001b[0m 'PPO' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .rollouts(num_rollout_workers=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"CartPole-v1\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(1):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))\n",
    "    #action = algo.compute_single_action(obs)\n",
    "\n",
    "    dense_logs = {}\n",
    "    # Note: worker 0 is reserved for the trainer actor\n",
    "    for worker in range((algo[\"num_workers\"] > 0), algo[\"num_workers\"] + 1):\n",
    "        for env_id in range(algo[\"num_envs_per_worker\"]):\n",
    "            dense_logs[\"worker={};env_id={}\".format(worker, env_id)] = \\\n",
    "            trainer.workers.foreach_worker(lambda w: w.async_env)[worker].envs[env_id].env.previous_episode_dense_log\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save()\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7432fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config=(PPOConfig()\n",
    ".rollouts(num_rollout_workers=1)\n",
    ".resources(num_gpus=0)\n",
    ".environment(env=\"CartPole-v1\"))\n",
    "#.build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4092217",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_config' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_37892\\4135580017.py\"\u001b[1;36m, line \u001b[1;32m5\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    for worker in range((trainer_config[\"num_workers\"] > 0), trainer_config[\"num_workers\"] + 1):\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'trainer_config' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Below, we fetch the dense logs for each rollout worker and environment within\n",
    "\n",
    "dense_logs = {}\n",
    "# Note: worker 0 is reserved for the trainer actor\n",
    "for worker in range((trainer_config[\"num_workers\"] > 0), trainer_config[\"num_workers\"] + 1):\n",
    "    for env_id in range(trainer_config[\"num_envs_per_worker\"]):\n",
    "        dense_logs[\"worker={};env_id={}\".format(worker, env_id)] = \\\n",
    "        trainer.workers.foreach_worker(lambda w: w.async_env)[worker].envs[env_id].env.previous_episode_dense_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc886fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 01:53:17,196\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evaluation': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.605, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 1000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8460604892216455, 'mean_inference_ms': 1.3377260434748712, 'mean_action_processing_ms': 0.14585050974780572, 'mean_env_wait_ms': 0.05333361238703054, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.013681983947753907, 'ViewRequirementAgentConnector_ms': 0.1203453540802002}, 'num_agent_steps_sampled_this_iter': 4000, 'num_env_steps_sampled_this_iter': 2000, 'timesteps_this_iter': 2000}}\n"
     ]
    }
   ],
   "source": [
    "print(algo.evaluate()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3110f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.5299568176269531, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.45, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 1.0, 1.0, 1.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 1.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 0.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 8.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 8.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8617230315706622, 'mean_inference_ms': 1.4389806718968632, 'mean_action_processing_ms': 0.1443238993782309, 'mean_env_wait_ms': 0.074595361206662, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.028244972229003906, 'ViewRequirementAgentConnector_ms': 0.2253970570034451}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.45, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 1.0, 1.0, 1.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 1.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 0.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 8.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 8.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8617230315706622, 'mean_inference_ms': 1.4389806718968632, 'mean_action_processing_ms': 0.1443238993782309, 'mean_env_wait_ms': 0.074595361206662, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.028244972229003906, 'ViewRequirementAgentConnector_ms': 0.2253970570034451}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 524.425, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 16.0, 'learn_throughput': 12499.975, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '932c5765559c47298597f6092e308461', 'date': '2023-06-19_00-50-55', 'timestamp': 1687161055, 'time_this_iter_s': 0.5254247188568115, 'time_total_s': 0.5254247188568115, 'pid': 24488, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x0000025019DC0FA0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x0000025019DD2940>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.5254247188568115, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 0.0199892520904541, 'perf': {'cpu_util_percent': 12.7, 'ram_util_percent': 48.3, 'gpu_util_percent0': 0.16, 'vram_util_percent0': 0.19140625}}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[64], line 7\u001b[1;36m\n\u001b[1;33m    obs, info = env.reset()\u001b[1;36m\n",
      "\u001b[1;31mAttributeError\u001b[0m\u001b[1;31m:\u001b[0m 'NoneType' object has no attribute 'reset'\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    action = algo.compute_single_action(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "ray.shutdown()\n",
    "\n",
    "#moderate prior precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2ff4087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 23:54:09,009\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-04-14 23:54:16,566\tINFO trainable.py:172 -- Trainable.setup took 12.545 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6795865297317505, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.88, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 1.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.039853736535827, 'mean_inference_ms': 1.5531155600476623, 'mean_action_processing_ms': 0.17674170916353288, 'mean_env_wait_ms': 0.04500773415636661, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.012067079544067383, 'ViewRequirementAgentConnector_ms': 0.22479356659783256}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.88, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 1.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.039853736535827, 'mean_inference_ms': 1.5531155600476623, 'mean_action_processing_ms': 0.17674170916353288, 'mean_env_wait_ms': 0.04500773415636661, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.012067079544067383, 'ViewRequirementAgentConnector_ms': 0.22479356659783256}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 577.822, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.064, 'learn_throughput': 16578.932, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-17', 'timestamp': 1681541657, 'time_this_iter_s': 0.5788228511810303, 'time_total_s': 0.5788228511810303, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BA90>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.5788228511810303, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 31.1, 'ram_util_percent': 47.4, 'gpu_util_percent0': 0.12, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7314061522483826, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 3.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.11, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.919299827252243, 'mean_inference_ms': 1.4818142774396406, 'mean_action_processing_ms': 0.1818664056107291, 'mean_env_wait_ms': 0.062393725958845535, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021955251693725586, 'ViewRequirementAgentConnector_ms': 0.16707277297973633}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.11, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.919299827252243, 'mean_inference_ms': 1.4818142774396406, 'mean_action_processing_ms': 0.1818664056107291, 'mean_env_wait_ms': 0.062393725958845535, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021955251693725586, 'ViewRequirementAgentConnector_ms': 0.16707277297973633}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 800, 'timers': {'training_iteration_time_ms': 545.348, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.532, 'learn_throughput': 15958.998, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'done': False, 'episodes_total': 200, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-17', 'timestamp': 1681541657, 'time_this_iter_s': 0.5138733386993408, 'time_total_s': 1.092696189880371, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BE20>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.092696189880371, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 46.2, 'ram_util_percent': 47.3, 'gpu_util_percent0': 0.07, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.5058264136314392, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 5.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.49, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 0.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 8.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8828798665381505, 'mean_inference_ms': 1.4100138240566666, 'mean_action_processing_ms': 0.1712384914201429, 'mean_env_wait_ms': 0.06160482193983334, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.017079591751098633, 'ViewRequirementAgentConnector_ms': 0.14663290977478027}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.49, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 0.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 8.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8828798665381505, 'mean_inference_ms': 1.4100138240566666, 'mean_action_processing_ms': 0.1712384914201429, 'mean_env_wait_ms': 0.06160482193983334, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.017079591751098633, 'ViewRequirementAgentConnector_ms': 0.14663290977478027}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1200, 'timers': {'training_iteration_time_ms': 521.626, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 11.69, 'learn_throughput': 17109.25, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'done': False, 'episodes_total': 300, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-18', 'timestamp': 1681541658, 'time_this_iter_s': 0.47518205642700195, 'time_total_s': 1.567878246307373, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BD60>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.567878246307373, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'warmup_time': 12.548984050750732, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6492184400558472, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 7.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.0, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9334712439262023, 'mean_inference_ms': 1.4415602856658671, 'mean_action_processing_ms': 0.1759609479582711, 'mean_env_wait_ms': 0.0549618819828486, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023952960968017578, 'ViewRequirementAgentConnector_ms': 0.22032475471496582}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.0, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9334712439262023, 'mean_inference_ms': 1.4415602856658671, 'mean_action_processing_ms': 0.1759609479582711, 'mean_env_wait_ms': 0.0549618819828486, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023952960968017578, 'ViewRequirementAgentConnector_ms': 0.22032475471496582}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1600, 'timers': {'training_iteration_time_ms': 538.458, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.656, 'learn_throughput': 15803.187, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'done': False, 'episodes_total': 400, 'training_iteration': 4, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-18', 'timestamp': 1681541658, 'time_this_iter_s': 0.5899109840393066, 'time_total_s': 2.1577892303466797, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B1382D1820>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.1577892303466797, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 37.9, 'ram_util_percent': 47.2, 'gpu_util_percent0': 0.11, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7665503025054932, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 9.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.52, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9231414947357327, 'mean_inference_ms': 1.4200236771132928, 'mean_action_processing_ms': 0.15987335266052308, 'mean_env_wait_ms': 0.058973943079625474, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.046994924545288086, 'ViewRequirementAgentConnector_ms': 0.14233922958374023}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.52, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9231414947357327, 'mean_inference_ms': 1.4200236771132928, 'mean_action_processing_ms': 0.15987335266052308, 'mean_env_wait_ms': 0.058973943079625474, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.046994924545288086, 'ViewRequirementAgentConnector_ms': 0.14233922958374023}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 529.815, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.929, 'learn_throughput': 15469.659, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'episodes_total': 500, 'training_iteration': 5, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-19', 'timestamp': 1681541659, 'time_this_iter_s': 0.4962425231933594, 'time_total_s': 2.654031753540039, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13FD5B580>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.654031753540039, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'warmup_time': 12.548984050750732, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8569368720054626, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 11.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9555395794947082, 'mean_inference_ms': 1.4787746607314338, 'mean_action_processing_ms': 0.17303054676167076, 'mean_env_wait_ms': 0.0628914860861188, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02704167366027832, 'ViewRequirementAgentConnector_ms': 0.2059648036956787}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9555395794947082, 'mean_inference_ms': 1.4787746607314338, 'mean_action_processing_ms': 0.17303054676167076, 'mean_env_wait_ms': 0.0628914860861188, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02704167366027832, 'ViewRequirementAgentConnector_ms': 0.2059648036956787}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2400, 'timers': {'training_iteration_time_ms': 550.907, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.771, 'learn_throughput': 15660.273, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'done': False, 'episodes_total': 600, 'training_iteration': 6, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-20', 'timestamp': 1681541660, 'time_this_iter_s': 0.6573679447174072, 'time_total_s': 3.3113996982574463, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13826E400>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.3113996982574463, 'timesteps_since_restore': 0, 'iterations_since_restore': 6, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 0.0, 'ram_util_percent': 47.1, 'gpu_util_percent0': 0.27, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7892510890960693, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 13.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.64, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9581208824685945, 'mean_inference_ms': 1.4988720885010631, 'mean_action_processing_ms': 0.17073016605744787, 'mean_env_wait_ms': 0.06317990239733548, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02521538734436035, 'ViewRequirementAgentConnector_ms': 0.1761481761932373}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.64, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9581208824685945, 'mean_inference_ms': 1.4988720885010631, 'mean_action_processing_ms': 0.17073016605744787, 'mean_env_wait_ms': 0.06317990239733548, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02521538734436035, 'ViewRequirementAgentConnector_ms': 0.1761481761932373}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2800, 'timers': {'training_iteration_time_ms': 555.635, 'load_time_ms': 0.143, 'load_throughput': 1398767.413, 'learn_time_ms': 13.375, 'learn_throughput': 14952.879, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'done': False, 'episodes_total': 700, 'training_iteration': 7, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-20', 'timestamp': 1681541660, 'time_this_iter_s': 0.5860385894775391, 'time_total_s': 3.8974382877349854, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BDF0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.8974382877349854, 'timesteps_since_restore': 0, 'iterations_since_restore': 7, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 0.0, 'ram_util_percent': 47.0, 'gpu_util_percent0': 0.41, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8371381759643555, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 15.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.03, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.944832427139806, 'mean_inference_ms': 1.4962974300539398, 'mean_action_processing_ms': 0.16498297620459987, 'mean_env_wait_ms': 0.06601082243672166, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.01466989517211914, 'ViewRequirementAgentConnector_ms': 0.12430882453918457}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.03, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.944832427139806, 'mean_inference_ms': 1.4962974300539398, 'mean_action_processing_ms': 0.16498297620459987, 'mean_env_wait_ms': 0.06601082243672166, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.01466989517211914, 'ViewRequirementAgentConnector_ms': 0.12430882453918457}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3200, 'timers': {'training_iteration_time_ms': 552.196, 'load_time_ms': 0.125, 'load_throughput': 1598591.329, 'learn_time_ms': 13.453, 'learn_throughput': 14866.137, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'done': False, 'episodes_total': 800, 'training_iteration': 8, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-21', 'timestamp': 1681541661, 'time_this_iter_s': 0.529125452041626, 'time_total_s': 4.426563739776611, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B138313160>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 4.426563739776611, 'timesteps_since_restore': 0, 'iterations_since_restore': 8, 'warmup_time': 12.548984050750732, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8821440935134888, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 17.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.31, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.960271534557014, 'mean_inference_ms': 1.5386448245390063, 'mean_action_processing_ms': 0.16615392101400103, 'mean_env_wait_ms': 0.06758842383537736, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.022125959396362305, 'ViewRequirementAgentConnector_ms': 0.2020738124847412}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.31, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.960271534557014, 'mean_inference_ms': 1.5386448245390063, 'mean_action_processing_ms': 0.16615392101400103, 'mean_env_wait_ms': 0.06758842383537736, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.022125959396362305, 'ViewRequirementAgentConnector_ms': 0.2020738124847412}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3600, 'timers': {'training_iteration_time_ms': 565.449, 'load_time_ms': 0.111, 'load_throughput': 1798415.245, 'learn_time_ms': 14.18, 'learn_throughput': 14104.005, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'done': False, 'episodes_total': 900, 'training_iteration': 9, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-22', 'timestamp': 1681541662, 'time_this_iter_s': 0.6734707355499268, 'time_total_s': 5.100034475326538, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B137FF9AF0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 5.100034475326538, 'timesteps_since_restore': 0, 'iterations_since_restore': 9, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 8.0, 'ram_util_percent': 47.1, 'gpu_util_percent0': 0.03, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8496705293655396, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 19.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.59, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9562232862526865, 'mean_inference_ms': 1.5362027524293271, 'mean_action_processing_ms': 0.17369788387666524, 'mean_env_wait_ms': 0.06433000331041754, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.013000011444091797, 'ViewRequirementAgentConnector_ms': 0.17013216018676758}}, 'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.59, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9562232862526865, 'mean_inference_ms': 1.5362027524293271, 'mean_action_processing_ms': 0.17369788387666524, 'mean_env_wait_ms': 0.06433000331041754, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.013000011444091797, 'ViewRequirementAgentConnector_ms': 0.17013216018676758}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 2000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 565.019, 'load_time_ms': 0.2, 'load_throughput': 1000430.292, 'learn_time_ms': 14.163, 'learn_throughput': 14121.76, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 1000, 'training_iteration': 10, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-22', 'timestamp': 1681541662, 'time_this_iter_s': 0.5621469020843506, 'time_total_s': 5.662181377410889, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13B131CD0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 5.662181377410889, 'timesteps_since_restore': 0, 'iterations_since_restore': 10, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 11.6, 'ram_util_percent': 47.1, 'gpu_util_percent0': 0.16, 'vram_util_percent0': 0.56298828125}}\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()\n",
    "\n",
    "#prior precision is large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b907663",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()\n",
    "\n",
    "#small prior precision\n",
    "#episode reward mean is 4.64 and policy loss is 1.6 under prior precision 20\n",
    "#policy loss increased to 1.65 and epsidoe reward mean increased to 4.79 under prior precison 0.0000000001\n",
    "#under prior precision 2 episdoe reward mean is 5.05 and loss is 1.7. second trial of same prior precison yielded 5.15 reward mean and 1.8 loss \n",
    "#trial 3 under prior precision 2 loss is 1.73 and episdoe reward mean is 5.04. trial 4 loss is 1.65 and epsidoe reward mean and reward mean 4.79\n",
    "#trial 5 is 1.43 policy loss and reward of 4.18 for prior precision 2 \n",
    "\n",
    "#reward is increasing but policy loss is also increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a054f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 23:46:46,491\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2023-04-19 23:46:50,066\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-04-19 23:46:55,310\tWARNING worker.py:846 -- `ray.get_gpu_ids()` will always return the empty list when called from the driver. This is because Ray does not manage GPU allocations to the driver process.\n",
      "2023-04-19 23:46:55,310\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-04-19 23:46:55,312\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property worker_index not supported.\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6482515335083008, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.79, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 0.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7618090406579167, 'mean_inference_ms': 1.0791999190600952, 'mean_action_processing_ms': 0.11994945469187263, 'mean_env_wait_ms': 0.031213855268943377, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.024434328079223633, 'ViewRequirementAgentConnector_ms': 0.15880955590142143}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.79, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 0.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7618090406579167, 'mean_inference_ms': 1.0791999190600952, 'mean_action_processing_ms': 0.11994945469187263, 'mean_env_wait_ms': 0.031213855268943377, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.024434328079223633, 'ViewRequirementAgentConnector_ms': 0.15880955590142143}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 411.886, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 9.463, 'learn_throughput': 21135.851, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-55', 'timestamp': 1681973215, 'time_this_iter_s': 0.4128868579864502, 'time_total_s': 0.4128868579864502, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB647F0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.4128868579864502, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 3.2, 'ram_util_percent': 40.3, 'gpu_util_percent0': 0.47, 'vram_util_percent0': 0.53369140625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.5196247100830078, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 3.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.47, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 8.0, 0.0, 8.0, 7.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 8.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 0.0, 0.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6959997210419389, 'mean_inference_ms': 1.0471641274164445, 'mean_action_processing_ms': 0.1251792669890825, 'mean_env_wait_ms': 0.03438102931453106, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008956432342529297, 'ViewRequirementAgentConnector_ms': 0.10797858238220215}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.47, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 8.0, 0.0, 8.0, 7.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 8.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 0.0, 0.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6959997210419389, 'mean_inference_ms': 1.0471641274164445, 'mean_action_processing_ms': 0.1251792669890825, 'mean_env_wait_ms': 0.03438102931453106, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008956432342529297, 'ViewRequirementAgentConnector_ms': 0.10797858238220215}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 800, 'timers': {'training_iteration_time_ms': 391.23, 'load_time_ms': 0.502, 'load_throughput': 398698.099, 'learn_time_ms': 8.234, 'learn_throughput': 24289.812, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'done': False, 'episodes_total': 200, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-56', 'timestamp': 1681973216, 'time_this_iter_s': 0.37207984924316406, 'time_total_s': 0.7849667072296143, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB53FD0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.7849667072296143, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 4.1, 'ram_util_percent': 40.4, 'gpu_util_percent0': 0.01, 'vram_util_percent0': 0.52978515625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.55398428440094, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 5.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.61, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 1.0, 8.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 0.0, 8.0, 0.0, 0.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 8.0, 8.0, 1.0, 8.0, 0.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6706936783877867, 'mean_inference_ms': 1.033522721733309, 'mean_action_processing_ms': 0.12198105429650936, 'mean_env_wait_ms': 0.029594053246217236, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.016854286193847656, 'ViewRequirementAgentConnector_ms': 0.1025397777557373}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.61, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 1.0, 8.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 0.0, 8.0, 0.0, 0.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 8.0, 8.0, 1.0, 8.0, 0.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6706936783877867, 'mean_inference_ms': 1.033522721733309, 'mean_action_processing_ms': 0.12198105429650936, 'mean_env_wait_ms': 0.029594053246217236, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.016854286193847656, 'ViewRequirementAgentConnector_ms': 0.1025397777557373}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1200, 'timers': {'training_iteration_time_ms': 381.445, 'load_time_ms': 0.334, 'load_throughput': 598047.148, 'learn_time_ms': 8.001, 'learn_throughput': 24998.335, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'done': False, 'episodes_total': 300, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-56', 'timestamp': 1681973216, 'time_this_iter_s': 0.3629014492034912, 'time_total_s': 1.1478681564331055, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB64430>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.1478681564331055, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8084396123886108, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 7.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.42, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6679702787363574, 'mean_inference_ms': 1.0462500778179191, 'mean_action_processing_ms': 0.12052639593345839, 'mean_env_wait_ms': 0.03543328703119514, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0290679931640625, 'ViewRequirementAgentConnector_ms': 0.12602901458740234}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.42, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6679702787363574, 'mean_inference_ms': 1.0462500778179191, 'mean_action_processing_ms': 0.12052639593345839, 'mean_env_wait_ms': 0.03543328703119514, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0290679931640625, 'ViewRequirementAgentConnector_ms': 0.12602901458740234}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1600, 'timers': {'training_iteration_time_ms': 384.338, 'load_time_ms': 0.251, 'load_throughput': 797396.198, 'learn_time_ms': 8.061, 'learn_throughput': 24811.026, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'done': False, 'episodes_total': 400, 'training_iteration': 4, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-56', 'timestamp': 1681973216, 'time_this_iter_s': 0.39403510093688965, 'time_total_s': 1.5419032573699951, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB57850>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.5419032573699951, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 2.8, 'ram_util_percent': 40.4, 'gpu_util_percent0': 0.04, 'vram_util_percent0': 0.52978515625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7519081234931946, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 9.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.36, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626207273561401, 'mean_inference_ms': 1.0419222977492484, 'mean_action_processing_ms': 0.1175179705395923, 'mean_env_wait_ms': 0.039889024092362724, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.009093046188354492, 'ViewRequirementAgentConnector_ms': 0.125152587890625}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.36, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626207273561401, 'mean_inference_ms': 1.0419222977492484, 'mean_action_processing_ms': 0.1175179705395923, 'mean_env_wait_ms': 0.039889024092362724, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.009093046188354492, 'ViewRequirementAgentConnector_ms': 0.125152587890625}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 383.847, 'load_time_ms': 0.201, 'load_throughput': 996745.247, 'learn_time_ms': 8.964, 'learn_throughput': 22311.433, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'episodes_total': 500, 'training_iteration': 5, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-57', 'timestamp': 1681973217, 'time_this_iter_s': 0.3828868865966797, 'time_total_s': 1.9247901439666748, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB946A0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.9247901439666748, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7848349213600159, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 11.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.51, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6609894056105791, 'mean_inference_ms': 1.0494113861770056, 'mean_action_processing_ms': 0.12151188497043072, 'mean_env_wait_ms': 0.03701780956055501, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021304607391357422, 'ViewRequirementAgentConnector_ms': 0.14121270179748535}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.51, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6609894056105791, 'mean_inference_ms': 1.0494113861770056, 'mean_action_processing_ms': 0.12151188497043072, 'mean_env_wait_ms': 0.03701780956055501, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021304607391357422, 'ViewRequirementAgentConnector_ms': 0.14121270179748535}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2400, 'timers': {'training_iteration_time_ms': 385.455, 'load_time_ms': 0.338, 'load_throughput': 592137.035, 'learn_time_ms': 8.901, 'learn_throughput': 22470.188, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'done': False, 'episodes_total': 600, 'training_iteration': 6, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-57', 'timestamp': 1681973217, 'time_this_iter_s': 0.394512414932251, 'time_total_s': 2.319302558898926, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB57F10>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.319302558898926, 'timesteps_since_restore': 0, 'iterations_since_restore': 6, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 3.7, 'ram_util_percent': 40.5, 'gpu_util_percent0': 0.05, 'vram_util_percent0': 0.52978515625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8710399270057678, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 13.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 1.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6585515966422213, 'mean_inference_ms': 1.055777711071856, 'mean_action_processing_ms': 0.12322258387694271, 'mean_env_wait_ms': 0.03422387236786434, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008939266204833984, 'ViewRequirementAgentConnector_ms': 0.12508893013000488}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 1.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6585515966422213, 'mean_inference_ms': 1.055777711071856, 'mean_action_processing_ms': 0.12322258387694271, 'mean_env_wait_ms': 0.03422387236786434, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008939266204833984, 'ViewRequirementAgentConnector_ms': 0.12508893013000488}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2800, 'timers': {'training_iteration_time_ms': 385.937, 'load_time_ms': 0.43, 'load_throughput': 465332.087, 'learn_time_ms': 8.849, 'learn_throughput': 22602.622, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'done': False, 'episodes_total': 700, 'training_iteration': 7, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-58', 'timestamp': 1681973218, 'time_this_iter_s': 0.38883185386657715, 'time_total_s': 2.708134412765503, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB64D00>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.708134412765503, 'timesteps_since_restore': 0, 'iterations_since_restore': 7, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.85731041431427, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 15.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.07, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6657106886797587, 'mean_inference_ms': 1.0772254748466532, 'mean_action_processing_ms': 0.12328861505816983, 'mean_env_wait_ms': 0.03620254330751226, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.010513544082641602, 'ViewRequirementAgentConnector_ms': 0.12743854522705078}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.07, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6657106886797587, 'mean_inference_ms': 1.0772254748466532, 'mean_action_processing_ms': 0.12328861505816983, 'mean_env_wait_ms': 0.03620254330751226, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.010513544082641602, 'ViewRequirementAgentConnector_ms': 0.12743854522705078}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3200, 'timers': {'training_iteration_time_ms': 393.099, 'load_time_ms': 0.376, 'load_throughput': 531808.099, 'learn_time_ms': 9.651, 'learn_throughput': 20723.678, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'done': False, 'episodes_total': 800, 'training_iteration': 8, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-58', 'timestamp': 1681973218, 'time_this_iter_s': 0.44522714614868164, 'time_total_s': 3.1533615589141846, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB53520>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.1533615589141846, 'timesteps_since_restore': 0, 'iterations_since_restore': 8, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 3.6, 'ram_util_percent': 40.6, 'gpu_util_percent0': 0.07, 'vram_util_percent0': 0.5296630859375}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9545664191246033, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 17.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.57, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6633380728917012, 'mean_inference_ms': 1.066249718208038, 'mean_action_processing_ms': 0.11748092032882122, 'mean_env_wait_ms': 0.038373476925457534, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008992910385131836, 'ViewRequirementAgentConnector_ms': 0.14661622047424316}}, 'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.57, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6633380728917012, 'mean_inference_ms': 1.066249718208038, 'mean_action_processing_ms': 0.11748092032882122, 'mean_env_wait_ms': 0.038373476925457534, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008992910385131836, 'ViewRequirementAgentConnector_ms': 0.14661622047424316}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3600, 'timers': {'training_iteration_time_ms': 389.767, 'load_time_ms': 0.334, 'load_throughput': 598284.111, 'learn_time_ms': 9.587, 'learn_throughput': 20862.513, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'done': False, 'episodes_total': 900, 'training_iteration': 9, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-59', 'timestamp': 1681973219, 'time_this_iter_s': 0.36411547660827637, 'time_total_s': 3.517477035522461, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB94910>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.517477035522461, 'timesteps_since_restore': 0, 'iterations_since_restore': 9, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9812411069869995, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 19.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.62, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626733001144689, 'mean_inference_ms': 1.061902172502311, 'mean_action_processing_ms': 0.11869277553758524, 'mean_env_wait_ms': 0.039210503009603596, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0190579891204834, 'ViewRequirementAgentConnector_ms': 0.1209249496459961}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.62, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626733001144689, 'mean_inference_ms': 1.061902172502311, 'mean_action_processing_ms': 0.11869277553758524, 'mean_env_wait_ms': 0.039210503009603596, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0190579891204834, 'ViewRequirementAgentConnector_ms': 0.1209249496459961}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 2000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 389.165, 'load_time_ms': 0.301, 'load_throughput': 664760.124, 'learn_time_ms': 9.579, 'learn_throughput': 20878.714, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 1000, 'training_iteration': 10, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-59', 'timestamp': 1681973219, 'time_this_iter_s': 0.3837430477142334, 'time_total_s': 3.9012200832366943, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F0002B7280>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.9012200832366943, 'timesteps_since_restore': 0, 'iterations_since_restore': 10, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 5.5, 'ram_util_percent': 40.4, 'gpu_util_percent0': 0.05, 'vram_util_percent0': 0.5296630859375}}\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70b544f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.models.torch.torch_action_dist.TorchCategorical'>\n"
     ]
    }
   ],
   "source": [
    "policy = algo.get_policy()\n",
    "print(policy.dist_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dedf576",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PG' object has no attribute 'observation_space'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_15792\\2622574447.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    prep = get_preprocessor(algo.observation_space)#(algo.observation_space)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m\u001b[1;31m:\u001b[0m 'PG' object has no attribute 'observation_space'\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "prep = get_preprocessor(algo.observation_space)#(algo.observation_space)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b45d9e97",
   "metadata": {},
   "source": [
    "WE can use tigramite to confirm and validate what we see with our primary method of causal inference\n",
    "\n",
    "However it is likely we will only be able to use tigramite only for a few time steps at a time and only for at most a few nodes even if we find ways to accelerate tigramite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f41a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_models(n,X,dx):\n",
    "        library_functions = [\n",
    "            #lambda x : (math.e)**x,\n",
    "            #lambda x : np.log(1-x),\n",
    "            #lambda x : np.log(1+x),\n",
    "            lambda x : np.sin(x),\n",
    "            lambda x : np.cos(x),\n",
    "            lambda x : 1/(1-x),\n",
    "            lambda x : 1/(1-x**2),\n",
    "            lambda x : 1/((1-x)**2),\n",
    "            lambda x : 1/(1+x),\n",
    "            lambda x : 1/(1+x**2),\n",
    "            lambda x : 1/((1+x)**2),\n",
    "            lambda x : 1/x,\n",
    "            #lambda x : 1/x**2,\n",
    "            #lambda x : 1/x**3\n",
    "        ]\n",
    "        library_function_names = [\n",
    "            #lambda x : 'e^' + x,\n",
    "    \t    #lambda x : 'ln(1-'+ x + ')',\n",
    "            #lambda x : 'ln(1+'+ x + ')',\n",
    "    \t    lambda x : 'sin(' + x + ')',\n",
    "    \t    lambda x : 'cos(' + x + ')',\n",
    "    \t    lambda x : '1/1-' + x,\n",
    "    \t    lambda x : '1/1-' + x + '^2',\n",
    "    \t    lambda x : '1/(1-' + x + ')^2',\n",
    "    \t    lambda x : '1/1+' + x,\n",
    "    \t    lambda x : '1/1+' + x + '^2',\n",
    "    \t    lambda x : '1/(1+' + x + ')^2',\n",
    "     \t    lambda x : '1/' + x,\n",
    "\t    #lambda x : '1/' + x + '^2',\n",
    "    \t    #lambda x : '1/' + x + '^3'\n",
    "        ]\n",
    "        \n",
    "        lib_custom = ps.CustomLibrary(library_functions=library_functions, function_names=library_function_names)\n",
    "        lib_poly = ps.PolynomialLibrary(degree=1, include_bias=True, include_interaction=False)\n",
    "        lib = ps.GeneralizedLibrary([lib_custom, lib_poly], inputs_per_library=np.array([[0,0],[0,1]]))\n",
    "\n",
    "        coeff = []\n",
    "        for i in range(n):\n",
    "            model = ps.SINDy(feature_library=lib, optimizer=Lasso(alpha=0.0001, fit_intercept=False,\n",
    "            max_iter=100000), discrete_time=True)\n",
    "            model.fit(X[i].T, x_dot=dx[i].T)\n",
    "            coeff.append(model.coefficients())\n",
    "        return np.array(coeff)\n",
    "\n",
    "#def predicted_equations(x,parameters):\n",
    "#        return parameters[0][0]/(1+x[0]**2) + parameters[0][1] + parameters[0][2]*x[0] + parameters[0][3]*x[1], \n",
    "#        parameters[1][3]*x[1] + parameters[1][2]*x[0] + parameters[1][1]\n",
    "\n",
    "#def predicted_series(n,x,coeffs):\n",
    "#        predicted_series = np.zeros_like((x))\n",
    "#        for i in range(n):\n",
    "#                predicted_series[i,:,0] = x[0,:,0]\n",
    "#                for t in range(x.shape[2]-1):\n",
    "#                        predicted_series[i,:,t+1] = predicted_equations(x[0,:,t],parameters=coeffs[i])\n",
    "#        return predicted_series\n",
    "\n",
    "def similarity(n,x,coeff,k_in):\n",
    "        #Check the correlation between gorund-truth time series u-component\n",
    "        corr_matrix_gt = np.corrcoef(x[:,0,:], x[:,0,:])[0:n,0:n]\n",
    "        distance_matrix = pairwise_distances(coeff[:,0,:], metric='seuclidean')\n",
    "        \n",
    "        #similarity analysis\n",
    "        s = np.sum(distance_matrix, axis=1)\n",
    "        s_gt = np.sum(np.abs(corr_matrix_gt), axis=1)\n",
    "        \n",
    "        #predicted low-degree node\n",
    "        print('one of the low degree nodes:', np.argmin(s))\n",
    "        \n",
    "        #predicted hub\n",
    "        print('predicted hub:', np.argmax(s))\n",
    "        \n",
    "        #check\n",
    "        print('Predicted low degree node', np.argmin(s), 'has', k_in[np.argmin(s)], 'in connection(s).')\n",
    "        print('The real hub is:', np.argmax(k_in))\n",
    "        \n",
    "        hub_id = np.argmax(s)\n",
    "        ld_id = np.argmin(s)\n",
    "        \n",
    "        return corr_matrix_gt, distance_matrix, s, s_gt, hub_id, ld_id\n",
    "\n",
    "def local_dynamics_function(n,m,time,x,beta,mu,sigma):\n",
    "\n",
    "        def rulkov_map(x):\n",
    "                x = x.reshape(n,m).T\n",
    "                return np.asarray([beta/(1+x[0]**2)+x[1],x[1]-mu*x[0]-sigma]).T.flatten()\n",
    "\n",
    "        y = x.reshape(n*m,time)\n",
    "        F_x = np.zeros_like(y)\n",
    "        for i in range(time):\n",
    "                F_x[:,i] = rulkov_map(y[:,i])\n",
    "        Fx = F_x.reshape(n,m,time)\n",
    "        return Fx\n",
    "\n",
    "def coupling_effect(dx,hub_id,Fx):\n",
    "        Y_hub = dx[hub_id,:,:] - Fx[hub_id,:,:-1]\n",
    "        Y = dx[:,:,:] - Fx[:,:,:-1]\n",
    "        return Y_hub, Y\n",
    "\n",
    "\n",
    "def reconstruction(n,m,time,x,y):\n",
    "        L_predicted = []\n",
    "        #model = ps.SINDy(feature_library=ps.IdentityLibrary(), optimizer=ps.STLSQ(threshold=0.0001))\n",
    "        model = ps.SINDy(feature_library=ps.IdentityLibrary(), optimizer=Lasso(alpha=0.001, fit_intercept=False))\n",
    "        model.fit(x.reshape(n*m,time-1).T, x_dot=y.reshape(n*m,time-1).T, quiet=True)\n",
    "        L_predicted.append(model.coefficients())\n",
    "        L_predicted = np.array(L_predicted)[0,:,:]\n",
    "        #remove kronecker product\n",
    "        xxx = np.arange(0,n*m,m)\n",
    "        xx, yy = np.meshgrid(xxx, xxx)\n",
    "        L_predicted = L_predicted[xx,yy]\n",
    "        return L_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734cfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#causal inference module\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from NEMtropy import UndirectedGraph, matrix_generator\n",
    "from NEMtropy.network_functions import build_adjacency_from_edgelist\n",
    "#generating null models\n",
    "graph.solve_tool(model=\"cm_exp\",\n",
    "                 method=\"fixed_point\",\n",
    "                 initial_guess=\"random\")\n",
    "\n",
    "# After that we have solved the model we can use it to generate random versions of zachary karate club.\n",
    "# The ensemble sampler function generates \"n\" random versions of zachary karate club using the model parameters computed by solve_tool.\n",
    "\n",
    "graph.ensemble_sampler(10, cpu_n=12, output_dir=\"sample/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d23daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory pre-allocation\n",
    "emp_dyads = np.zeros(n)\n",
    "emp_singles = np.zeros(n)\n",
    "emp_zeros = np.zeros(n)\n",
    "\n",
    "for i in range(n):\n",
    "    # Read the graph as an edgelist\n",
    "    edgelist_ens = np.loadtxt(f\"sample/{i}.txt\")\n",
    "    # read the graph in networkx \n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_edges_from(edgelist_ens.astype(int))\n",
    "    # create adjacency matrix as numpy array\n",
    "    a = nx.to_numpy_array(graph)\n",
    "    # compute dyads numerosity\n",
    "    emp_dyads[i] = count_2motif_2(a)\n",
    "    emp_singles[i] = count_2motif_1(a)\n",
    "    emp_zeros[i] = count_2motif_0(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score = {}\n",
    "emp_mu = np.mean(emp_dyads)\n",
    "emp_std = np.std(emp_dyads)\n",
    "z_score['dyads'] = (count_2motif_2(A) - emp_mu)/emp_std\n",
    "emp_mu = np.mean(emp_singles)\n",
    "emp_std = np.std(emp_singles)\n",
    "z_score['singles'] = (count_2motif_1(A) - emp_mu)/emp_std\n",
    "emp_mu = np.mean(emp_zeros)\n",
    "emp_std = np.std(emp_zeros)\n",
    "z_score['zeros'] = (count_2motif_0(A) - emp_mu)/emp_std\n",
    "\n",
    "print(z_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86254ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory pre-allocation\n",
    "sample_numerosity = list(range(10,1010,50))\n",
    "z_score_empirical ={\n",
    "    '2': [],\n",
    "    '1': [],\n",
    "    '0': []\n",
    "}\n",
    "for n in sample_numerosity: \n",
    "    emp_dyads = np.zeros(n)\n",
    "    emp_singles = np.zeros(n)\n",
    "    emp_zeros = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        # Read the graph as an edgelist\n",
    "        edgelist_ens = np.loadtxt(f\"sample/{i}.txt\")\n",
    "        # read the graph in networkx \n",
    "        graph = nx.DiGraph()\n",
    "        graph.add_edges_from(edgelist_ens.astype(int))\n",
    "        # create adjacency matrix as numpy array\n",
    "        a = nx.to_numpy_array(graph)\n",
    "        # compute dyads numerosity\n",
    "        emp_dyads[i] = count_2motif_2(a)\n",
    "        emp_singles[i] = count_2motif_1(a)\n",
    "        emp_zeros[i] = count_2motif_0(a)\n",
    "\n",
    "    emp_mu = np.mean(emp_dyads)\n",
    "    emp_std = np.std(emp_dyads)\n",
    "    z_score_empirical['2'].append((count_2motif_2(A) - emp_mu)/emp_std)\n",
    "    emp_mu = np.mean(emp_singles)\n",
    "    emp_std = np.std(emp_singles)\n",
    "    z_score_empirical['1'].append((count_2motif_1(A) - emp_mu)/emp_std)\n",
    "    emp_mu = np.mean(emp_zeros)\n",
    "    emp_std = np.std(emp_zeros)\n",
    "    z_score_empirical['0'].append((count_2motif_0(A) - emp_mu)/emp_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_map = {\n",
    "    '0': 'zeros',\n",
    "    '1': 'singles',\n",
    "    '2': 'dyads'\n",
    "}\n",
    "\n",
    "for data in ['2', '1', '0']:\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(\n",
    "        sample_numerosity,\n",
    "        z_score_empirical[data],\n",
    "        color = 'blue',\n",
    "        marker='o', \n",
    "        linestyle='dashed',\n",
    "        label = 'empirical'\n",
    "        )\n",
    "\n",
    "    plt.hlines(\n",
    "        y = z_score_analytical[data],\n",
    "        xmin = sample_numerosity[0],\n",
    "        xmax = sample_numerosity[-1],\n",
    "        color = 'red',\n",
    "        label = 'analytical'\n",
    "    )\n",
    "\n",
    "    plt.xlabel('sample size')\n",
    "    plt.ylabel('z score')\n",
    "    plt.legend(loc='center right')\n",
    "    plt.title(f'{data_map[data]}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
