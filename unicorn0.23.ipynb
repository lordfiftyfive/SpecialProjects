{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix for matmul error that is causing last bug with memory\n",
    "\n",
    "\n",
    "#this goes inside reset function \n",
    "#self.images = np.pad(self.images, ((0, 0), (0, 0), (0, 1)), mode='constant', constant_values=0)\n",
    "\n",
    "#these go inside step function\n",
    "\n",
    "\"\"\"\n",
    "obs[agent_id] = {\n",
    "    \"image\": self.images[self.current_step].astype(np.float32),  # Shape (175, 175)\n",
    "    \"other_action\": np.array(other_actions, dtype=np.float32).reshape(1,)\n",
    "}\n",
    "\n",
    "# Convert image to 1D and replace the last element (which was 0) with other_action\n",
    "obs[agent_id] = np.concatenate((\n",
    "    obs[agent_id][\"image\"].flatten(),  # Shape (30625,)\n",
    "    obs[agent_id][\"other_action\"]      # Shape (1,)\n",
    "))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntrained_pop, fitnesses = train_multi_agent(\\n    pop=pop_agent,\\n    env=env,\\n    algo=\"MATD3\",\\n    env_name=\"cichyenv\",\\n    memory=memory,\\n    swap_channels=False,\\n    max_steps=500000,\\n    evo_steps=10000,\\n    eval_steps=None,\\n    eval_loop=1,\\n    target=200.0,\\n    tournament=tournament,\\n    wb=False,\\n)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from pettingzoo import ParallelEnv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from agilerl.utils.utils import create_population as Population\n",
    "from agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\n",
    "from agilerl.training.train_offline import train_offline\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.training.train_multi_agent import train_multi_agent\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "#set device to the dual t4s. \n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "# -------------------------------\n",
    "# Environment Definition\n",
    "# -------------------------------\n",
    "class CichyEnv(ParallelEnv):\n",
    "    metadata = {\"name\": \"cichyenv\"}\n",
    "\n",
    "    def __init__(self, images, y1, y2):\n",
    "        self.images = images.reshape(92, 175, 175)  # Ensure correct shape\n",
    "        self.y1 = y1  # Rewards for agent IT\n",
    "        self.y2 = y2  # Rewards for agent EVC\n",
    "        self.agents = [\"IT\", \"EVC\"]\n",
    "        self.agent_ids = [\"IT\", \"EVC\"]\n",
    "        self.current_step = 0  # Initialize current_step\n",
    "        \n",
    "        # Merging observation spaces into a single space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"image\": spaces.Box(low=0, high=255, shape=(175, 175), dtype=np.uint8),  # Expecting image shape of (175, 175)\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(93,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0  # Ensure it's set to 0 at the start of each episode\n",
    "        self.actions = {agent: [] for agent in self.agent_ids}\n",
    "        \n",
    "        # Creating observation dictionary\n",
    "        obs = {}\n",
    "        for agent in self.agent_ids:\n",
    "            image = self.images[self.current_step]  # Get the image for the current step\n",
    "            if image.shape != (175, 175):\n",
    "                raise ValueError(f\"Unexpected image shape: {image.shape}, expected (175, 175)\")\n",
    "\n",
    "            # Expand the image dimensions to make it 5D for Conv3D (batch, channels, depth, height, width)\n",
    "            image_expanded = np.expand_dims(image, axis=0)  # Adds a batch dimension, (1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds a channel dimension, (1, 1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds depth dimension, (1, 1, 1, 175, 175)\n",
    "\n",
    "            obs[agent] = {\n",
    "                \"image\": image_expanded,  # Now shape (1, 1, 1, 175, 175)\n",
    "                \"other_action\": np.array([0.0]),  # Dummy value for the action of the other agent\n",
    "            }\n",
    "\n",
    "            #uncomment out soon \n",
    "\n",
    "        obs = {\n",
    "            agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "            for agent_id in self.agent_ids\n",
    "        }\n",
    "                    \n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        if self.current_step >= len(self.images):\n",
    "            self.current_step = len(self.images) - 1\n",
    "\n",
    "        obs, rewards, dones, infos = {}, {}, {}, {}\n",
    "\n",
    "        for agent_id in self.agent_ids:\n",
    "            other_agent_id = \"EVC\" if agent_id == \"IT\" else \"IT\"\n",
    "\n",
    "            full_action = action_dict.get(agent_id, np.zeros((93,)))\n",
    "            agent_actions = full_action[:92].reshape(-1, 1)#(full_action[:92]).reshape((92, 1))  \n",
    "            other_action = full_action[92:]  \n",
    "\n",
    "            self.actions[agent_id].append(agent_actions)  \n",
    "\n",
    "            other_actions = action_dict.get(other_agent_id, np.zeros((93,)))[92:] \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            # Padding other_action into the image array (assuming you want to treat the action as part of the input)\n",
    "            image_and_action = np.concatenate(\n",
    "                (self.images[self.current_step].flatten(), other_actions), axis=0\n",
    "            )\n",
    "            obs[agent_id] = torch.tensor(image_and_action, dtype=torch.float32)\n",
    "\n",
    "            \n",
    "            \"\"\" \n",
    "\n",
    "            obs[agent_id] = {\n",
    "                \"image\": torch.tensor(self.images[self.current_step], dtype=torch.float32),  # Convert to tensor\n",
    "                \"other_action\": torch.tensor(other_actions, dtype=torch.float32),  # Convert to tensor\n",
    "            }\n",
    "\n",
    "            #rewards[agent_id] = self._calculate_reward(agent_id)  \n",
    "\n",
    "            rewards[agent_id] = np.array([self._calculate_reward(agent_id)], dtype=np.float32)\n",
    "            dones[agent_id] = self.current_step >= len(self.images) - 1\n",
    "            infos[agent_id] = {}\n",
    "        dones = {agent_id: self.current_step >= len(self.images) - 1 for agent_id in self.agent_ids}\n",
    "        dones[\"__all__\"] = all(dones.values())\n",
    "        self.current_step += 1\n",
    "        obs = {\n",
    "            agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "            for agent_id in self.agent_ids\n",
    "            }  \n",
    "        \n",
    "        trunc = {agent_id: False for agent_id in self.agent_ids}\n",
    "        return obs, rewards, dones, trunc, infos\n",
    "    def _calculate_reward(self, agent_id):\n",
    "        \"\"\"Computes the reward based on similarity to the expert RDM using all 92 actions per step.\"\"\"\n",
    "        actions = np.array(self.actions[agent_id])  # Shape: (num_steps, 92, 1)\n",
    "        \n",
    "        num_steps = actions.shape[0]  \n",
    "        num_images = actions.shape[1]  \n",
    "\n",
    "        if num_steps < 2:  \n",
    "            return 0  \n",
    "\n",
    "        simulated_rdm = np.zeros((num_images, num_images))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for j in range(num_images):\n",
    "                if i != j:\n",
    "                    sim = cosine_similarity(actions[:, i].reshape(-1, 1), actions[:, j].reshape(-1, 1))[0][0]\n",
    "                    simulated_rdm[i, j] = 1 - sim\n",
    "\n",
    "        expert_rdm = self.y1 if agent_id == \"IT\" else self.y2\n",
    "\n",
    "        min_size = min(simulated_rdm.shape[0], expert_rdm.shape[0])\n",
    "        reward = -np.sum((simulated_rdm[:min_size, :min_size] - expert_rdm[:min_size, :min_size]) ** 2)\n",
    "        return reward\n",
    "\n",
    "#version number 2 \n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training Setup with AgileRL\n",
    "# -------------------------------8-=\n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "\n",
    "env = CichyEnv(x_train, y_train1, y_train2)\n",
    "\n",
    "# Initial Hyperparameters\n",
    "INIT_HP = {\n",
    "    \"DOUBLE\": True,\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"POPULATION_SIZE\": 2,\n",
    "    \"O_U_NOISE\": 0.2,\n",
    "    \"EXPL_NOISE\": 0.1,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"LR\": 0.001,\n",
    "    \"LR_ACTOR\": 0.002,\n",
    "    \"LR_CRITIC\": 0.002,\n",
    "    \"TAU\": 0.5,\n",
    "    \"GAMMA\": 1.0,\n",
    "    \"LAMBDA\": 1.0,\n",
    "    \"REG\": 0.0625,\n",
    "    \"LEARN_STEP\": 2,\n",
    "    \"MEAN_NOISE\": 1,\n",
    "    \"THETA\": 1,\n",
    "    \"DT\": 1,\n",
    "    \"POLICY_FREQ\": 2,\n",
    "    \"AGENT_IDS\": [\"IT\",\"EVC\"],\n",
    "    \"MEMORY_SIZE\": 100000\n",
    "        \n",
    "}\n",
    "\n",
    "hp_config = HyperparameterConfig(\n",
    "    #lr=RLParameter(min=6.25e-5, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n",
    ")\n",
    "\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "\n",
    "# Create populations for each agent\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# Tournament selection\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,\n",
    "    elitism=True,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    eval_loop=1,\n",
    ")\n",
    "\n",
    "# Mutation settings\n",
    "mutations = Mutations(\n",
    "    no_mutation=0.4,\n",
    "    architecture=0.2,\n",
    "    new_layer_prob=0.2,\n",
    "    parameters=0.2,\n",
    "    activation=0,\n",
    "    rl_hp=0.2,\n",
    "    mutation_sd=0.1,\n",
    "    rand_seed=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Offline training\n",
    "trained_pop, pop_fitnesses = train_multi_agent(#train_on_policy(#train_offline(\n",
    "    pop=[pop_agent1, pop_agent2],\n",
    "    env=env,\n",
    "    algo=\"MATD3\",\n",
    "    env_name=\"cichyenv\",\n",
    "    #dataset=[x_train, y_train1, y_train2],\n",
    "    memory=memory,  # Replay buffer if needed\n",
    "    swap_channels=False,  # Ensure channel order is correct\n",
    "    max_steps=500000,\n",
    "    evo_steps=10000,\n",
    "    eval_steps=None,\n",
    "    eval_loop=1,\n",
    "    target=200.0,\n",
    "    tournament=tournament,\n",
    "    #mutation=mutations,\n",
    "    wb=False,  # Weights & Biases logging\n",
    "    #accelerator=device\n",
    ")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "trained_pop, fitnesses = train_multi_agent(\n",
    "    pop=pop_agent,\n",
    "    env=env,\n",
    "    algo=\"MATD3\",\n",
    "    env_name=\"cichyenv\",\n",
    "    memory=memory,\n",
    "    swap_channels=False,\n",
    "    max_steps=500000,\n",
    "    evo_steps=10000,\n",
    "    eval_steps=None,\n",
    "    eval_loop=1,\n",
    "    target=200.0,\n",
    "    tournament=tournament,\n",
    "    wb=False,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research team suggests that the number of representative agents that are needed are 5 agents for EVC and 3 agents for IT. Therefore we will use 9 agents total since 1 needs to represent the Thalamus\n",
    "\n",
    "Core development TODOs\n",
    "\n",
    "TODO: figure out visualization and running the trained simulation on new data \n",
    "\n",
    "2nd TODO: finish the 9 agent simulation by doing ID based observation masking  \n",
    "\n",
    "other TODO: switch over from RDMs to MEG \n",
    "\n",
    "other TODO: swap out classical neural networks for UODEs\n",
    "\n",
    "Optimization TODOs\n",
    "\n",
    "TODO: vectorize enviroment and figure out how to increase number of enviroments\n",
    "\n",
    "\n",
    "\n",
    "In order to do this switch over the research team needs to figure out which channels correspond to which parts of the brain so that irrelavent channels can be removed. \n",
    "\n",
    "how to do groupings? the answer is to use hierarchical learning \n",
    "\n",
    "other TODO: swap out classical neural network for UODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary code for vectorizing pettingzoo enviroment\n",
    "\n",
    "\"\"\"\n",
    "from agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\n",
    "env = CustomEnv()\n",
    "num_envs = 4\n",
    "AsyncPettingZooVecEnv([lambda : env for _ in range(num_envs)])\n",
    "observations, infos = vec_env.reset()\n",
    "for step in range(25):\n",
    "    actions = {\n",
    "        agent: [vec_env.single_action_space(agent).sample() for n in range(num_envs)]\n",
    "        for agent in vec_env.agents\n",
    "    }\n",
    "    observations, rewards, terminations, truncations, infos = vec_env.step(actions)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IT': False, 'EVC': False, '__all__': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:19<?, ?step/s]\n",
      "250000step [00:27, 11942.54step/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global steps 20000 ---\n",
      "Steps [0, 20000]\n",
      "Scores: ['0 completed episodes']\n",
      "Fitnesses: ['-4656445.83']\n",
      "5 fitness avgs: ['nan', '-4656445.83']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subarno/miniconda3/envs/primary/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/subarno/miniconda3/envs/primary/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "250000step [07:23, 563.11step/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "max_steps = 20000 \n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "\n",
    "\n",
    "\n",
    "training_steps = 6\n",
    "env = CichyEnv(x_train, y_train1, y_train2)#.parallel_env()\n",
    "action_dict = {\n",
    "    \"IT\": np.random.uniform(-1, 1, 93),  # Random action for agent IT (size 93)\n",
    "    \"EVC\": np.random.uniform(-1, 1, 93)  # Random action for agent EVC (size 93)\n",
    "}\n",
    "#env.step(action_dict)\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]},}\n",
    "obs = env.reset()  # This ensures self.actions is initialized\n",
    "obs, rewards, dones,bb, infos = env.step(action_dict)\n",
    "\n",
    "print(dones)\n",
    "#print(type(obs), obs)\n",
    "\n",
    "\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "#print(type(obs), obs)\n",
    "num_envs = 4\n",
    "learning_delay = 0  # Steps before starting learning\n",
    "evo_steps = 100  # Evolution frequency\n",
    "eval_steps = None  # Evaluation steps per episode - go until done\n",
    "eval_loop = 1  \n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "agent_ids = [\"IT\", \"EVC\"]\n",
    "#agent = pop_agent[0]\n",
    "#print(agent)\n",
    "pbar = trange(max_steps, unit=\"step\")\n",
    "while np.less([agent.steps[-1] for agent in pop_agent], max_steps).all():\n",
    "    pop_episode_scores = []\n",
    "    for agent in pop_agent:  # Loop through population\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        scores = np.zeros(num_envs)\n",
    "        completed_episode_scores = []\n",
    "        steps = 0\n",
    "        \"\"\"\n",
    "        processed_obs = {\n",
    "            agent_id: spaces.flatten(env.observation_space, obs[agent_id]) \n",
    "            for agent_id in agent_ids\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    for idx_step in range(training_steps // num_envs):\n",
    "\n",
    "        # Ensure obs is flattened before passing to agent\n",
    "        cont_actions, discrete_action = agent.get_action(obs=obs, training=True, infos=info)\n",
    "        if agent.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "\n",
    "        # Act in environment\n",
    "        next_state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "        scores += np.sum(np.array(list(reward.values())).transpose(), axis=-1)\n",
    "        total_steps += num_envs\n",
    "        steps += num_envs\n",
    "        \n",
    "        # Save experiences to replay buffer\n",
    "\n",
    "        #memory is no longer erroring out but the elif right after is when memory.save_to_memory is uncommented out \n",
    "        \"\"\"\n",
    "        \n",
    "                \n",
    "        memory.save_to_memory(\n",
    "            state,\n",
    "            cont_actions,\n",
    "            reward,\n",
    "            next_state, \n",
    "            termination,\n",
    "            #is_vectorised=True,\n",
    "        )\n",
    "        \"\"\"    \n",
    "\n",
    "        # Learn according to learning frequency\n",
    "        # Handle learn steps > num_envs\n",
    "        if agent.learn_step > num_envs:\n",
    "            learn_step = agent.learn_step // num_envs\n",
    "            if (\n",
    "                idx_step % learn_step == 0\n",
    "                and len(memory) >= agent.batch_size\n",
    "                and memory.counter > learning_delay\n",
    "            ):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "        # Handle num_envs > learn step; learn multiple times per step in env\n",
    "        elif (\n",
    "            len(memory) >= agent.batch_size and memory.counter > learning_delay\n",
    "        ):\n",
    "            for _ in range(num_envs // agent.learn_step):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)#where bug is currently occuring.\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Calculate scores and reset noise for finished episodes\n",
    "        reset_noise_indices = []\n",
    "        term_array = np.array(list(termination.values())).transpose()\n",
    "        trunc_array = np.array(list(truncation.values())).transpose()\n",
    "        for idx, (d, t) in enumerate(zip(term_array, trunc_array)):\n",
    "            if np.any(d) or np.any(t):\n",
    "                completed_episode_scores.append(scores[idx])\n",
    "                agent.scores.append(scores[idx])\n",
    "                scores[idx] = 0\n",
    "                reset_noise_indices.append(idx)\n",
    "        agent.reset_action_noise(reset_noise_indices)\n",
    "\n",
    "    pbar.update(evo_steps // len(pop_agent))\n",
    "\n",
    "    agent.steps[-1] += steps\n",
    "    pop_episode_scores.append(completed_episode_scores)\n",
    "\n",
    "# Evaluate population\n",
    "fitnesses = [\n",
    "    agent.test( #agent.test is a potential bottleneck\n",
    "        env,\n",
    "        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "        max_steps=eval_steps,\n",
    "        loop=eval_loop,\n",
    "    )\n",
    "]\n",
    "mean_scores = [\n",
    "    (\n",
    "        np.mean(episode_scores)\n",
    "        if len(episode_scores) > 0\n",
    "        else \"0 completed episodes\"\n",
    "    )\n",
    "    for episode_scores in pop_episode_scores\n",
    "]\n",
    "\n",
    "print(f\"--- Global steps {total_steps} ---\")\n",
    "print(f\"Steps {[agent.steps[-1] for agent in pop_agent]}\")\n",
    "print(f\"Scores: {mean_scores}\")\n",
    "print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n",
    "print(\n",
    "    f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop_agent]}'\n",
    ")\n",
    "\n",
    "# Tournament selection and population mutation\n",
    "elite, pop = tournament.select(pop_agent)\n",
    "pop = mutations.mutation(pop)\n",
    "\n",
    "# Update step counter\n",
    "for agent in pop:\n",
    "    agent.steps.append(agent.steps[-1])\n",
    "\n",
    "\"\"\"\n",
    "path = \"\\\\wsl.localhost\\Debian\\home\\subarno\\SpecialProjects\\MATD3_trained_agent.pt\"\n",
    "filename = \"MADDPG_trained_agent.pt\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "save_path = os.path.join(path, filename)\n",
    "agent.save_checkpoint(save_path)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pbar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    path = \"\\\\wsl.localhost\\Debian\\home\\subarno\\SpecialProjects\\MATD3_trained_agent.pt\"\n",
    "    matd3 = MATD3.load(path, device)\n",
    "    episodes = 10\n",
    "    frames = []\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "        for _ in range(max_steps):\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = matd3.get_action(\n",
    "                state, training=False, infos=info\n",
    "            )\n",
    "            if matd3.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Save the frame for this step and append to frames list\n",
    "            #frame = env.render()\n",
    "            #frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(\n",
    "                {agent: a.squeeze() for agent, a in action.items()}\n",
    "            )\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Record agent specific episodic reward\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "n_samples, h, w = lfw_people.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "#resnet18 = models.resnet18()\n",
    "alexnet = models.alexnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IT': array([False]), 'EVC': array([False]), '__all__': array([False])}\n",
      "[False]\n",
      "<agilerl.algorithms.matd3.MATD3 object at 0x7f926c5d2bc0>\n"
     ]
    }
   ],
   "source": [
    "dones = {\n",
    "    agent: np.array([done]) for agent, done in dones.items()\n",
    "}\n",
    "print(dones)\n",
    "print(dones['IT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from pettingzoo import ParallelEnv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from agilerl.utils.utils import create_population as Population\n",
    "from agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\n",
    "from agilerl.training.train_offline import train_offline\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.training.train_multi_agent import train_multi_agent\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "#set device to the dual t4s. \n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "# -------------------------------\n",
    "# Environment Definition\n",
    "# -------------------------------\n",
    "class CichyEnv(ParallelEnv):\n",
    "    metadata = {\"name\": \"cichyenv\"}\n",
    "\n",
    "    def __init__(self, images, y1, y2):\n",
    "        self.images = images.reshape(92, 175, 175)  # Ensure correct shape\n",
    "        self.y1 = y1  # Rewards for agent IT\n",
    "        self.y2 = y2  # Rewards for agent EVC\n",
    "        self.agents = [\"IT1\", \"IT2\", \"IT3\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\"] #add one more agent reprsenting the occipital lobe bill kennedy talked about\n",
    "        self.agent_ids = [\"IT1\", \"IT2\", \"IT3\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\"]#[\"IT\", \"EVC\"]\n",
    "        self.current_step = 0  # Initialize current_step\n",
    "        \n",
    "        # Merging observation spaces into a single space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"image\": spaces.Box(low=0, high=255, shape=(175, 175), dtype=np.uint8),  # Expecting image shape of (175, 175)\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(93,), dtype=np.float32)\n",
    "    def reset(self):\n",
    "        self.current_step = 0  # Ensure it's set to 0 at the start of each episode\n",
    "        self.actions = {agent: [] for agent in self.agent_ids}\n",
    "        \n",
    "        # Creating observation dictionary\n",
    "        obs = {}\n",
    "        for agent in self.agent_ids:\n",
    "            image = self.images[self.current_step]  # Get the image for the current step\n",
    "            if image.shape != (175, 175):\n",
    "                raise ValueError(f\"Unexpected image shape: {image.shape}, expected (175, 175)\")\n",
    "\n",
    "            # Expand the image dimensions to make it 5D for Conv3D (batch, channels, depth, height, width)\n",
    "            image_expanded = np.expand_dims(image, axis=0)  # Adds a batch dimension, (1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds a channel dimension, (1, 1, 175, 175)\n",
    "            image_expanded = np.expand_dims(image_expanded, axis=0)  # Adds depth dimension, (1, 1, 1, 175, 175)\n",
    "\n",
    "            obs[agent] = {\n",
    "                \"image\": image_expanded,  # Now shape (1, 1, 1, 175, 175)\n",
    "                \"other_action\": np.array([0.0]),  # Dummy value for the action of the other agent\n",
    "            }\n",
    "\n",
    "            #uncomment out soon \n",
    "\n",
    "        obs = {\n",
    "            agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "            for agent_id in self.agent_ids\n",
    "        }\n",
    "                    \n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        if self.current_step >= len(self.images):\n",
    "            self.current_step = len(self.images) - 1\n",
    "\n",
    "        obs, rewards, dones, infos = {}, {}, {}, {}\n",
    "\n",
    "        for agent_id in self.agent_ids:\n",
    "            other_agent_id = \"EVC\" if agent_id == \"IT\" else \"IT\"\n",
    "\n",
    "            full_action = action_dict.get(agent_id, np.zeros((93,)))\n",
    "            agent_actions = full_action[:92].reshape(-1, 1)#(full_action[:92]).reshape((92, 1))  \n",
    "            other_action = full_action[92:]  \n",
    "\n",
    "            self.actions[agent_id].append(agent_actions)  \n",
    "\n",
    "            other_actions = action_dict.get(other_agent_id, np.zeros((93,)))[92:] \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            # Padding other_action into the image array (assuming you want to treat the action as part of the input)\n",
    "            image_and_action = np.concatenate(\n",
    "                (self.images[self.current_step].flatten(), other_actions), axis=0\n",
    "            )\n",
    "            obs[agent_id] = torch.tensor(image_and_action, dtype=torch.float32)\n",
    "\n",
    "            \n",
    "            \"\"\" \n",
    "\n",
    "            obs[agent_id] = {\n",
    "                \"image\": torch.tensor(self.images[self.current_step], dtype=torch.float32),  # Convert to tensor\n",
    "                \"other_action\": torch.tensor(other_actions, dtype=torch.float32),  # Convert to tensor\n",
    "            }\n",
    "\n",
    "            #rewards[agent_id] = self._calculate_reward(agent_id)  \n",
    "\n",
    "            rewards[agent_id] = np.array([self._calculate_reward(agent_id)], dtype=np.float32)\n",
    "            dones[agent_id] = self.current_step >= len(self.images) - 1\n",
    "            infos[agent_id] = {}\n",
    "        dones = {agent_id: self.current_step >= len(self.images) - 1 for agent_id in self.agent_ids}\n",
    "        dones[\"__all__\"] = all(dones.values())\n",
    "        self.current_step += 1\n",
    "        obs = {\n",
    "            agent_id: spaces.flatten(self.observation_space, obs[agent_id]) \n",
    "            for agent_id in self.agent_ids\n",
    "            }  \n",
    "        \n",
    "        trunc = {agent_id: False for agent_id in self.agent_ids}\n",
    "        return obs, rewards, dones, trunc, infos\n",
    "    def _calculate_reward(self, agent_id):\n",
    "        \"\"\"Computes the reward based on similarity to the expert RDM using all 92 actions per step.\"\"\"\n",
    "        actions = np.array(self.actions[agent_id])  # Shape: (num_steps, 92, 1)\n",
    "        \n",
    "        num_steps = actions.shape[0]  \n",
    "        num_images = actions.shape[1]  \n",
    "\n",
    "        if num_steps < 2:  \n",
    "            return 0  \n",
    "\n",
    "        simulated_rdm = np.zeros((num_images, num_images))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for j in range(num_images):\n",
    "                if i != j:\n",
    "                    sim = cosine_similarity(actions[:, i].reshape(-1, 1), actions[:, j].reshape(-1, 1))[0][0]\n",
    "                    simulated_rdm[i, j] = 1 - sim\n",
    "\n",
    "        expert_rdm = self.y1 if agent_id == \"IT\" else self.y2\n",
    "\n",
    "        min_size = min(simulated_rdm.shape[0], expert_rdm.shape[0])\n",
    "        reward = -np.sum((simulated_rdm[:min_size, :min_size] - expert_rdm[:min_size, :min_size]) ** 2)\n",
    "        return reward\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training Setup with AgileRL\n",
    "# -------------------------------8-=\n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "\n",
    "env = CichyEnv(x_train, y_train1, y_train2)\n",
    "\n",
    "# Initial Hyperparameters\n",
    "INIT_HP = {\n",
    "    \"DOUBLE\": True,\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"POPULATION_SIZE\": 8,\n",
    "    \"O_U_NOISE\": 0.2,\n",
    "    \"EXPL_NOISE\": 0.1,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"LR\": 0.001,\n",
    "    \"LR_ACTOR\": 0.002,\n",
    "    \"LR_CRITIC\": 0.002,\n",
    "    \"TAU\": 0.5,\n",
    "    \"GAMMA\": 1.0,\n",
    "    \"LAMBDA\": 1.0,\n",
    "    \"REG\": 0.0625,\n",
    "    \"LEARN_STEP\": 2,\n",
    "    \"MEAN_NOISE\": 1,\n",
    "    \"THETA\": 1,\n",
    "    \"DT\": 1,\n",
    "    \"POLICY_FREQ\": 2,\n",
    "    \"AGENT_IDS\": [\"IT1\", \"IT2\", \"IT3\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\"],\n",
    "    \"MEMORY_SIZE\": 100000\n",
    "        \n",
    "}\n",
    "\n",
    "hp_config = HyperparameterConfig(\n",
    "    #lr=RLParameter(min=6.25e-5, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n",
    ")\n",
    "\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "\n",
    "# Create populations for each agent\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# Tournament selection\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,\n",
    "    elitism=True,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    eval_loop=1,\n",
    ")\n",
    "\n",
    "# Mutation settings\n",
    "mutations = Mutations(\n",
    "    no_mutation=0.4,\n",
    "    architecture=0.2,\n",
    "    new_layer_prob=0.2,\n",
    "    parameters=0.2,\n",
    "    activation=0,\n",
    "    rl_hp=0.2,\n",
    "    mutation_sd=0.1,\n",
    "    rand_seed=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: output an ID along with the other action to make it visible to only one other agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'IT1': array([ 16.      ,  89.      , 159.      , ..., 188.      ,  55.      ,\n",
      "        -0.618883], dtype=float32), 'IT2': array([ 16.      ,  89.      , 159.      , ..., 188.      ,  55.      ,\n",
      "        -0.618883], dtype=float32), 'IT3': array([ 16.      ,  89.      , 159.      , ..., 188.      ,  55.      ,\n",
      "        -0.618883], dtype=float32), 'EVC1': array([ 16.      ,  89.      , 159.      , ..., 188.      ,  55.      ,\n",
      "        -0.618883], dtype=float32), 'EVC2': array([ 16.      ,  89.      , 159.      , ..., 188.      ,  55.      ,\n",
      "        -0.618883], dtype=float32), 'EVC3': array([ 16.      ,  89.      , 159.      , ..., 188.      ,  55.      ,\n",
      "        -0.618883], dtype=float32), 'EVC4': array([ 16.      ,  89.      , 159.      , ..., 188.      ,  55.      ,\n",
      "        -0.618883], dtype=float32), 'EVC5': array([ 16.      ,  89.      , 159.      , ..., 188.      ,  55.      ,\n",
      "        -0.618883], dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:37<?, ?step/s]\n",
      "6250000step [01:18, 105397.36step/s]                      "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 124\u001b[0m\n\u001b[1;32m    121\u001b[0m     pop_episode_scores\u001b[38;5;241m.\u001b[39mappend(completed_episode_scores)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Evaluate population\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    125\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtest(\n\u001b[1;32m    126\u001b[0m         env,\n\u001b[1;32m    127\u001b[0m         swap_channels\u001b[38;5;241m=\u001b[39mINIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHANNELS_LAST\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    128\u001b[0m         max_steps\u001b[38;5;241m=\u001b[39meval_steps,\n\u001b[1;32m    129\u001b[0m         loop\u001b[38;5;241m=\u001b[39meval_loop,\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m pop_agent\n\u001b[1;32m    132\u001b[0m ]\n\u001b[1;32m    133\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m     (\n\u001b[1;32m    135\u001b[0m         np\u001b[38;5;241m.\u001b[39mmean(episode_scores)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode_scores \u001b[38;5;129;01min\u001b[39;00m pop_episode_scores\n\u001b[1;32m    140\u001b[0m ]\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Global steps \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 125\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m     pop_episode_scores\u001b[38;5;241m.\u001b[39mappend(completed_episode_scores)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Evaluate population\u001b[39;00m\n\u001b[1;32m    124\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 125\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mswap_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINIT_HP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCHANNELS_LAST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_loop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m pop_agent\n\u001b[1;32m    132\u001b[0m ]\n\u001b[1;32m    133\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m     (\n\u001b[1;32m    135\u001b[0m         np\u001b[38;5;241m.\u001b[39mmean(episode_scores)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode_scores \u001b[38;5;129;01min\u001b[39;00m pop_episode_scores\n\u001b[1;32m    140\u001b[0m ]\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Global steps \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/agilerl/algorithms/matd3.py:975\u001b[0m, in \u001b[0;36mMATD3.test\u001b[0;34m(self, env, swap_channels, max_steps, loop, sum_scores)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_vectorised:\n\u001b[1;32m    974\u001b[0m     action \u001b[38;5;241m=\u001b[39m {agent: act[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m agent, act \u001b[38;5;129;01min\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 975\u001b[0m obs, reward, term, trunc, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m score_increment \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    977\u001b[0m     (\n\u001b[1;32m    978\u001b[0m         np\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(reward\u001b[38;5;241m.\u001b[39mvalues()))\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m    988\u001b[0m )\n\u001b[1;32m    989\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m score_increment\n",
      "Cell \u001b[0;32mIn[11], line 106\u001b[0m, in \u001b[0;36mCichyEnv.step\u001b[0;34m(self, action_dict)\u001b[0m\n\u001b[1;32m     99\u001b[0m obs[agent_id] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),  \u001b[38;5;66;03m# Convert to tensor\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother_action\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(other_actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),  \u001b[38;5;66;03m# Convert to tensor\u001b[39;00m\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m#rewards[agent_id] = self._calculate_reward(agent_id)  \u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m rewards[agent_id] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m)\u001b[49m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    107\u001b[0m dones[agent_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    108\u001b[0m infos[agent_id] \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[11], line 134\u001b[0m, in \u001b[0;36mCichyEnv._calculate_reward\u001b[0;34m(self, agent_id)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_images):\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[0;32m--> 134\u001b[0m             sim \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    135\u001b[0m             simulated_rdm[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m sim\n\u001b[1;32m    137\u001b[0m expert_rdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my1 \u001b[38;5;28;01mif\u001b[39;00m agent_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my2\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \n\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:209\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[0;32m--> 209\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/sklearn/utils/validation.py:116\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 116\u001b[0m     first_pass_isfinite \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39misfinite(\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/primary/lib/python3.10/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "max_steps = 20000 \n",
    "x_train = np.random.randint(0, 255, (175,175, 92), dtype=np.uint8)  # Dummy dataset\n",
    "y_train1 = np.random.randn(92, 92)  # Dummy RDM for agent IT\n",
    "y_train2 = np.random.randn(92, 92)  # Dummy RDM for agent EVC\n",
    "training_steps = 6\n",
    "env = CichyEnv(x_train, y_train1, y_train2)#.parallel_env()\n",
    "action_dict = {\n",
    "    \"IT\": np.random.uniform(-1, 1, 93),  # Random action for agent IT (size 93)\n",
    "    \"EVC\": np.random.uniform(-1, 1, 93)  # Random action for agent EVC (size 93)\n",
    "}\n",
    "#env.step(action_dict)\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "obs = env.reset()  # This ensures self.actions is initialized\n",
    "obs, rewards, dones,bb, infos = env.step(action_dict)\n",
    "print(type(obs), obs)\n",
    "\n",
    "\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space),spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "#print(type(obs), obs)\n",
    "num_envs = 4\n",
    "learning_delay = 0  # Steps before starting learning\n",
    "evo_steps = 100  # Evolution frequency\n",
    "eval_steps = None  # Evaluation steps per episode - go until done\n",
    "eval_loop = 1  \n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "agent_ids = [\"IT1\", \"IT2\", \"IT3\", \"EVC1\", \"EVC2\", \"EVC3\", \"EVC4\", \"EVC5\"]\n",
    "#agent = pop_agent[0]\n",
    "#print(agent)\n",
    "pbar = trange(max_steps, unit=\"step\")\n",
    "while np.less([agent.steps[-1] for agent in pop_agent], max_steps).all():\n",
    "    pop_episode_scores = []\n",
    "    for agent in pop_agent:  # Loop through population\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        scores = np.zeros(num_envs)\n",
    "        completed_episode_scores = []\n",
    "        steps = 0\n",
    "\n",
    "    for idx_step in range(training_steps // num_envs):\n",
    "        # Ensure obs is flattened before passing to agent\n",
    "        cont_actions, discrete_action = agent.get_action(obs=obs, training=True, infos=info)\n",
    "        if agent.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "\n",
    "        # Act in environment\n",
    "        next_state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "        scores += np.sum(np.array(list(reward.values())).transpose(), axis=-1)\n",
    "        total_steps += num_envs\n",
    "        steps += num_envs\n",
    "        \"\"\"\n",
    "        # Save experiences to replay buffer\n",
    "\n",
    "        #erroring out must fix\n",
    "\n",
    "        memory.save_to_memory(\n",
    "            state,\n",
    "            cont_actions,\n",
    "            reward,\n",
    "            next_state,\n",
    "            termination,\n",
    "            is_vectorised=True,\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Learn according to learning frequency\n",
    "        # Handle learn steps > num_envs\n",
    "        if agent.learn_step > num_envs:\n",
    "            learn_step = agent.learn_step // num_envs\n",
    "            if (\n",
    "                idx_step % learn_step == 0\n",
    "                and len(memory) >= agent.batch_size\n",
    "                and memory.counter > learning_delay\n",
    "            ):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "        # Handle num_envs > learn step; learn multiple times per step in env\n",
    "        elif (\n",
    "            len(memory) >= agent.batch_size and memory.counter > learning_delay\n",
    "        ):\n",
    "            for _ in range(num_envs // agent.learn_step):\n",
    "                # Sample replay buffer\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                # Learn according to agent's RL algorithm\n",
    "                agent.learn(experiences)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Calculate scores and reset noise for finished episodes\n",
    "        reset_noise_indices = []\n",
    "        term_array = np.array(list(termination.values())).transpose()\n",
    "        trunc_array = np.array(list(truncation.values())).transpose()\n",
    "        for idx, (d, t) in enumerate(zip(term_array, trunc_array)):\n",
    "            if np.any(d) or np.any(t):\n",
    "                completed_episode_scores.append(scores[idx])\n",
    "                agent.scores.append(scores[idx])\n",
    "                scores[idx] = 0\n",
    "                reset_noise_indices.append(idx)\n",
    "        agent.reset_action_noise(reset_noise_indices)\n",
    "\n",
    "    pbar.update(evo_steps // len(pop_agent))\n",
    "\n",
    "    agent.steps[-1] += steps\n",
    "    pop_episode_scores.append(completed_episode_scores)\n",
    "\n",
    "# Evaluate population\n",
    "fitnesses = [\n",
    "    agent.test(\n",
    "        env,\n",
    "        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "        max_steps=eval_steps,\n",
    "        loop=eval_loop,\n",
    "    )\n",
    "    for agent in pop_agent\n",
    "]\n",
    "mean_scores = [\n",
    "    (\n",
    "        np.mean(episode_scores)\n",
    "        if len(episode_scores) > 0\n",
    "        else \"0 completed episodes\"\n",
    "    )\n",
    "    for episode_scores in pop_episode_scores\n",
    "]\n",
    "\n",
    "print(f\"--- Global steps {total_steps} ---\")\n",
    "print(f\"Steps {[agent.steps[-1] for agent in pop_agent]}\")\n",
    "print(f\"Scores: {mean_scores}\")\n",
    "print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n",
    "print(\n",
    "    f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop_agent]}'\n",
    ")\n",
    "\n",
    "# Tournament selection and population mutation\n",
    "elite, pop = tournament.select(pop_agent)\n",
    "pop = mutations.mutation(pop)\n",
    "\n",
    "# Update step counter\n",
    "for agent in pop:\n",
    "    agent.steps.append(agent.steps[-1])\n",
    "\n",
    "pbar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prisoner': (0, 48, 24), 'guard': (0, 48, 24)}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import random\n",
    "from copy import copy\n",
    "prisoner_y = 0\n",
    "possible_agents = [\"prisoner\", \"guard\"]\n",
    "\n",
    "agents = copy(possible_agents)\n",
    "guard_x = 6\n",
    "guard_y = 6\n",
    "prisoner_x = 0\n",
    "escape_x = random.randint(2, 5)\n",
    "escape_y = random.randint(2, 5)\n",
    "pobs = {\n",
    "    a: (\n",
    "            prisoner_x + 7 * prisoner_y,\n",
    "            guard_x + 7 * guard_y,\n",
    "            escape_x + 7 * escape_y,\n",
    "            )\n",
    "            for a in agents\n",
    "}\n",
    "print(p)\"\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pop, fitnesses = train_multi_agent(\n",
    "    pop=pop_agent,\n",
    "    env=env,\n",
    "    algo=\"MATD3\",\n",
    "    env_name=\"cichyenv\",\n",
    "    memory=memory,\n",
    "    swap_channels=False,\n",
    "    max_steps=500000,\n",
    "    evo_steps=10000,\n",
    "    eval_steps=None,\n",
    "    eval_loop=1,\n",
    "    target=200.0,\n",
    "    tournament=tournament,\n",
    "    wb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CichyEnv(x_train, y_train1, y_train2)\n",
    "\n",
    "# Initial Hyperparameters\n",
    "INIT_HP = {\n",
    "    \"DOUBLE\": True,\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"POPULATION_SIZE\": 9,\n",
    "    \"O_U_NOISE\": 0.2,\n",
    "    \"EXPL_NOISE\": 0.1,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"LR\": 0.001,\n",
    "    \"LR_ACTOR\": 0.002,\n",
    "    \"LR_CRITIC\": 0.002,\n",
    "    \"TAU\": 0.5,\n",
    "    \"GAMMA\": 1.0,\n",
    "    \"LAMBDA\": 1.0,\n",
    "    \"REG\": 0.0625,\n",
    "    \"LEARN_STEP\": 2,\n",
    "    \"MEAN_NOISE\": 1,\n",
    "    \"THETA\": 1,\n",
    "    \"DT\": 1,\n",
    "    \"POLICY_FREQ\": 2,\n",
    "    \"AGENT_IDS\": [\"IT\",\"EVC\"],\n",
    "    \"MEMORY_SIZE\": 100000\n",
    "        \n",
    "}\n",
    "\n",
    "hp_config = HyperparameterConfig(\n",
    "    #lr=RLParameter(min=6.25e-5, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n",
    ")\n",
    "\n",
    "NET_CONFIG = {\"head_config\": {\"hidden_size\": [128]}}\n",
    "\n",
    "# Create populations for each agent\n",
    "pop_agent = Population(\n",
    "    algo=\"MATD3\",#NeuralTS\",\n",
    "    observation_space=[spaces.flatten_space(env.observation_space), spaces.flatten_space(env.observation_space)],\n",
    "    action_space=[env.action_space,env.action_space],\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    hp_config=hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# Tournament selection\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,\n",
    "    elitism=True,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    eval_loop=1,\n",
    ")\n",
    "\n",
    "# Mutation settings\n",
    "mutations = Mutations(\n",
    "    no_mutation=0.4,\n",
    "    architecture=0.2,\n",
    "    new_layer_prob=0.2,\n",
    "    parameters=0.2,\n",
    "    activation=0,\n",
    "    rl_hp=0.2,\n",
    "    mutation_sd=0.1,\n",
    "    rand_seed=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code below trains skills\n",
    "\n",
    "note: skills are not full envs. They are classes that connect to an env and only consist of a single reward function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.wrappers.learning import Skill\n",
    "import os\n",
    "from agilerl.algorithms.ppo import PPO\n",
    "from agilerl.training.train_on_policy import train_on_policy\n",
    "from agilerl.wrappers.learning import Skill\n",
    "from agilerl.utils.algo_utils import obs_channels_to_first\n",
    "from agilerl.utils.utils import (\n",
    "   create_population,\n",
    "   make_skill_vect_envs,\n",
    "   make_vect_envs,\n",
    "   observation_space_channels_to_first\n",
    ")\n",
    "NET_CONFIG = {\n",
    "   \"encoder_config\": {\"hidden_size\": [64, 64]}  # Actor encoder hidden size\n",
    "}\n",
    "\n",
    "INIT_HP = {\n",
    "   \"ENV_NAME\": \"LunarLander-v2\",\n",
    "   \"ALGO\": \"PPO\",\n",
    "   \"POPULATION_SIZE\": 1,  # Population size\n",
    "   \"BATCH_SIZE\": 128,  # Batch size\n",
    "   \"LR\": 1e-3,  # Learning rate\n",
    "   \"LEARN_STEP\": 128,  # Learning frequency\n",
    "   \"GAMMA\": 0.99,  # Discount factor\n",
    "   \"GAE_LAMBDA\": 0.95,  # Lambda for general advantage estimation\n",
    "   \"ACTION_STD_INIT\": 0.6,  # Initial action standard deviation\n",
    "   \"CLIP_COEF\": 0.2,  # Surrogate clipping coefficient\n",
    "   \"ENT_COEF\": 0.01,  # Entropy coefficient\n",
    "   \"VF_COEF\": 0.5,  # Value function coefficient\n",
    "   \"MAX_GRAD_NORM\": 0.5,  # Maximum norm for gradient clipping\n",
    "   \"TARGET_KL\": None,  # Target KL divergence threshold\n",
    "   \"TARGET_SCORE\": 2000,\n",
    "   \"MAX_STEPS\": 1_000_000,\n",
    "   \"EVO_STEPS\": 10_000,\n",
    "   \"UPDATE_EPOCHS\": 4,  # Number of policy update epochs\n",
    "   # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "   \"CHANNELS_LAST\": False,\n",
    "   \"WANDB\": True,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Directory to save trained agents and skills\n",
    "save_dir = \"./models/PPO\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "skills = {\n",
    "   \"stabilize\": StabilizeSkill,\n",
    "   \"center\": CenterSkill,\n",
    "   \"landing\": LandingSkill,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for skill in skills.keys():\n",
    "   env = make_skill_vect_envs(\n",
    "         INIT_HP[\"ENV_NAME\"], skills[skill], num_envs=1\n",
    "   )  # Create environment\n",
    "\n",
    "   observation_space = env.single_observation_space\n",
    "   action_space = env.single_action_space\n",
    "   if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "         observation_space = observation_space_channels_to_first(observation_space)\n",
    "\n",
    "   pop = create_population(\n",
    "         algo=\"PPO\",  # Algorithm\n",
    "         observation_space=observation_space,  # Observation space\n",
    "         action_space=action_space,  # Action space\n",
    "         net_config=NET_CONFIG,  # Network configuration\n",
    "         INIT_HP=INIT_HP,  # Initial hyperparameters\n",
    "         population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "         device=device,\n",
    "   )\n",
    "\n",
    "   trained_pop, pop_fitnesses = train_on_policy(\n",
    "         env=env,  # Gym-style environment\n",
    "         env_name=f\"{INIT_HP['ENV_NAME']}-{skill}\",  # Environment name\n",
    "         algo=INIT_HP[\"ALGO\"],  # Algorithm\n",
    "         pop=pop,  # Population of agents\n",
    "         swap_channels=INIT_HP[\n",
    "            \"CHANNELS_LAST\"\n",
    "         ],  # Swap image channel from last to first\n",
    "         max_steps=INIT_HP[\"MAX_STEPS\"],  # Max number of training episodes\n",
    "         evo_steps=INIT_HP[\"EVO_STEPS\"],  # Evolution frequency\n",
    "         evo_loop=3,  # Number of evaluation episodes per agent\n",
    "         target=INIT_HP[\"TARGET_SCORE\"],  # Target score for early stopping\n",
    "         tournament=None,  # Tournament selection object\n",
    "         mutation=None,  # Mutations object\n",
    "         wb=INIT_HP[\"WANDB\"],  # Weights and Biases tracking\n",
    "   )\n",
    "\n",
    "   # Save the trained algorithm\n",
    "   filename = f\"PPO_trained_agent_{skill}.pt\"\n",
    "   save_path = os.path.join(save_dir, filename)\n",
    "   trained_pop[0].save_checkpoint(save_path)\n",
    "\n",
    "   env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is for the meta selector agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stabilize_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_stabilize.pt\"))\n",
    "center_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_center.pt\"))\n",
    "landing_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_landing.pt\"))\n",
    "\n",
    "trained_skills = {\n",
    "   0: {\"skill\": \"stabilize\", \"agent\": stabilize_agent, \"skill_duration\": 40},\n",
    "   1: {\"skill\": \"center\", \"agent\": center_agent, \"skill_duration\": 40},\n",
    "   2: {\"skill\": \"landing\", \"agent\": landing_agent, \"skill_duration\": 40},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vect_envs(INIT_HP[\"ENV_NAME\"], num_envs=1)  # Create environment\n",
    "\n",
    "observation_space = env.single_observation_space\n",
    "\n",
    "action_dim = len(\n",
    "   trained_skills\n",
    ")  # Selector will be trained to choose which trained skill to use\n",
    "\n",
    "action_space = spaces.Discrete(action_dim)\n",
    "\n",
    "if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "   observation_space = observation_space_channels_to_first(observation_space)\n",
    "\n",
    "pop = create_population(\n",
    "   algo=\"PPO\",  # Algorithm\n",
    "   observation_space=observation_space,  # Observation space\n",
    "   action_space=action_space,  # Action space\n",
    "   net_config=NET_CONFIG,  # Network configuration\n",
    "   INIT_HP=INIT_HP,  # Initial hyperparameters\n",
    "   population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "   device=device,\n",
    ")\n",
    "\n",
    "if INIT_HP[\"WANDB\"]:\n",
    "   wandb.init(\n",
    "         # set the wandb project where this run will be logged\n",
    "         project=\"EvoWrappers\",\n",
    "         name=\"{}-EvoHPO-{}-{}\".format(\n",
    "            INIT_HP[\"ENV_NAME\"],\n",
    "            INIT_HP[\"ALGO\"],\n",
    "            datetime.now().strftime(\"%m%d%Y%H%M%S\"),\n",
    "         ),\n",
    "         # track hyperparameters and run metadata\n",
    "         config={\n",
    "            \"algo\": f\"Evo HPO {INIT_HP['ALGO']}\",\n",
    "            \"env\": INIT_HP[\"ENV_NAME\"],\n",
    "            \"INIT_HP\": INIT_HP,\n",
    "         },\n",
    "   )\n",
    "\n",
    "bar_format = \"{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]\"\n",
    "pbar = trange(\n",
    "  INIT_HP[\"MAX_STEPS\"],\n",
    "  unit=\"step\",\n",
    "  bar_format=bar_format,\n",
    "  ascii=True)\n",
    "\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.less([agent.steps[-1] for agent in pop], INIT_HP[\"MAX_STEPS\"]).all():\n",
    "   for agent in pop:  # Loop through population\n",
    "         state = env.reset()[0]  # Reset environment at start of episode\n",
    "         score = 0\n",
    "\n",
    "         states = []\n",
    "         actions = []\n",
    "         log_probs = []\n",
    "         rewards = []\n",
    "         terminations = []\n",
    "         values = []\n",
    "\n",
    "         for idx_step in range(500):\n",
    "            # Get next action from agent\n",
    "            action, log_prob, _, value = agent.get_action(state)\n",
    "\n",
    "            # Internal loop to execute trained skill\n",
    "            skill_agent = trained_skills[action[0]][\"agent\"]\n",
    "            skill_duration = trained_skills[action[0]][\"skill_duration\"]\n",
    "            reward = 0\n",
    "            for skill_step in range(skill_duration):\n",
    "               # If landed, do nothing\n",
    "               if state[0][6] or state[0][7]:\n",
    "                     next_state, skill_reward, termination, truncation, _ = env.step(\n",
    "                        [0]\n",
    "                     )\n",
    "               else:\n",
    "                     skill_action, _, _, _ = skill_agent.get_action(state)\n",
    "                     next_state, skill_reward, termination, truncation, _ = env.step(\n",
    "                        skill_action\n",
    "                     )  # Act in environment\n",
    "               reward += skill_reward\n",
    "               if np.any(termination) or np.any(truncation):\n",
    "                     break\n",
    "               state = next_state\n",
    "            score += reward\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            terminations.append(termination)\n",
    "            values.append(value)\n",
    "\n",
    "         agent.scores.append(score)\n",
    "\n",
    "         # Learn according to agent's RL algorithm\n",
    "         agent.learn(\n",
    "            (\n",
    "               states,\n",
    "               actions,\n",
    "               log_probs,\n",
    "               rewards,\n",
    "               terminations,\n",
    "               values,\n",
    "               next_state,\n",
    "            )\n",
    "         )\n",
    "\n",
    "         agent.steps[-1] += idx_step + 1\n",
    "         total_steps += idx_step + 1\n",
    "\n",
    "   if (agent.steps[-1]) % INIT_HP[\"EVO_STEPS\"] == 0:\n",
    "      mean_scores = np.mean([agent.scores[-20:] for agent in pop], axis=1)\n",
    "      if INIT_HP[\"WANDB\"]:\n",
    "          wandb.log(\n",
    "              {\n",
    "                  \"global_step\": total_steps,\n",
    "                  \"train/mean_score\": np.mean(mean_scores),\n",
    "              }\n",
    "          )\n",
    "      print(\n",
    "          f\"\"\"\n",
    "          --- Global Steps {total_steps} ---\n",
    "          Score:\\t\\t{mean_scores}\n",
    "          \"\"\",\n",
    "          end=\"\\r\",\n",
    "      )\n",
    "\n",
    "if INIT_HP[\"WANDB\"]:\n",
    "   wandb.finish()\n",
    "env.close()\n",
    "\n",
    "# Save the trained selector\n",
    "filename = \"PPO_trained_agent_selector.pt\"\n",
    "save_path = os.path.join(save_dir, filename)\n",
    "pop[0].save_checkpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
