{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0fcff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\nebulgym\\training_learners\\model_engines\\ort_engine.py:14: UserWarning: No ONNXRuntime training library for pytorch has been detected. The ORT backend won't be used.\n",
      "  warnings.warn(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\nebulgym\\training_learners\\model_engines\\rammer_engine.py:16: UserWarning: No valid Rammer installation found. Using the Rammer backend will result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.6.0, llvm 15.0.1, commit f1c6fbbd, win, python 3.9.15\n",
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "import functools\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "if we do this carefully we can use taichi to carry out speedup\n",
    "\n",
    "The mixer and the bmodel would be ti.funcs\n",
    "\n",
    "qmixpolicy would also be a ti.func\n",
    "\n",
    "qmix would be the ti.kernel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import torch\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "#from nebulgym.decorators.torch_decorators import accel\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "#from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE, make_multi_agent\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer.abstract_infer import TracePosterior\n",
    "import nitime\n",
    "from deeptime.sindy import SINDy\n",
    "\n",
    "#non causal counterfactuals\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers # helper functions\n",
    "\n",
    "#data visualization\n",
    "import pygwalker as pyg\n",
    "\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcae5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.ode import odeint\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.examples.datasets import LYNXHARE, load_dataset\n",
    "from numpyro.infer import MCMC, NUTS, Predictive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ab907a3",
   "metadata": {},
   "source": [
    "For the ESM the observations are stuff from observation space, x is the reward\n",
    "\n",
    "The epistemic value has form expectation of entropy of p(outcomes given states)-Q(outcomes given policy) \n",
    "\n",
    "pragmatic value has lnP(outcomes|c)\n",
    "\n",
    "APE = expected entropy of probability theta given observations and design\n",
    "\n",
    "Theta is q as defined by the guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d4a86f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create this like the outcome predictor. This is the expected free energy sub-module ESM\n",
    "#this will essentially calculate the q(x|y)\n",
    "class ESM(nn.Module,ivy.Module):\n",
    "     def __init__(self):     \n",
    "\n",
    "        super().__init__()\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior_entropy = dist.Bernoulli(prior_w_prob).entropy()\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        #model = tf.keras.Sequential([\n",
    "        u = tfkl.InputLayer(input_shape=input_shape),\n",
    "        \"\"\"\n",
    "        u = tf.keras.layers.LSTM(25,kernel_initializer='zeros',activation='tanh', dtype = x.dtype, use_bias=True)(u),\n",
    "        u = tfp.layers.VariationalGaussianProcess(\n",
    "                num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\n",
    "                inducing_index_points_initializer=tf.compat.v1.constant_initializer(\n",
    "                    np.linspace(0,x_range, num=1125,\n",
    "                                dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))), mean_fn=None,\n",
    "                jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)(u)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "        #in unconstrained thing replace astype with tf.dtype thing.    #tf.initializers.constant(-10.0)\n",
    "        #])\n",
    "        def compute_probability(self, y):\n",
    "            #fk.Model()\n",
    "            z = nn.functional.relu(self.h1(y))\n",
    "            z = nn.functional.relu(self.h2(z))\n",
    "            #o = tf.nn.relu(u)\n",
    "            return self.h3(z)\n",
    "        def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "            pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "            y = y_dict[\"y\"]\n",
    "\n",
    "            #bmodel = FullLaplace(bmodel,'regression',prior_precision=2)\n",
    "            dem_prob = self.compute_probability(y).squeeze()\n",
    "            pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d738f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.oed.eig import marginal_eig\n",
    "def ES(design, observation_labels, target_labels):\n",
    "    # This shape allows us to learn a different parameter for each candidate design l\n",
    "    q_logit = pyro.param(\"q_logit\", torch.zeros(design.shape[-2:])).entropy()\n",
    "    pyro.sample(\"y\", dist.Bernoulli(logits=q_logit).to_event(1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3aecd46",
   "metadata": {},
   "source": [
    "we are going to use the network design algorithm that uses pysindy and its physics informed neural networks. If we can use this in \n",
    "conjunction with judea perls algorithm for verifying any causal thing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ed8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoize(fn=None, **kwargs):\n",
    "    if fn is None:\n",
    "        return lambda _fn: memoize(_fn, **kwargs)\n",
    "    return functools.lru_cache(**kwargs)(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c21df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashingMarginal(dist.Distribution):\n",
    "    \"\"\"\n",
    "    :param trace_dist: a TracePosterior instance representing a Monte Carlo posterior\n",
    "\n",
    "    Marginal histogram distribution.\n",
    "    Turns a TracePosterior object into a Distribution\n",
    "    over the return values of the TracePosterior's model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trace_dist, sites=None):\n",
    "        assert isinstance(\n",
    "            trace_dist, TracePosterior\n",
    "        ), \"trace_dist must be trace posterior distribution object\"\n",
    "\n",
    "        if sites is None:\n",
    "            sites = \"_RETURN\"\n",
    "\n",
    "        assert isinstance(sites, (str, list)), \"sites must be either '_RETURN' or list\"\n",
    "\n",
    "        self.sites = sites\n",
    "        super().__init__()\n",
    "        self.trace_dist = trace_dist\n",
    "\n",
    "    has_enumerate_support = True\n",
    "\n",
    "    @memoize(maxsize=10)\n",
    "    def _dist_and_values(self):\n",
    "        # XXX currently this whole object is very inefficient\n",
    "        values_map, logits = collections.OrderedDict(), collections.OrderedDict()\n",
    "        for tr, logit in zip(self.trace_dist.exec_traces, self.trace_dist.log_weights):\n",
    "            if isinstance(self.sites, str):\n",
    "                value = tr.nodes[self.sites][\"value\"]\n",
    "            else:\n",
    "                value = {site: tr.nodes[site][\"value\"] for site in self.sites}\n",
    "            if not torch.is_tensor(logit):\n",
    "                logit = torch.tensor(logit)\n",
    "\n",
    "            if torch.is_tensor(value):\n",
    "                value_hash = hash(value.cpu().contiguous().numpy().tobytes())\n",
    "            elif isinstance(value, dict):\n",
    "                value_hash = hash(self._dict_to_tuple(value))\n",
    "            else:\n",
    "                value_hash = hash(value)\n",
    "            if value_hash in logits:\n",
    "                # Value has already been seen.\n",
    "                logits[value_hash] = dist.util.logsumexp(\n",
    "                    torch.stack([logits[value_hash], logit]), dim=-1\n",
    "                )\n",
    "            else:\n",
    "                logits[value_hash] = logit\n",
    "                values_map[value_hash] = value\n",
    "\n",
    "        logits = torch.stack(list(logits.values())).contiguous().view(-1)\n",
    "        logits = logits - dist.util.logsumexp(logits, dim=-1)\n",
    "        d = dist.Categorical(logits=logits)\n",
    "        return d, values_map\n",
    "\n",
    "    def sample(self):\n",
    "        d, values_map = self._dist_and_values()\n",
    "        ix = d.sample()\n",
    "        return list(values_map.values())[ix]\n",
    "\n",
    "    def log_prob(self, val):\n",
    "        d, values_map = self._dist_and_values()\n",
    "        if torch.is_tensor(val):\n",
    "            value_hash = hash(val.cpu().contiguous().numpy().tobytes())\n",
    "        elif isinstance(val, dict):\n",
    "            value_hash = hash(self._dict_to_tuple(val))\n",
    "        else:\n",
    "            value_hash = hash(val)\n",
    "        return d.log_prob(torch.tensor([list(values_map.keys()).index(value_hash)]))\n",
    "\n",
    "    def enumerate_support(self):\n",
    "        d, values_map = self._dist_and_values()\n",
    "        return list(values_map.values())[:]\n",
    "\n",
    "    def _dict_to_tuple(self, d):\n",
    "        \"\"\"\n",
    "        Recursively converts a dictionary to a list of key-value tuples\n",
    "        Only intended for use as a helper function inside HashingMarginal!!\n",
    "        May break when keys cant be sorted, but that is not an expected use-case\n",
    "        \"\"\"\n",
    "        if isinstance(d, dict):\n",
    "            return tuple([(k, self._dict_to_tuple(d[k])) for k in sorted(d.keys())])\n",
    "        else:\n",
    "            return d\n",
    "\n",
    "    def _weighted_mean(self, value, dim=0):\n",
    "        weights = self._log_weights.reshape([-1] + (value.dim() - 1) * [1])\n",
    "        max_weight = weights.max(dim=dim)[0]\n",
    "        relative_probs = (weights - max_weight).exp()\n",
    "        return (value * relative_probs).sum(dim=dim) / relative_probs.sum(dim=dim)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        samples = torch.stack(list(self._dist_and_values()[1].values()))\n",
    "        return self._weighted_mean(samples)\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        samples = torch.stack(list(self._dist_and_values()[1].values()))\n",
    "        deviation_squared = torch.pow(samples - self.mean, 2)\n",
    "        return self._weighted_mean(deviation_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ad275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Marginal(fn=None, **kwargs):\n",
    "    if fn is None:\n",
    "        return lambda _fn: Marginal(_fn, **kwargs)\n",
    "    return memoize(\n",
    "        lambda *args: HashingMarginal(BestFirstSearch(fn, **kwargs).run(*args))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ti.data_oriented\n",
    "class Search(TracePosterior):\n",
    "    \"\"\"\n",
    "    Exact inference by enumerating over all possible executions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, max_tries=int(1e6), **kwargs):\n",
    "        self.model = model\n",
    "        self.max_tries = max_tries\n",
    "        super().__init__(**kwargs)\n",
    "    #@ti.kernel\n",
    "    def _traces(self, *args, **kwargs):\n",
    "        q = queue.Queue()\n",
    "        q.put(poutine.Trace())\n",
    "        p = poutine.trace(poutine.queue(self.model, queue=q, max_tries=self.max_tries))\n",
    "        while not q.empty():\n",
    "            tr = p.get_trace(*args, **kwargs)\n",
    "            yield tr, tr.log_prob_sum()\n",
    "#@ti.data_oriented\n",
    "class BestFirstSearch(TracePosterior):\n",
    "    \"\"\"\n",
    "    Inference by enumerating executions ordered by their probabilities.\n",
    "    Exact (and results equivalent to Search) if all executions are enumerated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, num_samples=None, **kwargs):\n",
    "        if num_samples is None:\n",
    "            num_samples = 100\n",
    "        self.num_samples = num_samples\n",
    "        self.model = model\n",
    "        super().__init__(**kwargs)\n",
    "    #@ti.kernel\n",
    "    def _traces(self, *args, **kwargs):\n",
    "        q = queue.PriorityQueue()\n",
    "        # add a little bit of noise to the priority to break ties...\n",
    "        q.put((torch.zeros(1).item() - torch.rand(1).item() * 1e-2, poutine.Trace()))\n",
    "        q_fn = pqueue(self.model, queue=q)\n",
    "        for i in range(self.num_samples):\n",
    "            if q.empty():\n",
    "                # num_samples was too large!\n",
    "                break\n",
    "            tr = poutine.trace(q_fn).get_trace(*args, **kwargs)  # XXX should block\n",
    "            yield tr, tr.log_prob_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dab43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pragmatic value function \n",
    "\n",
    "@Marginal\n",
    "def speaker(state):\n",
    "    alpha = 1.\n",
    "    with poutine.scale(scale=torch.tensor(alpha)):\n",
    "        utterance = utterance_prior()\n",
    "        pyro.sample(\"listener\", literal_listener(utterance), obs=state)\n",
    "    return utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee6e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = DAdaptAdaGrad#torch.optim.SGD\n",
    "scheduler = pyro.optim.ExponentialLR({'optimizer': optimizer, 'optim_args': {}, 'gamma': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45659f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "@accelerate_model()\n",
    "class emodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "\n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        pragmatic value is probability of outcomes averaged over all policies or E(lnP*o|c) where c is prior\n",
    "        policies with outcomes close to the prior become more probable\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #our job here is to minimize the log probability of y given prior. \n",
    "        #input_dict[\"obs_flat\"].float()\n",
    "        #encoder = tfk.Sequential([\n",
    "        \"\"\"\n",
    "        i = tfkl.InputLayer(input_shape=input_shape)\n",
    "        x = tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "               activation=None)(i)\n",
    "        z = tfp.layers.VariationalGaussianProcess(\n",
    "                num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\n",
    "                inducing_index_points_initializer=tf.compat.v1.constant_initializer(\n",
    "                    np.linspace(0,x_range, num=1125,\n",
    "                                dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))), mean_fn=None,\n",
    "                jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)(x)\n",
    "\n",
    "        #in unconstrained thing replace astype with tf.dtype thing.    #tf.initializers.constant(-10.0)\n",
    "\n",
    "        #negloglik = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "\n",
    "        eig = marginal_eig(model,\n",
    "                           candidate_designs,       # design, or in this case, tensor of possible designs\n",
    "                           \"y\",                     # site label of observations, could be a list\n",
    "                           \"theta\",                 # site label of 'targets' (latent variables), could also be list\n",
    "                           num_samples=100,         # number of samples to draw per step in the expectation\n",
    "                           num_steps=num_steps,     # number of gradient steps\n",
    "                           guide=marginal_guide,    # guide q(y)\n",
    "                           optim=optimizer,         # optimizer with learning rate decay\n",
    "                           final_num_samples=10000  # at the last step, we draw more samples\n",
    "                                                    # for a more accurate EIG estimate\n",
    "                          )\n",
    "\n",
    "        expected free energy is minimized when when observations are slected that cause a large change in beliefs \n",
    "        \n",
    "        Expected free energy will consist of 3 components\n",
    "        \n",
    "        ESM -> q(y|d)\n",
    "        |\n",
    "        emodels -> initialization pragmatic value neural network. \n",
    "        This is going to be neural network for \n",
    "        log(p(y|prior)) +epistemic value  emodels + (epistemic value = ESM - expected ambiguity)\n",
    "        |\n",
    "        EFE -> expected free energy/temperature   \n",
    "        \n",
    "        pragmatic value will be calculated using a variational gaussian process regression tensorflow neural network \n",
    "        \n",
    "        we are going to explore The Rational Speech Act framework and its relation to calculating the pragmatic value\n",
    "        \n",
    "        We will actually calculate both epistemic value and define the neural network for pragmatic value in emodels\n",
    "        \n",
    "        but inputs, parameterization and outputs of pragmatic value network will happen on EFE\n",
    "        \n",
    "        Next objective: determine the probaility of hidden state given observation\n",
    "        \n",
    "        we should use a neural network to calculate the probability of reward given action \n",
    "        \n",
    "        if there is 0 correlation, that is to say reward does not change given a certain action then p(s|y) = 0\n",
    "        \n",
    "        pragmatic value observations expecteation of log probability of outcomes given prior \n",
    "        \n",
    "        theta = s=w=x is the hidden latent variable state. Here it is going to be the reward\n",
    "        \n",
    "        design is going to be action, outcome is observation, state is the reward\n",
    "        \n",
    "        also -H(P(s)) = E(lnP(s))\n",
    "        \n",
    "        in EFE we are going to use a relaxed bernoulli with temperature for distribution of entropy of p(o|s)\n",
    " \n",
    "        average predictive entropy = H(posteririor pdf) = entropy of entire epsitemic value \n",
    "        \n",
    "        for our purposes p(theta) = q(theta|design)\n",
    "        \n",
    "        pragmatic value is -E(ln(p(y|prior)) which could be converted to prior entropy \n",
    "        \n",
    "        we will implement enhanced expected free energy later to include learning. \n",
    "        \n",
    "        we for now can set the pragmatic value to 0. this is used to set objectives \n",
    "        \n",
    "        NOte: to clear up confusion theta can refer to states or to beliefs about parameters in model \n",
    "        \n",
    "        later for enhanced expected free energy we will have to include theta side by side with states outcomes,\n",
    "        designs and prior \n",
    "        \n",
    "        Reinforcement learning use value functions whereas active inference uses priors to set objectives\n",
    "        \n",
    "        the pragmatic value will be computed in emodels and will be the basis for the first q value because \n",
    "        \n",
    "        the pragmatic value deals with exploitation and selection based on realizing the objective set by prior\n",
    "        \n",
    "        note: observation noise is simply a counterpart of prior precision that allows us to adjust how much weight\n",
    "        to put on observations as updates to the prior. in short we can make updates small by making the weight of \n",
    "        prior precision extremely large or making observation noise large. conversely \n",
    "        we can make the updates large by making prior precision extremely small or by making the obs noise small \n",
    "        \n",
    "        we should note that prior precision acts as a gain control devide for amplifying signals. This may also be related\n",
    "        to attention which is related to consciousness ans learning. \n",
    "        \n",
    "        optimization of precisions is the statstical-mechanical basis of signal prioritisation in general\n",
    "        \n",
    "        to do this we need to be able to predict precisons so that we can weight the expected precision of error signals\n",
    "        over the precision of outgoing predictions\n",
    "        \n",
    "        optimization beyond action and perception, the adjustment of precision to match the ampltitude of incoming\n",
    "        prediction errors is the basis of consciousness in fristons work \n",
    "        \n",
    "        the next phase of pegasus will be the following: for learning we need to do the inverse of inference \n",
    "        \n",
    "        which involves optimizing beliefs about relationships between variables in the generative model. This may only\n",
    "        occur after states have been inferred \n",
    "        \n",
    "        to proceed with the next phase we will have to dispense with the mixer network \n",
    "        \n",
    "        we should use the memory example rather then the election example as our primary reference mainly due to the fact\n",
    "        that we are not using two latent variables \n",
    "        \n",
    "        we dont need tow wory about sample size or optimizer since here we are optimizing over psuedo observations\n",
    "        \n",
    "        at some point tasha would like to use this project to model the brain of a dog as well as a human\n",
    "        \n",
    "        tasha would like visualization for simulated nueurons \n",
    "        \n",
    "        \n",
    "        our final two options are:\n",
    "        \n",
    "        set explore setting to false. Do expected free energy optimization on emodels and jettison efe\n",
    "        \n",
    "        or\n",
    "        \n",
    "        keep efe. calculate only pragmatic value on emodels and do full expected free energy calculation on EFE\n",
    "        \n",
    "        or \n",
    "        \n",
    "        keep efe. do full calculations of epistemic and pragmatic in emodels.  \n",
    "        \n",
    "        \n",
    "        On may 15th 2023 let us try to cut out efe and do the full Expected free energy calculation in emodel \n",
    "        \"\"\"\n",
    "            \n",
    "        guide = ESM()# marginal_guide = posterior predictive entropy d can also be expressed as pi \n",
    "        pyro.clear_param_store()\n",
    "        # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "        # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "        # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)] where w = x  \n",
    "        with pyro.plate_stack(\"plate\", l.shape[:-1]):\n",
    "            theta = pyro.sample(\"theta\", dist.Dirichlet(1))#Normal(0, 0.8))# in our case this will represent the reward\n",
    "            # Share theta across the number of rounds of the experiment\n",
    "            # This represents repeatedly testing the same participant\n",
    "            theta = theta.unsqueeze(-1)\n",
    "            # This define a *logistic regression* model for y\n",
    "            #logit_p = sensitivity * (theta - l)\n",
    "            # The event shape represents responses from the same participant\n",
    "            y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n",
    "            return y\n",
    "            #ape = average posterior entropy\n",
    "        #we dont need tow wory about sample size or optimizer since here we are optimizing over psuedo observations\n",
    "        ape = posterior_eig(model, allocation, \"y\", \"theta\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "        \n",
    "        eigs[strategy] = prior_entropy - ape\n",
    "        print(eigs[strategy].item())\n",
    "\n",
    "        if eigs[strategy] > best_eig:\n",
    "            best_strategy, best_eig = strategy, eigs[strategy]\n",
    "\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        def utterance_prior():\n",
    "            ix = pyro.sample(\"utt\", dist.Categorical(probs=torch.ones(3) / 3))\n",
    "            return [\"none\",\"some\",\"all\"][ix]\n",
    "        @Marginal\n",
    "        def speaker():\n",
    "            alpha = 1.\n",
    "            with poutine.scale(scale=torch.tensor(alpha)):\n",
    "                state = utterance_prior()\n",
    "                pyro.sample(\"POMDP\",state , obs=input_dict[\"obs_flat\"])\n",
    "            return utterance\n",
    "        guide = ESM()\n",
    "\n",
    "        # Step 3: learn the posterior using all data seen so far\n",
    "        \n",
    "\n",
    "        conditioned_model = pyro.condition(model, {\"y\": input_dict[\"obs_flat\"]})\n",
    "        svi = SVI(conditioned_model,\n",
    "                  guide,\n",
    "                  scheduler,#Adam({\"lr\": .005}),\n",
    "                  loss=Trace_ELBO(),\n",
    "                  num_samples=1)\n",
    "        #num_iters = 2000\n",
    "        #for i in range(num_iters):\n",
    "        elbo = svi.step(ls)\n",
    "        \n",
    "        history.append((pyro.param(\"posterior_mean\").detach().clone().numpy(),\n",
    "                        pyro.param(\"posterior_sd\").detach().clone().numpy()))\n",
    "        current_model = make_model(pyro.param(\"posterior_mean\").detach().clone(),\n",
    "                                   pyro.param(\"posterior_sd\").detach().clone())\n",
    "        print(\"Estimate of \\u03b8: {:.3f} \\u00b1 {:.3f}\\n\".format(*history[-1]))     \n",
    "                                \n",
    "                                \n",
    "        utterance = speaker()\n",
    "        q = elbo - utterance \n",
    "        #####################################################################################\n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        \"\"\"\n",
    "        \n",
    "        model = gpflow.models.VGP((x, h_in),kernel=gpflow.kernels.SquaredExponential(),\n",
    "        likelihood=gpflow.likelihoods.Bernoulli())\n",
    "        opt = gpflow.optimizers.Scipy()\n",
    "        opt.minimize(model.training_loss, model.trainable_variables)\n",
    "        model = gpflow.models.SVGP(\n",
    "        kernel=gpflow.kernels.SquaredExponential(),\n",
    "        likelihood=gpflow.likelihoods.Bernoulli(),\n",
    "        inducing_variable=np.linspace(0.0, 1.0, 4)[:, None],)\n",
    "        opt = gpflow.optimizers.Scipy()\n",
    "        opt.minimize(model.training_loss_closure((X, Y)), model.trainable_variables)\n",
    "        gpflow.utilities.print_summary(model, \"notebook\")\n",
    "\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        #q = self.fc2(h)\n",
    "        #model = tfk.Model(inputs=i(input_dict[\"obs_flat\"]),outputs=z(h))\n",
    "        #i = tfkl.InputLayer(input_shape=input_shape)\n",
    "        #x = tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),activation=None)(i)\n",
    "        \"\"\"\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),reinterpreted_batch_ndims=1)\n",
    "        x  = tf.keras.layers.Dense(50,kernel_initializer='ones', use_bias=False,kernel_regularizer=tfpl.KLDivergenceRegularizer(prior))(h)\n",
    "        #x = tfpl.MultivariateNormalTriL(d)\n",
    "        z = tfp.layers.VariationalGaussianProcess(\n",
    "                num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\n",
    "                inducing_index_points_initializer=tf.compat.v1.constant_initializer(\n",
    "                    np.linspace(0,x_range, num=1125,\n",
    "                                dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))), mean_fn=None,\n",
    "                jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)(x)\n",
    "\n",
    "        model = tfk.Model(inputs=x,outputs=q)#decoder(input_dict[\"obs_flat\"][0]))\n",
    "        \n",
    "        yhat = model(x_tst)#Note that this is a distribution not a tensor\n",
    "        num_samples = 13 \n",
    "        for i in range(num_samples):\n",
    "            sample_ = yhat.sample().numpy()\n",
    "            sample_[..., 0].T\n",
    "            e = sample_[..., 0].T\n",
    "            r.append(e)\n",
    "  \n",
    "        rfinal = (r[0]+r[1]+r[2]+r[3]+r[4]+r[5]+r[6]+r[7]+r[8]+r[9]+r[10]+r[11]+r[12])/13\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        #\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))\n",
    "        \n",
    "        h = self.rnn(x,h_in)\n",
    "        q = self.fc2(h)\n",
    "        \"\"\"\n",
    "        #negloglik = lambda x, rv_x: -rv_x.log_prob(h)#tf.keras.losses.KLDivergence()\n",
    "        #model.add_loss(negloglik)\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n",
    "ivy.set_framework('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "968647d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "#@ti.func\n",
    "@accelerate_model()\n",
    "class bmodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        #under advice from Tasha and Dimitrov and given what we know about\n",
    "        #long term dependencce int he brain we are using LSTMs for neuronal agents\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        \n",
    "        \"\"\"\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(priora, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))#nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        #\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))\n",
    "        \n",
    "        h = self.rnn(x,h_in)\n",
    "        q = self.fc2(h)\n",
    "\n",
    "        \"\"\"\n",
    "        vae = tfk.Model(inputs=encoder(input_dict[\"obs_flat\"]),\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        \"\"\"\n",
    "\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a352678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "y = np.ones(30)\n",
    "y = np.expand_dims(y,1)\n",
    "x = tf.keras.layers.Dense(1)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "516ff7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\"\"\"\n",
    "next objective for april 8th 2023: we are going to have to modify the mixer network \n",
    "\n",
    "The mixer network is going to be a bayesian tensorflow network with a -ELBO builtin loss and a kullbacker leibeler loss\n",
    "that will be seen in the QMIX loss function\n",
    "\n",
    "We will have a configurable parameter that depending on whether the policy is high or low will have either -elbo or \n",
    "kullbacker leibler as builtin losses with a generic bayesian neural network . \n",
    "\n",
    "finally we will have emodel which will have a neural network guide\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#@ti.func\n",
    "#@ti.data_oriented\n",
    "class QMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape, mixing_embed_dim):\n",
    "        super(QMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.embed_dim = mixing_embed_dim\n",
    "        self.state_dim = int(np.prod(state_shape))\n",
    "\n",
    "        self.hyper_w_1 = nn.Linear(self.state_dim, self.embed_dim * self.n_agents)#tfkl.Embedding(self.state_dim,  self.embed_dim * self.n_agents)#\n",
    "        self.hyper_w_final = nn.Linear(self.state_dim, self.embed_dim)#tfkl.Embedding(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # State dependent bias for hidden layer\n",
    "        self.hyper_b_1 = nn.Linear(self.state_dim, self.embed_dim)#tfkl.Embedding(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # V(s) instead of a bias for the last layers\n",
    "        self.V = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.embed_dim),#tfkl.Embedding(self.state_dim, self.embed_dim)\n",
    "            nn.ReLU(),\n",
    "            # tf.nn.relu\n",
    "            nn.Linear(self.embed_dim, 1),#tfkl.Embedding(self.embed_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, agent_qs, states):\n",
    "        \"\"\"Forward pass for the mixer.\n",
    "        Args:\n",
    "            agent_qs: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            states: Tensor of shape [B, T, state_dim]\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        nn.Sequential(\n",
    "            torch.abs(self.hyper_w_1(states))\n",
    "            self.hyper_b_1(states)\n",
    "            nn.functional.elu(torch.bmm(agent_qs, w1) + b1)\n",
    "            torch.abs(self.hyper_w_final(states))\n",
    "            self.V\n",
    "            \n",
    "        )\n",
    "        tfkl.InputLayer(self.state_dim,self.embed_dim*self.n_agents)\n",
    "        \n",
    "        self.hyper_w_1 = tf.keras.activations.linear(self.state_dim, self.embed_dim * self.n_agents)\n",
    "        \n",
    "        tfkl.Embedding(1000, 64)#, input_length=10)\n",
    "        \n",
    "        x.numpy()\n",
    "        \n",
    "        #batch matrix multiplication\n",
    "        tf.matmul()\n",
    "        \n",
    "        model = keras.Model(inputs=[states, agent_qs, tags_input], outputs=output\n",
    "        \n",
    "        \n",
    "        #################################################################################################\n",
    "        w1 = tf.math.abs(tfkl.Embedding(self.state_dim,  self.embed_dim * self.n_agents)(states))\n",
    "        b1 = tfkl.Embedding(self.state_dim, self.embed_dim)(states)\n",
    "        w1 = tf.reshape(w1, [-1, self.n_agents, self.embed_dim])\n",
    "        b1 = tf.reshape(b1,[-1, 1, self.embed_dim])\n",
    "        hidden = tf.nn.elu(tf.matmul(agent_qs, w1)+ b1)\n",
    "        w_final = tf.math.abs(tfkl.Embedding(self.state_dim, self.embed_dim)(states))\n",
    "        w_final = tf.reshape(-1, self.embed_dim, 1)\n",
    "        v =  tfkl.Embedding(self.V(states),-1, 1, 1)\n",
    "        y = tf.matmul(hidden, w_final) + v\n",
    "        q_tot = tf.reshape(y,[bs, -1, 1])\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        bs = agent_qs.size(0)\n",
    "        states = states.reshape(-1, self.state_dim)#tf.keras.layers.Reshape((3, 4), input_shape=(12,))\n",
    "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
    "        # First layer\n",
    "        \n",
    "        w1 = torch.abs(self.hyper_w_1(states))\n",
    "        \n",
    "        b1 = self.hyper_b_1(states)\n",
    "        w1 = w1.view(-1, self.n_agents, self.embed_dim)\n",
    "        b1 = b1.view(-1, 1, self.embed_dim)\n",
    "        hidden = nn.functional.elu(torch.bmm(agent_qs, w1) + b1)#tf.nn.elu(tf.matmul(agent_qs, w1)+ b1)\n",
    "        # Second layer #tf.nn.elu()\n",
    "        w_final = torch.abs(self.hyper_w_final(states))\n",
    "        w_final = w_final.view(-1, self.embed_dim, 1)\n",
    "        # State-dependent bias\n",
    "        v = self.V(states).view(-1, 1, 1)\n",
    "        # Compute final output\n",
    "        y = torch.bmm(hidden, w_final) + v#tf.matmul(hidden, w_final)\n",
    "        \n",
    "        # Reshape and return\n",
    "        q_tot = y.view(bs, -1, 1)\n",
    "        return q_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4d3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dadaptation import DAdaptAdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c51aaa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ti.data_oriented\n",
    "class QMixLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        target_model,\n",
    "        mixer,\n",
    "        target_mixer,\n",
    "        n_agents,\n",
    "        n_actions,\n",
    "        double_q=True,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.mixer = mixer\n",
    "        self.target_mixer = target_mixer\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        self.double_q = double_q\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rewards,\n",
    "        actions,\n",
    "        terminated,\n",
    "        mask,\n",
    "        obs,\n",
    "        next_obs,\n",
    "        action_mask,\n",
    "        next_action_mask,\n",
    "        state=None,\n",
    "        next_state=None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the loss.\n",
    "        Args:\n",
    "            rewards: Tensor of shape [B, T, n_agents]\n",
    "            actions: Tensor of shape [B, T, n_agents]\n",
    "            terminated: Tensor of shape [B, T, n_agents]\n",
    "            mask: Tensor of shape [B, T, n_agents]\n",
    "            obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            next_obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            next_action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            state: Tensor of shape [B, T, state_dim] (optional)\n",
    "            next_state: Tensor of shape [B, T, state_dim] (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert either none or both of state and next_state are given\n",
    "        if state is None and next_state is None:\n",
    "            state = obs  # default to state being all agents' observations\n",
    "            next_state = next_obs\n",
    "        elif (state is None) != (next_state is None):\n",
    "            raise ValueError(\n",
    "                \"Expected either neither or both of `state` and \"\n",
    "                \"`next_state` to be given. Got: \"\n",
    "                \"\\n`state` = {}\\n`next_state` = {}\".format(state, next_state)\n",
    "            )\n",
    "\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = _unroll_mac(self.model, obs)\n",
    "\n",
    "        # Pick the Q-Values for the actions taken -> [B * n_agents, T]\n",
    "        chosen_action_qvals = torch.gather(\n",
    "            mac_out, dim=3, index=actions.unsqueeze(3)\n",
    "        ).squeeze(3)\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = _unroll_mac(self.target_model, next_obs)\n",
    "\n",
    "        # Mask out unavailable actions for the t+1 step\n",
    "        ignore_action_tp1 = (next_action_mask == 0) & (mask == 1).unsqueeze(-1)\n",
    "        target_mac_out[ignore_action_tp1] = -np.inf\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.double_q:\n",
    "            # Double Q learning computes the target Q values by selecting the\n",
    "            # t+1 timestep action according to the \"policy\" neural network and\n",
    "            # then estimating the Q-value of that action with the \"target\"\n",
    "            # neural network\n",
    "            \n",
    "            #target neural network does expected free energy while policy\n",
    "            #neural network will be variational free energy\n",
    "\n",
    "            # Compute the t+1 Q-values to be used in action selection\n",
    "            # using next_obs\n",
    "            mac_out_tp1 = _unroll_mac(self.model, next_obs)\n",
    "\n",
    "            # mask out unallowed actions\n",
    "            mac_out_tp1[ignore_action_tp1] = -np.inf\n",
    "\n",
    "            # obtain best actions at t+1 according to policy NN\n",
    "            cur_max_actions = mac_out_tp1.argmax(dim=3, keepdim=True)\n",
    "\n",
    "            # use the target network to estimate the Q-values of policy\n",
    "            # network's selected actions\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(\n",
    "                3\n",
    "            )\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "        assert (\n",
    "            target_max_qvals.min().item() != -np.inf\n",
    "        ), \"target_max_qvals contains a masked action; \\\n",
    "            there may be a state with no valid actions.\"\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state)\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, next_state)\n",
    "\n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n",
    "        \"\"\"\n",
    "        \n",
    "        guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        elbo = elbo_(model, guide)\n",
    "        \n",
    "        el = elbo(data)\n",
    "        \n",
    "        #rewards = {self.agent_1:kullbacker, self.agent_2: el}\n",
    "        #loss = lambda y, rv_y: rv_y.variational_loss(y, kl_weight=np.array(batch_size, x.dtype) / x.shape[0])\n",
    "                \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Td-error\n",
    "        #we need to replace this with a variational free energy error\n",
    "        td_error = chosen_action_qvals - targets.detach()\n",
    "        te_error= tf.keras.losses.KLDivergence(chosen_action_qvals - targets.detach()).numpy()\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)#this is ELBO \n",
    "        \n",
    "        #we are going to ahve a kldivergence loss and a -ELBO regularizer for the mixer network \n",
    "        \n",
    "        mask = mask.expand_as(te_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = te_error * mask\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        \n",
    "        #guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        #elbo_ = pyro.infer.Trace_ELBO(num_particles=1)\n",
    "\n",
    "        # Fix the model/guide pair\n",
    "        #elbo = elbo_(model, guide)        \n",
    "        \n",
    "        #data = obs + preds \n",
    "        #loss = -ln()\n",
    "        #loss = lambda y, rv_y: -rv_y.log_prob(y)\n",
    "        loss = masked_error#(masked_td_error**2).sum() / mask.sum()\n",
    "        return loss, mask, masked_td_error, chosen_action_qvals, targets\n",
    "\n",
    "    \n",
    "#this part just above is what we need to revise\n",
    "    \n",
    "#@ti.func\n",
    "#@ti.data_oriented\n",
    "class QMixTorchPolicy(TorchPolicy):\n",
    "    \"\"\"QMix impl. Assumes homogeneous agents for now.\n",
    "    You must use MultiAgentEnv.with_agent_groups() to group agents\n",
    "    together for QMix. This creates the proper Tuple obs/action spaces and\n",
    "    populates the '_group_rewards' info field.\n",
    "    Action masking: to specify an action mask for individual agents, use a\n",
    "    dict space with an action_mask key, e.g. {\"obs\": ob, \"action_mask\": mask}.\n",
    "    The mask space must be `Box(0, 1, (n_actions,))`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        # We want to error out on instantiation and not on import, because tune\n",
    "        # imports all RLlib algorithms when registering them\n",
    "        # TODO (Artur): Find a way to only import algorithms when needed\n",
    "        if not torch:\n",
    "            raise ImportError(\"Could not import PyTorch, which QMix requires.\")\n",
    "\n",
    "        _validate(obs_space, action_space)\n",
    "        config = dict(ray.rllib.algorithms.qmix.qmix.DEFAULT_CONFIG, **config)\n",
    "        self.framework = \"torch\"\n",
    "\n",
    "        self.n_agents = 3000#len(obs_space.original_space.spaces)\n",
    "        config[\"model\"][\"n_agents\"] = self.n_agents\n",
    "        self.n_actions = action_space.spaces[0].n\n",
    "        self.h_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "        self.has_env_global_state = False\n",
    "        self.has_action_mask = False\n",
    "\n",
    "        agent_obs_space = obs_space.original_space.spaces[0]\n",
    "        if isinstance(agent_obs_space, gym.spaces.Dict):\n",
    "            space_keys = set(agent_obs_space.spaces.keys())\n",
    "            if \"obs\" not in space_keys:\n",
    "                raise ValueError(\"Dict obs space must have subspace labeled `obs`\")\n",
    "            self.obs_size = _get_size(agent_obs_space.spaces[\"obs\"])\n",
    "            if \"action_mask\" in space_keys:\n",
    "                mask_shape = tuple(agent_obs_space.spaces[\"action_mask\"].shape)\n",
    "                if mask_shape != (self.n_actions,):\n",
    "                    raise ValueError(\n",
    "                        \"Action mask shape must be {}, got {}\".format(\n",
    "                            (self.n_actions,), mask_shape\n",
    "                        )\n",
    "                    )\n",
    "                self.has_action_mask = True\n",
    "            if ENV_STATE in space_keys:\n",
    "                self.env_global_state_shape = _get_size(\n",
    "                    agent_obs_space.spaces[ENV_STATE]\n",
    "                )\n",
    "                self.has_env_global_state = True\n",
    "            else:\n",
    "                self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            # The real agent obs space is nested inside the dict\n",
    "            config[\"model\"][\"full_obs_space\"] = agent_obs_space\n",
    "            agent_obs_space = agent_obs_space.spaces[\"obs\"]\n",
    "        else:\n",
    "            self.obs_size = _get_size(agent_obs_space)\n",
    "            self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "        #model = bmodel()#CModel()#CModel()\n",
    "        #bmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#\n",
    "        ivy.set_framework('torch')\n",
    "        #model = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')\n",
    "        bmodel = bmodel()\n",
    "        emodel = emodel()\n",
    "        bmodel = FullLaplace(bmodel,'regression',prior_precision=0.00000000000000000000001)#0.00000000000000000000001)#10000000000000000000000000)\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"model\",\n",
    "            default_model=emodel#a#RNNModel#bmodel()#RNNModel,\n",
    "        )\n",
    "\n",
    "        super().__init__(obs_space, action_space, config, model=self.model)\n",
    "\n",
    "        self.target_model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"target_model\",\n",
    "            default_model=bmodel#bmodel()#RNNModel\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.exploration = self._create_exploration()\n",
    "        \n",
    "        # Setup the mixer network.\n",
    "        if config[\"mixer\"] is None:\n",
    "            self.mixer = None\n",
    "            self.target_mixer = None\n",
    "        elif config[\"mixer\"] == \"qmix\":\n",
    "            self.mixer = FullLaplace(QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ),prior_precision=1).to(self.device)\n",
    "            self.target_mixer = FullLaplace(QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ),prior_precision=1).to(self.device)\n",
    "\n",
    "        self.cur_epsilon = 1.0\n",
    "        self.update_target()  # initial sync\n",
    "\n",
    "        # Setup optimizer\n",
    "        self.params = list(self.model.parameters())\n",
    "        if self.mixer:\n",
    "            self.params += list(self.mixer.parameters())\n",
    "        self.loss = QMixLoss(\n",
    "            self.model,\n",
    "            self.target_model,\n",
    "            self.mixer,\n",
    "            self.target_mixer,\n",
    "            self.n_agents,\n",
    "            self.n_actions,\n",
    "            self.config[\"double_q\"],\n",
    "            self.config[\"gamma\"],\n",
    "        )\n",
    "        from torch.optim import RMSprop\n",
    "        \n",
    "        self.rmsprop_optimizer = RMSprop(\n",
    "            params=self.params,\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"optim_alpha\"],\n",
    "            eps=config[\"optim_eps\"],\n",
    "        )#replace with dadptation \n",
    "        self.rs_prop_optimizer = DAdaptAdaGrad(\n",
    "            params=self.params,\n",
    "            alpha=config(\"optim_alpha\"),\n",
    "            eps = config[\"optim_eps\"],\n",
    "        \n",
    "        )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions_from_input_dict(\n",
    "        self,\n",
    "        input_dict: Dict[str, TensorType],\n",
    "        explore: bool = None,\n",
    "        timestep: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:\n",
    "\n",
    "        obs_batch = input_dict[SampleBatch.OBS]\n",
    "        state_batches = []\n",
    "        i = 0\n",
    "        while f\"state_in_{i}\" in input_dict:\n",
    "            state_batches.append(input_dict[f\"state_in_{i}\"])\n",
    "            i += 1\n",
    "\n",
    "        explore = explore if explore is not None else self.config[\"explore\"]\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        # We need to ensure we do not use the env global state\n",
    "        # to compute actions\n",
    "\n",
    "        # Compute actions\n",
    "        with torch.no_grad():\n",
    "            q_values, hiddens = _mac(\n",
    "                self.model,\n",
    "                torch.as_tensor(obs_batch, dtype=torch.float, device=self.device),\n",
    "                [\n",
    "                    torch.as_tensor(np.array(s), dtype=torch.float, device=self.device)\n",
    "                    for s in state_batches\n",
    "                ],\n",
    "            )\n",
    "            avail = torch.as_tensor(action_mask, dtype=torch.float, device=self.device)\n",
    "            masked_q_values = q_values.clone()\n",
    "            masked_q_values[avail == 0.0] = -float(\"inf\")\n",
    "            masked_q_values_folded = torch.reshape(\n",
    "                masked_q_values, [-1] + list(masked_q_values.shape)[2:]\n",
    "            )\n",
    "            actions, _ = self.exploration.get_exploration_action(\n",
    "                action_distribution=TorchCategorical(masked_q_values_folded),\n",
    "                timestep=timestep,\n",
    "                explore=explore,\n",
    "            )\n",
    "            actions = (\n",
    "                torch.reshape(actions, list(masked_q_values.shape)[:-1]).cpu().numpy()\n",
    "            )\n",
    "            hiddens = [s.cpu().numpy() for s in hiddens]\n",
    "\n",
    "        return tuple(actions.transpose([1, 0])), hiddens, {}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions(self, *args, **kwargs):\n",
    "        return self.compute_actions_from_input_dict(*args, **kwargs)\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "    ):\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        return np.zeros(obs_batch.size()[0])\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        obs_batch, action_mask, env_global_state = self._unpack_observation(\n",
    "            samples[SampleBatch.CUR_OBS]\n",
    "        )\n",
    "        (\n",
    "            next_obs_batch,\n",
    "            next_action_mask,\n",
    "            next_env_global_state,\n",
    "        ) = self._unpack_observation(samples[SampleBatch.NEXT_OBS])\n",
    "        group_rewards = self._get_group_rewards(samples[SampleBatch.INFOS])\n",
    "\n",
    "        input_list = [\n",
    "            group_rewards,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            samples[SampleBatch.ACTIONS],\n",
    "            samples[SampleBatch.TERMINATEDS],\n",
    "            obs_batch,\n",
    "            next_obs_batch,\n",
    "        ]\n",
    "        if self.has_env_global_state:\n",
    "            input_list.extend([env_global_state, next_env_global_state])\n",
    "\n",
    "        output_list, _, seq_lens = chop_into_sequences(\n",
    "            episode_ids=samples[SampleBatch.EPS_ID],\n",
    "            unroll_ids=samples[SampleBatch.UNROLL_ID],\n",
    "            agent_indices=samples[SampleBatch.AGENT_INDEX],\n",
    "            feature_columns=input_list,\n",
    "            state_columns=[],  # RNN states not used here\n",
    "            max_seq_len=self.config[\"model\"][\"max_seq_len\"],\n",
    "            dynamic_max=True,\n",
    "        )\n",
    "        # These will be padded to shape [B * T, ...]\n",
    "        if self.has_env_global_state:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "                env_global_state,\n",
    "                next_env_global_state,\n",
    "            ) = output_list\n",
    "        else:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "            ) = output_list\n",
    "        B, T = len(seq_lens), max(seq_lens)\n",
    "\n",
    "        def to_batches(arr, dtype):\n",
    "            new_shape = [B, T] + list(arr.shape[1:])\n",
    "            return torch.as_tensor(\n",
    "                np.reshape(arr, new_shape), dtype=dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        rewards = to_batches(rew, torch.float)\n",
    "        actions = to_batches(act, torch.long)\n",
    "        obs = to_batches(obs, torch.float).reshape([B, T, self.n_agents, self.obs_size])\n",
    "        action_mask = to_batches(action_mask, torch.float)\n",
    "        next_obs = to_batches(next_obs, torch.float).reshape(\n",
    "            [B, T, self.n_agents, self.obs_size]\n",
    "        )\n",
    "        next_action_mask = to_batches(next_action_mask, torch.float)\n",
    "        if self.has_env_global_state:\n",
    "            env_global_state = to_batches(env_global_state, torch.float)\n",
    "            next_env_global_state = to_batches(next_env_global_state, torch.float)\n",
    "\n",
    "        # TODO(ekl) this treats group termination as individual termination\n",
    "        terminated = (\n",
    "            to_batches(terminateds, torch.float)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Create mask for where index is < unpadded sequence length\n",
    "        filled = np.reshape(\n",
    "            np.tile(np.arange(T, dtype=np.float32), B), [B, T]\n",
    "        ) < np.expand_dims(seq_lens, 1)\n",
    "        mask = (\n",
    "            torch.as_tensor(filled, dtype=torch.float, device=self.device)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss_out, mask, masked_td_error, chosen_action_qvals, targets = self.loss(\n",
    "            rewards,\n",
    "            actions,\n",
    "            terminated,\n",
    "            mask,\n",
    "            obs,\n",
    "            next_obs,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            env_global_state,\n",
    "            next_env_global_state,\n",
    "        )\n",
    "\n",
    "        # Optimise\n",
    "        self.rsprop_optimizer.zero_grad()\n",
    "\n",
    "        loss_out.backward()\n",
    "        grad_norm_info = apply_grad_clipping(self, self.rsprop_optimizer, loss_out)\n",
    "        self.rsprop_optimizer.step()\n",
    "\n",
    "        mask_elems = mask.sum().item()\n",
    "        stats = {\n",
    "            \"loss\": loss_out.item(),\n",
    "            \"td_error_abs\": masked_td_error.abs().sum().item() / mask_elems,\n",
    "            \"q_taken_mean\": (chosen_action_qvals * mask).sum().item() / mask_elems,\n",
    "            \"target_mean\": (targets * mask).sum().item() / mask_elems,\n",
    "        }\n",
    "        stats.update(grad_norm_info)\n",
    "\n",
    "        return {LEARNER_STATS_KEY: stats}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_initial_state(self):  # initial RNN state\n",
    "        return [\n",
    "            s.expand([self.n_agents, -1]).cpu().numpy()\n",
    "            for s in self.model.get_initial_state()\n",
    "        ]\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            \"model\": self._cpu_dict(self.model.state_dict()),\n",
    "            \"target_model\": self._cpu_dict(self.target_model.state_dict()),\n",
    "            \"mixer\": self._cpu_dict(self.mixer.state_dict()) if self.mixer else None,\n",
    "            \"target_mixer\": self._cpu_dict(self.target_mixer.state_dict())\n",
    "            if self.mixer\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(self._device_dict(weights[\"model\"]))\n",
    "        self.target_model.load_state_dict(self._device_dict(weights[\"target_model\"]))\n",
    "        if weights[\"mixer\"] is not None:\n",
    "            self.mixer.load_state_dict(self._device_dict(weights[\"mixer\"]))\n",
    "            self.target_mixer.load_state_dict(\n",
    "                self._device_dict(weights[\"target_mixer\"])\n",
    "            )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_state(self):\n",
    "        state = self.get_weights()\n",
    "        state[\"cur_epsilon\"] = self.cur_epsilon\n",
    "        return state\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_state(self, state):\n",
    "        self.set_weights(state)\n",
    "        self.set_epsilon(state[\"cur_epsilon\"])\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        logger.debug(\"Updated target networks\")\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cur_epsilon = epsilon\n",
    "\n",
    "    def _get_group_rewards(self, info_batch):\n",
    "        group_rewards = np.array(\n",
    "            [info.get(GROUP_REWARDS, [0.0] * self.n_agents) for info in info_batch]\n",
    "        )\n",
    "        return group_rewards\n",
    "\n",
    "    def _device_dict(self, state_dict):\n",
    "        return {\n",
    "            k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _cpu_dict(state_dict):\n",
    "        return {k: v.cpu().detach().numpy() for k, v in state_dict.items()}\n",
    "\n",
    "    def _unpack_observation(self, obs_batch):\n",
    "        \"\"\"Unpacks the observation, action mask, and state (if present)\n",
    "        from agent grouping.\n",
    "        Returns:\n",
    "            obs (np.ndarray): obs tensor of shape [B, n_agents, obs_size]\n",
    "            mask (np.ndarray): action mask, if any\n",
    "            state (np.ndarray or None): state tensor of shape [B, state_size]\n",
    "                or None if it is not in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        unpacked = _unpack_obs(\n",
    "            np.array(obs_batch, dtype=np.float32),\n",
    "            self.observation_space.original_space,\n",
    "            tensorlib=np,\n",
    "        )\n",
    "\n",
    "        if isinstance(unpacked[0], dict):\n",
    "            assert \"obs\" in unpacked[0]\n",
    "            unpacked_obs = [np.concatenate(tree.flatten(u[\"obs\"]), 1) for u in unpacked]\n",
    "        else:\n",
    "            unpacked_obs = unpacked\n",
    "\n",
    "        obs = np.concatenate(unpacked_obs, axis=1).reshape(\n",
    "            [len(obs_batch), self.n_agents, self.obs_size]\n",
    "        )\n",
    "\n",
    "        if self.has_action_mask:\n",
    "            action_mask = np.concatenate(\n",
    "                [o[\"action_mask\"] for o in unpacked], axis=1\n",
    "            ).reshape([len(obs_batch), self.n_agents, self.n_actions])\n",
    "        else:\n",
    "            action_mask = np.ones(\n",
    "                [len(obs_batch), self.n_agents, self.n_actions], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.has_env_global_state:\n",
    "            state = np.concatenate(tree.flatten(unpacked[0][ENV_STATE]), 1)\n",
    "        else:\n",
    "            state = None\n",
    "        return obs, action_mask, state\n",
    "\n",
    "#@ti.func\n",
    "def _validate(obs_space, action_space):\n",
    "    if not hasattr(obs_space, \"original_space\") or not isinstance(\n",
    "        obs_space.original_space, gym.spaces.Tuple\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Obs space must be a Tuple, got {}. Use \".format(obs_space)\n",
    "            + \"MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space, gym.spaces.Tuple):\n",
    "        raise ValueError(\n",
    "            \"Action space must be a Tuple, got {}. \".format(action_space)\n",
    "            + \"Use MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space.spaces[0], gym.spaces.Discrete):\n",
    "        raise ValueError(\n",
    "            \"QMix requires a discrete action space, got {}\".format(\n",
    "                action_space.spaces[0]\n",
    "            )\n",
    "        )\n",
    "    if len({str(x) for x in obs_space.original_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: observations of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(obs_space.original_space.spaces)\n",
    "        )\n",
    "    if len({str(x) for x in action_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: action space of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(action_space.spaces)\n",
    "        )\n",
    "\n",
    "#@ti.func\n",
    "def _mac(model, obs, h):\n",
    "    \"\"\"Forward pass of the multi-agent controller.\n",
    "    Args:\n",
    "        model: TorchModelV2 class\n",
    "        obs: Tensor of shape [B, n_agents, obs_size]\n",
    "        h: List of tensors of shape [B, n_agents, h_size]\n",
    "    Returns:\n",
    "        q_vals: Tensor of shape [B, n_agents, n_actions]\n",
    "        h: Tensor of shape [B, n_agents, h_size]\n",
    "    \"\"\"\n",
    "    B, n_agents = obs.size(0), obs.size(1)\n",
    "    if not isinstance(obs, dict):\n",
    "        obs = {\"obs\": obs}\n",
    "    obs_agents_as_batches = {k: _drop_agent_dim(v) for k, v in obs.items()}\n",
    "    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n",
    "    q_flat, h_flat = model(obs_agents_as_batches, h_flat, None)\n",
    "    return q_flat.reshape([B, n_agents, -1]), [\n",
    "        s.reshape([B, n_agents, -1]) for s in h_flat\n",
    "    ]\n",
    "#@ti.func\n",
    "def _unroll_mac(model, obs_tensor):\n",
    "    \"\"\"Computes the estimated Q values for an entire trajectory batch\"\"\"\n",
    "    B = obs_tensor.size(0)\n",
    "    T = obs_tensor.size(1)\n",
    "    n_agents = obs_tensor.size(2)\n",
    "\n",
    "    mac_out = []\n",
    "    h = [s.expand([B, n_agents, -1]) for s in model.get_initial_state()]\n",
    "    for t in range(T):\n",
    "        q, h = _mac(model, obs_tensor[:, t], h)\n",
    "        mac_out.append(q)\n",
    "    mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "    return mac_out\n",
    "#@ti.func\n",
    "def _drop_agent_dim(T):\n",
    "    shape = list(T.shape)\n",
    "    B, n_agents = shape[0], shape[1]\n",
    "    return T.reshape([B * n_agents] + shape[2:])\n",
    "#@ti.func\n",
    "def _add_agent_dim(T, n_agents):\n",
    "    shape = list(T.shape)\n",
    "    B = shape[0] // n_agents\n",
    "    assert shape[0] % n_agents == 0\n",
    "    return T.reshape([B, n_agents] + shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1fc5cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass TwoStepGameWithGroupedAgents(MultiAgentEnv):\\n    def __init__(self, env_config):\\n        super().__init__()\\n        env = TwoStepGame(env_config)\\n        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\\n        tuple_act_space = Tuple([env.action_space, env.action_space])\\n\\n        self.env = env.with_agent_groups(\\n            groups={\"agents\": [0, 1]},\\n            obs_space=tuple_obs_space,\\n            act_space=tuple_act_space,\\n        )\\n        self.observation_space = self.env.observation_space\\n        self.action_space = self.env.action_space\\n        self._agent_ids = {\"agents\"}\\n\\n    def reset(self, *, seed=None, options=None):\\n        return self.env.reset(seed=seed, options=options)\\n\\n    def step(self, actions):\\n        return self.env.step(actions)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@ti.data_oriented\n",
    "class TwoStepGame(MultiAgentEnv):\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(2)\n",
    "        self.state = None\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        #self.agent_3=2\n",
    "        self._skip_env_checking = True\n",
    "        gpus_available = ray.get_gpu_ids()\n",
    "        # MADDPG emits action logits instead of actual discrete actions\n",
    "        self.actions_are_logits = env_config.get(\"actions_are_logits\", False)\n",
    "        self.one_hot_state_encoding = env_config.get(\"one_hot_state_encoding\", False)\n",
    "        self.with_state = env_config.get(\"separate_state_space\", False)\n",
    "        self._agent_ids = {0, 1}\n",
    "        if not self.one_hot_state_encoding:\n",
    "            self.observation_space = Discrete(6)\n",
    "            self.with_state = False\n",
    "        else:\n",
    "            # Each agent gets the full state (one-hot encoding of which of the\n",
    "            # three states are active) as input with the receiving agent's\n",
    "            # ID (1 or 2) concatenated onto the end.\n",
    "            if self.with_state:\n",
    "                self.observation_space = Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.observation_space = MultiDiscrete([2, 2, 2, 3])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = np.array([1, 0, 0])\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "\n",
    "        state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "        \n",
    "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
    "        obs = self._obs()\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        truncateds = {\"__all__\": False}\n",
    "        infos = {\n",
    "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
    "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "        }\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "\n",
    "    def _obs(self):\n",
    "        if self.with_state:\n",
    "            return {\n",
    "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
    "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
    "            }\n",
    "        else:\n",
    "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
    "\n",
    "    def agent_1_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [1]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0]\n",
    "\n",
    "    def agent_2_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [2]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0] + 3\n",
    "\n",
    "        #if self.render_mode == \"rgb_array\":\n",
    "            #return self._render_frame()\n",
    "\n",
    "\"\"\"\n",
    "class TwoStepGameWithGroupedAgents(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        env = TwoStepGame(env_config)\n",
    "        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\n",
    "        tuple_act_space = Tuple([env.action_space, env.action_space])\n",
    "\n",
    "        self.env = env.with_agent_groups(\n",
    "            groups={\"agents\": [0, 1]},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self._agent_ids = {\"agents\"}\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        return self.env.reset(seed=seed, options=options)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ti.data_oriented\n",
    "class PEnv(MultiAgentEnv):\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        #maps env states to observations. we may want to have observations that are correlated to but not the same as env st\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=np.float32),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=np.float32),\n",
    "            }\n",
    "        )\n",
    "        self.observation_space = Box(-inf, inf, (1,), np.float32)\n",
    "        self.action_space = Box(-3.0, 3.0, (1,), np.float32)\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    #this converts enviroments state to observation \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    #We can also implement a similar method for the auxiliary information that is returned by step and reset\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        terminated = 0\n",
    "        reward = 1 if terminated else 0  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "    def render():\n",
    "        print(\"panda3d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc66031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "ti.init(arch=ti.gpu)\n",
    "n = 128\n",
    "val = ti.field(ti.i32, shape=n)\n",
    "@ti.kernel\n",
    "def fill():\n",
    "    #ti.loop_config(parallelize=8, block_dim=16)\n",
    "    # If the kernel is run on the CPU backend, 8 threads will be used to run it\n",
    "    # If the kernel is run on the CUDA backend, each block will have 16 threads.\n",
    "    ti.loop_config(parallelize=8,block_dim=128)\n",
    "    for i in range(n):\n",
    "        val[i] = i\n",
    "fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56dbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "#@ti.data_oriented\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b23e69c",
   "metadata": {},
   "source": [
    "1.finish modifying mixer network - status: basically complete\n",
    "2. visualization - status: ongoing\n",
    "3. expected free energy module Status: 90% done \n",
    "4. integrate reloadium to reduce testing time \n",
    "\n",
    "note: there are actually two mixers: a regular mixer network and a target mixer network. self.target_mixer and self.mixer\n",
    "\n",
    "we should probably use umato to preprocess the inputs going into the neural network for the pragmatic value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebbcfa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class EFE(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \n",
    "    In our case we will be this is where we will create the pragmatic value and together with the epistemic value\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a EFE Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        #pragmatic value function \n",
    "\n",
    "        @Marginal\n",
    "        def speaker(state):\n",
    "            alpha = 1.\n",
    "            with poutine.scale(scale=torch.tensor(alpha)):\n",
    "                utterance = utterance_prior()\n",
    "                pyro.sample(\"listener\", literal_listener(utterance), obs=state)\n",
    "            return utterance\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        # Step 3: learn the posterior using all data seen so far\n",
    "        guide = ESM()\n",
    "        model = self.model\n",
    "        conditioned_model = pyro.condition(model, {\"y\": action_distribution.inputs})\n",
    "        svi = SVI(conditioned_model,\n",
    "                  guide,\n",
    "                  Adam({\"lr\": .005}),\n",
    "                  loss=Trace_ELBO(),\n",
    "                  num_samples=100)\n",
    "        #num_iters = 2000\n",
    "        #for i in range(num_iters):\n",
    "        elbo = svi.step(ls)\n",
    "        \n",
    "        history.append((pyro.param(\"posterior_mean\").detach().clone().numpy(),\n",
    "                        pyro.param(\"posterior_sd\").detach().clone().numpy()))\n",
    "        current_model = make_model(pyro.param(\"posterior_mean\").detach().clone(),\n",
    "                                   pyro.param(\"posterior_sd\").detach().clone())\n",
    "        print(\"Estimate of \\u03b8: {:.3f} \\u00b1 {:.3f}\\n\".format(*history[-1]))\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        for pragmatic value we can use soem of the math for Rational Speech Act framework\n",
    "        \n",
    "        note: \"allocation\" is used together with alpha to create a distribution which is the variable y. \n",
    "        \n",
    "        eig = marginal_eig(model,\n",
    "                           candidate_designs,       # design, or in this case, tensor of possible designs\n",
    "                           \"y\",                     # site label of observations, could be a list\n",
    "                           \"theta\",                 # site label of 'targets' (latent variables), could also be list\n",
    "                           num_samples=100,         # number of samples to draw per step in the expectation\n",
    "                           num_steps=num_steps,     # number of gradient steps\n",
    "                           guide=marginal_guide,    # guide q(y)\n",
    "                           optim=optimizer,         # optimizer with learning rate decay\n",
    "                           final_num_samples=10000  # at the last step, we draw more samples\n",
    "                                                    # for a more accurate EIG estimate\n",
    "                          )\n",
    "                          \n",
    "        we should use pragmatic for emodels and complete either full or last half or first half of epistemic for efe \n",
    "        \"\"\"\n",
    "\n",
    "        #self.model  is the first agent model we use to get the first q value used for model selection\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "104693cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdedf088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our custom replacement for softq in explore config\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class SoftQ(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a SoftQ Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1dd53c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig, NotProvided\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    "    SAMPLE_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "#@ti.data_oriented\n",
    "class QMixConfig(SimpleQConfig):\n",
    "    \"\"\"Defines a configuration class from which QMix can be built.\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> config = QMixConfig()  # doctest: +SKIP\n",
    "        >>> config = config.training(gamma=0.9, lr=0.01, kl_coeff=0.3)  # doctest: +SKIP\n",
    "        >>> config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "        >>> config = config.rollouts(num_rollout_workers=4)  # doctest: +SKIP\n",
    "        >>> print(config.to_dict())  # doctest: +SKIP\n",
    "        >>> # Build an Algorithm object from the config and run 1 training iteration.\n",
    "        >>> algo = config.build(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> algo.train()  # doctest: +SKIP\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> from ray import air\n",
    "        >>> from ray import tune\n",
    "        >>> config = QMixConfig()\n",
    "        >>> # Print out some default values.\n",
    "        >>> print(config.optim_alpha)  # doctest: +SKIP\n",
    "        >>> # Update the config object.\n",
    "        >>> config.training(  # doctest: +SKIP\n",
    "        ...     lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97\n",
    "        ... )\n",
    "        >>> # Set the config object's env.\n",
    "        >>> config.environment(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> # Use to_dict() to get the old-style python config dict\n",
    "        >>> # when running with tune.\n",
    "        >>> tune.Tuner(  # doctest: +SKIP\n",
    "        ...     \"QMix\",\n",
    "        ...     run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "        ...     param_space=config.to_dict(),\n",
    "        ... ).fit()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PPOConfig instance.\"\"\"\n",
    "        super().__init__(algo_class=QMix)\n",
    "\n",
    "        # fmt: off\n",
    "        # __sphinx_doc_begin__\n",
    "        # QMix specific settings:\n",
    "        self.mixer = \"qmix\"\n",
    "        self.mixing_embed_dim = 32\n",
    "        self.double_q = True\n",
    "        self.optim_alpha = 0.99\n",
    "        self.optim_eps = 0.00001\n",
    "        self.grad_clip = 10\n",
    "        #self.render_mode = 'rgb_array'\n",
    "        # QMix-torch overrides the TorchPolicy's learn_on_batch w/o specifying a\n",
    "        # alternative `learn_on_loaded_batch` alternative for the GPU.\n",
    "        # TODO: This hack will be resolved once we move all algorithms to the new\n",
    "        #  RLModule/Learner APIs.\n",
    "        self.simple_optimizer = True\n",
    "\n",
    "        # Override some of AlgorithmConfig's default values with QMix-specific values.\n",
    "        # .training()\n",
    "        self.lr = 0.0005\n",
    "        self.train_batch_size = 32\n",
    "        self.target_network_update_freq = 500\n",
    "        self.num_steps_sampled_before_learning_starts = 1000\n",
    "        self.replay_buffer_config = {\n",
    "            \"type\": \"ReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "            \"prioritized_replay\": 'MultiAgentPrioritizedReplayBuffer',\n",
    "            # Size of the replay buffer in batches (not timesteps!).\n",
    "            \"capacity\": 1000,\n",
    "            # Choosing `fragments` here makes it so that the buffer stores entire\n",
    "            # batches, instead of sequences, episodes or timesteps.\n",
    "            \"storage_unit\": \"fragments\",\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.model = {\n",
    "            \"lstm_cell_size\": 64,\n",
    "            \"max_seq_len\": 999999,\n",
    "        }\n",
    "        \"\"\"\n",
    "        # .framework()\n",
    "        self.framework_str = \"torch\"\n",
    "\n",
    "        # .rollouts()\n",
    "        self.rollout_fragment_length = 4\n",
    "        self.batch_mode = \"complete_episodes\"\n",
    "\n",
    "        # .reporting()\n",
    "        self.min_time_s_per_iteration = 1\n",
    "        self.min_sample_timesteps_per_iteration = 1000\n",
    "\n",
    "        # .exploration()\n",
    "        \"\"\"\n",
    "        self.exploration_config = {\n",
    "           \n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\", #replace this with SoftQ\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.01,\n",
    "            # Timesteps over which to anneal epsilon.\n",
    "            \"epsilon_timesteps\": 40000,\n",
    "            \n",
    "            \"type\": \"EFE\",\n",
    "            \"temperature\": 1.0\n",
    "            # For soft_q, use:\n",
    "            # \"exploration_config\" = {\n",
    "            #   \"type\": \"SoftQ\"\n",
    "            #   \"temperature\": [float, e.g. 1.0]\n",
    "            # }\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        # .evaluation()\n",
    "        # Evaluate with epsilon=0 every `evaluation_interval` training iterations.\n",
    "        # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "        self.evaluation(\n",
    "            evaluation_config=AlgorithmConfig.overrides(explore=False)#False\n",
    "        )\n",
    "        # __sphinx_doc_end__\n",
    "        # fmt: on\n",
    "\n",
    "        self.worker_side_prioritization = DEPRECATED_VALUE\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        mixer: Optional[str] = NotProvided,\n",
    "        mixing_embed_dim: Optional[int] = NotProvided,\n",
    "        double_q: Optional[bool] = NotProvided,\n",
    "        target_network_update_freq: Optional[int] = NotProvided,\n",
    "        replay_buffer_config: Optional[dict] = NotProvided,\n",
    "        optim_alpha: Optional[float] = NotProvided,\n",
    "        optim_eps: Optional[float] = NotProvided,\n",
    "        grad_clip: Optional[float] = NotProvided,\n",
    "        # Deprecated args.\n",
    "        grad_norm_clipping=DEPRECATED_VALUE,\n",
    "        **kwargs,\n",
    "    ) -> \"QMixConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            mixer: Mixing network. Either \"qmix\", \"vdn\", or None.\n",
    "            mixing_embed_dim: Size of the mixing network embedding.\n",
    "            double_q: Whether to use Double_Q learning.\n",
    "            target_network_update_freq: Update the target network every\n",
    "                `target_network_update_freq` sample steps.\n",
    "            replay_buffer_config:\n",
    "            optim_alpha: RMSProp alpha.\n",
    "            optim_eps: RMSProp epsilon.\n",
    "            grad_clip: If not None, clip gradients during optimization at\n",
    "                this value.\n",
    "            grad_norm_clipping: Depcrecated in favor of grad_clip\n",
    "        Returns:\n",
    "            This updated AlgorithmConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if grad_norm_clipping != DEPRECATED_VALUE:\n",
    "            deprecation_warning(\n",
    "                old=\"grad_norm_clipping\",\n",
    "                new=\"grad_clip\",\n",
    "                help=\"Parameter `grad_norm_clipping` has been \"\n",
    "                \"deprecated in favor of grad_clip in QMix. \"\n",
    "                \"This is now the same parameter as in other \"\n",
    "                \"algorithms. `grad_clip` will be overwritten by \"\n",
    "                \"`grad_norm_clipping={}`\".format(grad_norm_clipping),\n",
    "                error=True,\n",
    "            )\n",
    "            grad_clip = grad_norm_clipping\n",
    "\n",
    "        if mixer is not NotProvided:\n",
    "            self.mixer = mixer\n",
    "        if mixing_embed_dim is not NotProvided:\n",
    "            self.mixing_embed_dim = mixing_embed_dim\n",
    "        if double_q is not NotProvided:\n",
    "            self.double_q = double_q\n",
    "        if target_network_update_freq is not NotProvided:\n",
    "            self.target_network_update_freq = target_network_update_freq\n",
    "        if replay_buffer_config is not NotProvided:\n",
    "            self.replay_buffer_config = replay_buffer_config\n",
    "        if optim_alpha is not NotProvided:\n",
    "            self.optim_alpha = optim_alpha\n",
    "        if optim_eps is not NotProvided:\n",
    "            self.optim_eps = optim_eps\n",
    "        if grad_clip is not NotProvided:\n",
    "            self.grad_clip = grad_clip\n",
    "\n",
    "        return self\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def validate(self) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate()\n",
    "\n",
    "        if self.framework_str != \"torch\":\n",
    "            raise ValueError(\n",
    "                \"Only `config.framework('torch')` supported so far for QMix!\"\n",
    "            )\n",
    "\n",
    "class QMix(SimpleQ):\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_config(cls) -> AlgorithmConfig:\n",
    "        return QMixConfig()\n",
    "\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_policy_class(\n",
    "        cls, config: AlgorithmConfig\n",
    "    ) -> Optional[Type[Policy]]:\n",
    "        return QMixTorchPolicy\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def training_step(self) -> ResultDict:\n",
    "        \"\"\"QMIX training iteration function.\n",
    "        - Sample n MultiAgentBatches from n workers synchronously.\n",
    "        - Store new samples in the replay buffer.\n",
    "        - Sample one training MultiAgentBatch from the replay buffer.\n",
    "        - Learn on the training batch.\n",
    "        - Update the target network every `target_network_update_freq` sample steps.\n",
    "        - Return all collected training metrics for the iteration.\n",
    "        Returns:\n",
    "            The results dict from executing the training iteration.\n",
    "        \"\"\"\n",
    "        # Sample n batches from n workers.\n",
    "        with self._timers[SAMPLE_TIMER]:\n",
    "            new_sample_batches = synchronous_parallel_sample(\n",
    "                worker_set=self.workers, concat=False\n",
    "            )\n",
    "\n",
    "        for batch in new_sample_batches:\n",
    "            # Update counters.\n",
    "            self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n",
    "            self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n",
    "            # Store new samples in the replay buffer.\n",
    "            self.local_replay_buffer.add(batch)\n",
    "\n",
    "        # Update target network every `target_network_update_freq` sample steps.\n",
    "        cur_ts = self._counters[\n",
    "            NUM_AGENT_STEPS_SAMPLED\n",
    "            if self.config.count_steps_by == \"agent_steps\"\n",
    "            else NUM_ENV_STEPS_SAMPLED\n",
    "        ]\n",
    "\n",
    "        train_results = {}\n",
    "\n",
    "        if cur_ts > self.config.num_steps_sampled_before_learning_starts:\n",
    "            # Sample n batches from replay buffer until the total number of timesteps\n",
    "            # reaches `train_batch_size`.\n",
    "            train_batch = sample_min_n_steps_from_buffer(\n",
    "                replay_buffer=self.local_replay_buffer,\n",
    "                min_steps=self.config.train_batch_size,\n",
    "                count_by_agent_steps=self.config.count_steps_by == \"agent_steps\",\n",
    "            )\n",
    "\n",
    "            # Learn on the training batch.\n",
    "            # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
    "            # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
    "            if self.config.get(\"simple_optimizer\") is True:\n",
    "                train_results = train_one_step(self, train_batch)\n",
    "            else:\n",
    "                train_results = multi_gpu_train_one_step(self, train_batch)\n",
    "\n",
    "            # Update target network every `target_network_update_freq` sample steps.\n",
    "            last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
    "            if cur_ts - last_update >= self.config.target_network_update_freq:\n",
    "                to_update = self.workers.local_worker().get_policies_to_train()\n",
    "                self.workers.local_worker().foreach_policy_to_train(\n",
    "                    lambda p, pid: pid in to_update and p.update_target()\n",
    "                )\n",
    "                self._counters[NUM_TARGET_UPDATES] += 1\n",
    "                self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
    "\n",
    "            update_priorities_in_replay_buffer(\n",
    "                self.local_replay_buffer, self.config, train_batch, train_results\n",
    "            )\n",
    "\n",
    "            # Update weights and global_vars - after learning on the local worker -\n",
    "            # on all remote workers.\n",
    "            global_vars = {\n",
    "                \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
    "            }\n",
    "            # Update remote workers' weights and global vars after learning on local\n",
    "            # worker.\n",
    "            with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
    "                self.workers.sync_weights(global_vars=global_vars)\n",
    "\n",
    "        # Return all collected metrics for the iteration.\n",
    "        return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9020d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "#b = np.ones(3000).tolist()\n",
    "#obs_space= Tuple({MultiDiscrete([])})\n",
    "\n",
    "observation_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]), \n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]),\n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "action_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([1,3]), \n",
    "                ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,1]),\n",
    "                ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "obs_space = Tuple([\n",
    "    Dict({\n",
    "        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    \n",
    "    }\n",
    "    )\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b172d234",
   "metadata": {},
   "source": [
    "plan for visualization and causal inference\n",
    "\n",
    "we are going to sample the replay buffer and then feed it into the pysindy based algorithm and use that for network reconstruction. This should cover internal visualization \n",
    "\n",
    "for causal inference we explore using either tigramite or transfer entropy. \n",
    "\n",
    "However our current primary method we are going to explore is using BICM library with what we can determine from the \n",
    "comparative advantage paper to for causal and statisical validation \n",
    "\n",
    "paper for internal visualization: reconstructing network dynamics of coupled discrete chaotic units of data\n",
    "\n",
    "paper for causal inference on this: Inferring comparative advantage via entropy maximization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "195b9607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 00:59:30,006\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\tune.py:562: UserWarning: Consider boosting PBT performance by enabling `reuse_actors` as well as implementing `reset_config` for Trainable.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-05-19 01:06:26</td></tr>\n",
       "<tr><td>Running for: </td><td>00:06:52.04        </td></tr>\n",
       "<tr><td>Memory:      </td><td>28.4/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 361 checkpoints, 44 perturbs<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/22.39 GiB heap, 0.0/11.2 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">    alpha</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_181f9_00000</td><td>TERMINATED</td><td>127.0.0.1:49472</td><td style=\"text-align: right;\">0.285522 </td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         97.7758</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_181f9_00001</td><td>TERMINATED</td><td>127.0.0.1:42304</td><td style=\"text-align: right;\">0.673583 </td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         98.1833</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_181f9_00002</td><td>TERMINATED</td><td>127.0.0.1:51424</td><td style=\"text-align: right;\">0.0670923</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         98.9411</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    7.01</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_181f9_00003</td><td>TERMINATED</td><td>127.0.0.1:13504</td><td style=\"text-align: right;\">0.0536738</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         98.1331</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    7.01</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39228)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=39228)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=39228)\u001b[0m 2023-05-19 00:59:44,286\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=39228)\u001b[0m 2023-05-19 00:59:44,559\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=39228)\u001b[0m 2023-05-19 00:59:44,582\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=43544)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43544)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=43544)\u001b[0m 2023-05-19 00:59:53,580\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43544)\u001b[0m 2023-05-19 00:59:53,846\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43544)\u001b[0m 2023-05-19 00:59:53,861\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=35860)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=35860)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=35860)\u001b[0m 2023-05-19 01:00:02,894\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=35860)\u001b[0m 2023-05-19 01:00:03,139\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=35860)\u001b[0m 2023-05-19 01:00:03,153\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=26204)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=26204)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=26204)\u001b[0m 2023-05-19 01:00:12,528\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26204)\u001b[0m 2023-05-19 01:00:12,790\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=26204)\u001b[0m 2023-05-19 01:00:12,806\tWARNING env.py:53 -- Skipping env checking for this experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                         </th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                            </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                               </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                      </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </th><th>timers                                                                                                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_181f9_00000</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.0, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.11618924140930176}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.2975196987390518, &#x27;cur_lr&#x27;: 0.0004}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 13.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000} </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 6.9, &#x27;ram_util_percent&#x27;: 44.7, &#x27;gpu_util_percent0&#x27;: 0.02, &#x27;vram_util_percent0&#x27;: 0.7529296875} </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.5888814674284866, &#x27;mean_inference_ms&#x27;: 0.7720049749860416, &#x27;mean_action_processing_ms&#x27;: 0.11981631244275498, &#x27;mean_env_wait_ms&#x27;: 0.02624952818647952, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.5888814674284866, &#x27;mean_inference_ms&#x27;: 0.7720049749860416, &#x27;mean_action_processing_ms&#x27;: 0.11981631244275498, &#x27;mean_env_wait_ms&#x27;: 0.02624952818647952, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.0, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.11618924140930176}}                  </td><td>{&#x27;training_iteration_time_ms&#x27;: 315.134, &#x27;load_time_ms&#x27;: 0.0, &#x27;load_throughput&#x27;: 0.0, &#x27;learn_time_ms&#x27;: 10.856, &#x27;learn_throughput&#x27;: 18422.503, &#x27;synch_weights_time_ms&#x27;: 0.0}          </td></tr>\n",
       "<tr><td>PG_TwoStepGame_181f9_00001</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.014961719512939453, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.14995718002319336}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 7   </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.0989127829670906, &#x27;cur_lr&#x27;: 0.0004}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 25.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000} </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 13.3, &#x27;ram_util_percent&#x27;: 45.6, &#x27;gpu_util_percent0&#x27;: 0.03, &#x27;vram_util_percent0&#x27;: 0.7529296875}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.7343822972034779, &#x27;mean_inference_ms&#x27;: 1.0502496988486438, &#x27;mean_action_processing_ms&#x27;: 0.12939849847649113, &#x27;mean_env_wait_ms&#x27;: 0.044492243437160205, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.7343822972034779, &#x27;mean_inference_ms&#x27;: 1.0502496988486438, &#x27;mean_action_processing_ms&#x27;: 0.12939849847649113, &#x27;mean_env_wait_ms&#x27;: 0.044492243437160205, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.014961719512939453, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.14995718002319336}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 403.113, &#x27;load_time_ms&#x27;: 0.2, &#x27;load_throughput&#x27;: 1000430.292, &#x27;learn_time_ms&#x27;: 9.262, &#x27;learn_throughput&#x27;: 21592.521, &#x27;synch_weights_time_ms&#x27;: 0.0}   </td></tr>\n",
       "<tr><td>PG_TwoStepGame_181f9_00002</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.011508703231811523, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1658191680908203} </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                 7.01</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.13782462291419506, &#x27;cur_lr&#x27;: 0.0004}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 65.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 11.3, &#x27;ram_util_percent&#x27;: 46.9, &#x27;gpu_util_percent0&#x27;: 0.02, &#x27;vram_util_percent0&#x27;: 0.7529296875}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.8960258164309748, &#x27;mean_inference_ms&#x27;: 1.2982123152015246, &#x27;mean_action_processing_ms&#x27;: 0.17220990944226539, &#x27;mean_env_wait_ms&#x27;: 0.05441895941318374, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 8.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.01, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.8960258164309748, &#x27;mean_inference_ms&#x27;: 1.2982123152015246, &#x27;mean_action_processing_ms&#x27;: 0.17220990944226539, &#x27;mean_env_wait_ms&#x27;: 0.05441895941318374, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.011508703231811523, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1658191680908203}} </td><td>{&#x27;training_iteration_time_ms&#x27;: 473.852, &#x27;load_time_ms&#x27;: 0.1, &#x27;load_throughput&#x27;: 2000144.969, &#x27;learn_time_ms&#x27;: 10.404, &#x27;learn_throughput&#x27;: 19222.732, &#x27;synch_weights_time_ms&#x27;: 0.0}  </td></tr>\n",
       "<tr><td>PG_TwoStepGame_181f9_00003</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.009050130844116211, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12832903861999512}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                 7.01</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.2468101978302002, &#x27;cur_lr&#x27;: 0.0004}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 15.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000} </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{}                                                                                                                 </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6886943290562723, &#x27;mean_inference_ms&#x27;: 0.9557621543069393, &#x27;mean_action_processing_ms&#x27;: 0.1254683356967142, &#x27;mean_env_wait_ms&#x27;: 0.043469619036167556, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 8.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.01, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6886943290562723, &#x27;mean_inference_ms&#x27;: 0.9557621543069393, &#x27;mean_action_processing_ms&#x27;: 0.1254683356967142, &#x27;mean_env_wait_ms&#x27;: 0.043469619036167556, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.009050130844116211, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12832903861999512}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 380.051, &#x27;load_time_ms&#x27;: 0.125, &#x27;load_throughput&#x27;: 1602408.405, &#x27;learn_time_ms&#x27;: 13.527, &#x27;learn_throughput&#x27;: 14785.629, &#x27;synch_weights_time_ms&#x27;: 0.0}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n",
      "2023-05-19 01:00:13,339\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00000 (score = -4.610000) into trial 181f9_00001 (score = -4.760000)\n",
      "\n",
      "2023-05-19 01:00:13,340\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00001:\n",
      "alpha : 0.4499777580529346 --- (resample) --> 0.06582733674984143\n",
      "\n",
      "2023-05-19 01:00:13,543\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00000 (score = -4.610000) into trial 181f9_00003 (score = -5.040000)\n",
      "\n",
      "2023-05-19 01:00:13,544\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.4499777580529346 --- (* 1.2) --> 0.5399733096635215\n",
      "\n",
      "2023-05-19 01:00:14,651\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:15,035\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:15,227\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:00:15,853\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:16,280\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:16,663\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:17,411\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:17,791\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:18,216\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:18,613\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:18,757\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:00:19,129\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:19,680\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:19,829\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:00:20,308\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:20,903\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:21,587\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:00:22,862\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:00:23,469\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:00:23,764\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "\u001b[2m\u001b[36m(pid=48104)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=48104)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-05-19 01:00:24,460\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "\u001b[2m\u001b[36m(pid=7028)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=7028)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-05-19 01:00:24,912\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:00:25,400\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "\u001b[2m\u001b[36m(PG pid=7028)\u001b[0m 2023-05-19 01:00:25,523\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=48104)\u001b[0m 2023-05-19 01:00:25,482\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "2023-05-19 01:00:25,908\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "\u001b[2m\u001b[36m(PG pid=7028)\u001b[0m 2023-05-19 01:00:25,882\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=7028)\u001b[0m 2023-05-19 01:00:25,903\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=48104)\u001b[0m 2023-05-19 01:00:25,846\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=48104)\u001b[0m 2023-05-19 01:00:25,877\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=7028)\u001b[0m 2023-05-19 01:00:26,018\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_700de0d20df345908c668e8d896a04df\n",
      "\u001b[2m\u001b[36m(PG pid=7028)\u001b[0m 2023-05-19 01:00:26,018\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.36504650115966797, '_episodes_total': 100}\n",
      "\u001b[2m\u001b[36m(PG pid=48104)\u001b[0m 2023-05-19 01:00:25,978\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_1eff208e29224bdc84d483ebb93113b6\n",
      "\u001b[2m\u001b[36m(PG pid=48104)\u001b[0m 2023-05-19 01:00:25,978\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.36504650115966797, '_episodes_total': 100}\n",
      "2023-05-19 01:00:26,183\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:00:26,717\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -4.840000) into trial 181f9_00002 (score = -7.000000)\n",
      "\n",
      "2023-05-19 01:00:26,718\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00002:\n",
      "alpha : 0.06582733674984143 --- (* 1.2) --> 0.0789928040998097\n",
      "\n",
      "2023-05-19 01:00:26,943\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -4.840000) into trial 181f9_00000 (score = -7.040000)\n",
      "\n",
      "2023-05-19 01:00:26,944\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.06582733674984143 --- (* 1.2) --> 0.0789928040998097\n",
      "\n",
      "2023-05-19 01:00:36,859\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "\u001b[2m\u001b[36m(pid=50000)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=50000)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=47416)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47416)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=50000)\u001b[0m 2023-05-19 01:00:39,596\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47416)\u001b[0m 2023-05-19 01:00:39,621\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=50000)\u001b[0m 2023-05-19 01:00:39,909\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=50000)\u001b[0m 2023-05-19 01:00:39,927\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=50000)\u001b[0m 2023-05-19 01:00:40,009\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_7d8d467ad63f4cff92f6b3cf91cedff4\n",
      "\u001b[2m\u001b[36m(PG pid=50000)\u001b[0m 2023-05-19 01:00:40,009\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 0.8931653499603271, '_episodes_total': 200}\n",
      "\u001b[2m\u001b[36m(PG pid=47416)\u001b[0m 2023-05-19 01:00:39,932\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47416)\u001b[0m 2023-05-19 01:00:39,950\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47416)\u001b[0m 2023-05-19 01:00:40,056\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_002b012bc8c1400bbc15653c0e4c3e06\n",
      "\u001b[2m\u001b[36m(PG pid=47416)\u001b[0m 2023-05-19 01:00:40,056\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 0.8931653499603271, '_episodes_total': 200}\n",
      "2023-05-19 01:00:41,053\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -4.970000) into trial 181f9_00003 (score = -7.030000)\n",
      "\n",
      "2023-05-19 01:00:41,054\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.0789928040998097 --- (* 0.8) --> 0.06319424327984777\n",
      "\n",
      "2023-05-19 01:00:41,303\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -4.970000) into trial 181f9_00001 (score = -7.040000)\n",
      "\n",
      "2023-05-19 01:00:41,304\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00001:\n",
      "alpha : 0.0789928040998097 --- (* 1.2) --> 0.09479136491977164\n",
      "\n",
      "2023-05-19 01:00:47,443\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00000 (score = -6.790000) into trial 181f9_00002 (score = -7.080000)\n",
      "\n",
      "2023-05-19 01:00:47,445\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00002:\n",
      "alpha : 0.0789928040998097 --- (* 1.2) --> 0.09479136491977164\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=51532)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=51532)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=52168)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=52168)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=51532)\u001b[0m 2023-05-19 01:00:53,913\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=52168)\u001b[0m 2023-05-19 01:00:53,895\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=51532)\u001b[0m 2023-05-19 01:00:54,271\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=51532)\u001b[0m 2023-05-19 01:00:54,290\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=52168)\u001b[0m 2023-05-19 01:00:54,252\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=52168)\u001b[0m 2023-05-19 01:00:54,277\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=51532)\u001b[0m 2023-05-19 01:00:54,390\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f06d94cf48f9435bb1ff9c0c4b55759c\n",
      "\u001b[2m\u001b[36m(PG pid=51532)\u001b[0m 2023-05-19 01:00:54,390\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 1.6263978481292725, '_episodes_total': 300}\n",
      "\u001b[2m\u001b[36m(PG pid=52168)\u001b[0m 2023-05-19 01:00:54,393\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_9a13d45ada264ca5911c936bb3dcdc3b\n",
      "\u001b[2m\u001b[36m(PG pid=52168)\u001b[0m 2023-05-19 01:00:54,393\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 1.6263978481292725, '_episodes_total': 300}\n",
      "\u001b[2m\u001b[36m(pid=47924)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47924)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=47924)\u001b[0m 2023-05-19 01:01:00,029\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47924)\u001b[0m 2023-05-19 01:01:00,383\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47924)\u001b[0m 2023-05-19 01:01:00,399\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47924)\u001b[0m 2023-05-19 01:01:00,482\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3ae0a00e2a1f43329070b2c8da3d2ff5\n",
      "\u001b[2m\u001b[36m(PG pid=47924)\u001b[0m 2023-05-19 01:01:00,482\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 13, '_timesteps_total': None, '_time_total': 6.379026651382446, '_episodes_total': 1300}\n",
      "2023-05-19 01:01:00,987\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00003 (score = -6.730000) into trial 181f9_00002 (score = -6.980000)\n",
      "\n",
      "2023-05-19 01:01:00,988\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00002:\n",
      "alpha : 0.06319424327984777 --- (* 0.8) --> 0.050555394623878216\n",
      "\n",
      "2023-05-19 01:01:01,692\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00003 (score = -6.860000) into trial 181f9_00000 (score = -7.010000)\n",
      "\n",
      "2023-05-19 01:01:01,693\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.06319424327984777 --- (resample) --> 0.05536418666933396\n",
      "\n",
      "2023-05-19 01:01:04,125\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:06,340\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:06,822\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:07,339\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:07,864\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:08,434\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:08,980\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:09,567\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:10,097\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:10,641\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:11,166\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:12,307\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:12,772\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "\u001b[2m\u001b[36m(pid=47448)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47448)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=39940)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=39940)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=46088)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46088)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-05-19 01:01:13,686\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "\u001b[2m\u001b[36m(PG pid=47448)\u001b[0m 2023-05-19 01:01:14,107\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=39940)\u001b[0m 2023-05-19 01:01:14,126\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46088)\u001b[0m 2023-05-19 01:01:14,164\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "2023-05-19 01:01:14,193\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "\u001b[2m\u001b[36m(PG pid=47448)\u001b[0m 2023-05-19 01:01:14,456\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47448)\u001b[0m 2023-05-19 01:01:14,479\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=39940)\u001b[0m 2023-05-19 01:01:14,475\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=39940)\u001b[0m 2023-05-19 01:01:14,493\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47448)\u001b[0m 2023-05-19 01:01:14,594\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_04be5fb0138c4c50a4bb7dd5837efbdd\n",
      "\u001b[2m\u001b[36m(PG pid=47448)\u001b[0m 2023-05-19 01:01:14,594\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 13, '_timesteps_total': None, '_time_total': 6.415512561798096, '_episodes_total': 1300}\n",
      "\u001b[2m\u001b[36m(PG pid=39940)\u001b[0m 2023-05-19 01:01:14,620\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_782a14157c0047b9bafd5521e9aaee14\n",
      "\u001b[2m\u001b[36m(PG pid=39940)\u001b[0m 2023-05-19 01:01:14,620\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 15, '_timesteps_total': None, '_time_total': 7.316965341567993, '_episodes_total': 1500}\n",
      "\u001b[2m\u001b[36m(PG pid=46088)\u001b[0m 2023-05-19 01:01:14,519\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46088)\u001b[0m 2023-05-19 01:01:14,539\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=46088)\u001b[0m 2023-05-19 01:01:14,657\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f91dc3201f4a4e86802214822e068f60\n",
      "\u001b[2m\u001b[36m(PG pid=46088)\u001b[0m 2023-05-19 01:01:14,657\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 14, '_timesteps_total': None, '_time_total': 6.858299970626831, '_episodes_total': 1400}\n",
      "2023-05-19 01:01:14,790\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:15,676\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -6.860000) into trial 181f9_00003 (score = -7.050000)\n",
      "\n",
      "2023-05-19 01:01:15,677\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.09479136491977164 --- (* 1.2) --> 0.11374963790372596\n",
      "\n",
      "2023-05-19 01:01:22,395\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:01:22,886\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:01:23,563\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:01:24,573\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:01:24,640\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "\u001b[2m\u001b[36m(pid=46448)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46448)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=45012)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45012)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-05-19 01:01:27,978\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "\u001b[2m\u001b[36m(PG pid=46448)\u001b[0m 2023-05-19 01:01:28,887\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-05-19 01:01:28,910\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46448)\u001b[0m 2023-05-19 01:01:29,225\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46448)\u001b[0m 2023-05-19 01:01:29,243\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-05-19 01:01:29,244\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-05-19 01:01:29,264\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=46448)\u001b[0m 2023-05-19 01:01:29,346\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3532d720112241df990c7237649dd161\n",
      "\u001b[2m\u001b[36m(PG pid=46448)\u001b[0m 2023-05-19 01:01:29,347\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 14, '_timesteps_total': None, '_time_total': 7.012966156005859, '_episodes_total': 1400}\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-05-19 01:01:29,364\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_54d296e4fd4e497b97b9b0dbcef1e965\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-05-19 01:01:29,365\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 17, '_timesteps_total': None, '_time_total': 8.282976865768433, '_episodes_total': 1700}\n",
      "2023-05-19 01:01:30,158\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00003 (score = -6.830000) into trial 181f9_00001 (score = -7.020000)\n",
      "\n",
      "2023-05-19 01:01:30,159\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00001:\n",
      "alpha : 0.11374963790372596 --- (* 0.8) --> 0.09099971032298076\n",
      "\n",
      "2023-05-19 01:01:30,622\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:01:31,773\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:01:31,793\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:01:32,287\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00000 (score = -6.890000) into trial 181f9_00003 (score = -7.090000)\n",
      "\n",
      "2023-05-19 01:01:32,288\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.05536418666933396 --- (* 0.8) --> 0.04429134933546717\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=46544)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46544)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=46544)\u001b[0m 2023-05-19 01:01:42,741\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46544)\u001b[0m 2023-05-19 01:01:43,064\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46544)\u001b[0m 2023-05-19 01:01:43,081\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=46544)\u001b[0m 2023-05-19 01:01:43,169\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_63dc929ea54c4ee48edfbd58f036af0e\n",
      "\u001b[2m\u001b[36m(PG pid=46544)\u001b[0m 2023-05-19 01:01:43,169\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 15, '_timesteps_total': None, '_time_total': 7.566635847091675, '_episodes_total': 1500}\n",
      "\u001b[2m\u001b[36m(pid=26880)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=26880)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=48640)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=48640)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=26880)\u001b[0m 2023-05-19 01:01:44,989\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=48640)\u001b[0m 2023-05-19 01:01:45,010\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=26880)\u001b[0m 2023-05-19 01:01:45,331\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=26880)\u001b[0m 2023-05-19 01:01:45,349\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=48640)\u001b[0m 2023-05-19 01:01:45,351\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=48640)\u001b[0m 2023-05-19 01:01:45,369\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=26880)\u001b[0m 2023-05-19 01:01:45,442\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_1998f84ba7e74e22afa9c95150a2b22d\n",
      "\u001b[2m\u001b[36m(PG pid=26880)\u001b[0m 2023-05-19 01:01:45,442\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 43, '_timesteps_total': None, '_time_total': 21.133371829986572, '_episodes_total': 4300}\n",
      "\u001b[2m\u001b[36m(PG pid=48640)\u001b[0m 2023-05-19 01:01:45,460\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_9bfd5b7481b440aabbfaa4706b95a415\n",
      "\u001b[2m\u001b[36m(PG pid=48640)\u001b[0m 2023-05-19 01:01:45,460\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 21, '_timesteps_total': None, '_time_total': 10.297337055206299, '_episodes_total': 2100}\n",
      "2023-05-19 01:01:46,085\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -6.940000) into trial 181f9_00000 (score = -7.030000)\n",
      "\n",
      "2023-05-19 01:01:46,086\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.09099971032298076 --- (* 1.2) --> 0.10919965238757691\n",
      "\n",
      "2023-05-19 01:01:46,790\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -6.840000) into trial 181f9_00002 (score = -7.090000)\n",
      "\n",
      "2023-05-19 01:01:46,792\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00002:\n",
      "alpha : 0.09099971032298076 --- (* 1.2) --> 0.10919965238757691\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=11608)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=11608)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=45684)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45684)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=11608)\u001b[0m 2023-05-19 01:01:58,942\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=11608)\u001b[0m 2023-05-19 01:01:59,302\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=11608)\u001b[0m 2023-05-19 01:01:59,318\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=11608)\u001b[0m 2023-05-19 01:01:59,405\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_93510f20d2b544b38d50fd0b6cc9b9cc\n",
      "\u001b[2m\u001b[36m(PG pid=11608)\u001b[0m 2023-05-19 01:01:59,405\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 19, '_timesteps_total': None, '_time_total': 9.518532276153564, '_episodes_total': 1900}\n",
      "\u001b[2m\u001b[36m(PG pid=45684)\u001b[0m 2023-05-19 01:01:59,694\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45684)\u001b[0m 2023-05-19 01:02:00,102\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45684)\u001b[0m 2023-05-19 01:02:00,124\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=45684)\u001b[0m 2023-05-19 01:02:00,252\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b06463d864f1407eb00d0819fe286146\n",
      "\u001b[2m\u001b[36m(PG pid=45684)\u001b[0m 2023-05-19 01:02:00,252\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 10.022031307220459, '_episodes_total': 2000}\n",
      "2023-05-19 01:02:00,766\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:02,000\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00000 (score = -6.750000) into trial 181f9_00003 (score = -7.000000)\n",
      "\n",
      "2023-05-19 01:02:02,001\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.10919965238757691 --- (resample) --> 0.599012354406006\n",
      "\n",
      "2023-05-19 01:02:02,459\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:03,633\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:04,157\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:05,407\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:02:05,920\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:02:07,567\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:07,743\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:02:08,276\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:02:09,262\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:02:10,089\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:10,358\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:02:10,591\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:11,060\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:11,523\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:11,975\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:02:12,774\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:02:13,682\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -6.900000) into trial 181f9_00000 (score = -7.040000)\n",
      "\n",
      "2023-05-19 01:02:13,683\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.09099971032298076 --- (* 0.8) --> 0.07279976825838462\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=45140)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45140)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=47728)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47728)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-05-19 01:02:14,074\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "\u001b[2m\u001b[36m(PG pid=45140)\u001b[0m 2023-05-19 01:02:15,084\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47728)\u001b[0m 2023-05-19 01:02:15,099\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45140)\u001b[0m 2023-05-19 01:02:15,477\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45140)\u001b[0m 2023-05-19 01:02:15,501\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-05-19 01:02:15,518\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "\u001b[2m\u001b[36m(PG pid=45140)\u001b[0m 2023-05-19 01:02:15,609\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_e13913b948d744d3aacf02c6a642807c\n",
      "\u001b[2m\u001b[36m(PG pid=45140)\u001b[0m 2023-05-19 01:02:15,609\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 11.12543272972107, '_episodes_total': 2200}\n",
      "\u001b[2m\u001b[36m(PG pid=47728)\u001b[0m 2023-05-19 01:02:15,518\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47728)\u001b[0m 2023-05-19 01:02:15,542\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47728)\u001b[0m 2023-05-19 01:02:15,654\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_90ce4a8330ad4999bc8cd338c8ad26ba\n",
      "\u001b[2m\u001b[36m(PG pid=47728)\u001b[0m 2023-05-19 01:02:15,654\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 23, '_timesteps_total': None, '_time_total': 11.670880556106567, '_episodes_total': 2300}\n",
      "2023-05-19 01:02:16,109\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:02:16,734\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -6.810000) into trial 181f9_00001 (score = -7.050000)\n",
      "\n",
      "2023-05-19 01:02:16,736\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00001:\n",
      "alpha : 0.10919965238757691 --- (* 0.8) --> 0.08735972191006153\n",
      "\n",
      "2023-05-19 01:02:17,779\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:02:18,275\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00003 (score = -7.040000) into trial 181f9_00002 (score = -7.080000)\n",
      "\n",
      "2023-05-19 01:02:18,276\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00002:\n",
      "alpha : 0.599012354406006 --- (* 0.8) --> 0.4792098835248048\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=42932)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=42932)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=42932)\u001b[0m 2023-05-19 01:02:27,159\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=42932)\u001b[0m 2023-05-19 01:02:27,488\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=42932)\u001b[0m 2023-05-19 01:02:27,513\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=42932)\u001b[0m 2023-05-19 01:02:27,601\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_6dade769c4f246fe8ec8390bbfc9687f\n",
      "\u001b[2m\u001b[36m(PG pid=42932)\u001b[0m 2023-05-19 01:02:27,601\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 64, '_timesteps_total': None, '_time_total': 31.125874280929565, '_episodes_total': 6400}\n",
      "\u001b[2m\u001b[36m(pid=21056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=21056)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-05-19 01:02:29,294\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "\u001b[2m\u001b[36m(PG pid=21056)\u001b[0m 2023-05-19 01:02:30,375\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=21056)\u001b[0m 2023-05-19 01:02:30,750\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=21056)\u001b[0m 2023-05-19 01:02:30,767\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=10752)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=10752)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=21056)\u001b[0m 2023-05-19 01:02:30,856\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_8b305872e67e4b1eb20b51be85afff1e\n",
      "\u001b[2m\u001b[36m(PG pid=21056)\u001b[0m 2023-05-19 01:02:30,856\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 24, '_timesteps_total': None, '_time_total': 12.267016410827637, '_episodes_total': 2400}\n",
      "\u001b[2m\u001b[36m(PG pid=10752)\u001b[0m 2023-05-19 01:02:31,897\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=10752)\u001b[0m 2023-05-19 01:02:32,246\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=10752)\u001b[0m 2023-05-19 01:02:32,269\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=10752)\u001b[0m 2023-05-19 01:02:32,377\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_5e039118f06f41368390569aae152293\n",
      "\u001b[2m\u001b[36m(PG pid=10752)\u001b[0m 2023-05-19 01:02:32,377\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 26, '_timesteps_total': None, '_time_total': 13.250373840332031, '_episodes_total': 2600}\n",
      "2023-05-19 01:02:33,659\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -6.890000) into trial 181f9_00003 (score = -7.020000)\n",
      "\n",
      "2023-05-19 01:02:33,660\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.4792098835248048 --- (* 0.8) --> 0.3833679068198439\n",
      "\n",
      "2023-05-19 01:02:33,917\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -6.890000) into trial 181f9_00000 (score = -7.030000)\n",
      "\n",
      "2023-05-19 01:02:33,918\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.4792098835248048 --- (* 0.8) --> 0.3833679068198439\n",
      "\n",
      "2023-05-19 01:02:35,165\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:02:36,107\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:02:38,364\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -6.910000) into trial 181f9_00002 (score = -7.050000)\n",
      "\n",
      "2023-05-19 01:02:38,365\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00002:\n",
      "alpha : 0.08735972191006153 --- (* 1.2) --> 0.10483166629207383\n",
      "\n",
      "2023-05-19 01:02:40,643\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "\u001b[2m\u001b[36m(pid=36900)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=36900)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=3200)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=3200)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=3200)\u001b[0m 2023-05-19 01:02:46,737\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=36900)\u001b[0m 2023-05-19 01:02:46,786\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "2023-05-19 01:02:47,053\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "\u001b[2m\u001b[36m(PG pid=36900)\u001b[0m 2023-05-19 01:02:47,124\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=36900)\u001b[0m 2023-05-19 01:02:47,145\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=3200)\u001b[0m 2023-05-19 01:02:47,074\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=3200)\u001b[0m 2023-05-19 01:02:47,098\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=36900)\u001b[0m 2023-05-19 01:02:47,235\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_75595048c51443d9878a4925c36a9e66\n",
      "\u001b[2m\u001b[36m(PG pid=36900)\u001b[0m 2023-05-19 01:02:47,235\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 14.19337511062622, '_episodes_total': 2800}\n",
      "\u001b[2m\u001b[36m(PG pid=3200)\u001b[0m 2023-05-19 01:02:47,188\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_8ae0358436ef4c45bc0be3ee6ceec579\n",
      "\u001b[2m\u001b[36m(PG pid=3200)\u001b[0m 2023-05-19 01:02:47,188\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 14.19337511062622, '_episodes_total': 2800}\n",
      "2023-05-19 01:02:49,475\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00000 (score = -6.830000) into trial 181f9_00003 (score = -7.060000)\n",
      "\n",
      "2023-05-19 01:02:49,477\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.3833679068198439 --- (* 0.8) --> 0.30669432545587516\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=49440)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=49440)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-05-19 01:02:50,816\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "\u001b[2m\u001b[36m(PG pid=49440)\u001b[0m 2023-05-19 01:02:50,968\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=49440)\u001b[0m 2023-05-19 01:02:51,373\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=49440)\u001b[0m 2023-05-19 01:02:51,391\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=49440)\u001b[0m 2023-05-19 01:02:51,484\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_6370ac07fbd344b39504bffbae79dab4\n",
      "\u001b[2m\u001b[36m(PG pid=49440)\u001b[0m 2023-05-19 01:02:51,484\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 34, '_timesteps_total': None, '_time_total': 16.93993854522705, '_episodes_total': 3400}\n",
      "2023-05-19 01:02:52,168\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -6.850000) into trial 181f9_00000 (score = -7.080000)\n",
      "\n",
      "2023-05-19 01:02:52,169\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.10483166629207383 --- (* 1.2) --> 0.1257979995504886\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=50684)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=50684)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=19008)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19008)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=50684)\u001b[0m 2023-05-19 01:03:02,661\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=19008)\u001b[0m 2023-05-19 01:03:02,745\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=50684)\u001b[0m 2023-05-19 01:03:03,031\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=50684)\u001b[0m 2023-05-19 01:03:03,057\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=19008)\u001b[0m 2023-05-19 01:03:03,111\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=19008)\u001b[0m 2023-05-19 01:03:03,128\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=50684)\u001b[0m 2023-05-19 01:03:03,143\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_948701ec84564e24b97e4fd6e41b05b0\n",
      "\u001b[2m\u001b[36m(PG pid=50684)\u001b[0m 2023-05-19 01:03:03,143\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 52, '_timesteps_total': None, '_time_total': 25.581679105758667, '_episodes_total': 5200}\n",
      "\u001b[2m\u001b[36m(PG pid=19008)\u001b[0m 2023-05-19 01:03:03,239\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_efcdb24e4afa453ea47a9ea69409b417\n",
      "\u001b[2m\u001b[36m(PG pid=19008)\u001b[0m 2023-05-19 01:03:03,239\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 31, '_timesteps_total': None, '_time_total': 15.573407173156738, '_episodes_total': 3100}\n",
      "\u001b[2m\u001b[36m(pid=6008)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=6008)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=6008)\u001b[0m 2023-05-19 01:03:04,851\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=6008)\u001b[0m 2023-05-19 01:03:05,165\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=6008)\u001b[0m 2023-05-19 01:03:05,180\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=6008)\u001b[0m 2023-05-19 01:03:05,267\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3043c0a7627f46969ae2a141253a7098\n",
      "\u001b[2m\u001b[36m(PG pid=6008)\u001b[0m 2023-05-19 01:03:05,267\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 35, '_timesteps_total': None, '_time_total': 17.393590688705444, '_episodes_total': 3500}\n",
      "2023-05-19 01:03:05,912\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -7.000000) into trial 181f9_00000 (score = -7.060000)\n",
      "\n",
      "2023-05-19 01:03:05,913\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.08735972191006153 --- (* 1.2) --> 0.10483166629207383\n",
      "\n",
      "2023-05-19 01:03:14,356\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:03:14,838\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:03:17,413\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -6.940000) into trial 181f9_00003 (score = -7.090000)\n",
      "\n",
      "2023-05-19 01:03:17,414\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.08735972191006153 --- (* 0.8) --> 0.06988777752804923\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=44224)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=44224)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=15284)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=15284)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=44224)\u001b[0m 2023-05-19 01:03:19,003\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=15284)\u001b[0m 2023-05-19 01:03:18,967\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=44224)\u001b[0m 2023-05-19 01:03:19,375\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=44224)\u001b[0m 2023-05-19 01:03:19,396\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=15284)\u001b[0m 2023-05-19 01:03:19,348\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=15284)\u001b[0m 2023-05-19 01:03:19,382\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=44224)\u001b[0m 2023-05-19 01:03:19,505\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3f44a3e00d5846f1a7d354a921f390ac\n",
      "\u001b[2m\u001b[36m(PG pid=44224)\u001b[0m 2023-05-19 01:03:19,505\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 57, '_timesteps_total': None, '_time_total': 27.68792700767517, '_episodes_total': 5700}\n",
      "\u001b[2m\u001b[36m(PG pid=15284)\u001b[0m 2023-05-19 01:03:19,474\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_127dfa4030d74fdebb6c8b235bcd218e\n",
      "\u001b[2m\u001b[36m(PG pid=15284)\u001b[0m 2023-05-19 01:03:19,474\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 56, '_timesteps_total': None, '_time_total': 27.568398475646973, '_episodes_total': 5600}\n",
      "\u001b[2m\u001b[36m(pid=46604)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46604)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=46604)\u001b[0m 2023-05-19 01:03:30,523\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46604)\u001b[0m 2023-05-19 01:03:30,847\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46604)\u001b[0m 2023-05-19 01:03:30,869\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=46604)\u001b[0m 2023-05-19 01:03:30,949\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_0091352baec7443599cd4a6b9cc4393a\n",
      "\u001b[2m\u001b[36m(PG pid=46604)\u001b[0m 2023-05-19 01:03:30,949\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 75, '_timesteps_total': None, '_time_total': 36.31151223182678, '_episodes_total': 7500}\n",
      "2023-05-19 01:03:31,924\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:03:32,464\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00000 (score = -6.960000) into trial 181f9_00002 (score = -7.010000)\n",
      "\n",
      "2023-05-19 01:03:32,465\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00002:\n",
      "alpha : 0.10483166629207383 --- (* 1.2) --> 0.1257979995504886\n",
      "\n",
      "2023-05-19 01:03:32,745\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00000 (score = -6.960000) into trial 181f9_00001 (score = -7.010000)\n",
      "\n",
      "2023-05-19 01:03:32,747\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00001:\n",
      "alpha : 0.10483166629207383 --- (resample) --> 0.09810782317565658\n",
      "\n",
      "2023-05-19 01:03:33,878\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00000\n",
      "2023-05-19 01:03:34,292\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:03:35,312\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00003 (score = -6.960000) into trial 181f9_00000 (score = -7.040000)\n",
      "\n",
      "2023-05-19 01:03:35,312\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.06988777752804923 --- (* 1.2) --> 0.08386533303365908\n",
      "\n",
      "2023-05-19 01:03:41,478\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:03:43,916\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "\u001b[2m\u001b[36m(pid=41016)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=41016)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=47496)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47496)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=41016)\u001b[0m 2023-05-19 01:03:45,462\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47496)\u001b[0m 2023-05-19 01:03:45,458\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=41016)\u001b[0m 2023-05-19 01:03:45,799\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=41016)\u001b[0m 2023-05-19 01:03:45,820\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47496)\u001b[0m 2023-05-19 01:03:45,793\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47496)\u001b[0m 2023-05-19 01:03:45,823\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=41016)\u001b[0m 2023-05-19 01:03:45,935\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_be175a26504843f4b1c1c8a977f572d2\n",
      "\u001b[2m\u001b[36m(PG pid=41016)\u001b[0m 2023-05-19 01:03:45,935\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 75, '_timesteps_total': None, '_time_total': 36.923712968826294, '_episodes_total': 7500}\n",
      "\u001b[2m\u001b[36m(PG pid=47496)\u001b[0m 2023-05-19 01:03:45,905\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3e60f8f181644ff1ba9dd0bee3816e8d\n",
      "\u001b[2m\u001b[36m(PG pid=47496)\u001b[0m 2023-05-19 01:03:45,905\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 75, '_timesteps_total': None, '_time_total': 36.923712968826294, '_episodes_total': 7500}\n",
      "2023-05-19 01:03:46,883\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -6.980000) into trial 181f9_00003 (score = -7.050000)\n",
      "\n",
      "2023-05-19 01:03:46,883\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.1257979995504886 --- (* 0.8) --> 0.10063839964039088\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20264)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=20264)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-05-19 01:03:47,150\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "\u001b[2m\u001b[36m(PG pid=20264)\u001b[0m 2023-05-19 01:03:48,153\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "2023-05-19 01:03:48,639\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -7.020000) into trial 181f9_00001 (score = -7.060000)\n",
      "\n",
      "2023-05-19 01:03:48,640\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00001:\n",
      "alpha : 0.1257979995504886 --- (* 1.2) --> 0.15095759946058632\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=20264)\u001b[0m 2023-05-19 01:03:48,583\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=20264)\u001b[0m 2023-05-19 01:03:48,602\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=20264)\u001b[0m 2023-05-19 01:03:48,706\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_5dd1c2958063451790dd6cc6ce436f95\n",
      "\u001b[2m\u001b[36m(PG pid=20264)\u001b[0m 2023-05-19 01:03:48,706\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 81, '_timesteps_total': None, '_time_total': 39.67346262931824, '_episodes_total': 8100}\n",
      "2023-05-19 01:03:49,289\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00002\n",
      "2023-05-19 01:03:51,205\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00000 (score = -7.000000) into trial 181f9_00002 (score = -7.090000)\n",
      "\n",
      "2023-05-19 01:03:51,205\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00002:\n",
      "alpha : 0.08386533303365908 --- (* 0.8) --> 0.06709226642692727\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=50892)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=50892)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=50892)\u001b[0m 2023-05-19 01:03:59,870\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=50892)\u001b[0m 2023-05-19 01:04:00,184\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=50892)\u001b[0m 2023-05-19 01:04:00,200\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=50892)\u001b[0m 2023-05-19 01:04:00,289\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_350811632d2a4aff95bf77940623329c\n",
      "\u001b[2m\u001b[36m(PG pid=50892)\u001b[0m 2023-05-19 01:04:00,289\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 76, '_timesteps_total': None, '_time_total': 37.421932220458984, '_episodes_total': 7600}\n",
      "\u001b[2m\u001b[36m(pid=48396)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=48396)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=48396)\u001b[0m 2023-05-19 01:04:01,580\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=48396)\u001b[0m 2023-05-19 01:04:01,901\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=48396)\u001b[0m 2023-05-19 01:04:01,920\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=48396)\u001b[0m 2023-05-19 01:04:02,008\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f1ee7d9e44cd468dbc97a992b9f6bc48\n",
      "\u001b[2m\u001b[36m(PG pid=48396)\u001b[0m 2023-05-19 01:04:02,008\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 79, '_timesteps_total': None, '_time_total': 38.90004777908325, '_episodes_total': 7900}\n",
      "\u001b[2m\u001b[36m(pid=22156)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=22156)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=22156)\u001b[0m 2023-05-19 01:04:03,575\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=22156)\u001b[0m 2023-05-19 01:04:03,972\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=22156)\u001b[0m 2023-05-19 01:04:04,007\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=22156)\u001b[0m 2023-05-19 01:04:04,202\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ea0b5f123dd646ec976dc9277d7195b4\n",
      "\u001b[2m\u001b[36m(PG pid=22156)\u001b[0m 2023-05-19 01:04:04,202\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 84, '_timesteps_total': None, '_time_total': 41.46117186546326, '_episodes_total': 8400}\n",
      "2023-05-19 01:04:05,347\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -7.020000) into trial 181f9_00001 (score = -7.060000)\n",
      "\n",
      "2023-05-19 01:04:05,347\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00001:\n",
      "alpha : 0.06709226642692727 --- (* 1.2) --> 0.08051071971231272\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=40552)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=40552)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=37068)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=37068)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=40552)\u001b[0m 2023-05-19 01:04:18,394\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=37068)\u001b[0m 2023-05-19 01:04:18,403\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=40552)\u001b[0m 2023-05-19 01:04:18,736\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=40552)\u001b[0m 2023-05-19 01:04:18,760\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=37068)\u001b[0m 2023-05-19 01:04:18,751\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=37068)\u001b[0m 2023-05-19 01:04:18,766\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=40552)\u001b[0m 2023-05-19 01:04:18,860\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_0c2b67ad34e74bc2b69adbc5a8c3e256\n",
      "\u001b[2m\u001b[36m(PG pid=40552)\u001b[0m 2023-05-19 01:04:18,860\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 85, '_timesteps_total': None, '_time_total': 42.18604326248169, '_episodes_total': 8500}\n",
      "\u001b[2m\u001b[36m(PG pid=37068)\u001b[0m 2023-05-19 01:04:18,852\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_8eea85fcb5444056bf55cb555f028db9\n",
      "\u001b[2m\u001b[36m(PG pid=37068)\u001b[0m 2023-05-19 01:04:18,852\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 84, '_timesteps_total': None, '_time_total': 41.78028845787048, '_episodes_total': 8400}\n",
      "2023-05-19 01:04:19,983\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -6.950000) into trial 181f9_00003 (score = -7.080000)\n",
      "\n",
      "2023-05-19 01:04:19,984\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.08051071971231272 --- (* 1.2) --> 0.09661286365477526\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=42900)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=42900)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=42900)\u001b[0m 2023-05-19 01:04:33,700\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=42900)\u001b[0m 2023-05-19 01:04:34,104\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=42900)\u001b[0m 2023-05-19 01:04:34,124\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=42900)\u001b[0m 2023-05-19 01:04:34,256\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_0a97cdbadffb4efcb062119832bf5146\n",
      "\u001b[2m\u001b[36m(PG pid=42900)\u001b[0m 2023-05-19 01:04:34,256\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 86, '_timesteps_total': None, '_time_total': 42.74971914291382, '_episodes_total': 8600}\n",
      "2023-05-19 01:04:35,389\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -7.010000) into trial 181f9_00000 (score = -7.040000)\n",
      "\n",
      "2023-05-19 01:04:35,390\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.06709226642692727 --- (* 0.8) --> 0.05367381314154182\n",
      "\n",
      "2023-05-19 01:04:38,891\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -7.010000) into trial 181f9_00001 (score = -7.080000)\n",
      "\n",
      "2023-05-19 01:04:38,892\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00001:\n",
      "alpha : 0.06709226642692727 --- (resample) --> 0.5043367026496751\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25528)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=25528)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=52172)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=52172)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=25528)\u001b[0m 2023-05-19 01:04:48,330\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=52172)\u001b[0m 2023-05-19 01:04:48,329\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=25528)\u001b[0m 2023-05-19 01:04:48,624\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=25528)\u001b[0m 2023-05-19 01:04:48,643\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=52172)\u001b[0m 2023-05-19 01:04:48,625\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=52172)\u001b[0m 2023-05-19 01:04:48,641\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=25528)\u001b[0m 2023-05-19 01:04:48,752\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f53713ed06444bd0af324e8004959625\n",
      "\u001b[2m\u001b[36m(PG pid=25528)\u001b[0m 2023-05-19 01:04:48,752\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 136, '_timesteps_total': None, '_time_total': 67.31926274299622, '_episodes_total': 13600}\n",
      "\u001b[2m\u001b[36m(PG pid=52172)\u001b[0m 2023-05-19 01:04:48,739\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_02abb1f9246d4dddab7fa4bd16385f25\n",
      "\u001b[2m\u001b[36m(PG pid=52172)\u001b[0m 2023-05-19 01:04:48,739\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 135, '_timesteps_total': None, '_time_total': 66.78749084472656, '_episodes_total': 13500}\n",
      "\u001b[2m\u001b[36m(pid=50776)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=50776)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=17152)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=17152)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=50776)\u001b[0m 2023-05-19 01:04:51,437\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=17152)\u001b[0m 2023-05-19 01:04:51,522\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=50776)\u001b[0m 2023-05-19 01:04:51,740\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=50776)\u001b[0m 2023-05-19 01:04:51,763\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=50776)\u001b[0m 2023-05-19 01:04:51,852\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_edb1c4f0a1e64932b1f407bf906a504f\n",
      "\u001b[2m\u001b[36m(PG pid=50776)\u001b[0m 2023-05-19 01:04:51,852\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 92, '_timesteps_total': None, '_time_total': 46.05935764312744, '_episodes_total': 9200}\n",
      "\u001b[2m\u001b[36m(PG pid=17152)\u001b[0m 2023-05-19 01:04:51,845\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=17152)\u001b[0m 2023-05-19 01:04:51,867\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=17152)\u001b[0m 2023-05-19 01:04:51,959\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_32ebaecf8655442c9f08c58de14e6f8c\n",
      "\u001b[2m\u001b[36m(PG pid=17152)\u001b[0m 2023-05-19 01:04:51,959\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 136, '_timesteps_total': None, '_time_total': 67.31926274299622, '_episodes_total': 13600}\n",
      "2023-05-19 01:04:53,384\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -7.000000) into trial 181f9_00000 (score = -7.050000)\n",
      "\n",
      "2023-05-19 01:04:53,385\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.06709226642692727 --- (* 0.8) --> 0.05367381314154182\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=50080)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=50080)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=7984)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=7984)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=7984)\u001b[0m 2023-05-19 01:05:06,143\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=50080)\u001b[0m 2023-05-19 01:05:06,108\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=7984)\u001b[0m 2023-05-19 01:05:06,509\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=7984)\u001b[0m 2023-05-19 01:05:06,531\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=50080)\u001b[0m 2023-05-19 01:05:06,461\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=50080)\u001b[0m 2023-05-19 01:05:06,497\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=7984)\u001b[0m 2023-05-19 01:05:06,645\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_4ae7078b72ca4bef84f33021f32beeee\n",
      "\u001b[2m\u001b[36m(PG pid=7984)\u001b[0m 2023-05-19 01:05:06,645\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 144, '_timesteps_total': None, '_time_total': 71.36436176300049, '_episodes_total': 14400}\n",
      "\u001b[2m\u001b[36m(PG pid=50080)\u001b[0m 2023-05-19 01:05:06,604\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_d7e384caec6c4a4cb126b1babd4f55c7\n",
      "\u001b[2m\u001b[36m(PG pid=50080)\u001b[0m 2023-05-19 01:05:06,605\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 138, '_timesteps_total': None, '_time_total': 68.60140490531921, '_episodes_total': 13800}\n",
      "2023-05-19 01:05:07,215\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00003 (score = -6.990000) into trial 181f9_00000 (score = -7.020000)\n",
      "\n",
      "2023-05-19 01:05:07,217\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.09661286365477526 --- (* 1.2) --> 0.1159354363857303\n",
      "\n",
      "2023-05-19 01:05:07,758\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:05:09,076\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "2023-05-19 01:05:10,016\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -7.010000) into trial 181f9_00003 (score = -7.030000)\n",
      "\n",
      "2023-05-19 01:05:10,018\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.5043367026496751 --- (* 0.8) --> 0.4034693621197401\n",
      "\n",
      "2023-05-19 01:05:12,686\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00001\n",
      "\u001b[2m\u001b[36m(pid=51424)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=51424)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=49260)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=49260)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=51424)\u001b[0m 2023-05-19 01:05:20,200\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=49260)\u001b[0m 2023-05-19 01:05:20,206\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=51424)\u001b[0m 2023-05-19 01:05:20,565\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=51424)\u001b[0m 2023-05-19 01:05:20,585\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=49260)\u001b[0m 2023-05-19 01:05:20,571\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=49260)\u001b[0m 2023-05-19 01:05:20,591\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=51424)\u001b[0m 2023-05-19 01:05:20,682\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_87fd0c5cd83e4c62b38719c4a86f8634\n",
      "\u001b[2m\u001b[36m(PG pid=51424)\u001b[0m 2023-05-19 01:05:20,682\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 167, '_timesteps_total': None, '_time_total': 82.38725996017456, '_episodes_total': 16700}\n",
      "\u001b[2m\u001b[36m(PG pid=49260)\u001b[0m 2023-05-19 01:05:20,707\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_275baa82f9744a01b4e44724c6d8b675\n",
      "\u001b[2m\u001b[36m(PG pid=49260)\u001b[0m 2023-05-19 01:05:20,707\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 115, '_timesteps_total': None, '_time_total': 57.210004568099976, '_episodes_total': 11500}\n",
      "\u001b[2m\u001b[36m(pid=45124)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45124)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=45124)\u001b[0m 2023-05-19 01:05:22,819\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "2023-05-19 01:05:23,073\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -7.000000) into trial 181f9_00000 (score = -7.030000)\n",
      "\n",
      "2023-05-19 01:05:23,074\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.5043367026496751 --- (* 0.8) --> 0.4034693621197401\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=45124)\u001b[0m 2023-05-19 01:05:23,463\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45124)\u001b[0m 2023-05-19 01:05:23,480\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=45124)\u001b[0m 2023-05-19 01:05:23,606\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3280d1186a8f4599940f59e948f747c0\n",
      "\u001b[2m\u001b[36m(PG pid=45124)\u001b[0m 2023-05-19 01:05:23,606\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 143, '_timesteps_total': None, '_time_total': 71.40462684631348, '_episodes_total': 14300}\n",
      "2023-05-19 01:05:29,744\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:05:32,508\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00002 (score = -7.000000) into trial 181f9_00003 (score = -7.040000)\n",
      "\n",
      "2023-05-19 01:05:32,509\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00003:\n",
      "alpha : 0.06709226642692727 --- (* 0.8) --> 0.05367381314154182\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=42152)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=42152)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=42152)\u001b[0m 2023-05-19 01:05:36,350\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=42152)\u001b[0m 2023-05-19 01:05:36,721\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=42152)\u001b[0m 2023-05-19 01:05:36,759\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=42152)\u001b[0m 2023-05-19 01:05:36,873\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_9ce10adfd45342d6a40e49153e2cb272\n",
      "\u001b[2m\u001b[36m(PG pid=42152)\u001b[0m 2023-05-19 01:05:36,873\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 161, '_timesteps_total': None, '_time_total': 80.03872513771057, '_episodes_total': 16100}\n",
      "\u001b[2m\u001b[36m(pid=43392)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43392)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=50460)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=50460)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=43392)\u001b[0m 2023-05-19 01:05:45,528\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=50460)\u001b[0m 2023-05-19 01:05:45,503\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43392)\u001b[0m 2023-05-19 01:05:45,827\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43392)\u001b[0m 2023-05-19 01:05:45,842\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=50460)\u001b[0m 2023-05-19 01:05:45,805\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=50460)\u001b[0m 2023-05-19 01:05:45,820\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=50460)\u001b[0m 2023-05-19 01:05:45,909\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_4f8abdf8b212462ca898cedc8c10c76c\n",
      "\u001b[2m\u001b[36m(PG pid=50460)\u001b[0m 2023-05-19 01:05:45,909\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 185, '_timesteps_total': None, '_time_total': 91.81706404685974, '_episodes_total': 18500}\n",
      "\u001b[2m\u001b[36m(PG pid=43392)\u001b[0m 2023-05-19 01:05:45,944\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_d873f6527547430bbb7487cd0f354b0f\n",
      "\u001b[2m\u001b[36m(PG pid=43392)\u001b[0m 2023-05-19 01:05:45,944\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 176, '_timesteps_total': None, '_time_total': 87.88049149513245, '_episodes_total': 17600}\n",
      "2023-05-19 01:05:46,495\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_181f9_00003\n",
      "2023-05-19 01:05:46,952\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -7.000000) into trial 181f9_00000 (score = -7.010000)\n",
      "\n",
      "2023-05-19 01:05:46,953\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.5043367026496751 --- (* 0.8) --> 0.4034693621197401\n",
      "\n",
      "2023-05-19 01:05:49,003\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00003 (score = -7.000000) into trial 181f9_00001 (score = -7.020000)\n",
      "\n",
      "2023-05-19 01:05:49,004\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00001:\n",
      "alpha : 0.05367381314154182 --- (resample) --> 0.673583429334756\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=32128)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=32128)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=37724)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=37724)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=37724)\u001b[0m 2023-05-19 01:05:58,728\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=32128)\u001b[0m 2023-05-19 01:05:58,729\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=37724)\u001b[0m 2023-05-19 01:05:59,003\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=37724)\u001b[0m 2023-05-19 01:05:59,025\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=32128)\u001b[0m 2023-05-19 01:05:59,008\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=32128)\u001b[0m 2023-05-19 01:05:59,029\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=37724)\u001b[0m 2023-05-19 01:05:59,127\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_62f1821bbfe9451d938011a4e2af7525\n",
      "\u001b[2m\u001b[36m(PG pid=37724)\u001b[0m 2023-05-19 01:05:59,127\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 187, '_timesteps_total': None, '_time_total': 92.87922549247742, '_episodes_total': 18700}\n",
      "\u001b[2m\u001b[36m(PG pid=32128)\u001b[0m 2023-05-19 01:05:59,150\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_02370dabb78f425492c1081f9a49d1ac\n",
      "\u001b[2m\u001b[36m(PG pid=32128)\u001b[0m 2023-05-19 01:05:59,150\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 177, '_timesteps_total': None, '_time_total': 88.43911719322205, '_episodes_total': 17700}\n",
      "\u001b[2m\u001b[36m(pid=42304)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=42304)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=42304)\u001b[0m 2023-05-19 01:06:00,869\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=42304)\u001b[0m 2023-05-19 01:06:01,166\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=42304)\u001b[0m 2023-05-19 01:06:01,182\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=42304)\u001b[0m 2023-05-19 01:06:01,258\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_07209d72a5254a0ca6de6a9a8026f3a9\n",
      "\u001b[2m\u001b[36m(PG pid=42304)\u001b[0m 2023-05-19 01:06:01,258\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 187, '_timesteps_total': None, '_time_total': 92.87922549247742, '_episodes_total': 18700}\n",
      "2023-05-19 01:06:01,961\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00001 (score = -7.000000) into trial 181f9_00000 (score = -7.010000)\n",
      "\n",
      "2023-05-19 01:06:01,962\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.673583429334756 --- (resample) --> 0.991834466595219\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=13504)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=13504)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=28188)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=28188)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=13504)\u001b[0m 2023-05-19 01:06:13,080\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=28188)\u001b[0m 2023-05-19 01:06:13,075\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=13504)\u001b[0m 2023-05-19 01:06:13,353\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=13504)\u001b[0m 2023-05-19 01:06:13,373\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=28188)\u001b[0m 2023-05-19 01:06:13,352\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=28188)\u001b[0m 2023-05-19 01:06:13,375\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=13504)\u001b[0m 2023-05-19 01:06:13,453\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ac1c829451c741ecae72679f17c06f8a\n",
      "\u001b[2m\u001b[36m(PG pid=13504)\u001b[0m 2023-05-19 01:06:13,453\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 192, '_timesteps_total': None, '_time_total': 95.08670091629028, '_episodes_total': 19200}\n",
      "\u001b[2m\u001b[36m(PG pid=28188)\u001b[0m 2023-05-19 01:06:13,474\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b533887032f34cbba7d65591a00c7ca5\n",
      "\u001b[2m\u001b[36m(PG pid=28188)\u001b[0m 2023-05-19 01:06:13,474\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 188, '_timesteps_total': None, '_time_total': 93.32641696929932, '_episodes_total': 18800}\n",
      "2023-05-19 01:06:14,246\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 181f9_00003 (score = -7.000000) into trial 181f9_00000 (score = -7.010000)\n",
      "\n",
      "2023-05-19 01:06:14,246\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial181f9_00000:\n",
      "alpha : 0.05367381314154182 --- (resample) --> 0.28552177411366286\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=49472)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=49472)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=49472)\u001b[0m 2023-05-19 01:06:23,986\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=49472)\u001b[0m 2023-05-19 01:06:24,243\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=49472)\u001b[0m 2023-05-19 01:06:24,256\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=49472)\u001b[0m 2023-05-19 01:06:24,345\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_64bbe5ef57ac42e68236624074e5bb32\n",
      "\u001b[2m\u001b[36m(PG pid=49472)\u001b[0m 2023-05-19 01:06:24,345\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 193, '_timesteps_total': None, '_time_total': 95.56337642669678, '_episodes_total': 19300}\n",
      "2023-05-19 01:06:27,355\tINFO tune.py:798 -- Total run time: 412.59 seconds (412.00 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "#from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "from gymnasium.spaces import Dict, Discrete, MultiDiscrete, Tuple,Box\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"PG\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=16)\n",
    "#parser.add_argument(\"--num-gpus\", type=int, default=1)\n",
    "parser.add_argument(\"--num-gpus\", type=float, default=0)#0.25)\n",
    "\n",
    "parser.add_argument(\"--num-workers\", type=int, default=6)\n",
    "parser.add_argument(\"--num-gpus-per-worker\", type=float, default=0.0)\n",
    "#parser.add_argument(\"render_mode\", type=int, default=1)\n",
    "parser.add_argument(\n",
    "    \"--mixer\",\n",
    "    type=str,\n",
    "    default=\"qmix\",\n",
    "    choices=[\"qmix\", \"vdn\", \"none\"],\n",
    "    help=\"The mixer model to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=70000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=9.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "            #if agent_id.startswith(\"low_level_\"):\n",
    "                #return \"low_level_policy\"\n",
    "            #else:\n",
    "                #return \"high_level_policy\"\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ray.init(num_cpus=16, num_gpus=0,local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0],\n",
    "        \"group_2\": [0,1],\n",
    "        #\"group_3\": [0]\n",
    "    }\n",
    "\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=observation_space, act_space=action_space\n",
    "        ),\n",
    "    )\n",
    "    \"\"\"\n",
    "    from ray.tune import register_env\n",
    "    from ray.rllib.algorithms.dqn import DQN \n",
    "    YourExternalEnv = ... \n",
    "    register_env(\"my_env\", \n",
    "        lambda config: YourExternalEnv(config))\n",
    "    trainer = DQN(env=\"my_env\") \n",
    "    while True: \n",
    "        print(trainer.train()) \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(TwoStepGame)\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=args.num_gpus,num_gpus_per_worker=args.num_gpus_per_worker,)\n",
    "        #.reset_config(reuse_actors=True)\n",
    "    )\n",
    "    def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "        #if agent_id.startswith(\"low_level_\"):\n",
    "        if agent_id.startswith(\"group_1\"):\n",
    "            return \"low_level_policy\"\n",
    "        else:\n",
    "            return \"high_level_policy\"\n",
    "\n",
    "    if args.run == \"QMIX\":\n",
    "        \n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "\n",
    "            .training(mixer=args.mixer, train_batch_size=32)\n",
    "            .resources(\n",
    "                # How many GPUs does the local worker (driver) need? For most algos,\n",
    "                # this is where the learning updates happen.\n",
    "                # Set this to > 1 for multi-GPU learning.\n",
    "                num_gpus=args.num_gpus,\n",
    "                # How many GPUs does each RolloutWorker (`num_workers`) need?\n",
    "                num_gpus_per_worker=args.num_gpus_per_worker,#0.25,\n",
    "        )           \n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        observation_space,\n",
    "                        action_space,\n",
    "                        config.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        #Tuple([observation_space,Box(-inf, inf, (1,), float64)]),#,Box()\n",
    "                        Tuple([observation_space,obs_space]),\n",
    "                        Tuple([action_space,obs_space]),#action_space,\n",
    "                        config.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn#lambda agent_id, episode, worker, **kwargs: \"pol2\"\n",
    "                #policy_mapping_fn=(lambda agent_id, episode, worker, **kw: (\"pol1\" if agent_id == \"agent1\" else \"pol2\")\n",
    "    #)#policy_mapping_fn,\n",
    "            )\n",
    "            .rollouts(num_rollout_workers=args.num_workers, num_envs_per_worker=args.num_envs_per_worker,)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "                }\n",
    "            )\n",
    "            .environment(\n",
    "                env=\"grouped_twostep\",\n",
    "                env_config={\n",
    "                    \"separate_state_space\": True,\n",
    "                    \"one_hot_state_encoding\": True,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr='training_iteration',\n",
    "        metric='episode_reward_mean',#'loss',\n",
    "        mode='min',\n",
    "        perturbation_interval=1,\n",
    "        hyperparam_mutations={\n",
    "            #\"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"alpha\": tune.uniform(0.0, 1.0),\n",
    "        }\n",
    "    )\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=4,\n",
    "            scheduler=pbt_scheduler,\n",
    "            #reuse_actors=True,\n",
    "        ),\n",
    "        \n",
    "        \n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ff70435",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5719debc",
   "metadata": {},
   "source": [
    "#episode reward mean is 4.64 and policy loss is 1.6 under prior precision 20\n",
    "#policy loss increased to 1.65 and epsidoe reward mean increased to 4.79 under prior precison 0.0000000001\n",
    "#under prior precision 2 episdoe reward mean is 5.05 and loss is 1.7. second trial of same prior precison yielded 5.15 reward mean and 1.8 loss \n",
    "#trial 3 under prior precision 2 loss is 1.73 and episdoe reward mean is 5.04. trial 4 loss is 1.65 and epsidoe reward mean and reward mean 4.79\n",
    "#trial 5 is 1.43 policy loss and reward of 4.18 for prior precision 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.train.rl import RLTrainer\n",
    "\n",
    "trainer = RLTrainer(\n",
    "    run_config=air.RunConfig(stop=stop, verbose=2),#RunConfig(stop={\"training_iteration\": 5}),\n",
    "    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),\n",
    "    algorithm=\"QMIX\",\n",
    "    config=config\n",
    "\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5126cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#offline learning\n",
    "\n",
    "import ray\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.train.rl import RLTrainer\n",
    "from ray.rllib.algorithms.bc.bc import BC\n",
    "\"\"\"\n",
    "dataset = ray.data.read_json(\n",
    "    \"/tmp/data-dir\", parallelism=2, ray_remote_args={\"num_cpus\": 1}\n",
    ")\n",
    "\n",
    "trainer = RLTrainer(\n",
    "    run_config=RunConfig(stop={\"training_iteration\": 5}),\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    datasets={\"train\": dataset},\n",
    "    algorithm=BCTrainer,\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"framework\": \"tf\",\n",
    "        \"evaluation_num_workers\": 1,\n",
    "        \"evaluation_interval\": 1,\n",
    "        \"evaluation_config\": {\"input\": \"sampler\"},\n",
    "    },\n",
    ")\n",
    "result = trainer.fit()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "776754a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 11:10:27,763\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-04-15 11:10:36,355\tINFO trainable.py:172 -- Trainable.setup took 13.594 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7622193694114685, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.13, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 0.0, 0.0, 8.0, 1.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 0.0, 1.0, 1.0, 0.0, 1.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 1.0, 0.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0889644053444933, 'mean_inference_ms': 1.6715585888914801, 'mean_action_processing_ms': 0.1555366895685148, 'mean_env_wait_ms': 0.0685827055973793, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02419821421305339, 'ViewRequirementAgentConnector_ms': 0.2105109426710341}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.13, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 0.0, 0.0, 8.0, 1.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 0.0, 1.0, 1.0, 0.0, 1.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 1.0, 0.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0889644053444933, 'mean_inference_ms': 1.6715585888914801, 'mean_action_processing_ms': 0.1555366895685148, 'mean_env_wait_ms': 0.0685827055973793, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02419821421305339, 'ViewRequirementAgentConnector_ms': 0.2105109426710341}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 624.493, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 19.055, 'learn_throughput': 10495.731, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-36', 'timestamp': 1681582236, 'time_this_iter_s': 0.6264936923980713, 'time_total_s': 0.6264936923980713, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B1274533A0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.6264936923980713, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 15.0, 'ram_util_percent': 51.9, 'gpu_util_percent0': 0.0, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7117332816123962, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 3.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.02, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 0.0, 8.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 0.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 0.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 1.0, 0.0, 8.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.131954930369693, 'mean_inference_ms': 1.690488206478129, 'mean_action_processing_ms': 0.194280819405344, 'mean_env_wait_ms': 0.06201558576854982, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.04031777381896973, 'ViewRequirementAgentConnector_ms': 0.242478609085083}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.02, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 0.0, 8.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 0.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 0.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 1.0, 0.0, 8.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.131954930369693, 'mean_inference_ms': 1.690488206478129, 'mean_action_processing_ms': 0.194280819405344, 'mean_env_wait_ms': 0.06201558576854982, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.04031777381896973, 'ViewRequirementAgentConnector_ms': 0.242478609085083}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 800, 'timers': {'training_iteration_time_ms': 640.844, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 16.785, 'learn_throughput': 11915.721, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'done': False, 'episodes_total': 200, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-37', 'timestamp': 1681582237, 'time_this_iter_s': 0.658195972442627, 'time_total_s': 1.2846896648406982, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B130C0EB80>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.2846896648406982, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 19.5, 'ram_util_percent': 52.1, 'gpu_util_percent0': 0.07, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7356886267662048, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 5.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.17, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 1.0, 1.0, 1.0, 8.0, 7.0, 1.0, 1.0, 8.0, 0.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 0.0, 0.0, 7.0, 7.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 7.0, 8.0, 0.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0731271816767787, 'mean_inference_ms': 1.6306672437417138, 'mean_action_processing_ms': 0.19447577376532274, 'mean_env_wait_ms': 0.05301223221713811, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008994340896606445, 'ViewRequirementAgentConnector_ms': 0.21720051765441895}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.17, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 1.0, 1.0, 1.0, 8.0, 7.0, 1.0, 1.0, 8.0, 0.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 0.0, 0.0, 7.0, 7.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 7.0, 8.0, 0.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0731271816767787, 'mean_inference_ms': 1.6306672437417138, 'mean_action_processing_ms': 0.19447577376532274, 'mean_env_wait_ms': 0.05301223221713811, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008994340896606445, 'ViewRequirementAgentConnector_ms': 0.21720051765441895}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1200, 'timers': {'training_iteration_time_ms': 613.278, 'load_time_ms': 0.333, 'load_throughput': 600043.491, 'learn_time_ms': 15.523, 'learn_throughput': 12884.077, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'done': False, 'episodes_total': 300, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-38', 'timestamp': 1681582238, 'time_this_iter_s': 0.5591464042663574, 'time_total_s': 1.8438360691070557, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B12C398E50>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.8438360691070557, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'warmup_time': 13.599156856536865, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6982377171516418, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 7.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.13, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 8.0, 8.0, 7.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 0.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.037116056673238, 'mean_inference_ms': 1.5966630309410912, 'mean_action_processing_ms': 0.17776501163858893, 'mean_env_wait_ms': 0.05970733442556546, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.027981042861938477, 'ViewRequirementAgentConnector_ms': 0.15546607971191406}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.13, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 8.0, 8.0, 7.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 0.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.037116056673238, 'mean_inference_ms': 1.5966630309410912, 'mean_action_processing_ms': 0.17776501163858893, 'mean_env_wait_ms': 0.05970733442556546, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.027981042861938477, 'ViewRequirementAgentConnector_ms': 0.15546607971191406}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1600, 'timers': {'training_iteration_time_ms': 595.497, 'load_time_ms': 0.25, 'load_throughput': 800057.988, 'learn_time_ms': 14.142, 'learn_throughput': 14142.473, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'done': False, 'episodes_total': 400, 'training_iteration': 4, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-38', 'timestamp': 1681582238, 'time_this_iter_s': 0.5441555976867676, 'time_total_s': 2.3879916667938232, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B10F00D670>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.3879916667938232, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 12.5, 'ram_util_percent': 52.1, 'gpu_util_percent0': 0.1, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7672795057296753, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 9.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.39, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 8.0, 7.0, 8.0, 7.0, 8.0, 8.0, 8.0, 7.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0361888191916726, 'mean_inference_ms': 1.6117643762182647, 'mean_action_processing_ms': 0.1854994199373624, 'mean_env_wait_ms': 0.05881126586731142, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.026178359985351562, 'ViewRequirementAgentConnector_ms': 0.19472694396972656}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.39, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 8.0, 7.0, 8.0, 7.0, 8.0, 8.0, 8.0, 7.0, 8.0, 8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0361888191916726, 'mean_inference_ms': 1.6117643762182647, 'mean_action_processing_ms': 0.1854994199373624, 'mean_env_wait_ms': 0.05881126586731142, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.026178359985351562, 'ViewRequirementAgentConnector_ms': 0.19472694396972656}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 600.055, 'load_time_ms': 0.2, 'load_throughput': 1000072.485, 'learn_time_ms': 14.114, 'learn_throughput': 14170.233, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'episodes_total': 500, 'training_iteration': 5, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-39', 'timestamp': 1681582239, 'time_this_iter_s': 0.6192898750305176, 'time_total_s': 3.007281541824341, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B130BFA250>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.007281541824341, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 28.6, 'ram_util_percent': 52.1, 'gpu_util_percent0': 0.0, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7927850484848022, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 11.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.61, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.029419958541832, 'mean_inference_ms': 1.578336552120466, 'mean_action_processing_ms': 0.17962388253827366, 'mean_env_wait_ms': 0.05886536851512898, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.020925521850585938, 'ViewRequirementAgentConnector_ms': 0.19593262672424316}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.61, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.029419958541832, 'mean_inference_ms': 1.578336552120466, 'mean_action_processing_ms': 0.17962388253827366, 'mean_env_wait_ms': 0.05886536851512898, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.020925521850585938, 'ViewRequirementAgentConnector_ms': 0.19593262672424316}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2400, 'timers': {'training_iteration_time_ms': 590.246, 'load_time_ms': 0.167, 'load_throughput': 1200086.981, 'learn_time_ms': 13.763, 'learn_throughput': 14531.725, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'done': False, 'episodes_total': 600, 'training_iteration': 6, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-40', 'timestamp': 1681582240, 'time_this_iter_s': 0.5431962013244629, 'time_total_s': 3.5504777431488037, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B12742EA90>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.5504777431488037, 'timesteps_since_restore': 0, 'iterations_since_restore': 6, 'warmup_time': 13.599156856536865, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.862657368183136, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 13.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.98, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0451639149548073, 'mean_inference_ms': 1.5994893236725263, 'mean_action_processing_ms': 0.18306513669914554, 'mean_env_wait_ms': 0.06176930168200864, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0386810302734375, 'ViewRequirementAgentConnector_ms': 0.25295305252075195}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.98, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0451639149548073, 'mean_inference_ms': 1.5994893236725263, 'mean_action_processing_ms': 0.18306513669914554, 'mean_env_wait_ms': 0.06176930168200864, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0386810302734375, 'ViewRequirementAgentConnector_ms': 0.25295305252075195}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2800, 'timers': {'training_iteration_time_ms': 597.495, 'load_time_ms': 0.143, 'load_throughput': 1400101.478, 'learn_time_ms': 13.225, 'learn_throughput': 15122.628, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'done': False, 'episodes_total': 700, 'training_iteration': 7, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-40', 'timestamp': 1681582240, 'time_this_iter_s': 0.6419870853424072, 'time_total_s': 4.192464828491211, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B12C46FA60>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 4.192464828491211, 'timesteps_since_restore': 0, 'iterations_since_restore': 7, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 19.5, 'ram_util_percent': 52.3, 'gpu_util_percent0': 0.0, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9490025639533997, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 15.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.23, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0629878201982064, 'mean_inference_ms': 1.6329474630838332, 'mean_action_processing_ms': 0.18956093844736022, 'mean_env_wait_ms': 0.06399357192297415, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.03689146041870117, 'ViewRequirementAgentConnector_ms': 0.21502208709716797}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.23, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0629878201982064, 'mean_inference_ms': 1.6329474630838332, 'mean_action_processing_ms': 0.18956093844736022, 'mean_env_wait_ms': 0.06399357192297415, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.03689146041870117, 'ViewRequirementAgentConnector_ms': 0.21502208709716797}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3200, 'timers': {'training_iteration_time_ms': 610.001, 'load_time_ms': 0.125, 'load_throughput': 1600115.975, 'learn_time_ms': 13.762, 'learn_throughput': 14532.334, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'done': False, 'episodes_total': 800, 'training_iteration': 8, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-41', 'timestamp': 1681582241, 'time_this_iter_s': 0.6985442638397217, 'time_total_s': 4.891009092330933, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B12C398E20>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 4.891009092330933, 'timesteps_since_restore': 0, 'iterations_since_restore': 8, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 7.5, 'ram_util_percent': 51.8, 'gpu_util_percent0': 0.0, 'vram_util_percent0': 0.5096435546875}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8179271221160889, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 17.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.04, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.050259920042929, 'mean_inference_ms': 1.6232797134458716, 'mean_action_processing_ms': 0.18790561182508203, 'mean_env_wait_ms': 0.06514095982070235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023944616317749023, 'ViewRequirementAgentConnector_ms': 0.19333314895629883}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.04, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.050259920042929, 'mean_inference_ms': 1.6232797134458716, 'mean_action_processing_ms': 0.18790561182508203, 'mean_env_wait_ms': 0.06514095982070235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023944616317749023, 'ViewRequirementAgentConnector_ms': 0.19333314895629883}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3600, 'timers': {'training_iteration_time_ms': 605.295, 'load_time_ms': 0.111, 'load_throughput': 1800130.472, 'learn_time_ms': 13.789, 'learn_throughput': 14504.101, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'done': False, 'episodes_total': 900, 'training_iteration': 9, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-42', 'timestamp': 1681582242, 'time_this_iter_s': 0.5686533451080322, 'time_total_s': 5.459662437438965, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B10EEF36A0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 5.459662437438965, 'timesteps_since_restore': 0, 'iterations_since_restore': 9, 'warmup_time': 13.599156856536865, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9234033226966858, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 19.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.38, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0634884126540245, 'mean_inference_ms': 1.6418654343177537, 'mean_action_processing_ms': 0.18953979164287488, 'mean_env_wait_ms': 0.0646500394440841, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.039147377014160156, 'ViewRequirementAgentConnector_ms': 0.21978545188903809}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.38, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0634884126540245, 'mean_inference_ms': 1.6418654343177537, 'mean_action_processing_ms': 0.18953979164287488, 'mean_env_wait_ms': 0.0646500394440841, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.039147377014160156, 'ViewRequirementAgentConnector_ms': 0.21978545188903809}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 2000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 611.693, 'load_time_ms': 0.1, 'load_throughput': 2000144.969, 'learn_time_ms': 13.81, 'learn_throughput': 14481.844, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 1000, 'training_iteration': 10, 'trial_id': 'default', 'experiment_id': '230ff5d8f39d4551857dd0ace75dc0c1', 'date': '2023-04-15_11-10-43', 'timestamp': 1681582243, 'time_this_iter_s': 0.6702642440795898, 'time_total_s': 6.129926681518555, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B130BF5490>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B12B23A0D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 6.129926681518555, 'timesteps_since_restore': 0, 'iterations_since_restore': 10, 'warmup_time': 13.599156856536865, 'perf': {'cpu_util_percent': 8.3, 'ram_util_percent': 51.7, 'gpu_util_percent0': 0.08, 'vram_util_percent0': 0.5096435546875}}\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()\n",
    "\n",
    "#moderate prior precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7df007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 23:54:09,009\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-04-14 23:54:16,566\tINFO trainable.py:172 -- Trainable.setup took 12.545 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6795865297317505, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.88, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 1.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.039853736535827, 'mean_inference_ms': 1.5531155600476623, 'mean_action_processing_ms': 0.17674170916353288, 'mean_env_wait_ms': 0.04500773415636661, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.012067079544067383, 'ViewRequirementAgentConnector_ms': 0.22479356659783256}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.88, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 8.0, 8.0, 0.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 1.0, 8.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.039853736535827, 'mean_inference_ms': 1.5531155600476623, 'mean_action_processing_ms': 0.17674170916353288, 'mean_env_wait_ms': 0.04500773415636661, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.012067079544067383, 'ViewRequirementAgentConnector_ms': 0.22479356659783256}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 577.822, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.064, 'learn_throughput': 16578.932, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-17', 'timestamp': 1681541657, 'time_this_iter_s': 0.5788228511810303, 'time_total_s': 0.5788228511810303, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BA90>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.5788228511810303, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 31.1, 'ram_util_percent': 47.4, 'gpu_util_percent0': 0.12, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7314061522483826, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 3.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.11, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.919299827252243, 'mean_inference_ms': 1.4818142774396406, 'mean_action_processing_ms': 0.1818664056107291, 'mean_env_wait_ms': 0.062393725958845535, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021955251693725586, 'ViewRequirementAgentConnector_ms': 0.16707277297973633}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.11, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.919299827252243, 'mean_inference_ms': 1.4818142774396406, 'mean_action_processing_ms': 0.1818664056107291, 'mean_env_wait_ms': 0.062393725958845535, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021955251693725586, 'ViewRequirementAgentConnector_ms': 0.16707277297973633}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 800, 'timers': {'training_iteration_time_ms': 545.348, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.532, 'learn_throughput': 15958.998, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'done': False, 'episodes_total': 200, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-17', 'timestamp': 1681541657, 'time_this_iter_s': 0.5138733386993408, 'time_total_s': 1.092696189880371, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BE20>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.092696189880371, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 46.2, 'ram_util_percent': 47.3, 'gpu_util_percent0': 0.07, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.5058264136314392, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 5.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.49, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 0.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 8.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8828798665381505, 'mean_inference_ms': 1.4100138240566666, 'mean_action_processing_ms': 0.1712384914201429, 'mean_env_wait_ms': 0.06160482193983334, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.017079591751098633, 'ViewRequirementAgentConnector_ms': 0.14663290977478027}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.49, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 0.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 8.0, 7.0, 1.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8828798665381505, 'mean_inference_ms': 1.4100138240566666, 'mean_action_processing_ms': 0.1712384914201429, 'mean_env_wait_ms': 0.06160482193983334, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.017079591751098633, 'ViewRequirementAgentConnector_ms': 0.14663290977478027}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1200, 'timers': {'training_iteration_time_ms': 521.626, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 11.69, 'learn_throughput': 17109.25, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'done': False, 'episodes_total': 300, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-18', 'timestamp': 1681541658, 'time_this_iter_s': 0.47518205642700195, 'time_total_s': 1.567878246307373, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BD60>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.567878246307373, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'warmup_time': 12.548984050750732, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6492184400558472, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 7.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.0, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9334712439262023, 'mean_inference_ms': 1.4415602856658671, 'mean_action_processing_ms': 0.1759609479582711, 'mean_env_wait_ms': 0.0549618819828486, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023952960968017578, 'ViewRequirementAgentConnector_ms': 0.22032475471496582}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.0, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9334712439262023, 'mean_inference_ms': 1.4415602856658671, 'mean_action_processing_ms': 0.1759609479582711, 'mean_env_wait_ms': 0.0549618819828486, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.023952960968017578, 'ViewRequirementAgentConnector_ms': 0.22032475471496582}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1600, 'timers': {'training_iteration_time_ms': 538.458, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.656, 'learn_throughput': 15803.187, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'done': False, 'episodes_total': 400, 'training_iteration': 4, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-18', 'timestamp': 1681541658, 'time_this_iter_s': 0.5899109840393066, 'time_total_s': 2.1577892303466797, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B1382D1820>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.1577892303466797, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 37.9, 'ram_util_percent': 47.2, 'gpu_util_percent0': 0.11, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7665503025054932, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 9.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.52, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9231414947357327, 'mean_inference_ms': 1.4200236771132928, 'mean_action_processing_ms': 0.15987335266052308, 'mean_env_wait_ms': 0.058973943079625474, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.046994924545288086, 'ViewRequirementAgentConnector_ms': 0.14233922958374023}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.52, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9231414947357327, 'mean_inference_ms': 1.4200236771132928, 'mean_action_processing_ms': 0.15987335266052308, 'mean_env_wait_ms': 0.058973943079625474, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.046994924545288086, 'ViewRequirementAgentConnector_ms': 0.14233922958374023}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 529.815, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.929, 'learn_throughput': 15469.659, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'episodes_total': 500, 'training_iteration': 5, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-19', 'timestamp': 1681541659, 'time_this_iter_s': 0.4962425231933594, 'time_total_s': 2.654031753540039, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13FD5B580>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.654031753540039, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'warmup_time': 12.548984050750732, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8569368720054626, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 11.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9555395794947082, 'mean_inference_ms': 1.4787746607314338, 'mean_action_processing_ms': 0.17303054676167076, 'mean_env_wait_ms': 0.0628914860861188, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02704167366027832, 'ViewRequirementAgentConnector_ms': 0.2059648036956787}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9555395794947082, 'mean_inference_ms': 1.4787746607314338, 'mean_action_processing_ms': 0.17303054676167076, 'mean_env_wait_ms': 0.0628914860861188, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02704167366027832, 'ViewRequirementAgentConnector_ms': 0.2059648036956787}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2400, 'timers': {'training_iteration_time_ms': 550.907, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 12.771, 'learn_throughput': 15660.273, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'done': False, 'episodes_total': 600, 'training_iteration': 6, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-20', 'timestamp': 1681541660, 'time_this_iter_s': 0.6573679447174072, 'time_total_s': 3.3113996982574463, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13826E400>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.3113996982574463, 'timesteps_since_restore': 0, 'iterations_since_restore': 6, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 0.0, 'ram_util_percent': 47.1, 'gpu_util_percent0': 0.27, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7892510890960693, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 13.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.64, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9581208824685945, 'mean_inference_ms': 1.4988720885010631, 'mean_action_processing_ms': 0.17073016605744787, 'mean_env_wait_ms': 0.06317990239733548, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02521538734436035, 'ViewRequirementAgentConnector_ms': 0.1761481761932373}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.64, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 1.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9581208824685945, 'mean_inference_ms': 1.4988720885010631, 'mean_action_processing_ms': 0.17073016605744787, 'mean_env_wait_ms': 0.06317990239733548, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.02521538734436035, 'ViewRequirementAgentConnector_ms': 0.1761481761932373}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2800, 'timers': {'training_iteration_time_ms': 555.635, 'load_time_ms': 0.143, 'load_throughput': 1398767.413, 'learn_time_ms': 13.375, 'learn_throughput': 14952.879, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'done': False, 'episodes_total': 700, 'training_iteration': 7, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-20', 'timestamp': 1681541660, 'time_this_iter_s': 0.5860385894775391, 'time_total_s': 3.8974382877349854, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13827BDF0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.8974382877349854, 'timesteps_since_restore': 0, 'iterations_since_restore': 7, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 0.0, 'ram_util_percent': 47.0, 'gpu_util_percent0': 0.41, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8371381759643555, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 15.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.03, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.944832427139806, 'mean_inference_ms': 1.4962974300539398, 'mean_action_processing_ms': 0.16498297620459987, 'mean_env_wait_ms': 0.06601082243672166, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.01466989517211914, 'ViewRequirementAgentConnector_ms': 0.12430882453918457}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.03, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.944832427139806, 'mean_inference_ms': 1.4962974300539398, 'mean_action_processing_ms': 0.16498297620459987, 'mean_env_wait_ms': 0.06601082243672166, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.01466989517211914, 'ViewRequirementAgentConnector_ms': 0.12430882453918457}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3200, 'timers': {'training_iteration_time_ms': 552.196, 'load_time_ms': 0.125, 'load_throughput': 1598591.329, 'learn_time_ms': 13.453, 'learn_throughput': 14866.137, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'done': False, 'episodes_total': 800, 'training_iteration': 8, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-21', 'timestamp': 1681541661, 'time_this_iter_s': 0.529125452041626, 'time_total_s': 4.426563739776611, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B138313160>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 4.426563739776611, 'timesteps_since_restore': 0, 'iterations_since_restore': 8, 'warmup_time': 12.548984050750732, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8821440935134888, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 17.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.31, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.960271534557014, 'mean_inference_ms': 1.5386448245390063, 'mean_action_processing_ms': 0.16615392101400103, 'mean_env_wait_ms': 0.06758842383537736, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.022125959396362305, 'ViewRequirementAgentConnector_ms': 0.2020738124847412}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.31, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.960271534557014, 'mean_inference_ms': 1.5386448245390063, 'mean_action_processing_ms': 0.16615392101400103, 'mean_env_wait_ms': 0.06758842383537736, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.022125959396362305, 'ViewRequirementAgentConnector_ms': 0.2020738124847412}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3600, 'timers': {'training_iteration_time_ms': 565.449, 'load_time_ms': 0.111, 'load_throughput': 1798415.245, 'learn_time_ms': 14.18, 'learn_throughput': 14104.005, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'done': False, 'episodes_total': 900, 'training_iteration': 9, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-22', 'timestamp': 1681541662, 'time_this_iter_s': 0.6734707355499268, 'time_total_s': 5.100034475326538, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B137FF9AF0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 5.100034475326538, 'timesteps_since_restore': 0, 'iterations_since_restore': 9, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 8.0, 'ram_util_percent': 47.1, 'gpu_util_percent0': 0.03, 'vram_util_percent0': 0.56298828125}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8496705293655396, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 19.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.59, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9562232862526865, 'mean_inference_ms': 1.5362027524293271, 'mean_action_processing_ms': 0.17369788387666524, 'mean_env_wait_ms': 0.06433000331041754, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.013000011444091797, 'ViewRequirementAgentConnector_ms': 0.17013216018676758}}, 'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.59, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9562232862526865, 'mean_inference_ms': 1.5362027524293271, 'mean_action_processing_ms': 0.17369788387666524, 'mean_env_wait_ms': 0.06433000331041754, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.013000011444091797, 'ViewRequirementAgentConnector_ms': 0.17013216018676758}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 2000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 565.019, 'load_time_ms': 0.2, 'load_throughput': 1000430.292, 'learn_time_ms': 14.163, 'learn_throughput': 14121.76, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 1000, 'training_iteration': 10, 'trial_id': 'default', 'experiment_id': '4cecb59f0f624d73878be22e1b616af9', 'date': '2023-04-14_23-54-22', 'timestamp': 1681541662, 'time_this_iter_s': 0.5621469020843506, 'time_total_s': 5.662181377410889, 'pid': 37820, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000002B13B131CD0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000002B13B02C9D0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 5.662181377410889, 'timesteps_since_restore': 0, 'iterations_since_restore': 10, 'warmup_time': 12.548984050750732, 'perf': {'cpu_util_percent': 11.6, 'ram_util_percent': 47.1, 'gpu_util_percent0': 0.16, 'vram_util_percent0': 0.56298828125}}\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()\n",
    "\n",
    "#prior precision is large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()\n",
    "\n",
    "#small prior precision\n",
    "#episode reward mean is 4.64 and policy loss is 1.6 under prior precision 20\n",
    "#policy loss increased to 1.65 and epsidoe reward mean increased to 4.79 under prior precison 0.0000000001\n",
    "#under prior precision 2 episdoe reward mean is 5.05 and loss is 1.7. second trial of same prior precison yielded 5.15 reward mean and 1.8 loss \n",
    "#trial 3 under prior precision 2 loss is 1.73 and episdoe reward mean is 5.04. trial 4 loss is 1.65 and epsidoe reward mean and reward mean 4.79\n",
    "#trial 5 is 1.43 policy loss and reward of 4.18 for prior precision 2 \n",
    "\n",
    "#reward is increasing but policy loss is also increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4ae579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 23:46:46,491\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2023-04-19 23:46:50,066\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-04-19 23:46:55,310\tWARNING worker.py:846 -- `ray.get_gpu_ids()` will always return the empty list when called from the driver. This is because Ray does not manage GPU allocations to the driver process.\n",
      "2023-04-19 23:46:55,310\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-04-19 23:46:55,312\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property worker_index not supported.\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.6482515335083008, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.79, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 0.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7618090406579167, 'mean_inference_ms': 1.0791999190600952, 'mean_action_processing_ms': 0.11994945469187263, 'mean_env_wait_ms': 0.031213855268943377, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.024434328079223633, 'ViewRequirementAgentConnector_ms': 0.15880955590142143}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.79, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 0.0, 7.0, 0.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 8.0, 0.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7618090406579167, 'mean_inference_ms': 1.0791999190600952, 'mean_action_processing_ms': 0.11994945469187263, 'mean_env_wait_ms': 0.031213855268943377, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.024434328079223633, 'ViewRequirementAgentConnector_ms': 0.15880955590142143}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 411.886, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 9.463, 'learn_throughput': 21135.851, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-55', 'timestamp': 1681973215, 'time_this_iter_s': 0.4128868579864502, 'time_total_s': 0.4128868579864502, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB647F0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.4128868579864502, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 3.2, 'ram_util_percent': 40.3, 'gpu_util_percent0': 0.47, 'vram_util_percent0': 0.53369140625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.5196247100830078, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 3.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.47, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 8.0, 0.0, 8.0, 7.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 8.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 0.0, 0.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6959997210419389, 'mean_inference_ms': 1.0471641274164445, 'mean_action_processing_ms': 0.1251792669890825, 'mean_env_wait_ms': 0.03438102931453106, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008956432342529297, 'ViewRequirementAgentConnector_ms': 0.10797858238220215}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.47, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 1.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 1.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 8.0, 0.0, 8.0, 7.0, 0.0, 0.0, 7.0, 0.0, 7.0, 7.0, 8.0, 0.0, 8.0, 7.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 8.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 0.0, 0.0, 1.0, 0.0, 0.0, 8.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6959997210419389, 'mean_inference_ms': 1.0471641274164445, 'mean_action_processing_ms': 0.1251792669890825, 'mean_env_wait_ms': 0.03438102931453106, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008956432342529297, 'ViewRequirementAgentConnector_ms': 0.10797858238220215}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800, 'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 800, 'timers': {'training_iteration_time_ms': 391.23, 'load_time_ms': 0.502, 'load_throughput': 398698.099, 'learn_time_ms': 8.234, 'learn_throughput': 24289.812, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 400, 'num_env_steps_trained': 400, 'num_agent_steps_sampled': 800, 'num_agent_steps_trained': 800}, 'done': False, 'episodes_total': 200, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-56', 'timestamp': 1681973216, 'time_this_iter_s': 0.37207984924316406, 'time_total_s': 0.7849667072296143, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB53FD0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.7849667072296143, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 4.1, 'ram_util_percent': 40.4, 'gpu_util_percent0': 0.01, 'vram_util_percent0': 0.52978515625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.55398428440094, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 5.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.61, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 1.0, 8.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 0.0, 8.0, 0.0, 0.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 8.0, 8.0, 1.0, 8.0, 0.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6706936783877867, 'mean_inference_ms': 1.033522721733309, 'mean_action_processing_ms': 0.12198105429650936, 'mean_env_wait_ms': 0.029594053246217236, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.016854286193847656, 'ViewRequirementAgentConnector_ms': 0.1025397777557373}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 4.61, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 1.0, 8.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 0.0, 8.0, 0.0, 0.0, 7.0, 7.0, 7.0, 0.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 1.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 0.0, 7.0, 0.0, 7.0, 8.0, 8.0, 1.0, 8.0, 0.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6706936783877867, 'mean_inference_ms': 1.033522721733309, 'mean_action_processing_ms': 0.12198105429650936, 'mean_env_wait_ms': 0.029594053246217236, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.016854286193847656, 'ViewRequirementAgentConnector_ms': 0.1025397777557373}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200, 'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1200, 'timers': {'training_iteration_time_ms': 381.445, 'load_time_ms': 0.334, 'load_throughput': 598047.148, 'learn_time_ms': 8.001, 'learn_throughput': 24998.335, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 600, 'num_env_steps_trained': 600, 'num_agent_steps_sampled': 1200, 'num_agent_steps_trained': 1200}, 'done': False, 'episodes_total': 300, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-56', 'timestamp': 1681973216, 'time_this_iter_s': 0.3629014492034912, 'time_total_s': 1.1478681564331055, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB64430>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.1478681564331055, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8084396123886108, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 7.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.42, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6679702787363574, 'mean_inference_ms': 1.0462500778179191, 'mean_action_processing_ms': 0.12052639593345839, 'mean_env_wait_ms': 0.03543328703119514, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0290679931640625, 'ViewRequirementAgentConnector_ms': 0.12602901458740234}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.42, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 7.0, 8.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 0.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 8.0, 1.0, 7.0, 7.0, 7.0, 0.0, 8.0, 8.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 8.0, 1.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 0.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6679702787363574, 'mean_inference_ms': 1.0462500778179191, 'mean_action_processing_ms': 0.12052639593345839, 'mean_env_wait_ms': 0.03543328703119514, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0290679931640625, 'ViewRequirementAgentConnector_ms': 0.12602901458740234}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 1600, 'timers': {'training_iteration_time_ms': 384.338, 'load_time_ms': 0.251, 'load_throughput': 797396.198, 'learn_time_ms': 8.061, 'learn_throughput': 24811.026, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'done': False, 'episodes_total': 400, 'training_iteration': 4, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-56', 'timestamp': 1681973216, 'time_this_iter_s': 0.39403510093688965, 'time_total_s': 1.5419032573699951, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB57850>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.5419032573699951, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 2.8, 'ram_util_percent': 40.4, 'gpu_util_percent0': 0.04, 'vram_util_percent0': 0.52978515625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7519081234931946, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 9.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.36, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626207273561401, 'mean_inference_ms': 1.0419222977492484, 'mean_action_processing_ms': 0.1175179705395923, 'mean_env_wait_ms': 0.039889024092362724, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.009093046188354492, 'ViewRequirementAgentConnector_ms': 0.125152587890625}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.36, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 1.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 0.0, 1.0, 7.0, 7.0, 0.0, 8.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 8.0, 8.0, 7.0, 7.0, 1.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 8.0, 0.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 1.0, 1.0, 8.0, 7.0, 0.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626207273561401, 'mean_inference_ms': 1.0419222977492484, 'mean_action_processing_ms': 0.1175179705395923, 'mean_env_wait_ms': 0.039889024092362724, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.009093046188354492, 'ViewRequirementAgentConnector_ms': 0.125152587890625}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 383.847, 'load_time_ms': 0.201, 'load_throughput': 996745.247, 'learn_time_ms': 8.964, 'learn_throughput': 22311.433, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1000, 'num_env_steps_trained': 1000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'episodes_total': 500, 'training_iteration': 5, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-57', 'timestamp': 1681973217, 'time_this_iter_s': 0.3828868865966797, 'time_total_s': 1.9247901439666748, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB946A0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 1.9247901439666748, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.7848349213600159, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 11.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.51, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6609894056105791, 'mean_inference_ms': 1.0494113861770056, 'mean_action_processing_ms': 0.12151188497043072, 'mean_env_wait_ms': 0.03701780956055501, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021304607391357422, 'ViewRequirementAgentConnector_ms': 0.14121270179748535}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.51, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 1.0, 7.0, 0.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 8.0, 7.0, 7.0, 0.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 1.0, 1.0, 1.0, 7.0, 7.0, 0.0, 7.0, 7.0, 0.0, 8.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 7.0, 0.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 8.0, 8.0, 7.0, 1.0, 8.0, 8.0, 7.0, 8.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6609894056105791, 'mean_inference_ms': 1.0494113861770056, 'mean_action_processing_ms': 0.12151188497043072, 'mean_env_wait_ms': 0.03701780956055501, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021304607391357422, 'ViewRequirementAgentConnector_ms': 0.14121270179748535}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400, 'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2400, 'timers': {'training_iteration_time_ms': 385.455, 'load_time_ms': 0.338, 'load_throughput': 592137.035, 'learn_time_ms': 8.901, 'learn_throughput': 22470.188, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1200, 'num_env_steps_trained': 1200, 'num_agent_steps_sampled': 2400, 'num_agent_steps_trained': 2400}, 'done': False, 'episodes_total': 600, 'training_iteration': 6, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-57', 'timestamp': 1681973217, 'time_this_iter_s': 0.394512414932251, 'time_total_s': 2.319302558898926, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB57F10>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.319302558898926, 'timesteps_since_restore': 0, 'iterations_since_restore': 6, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 3.7, 'ram_util_percent': 40.5, 'gpu_util_percent0': 0.05, 'vram_util_percent0': 0.52978515625}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.8710399270057678, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 13.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 1.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6585515966422213, 'mean_inference_ms': 1.055777711071856, 'mean_action_processing_ms': 0.12322258387694271, 'mean_env_wait_ms': 0.03422387236786434, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008939266204833984, 'ViewRequirementAgentConnector_ms': 0.12508893013000488}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.87, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 7.0, 7.0, 0.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 1.0, 7.0, 0.0, 0.0, 1.0, 7.0, 1.0, 1.0, 0.0, 7.0, 8.0, 0.0, 1.0, 1.0, 7.0, 7.0, 8.0, 0.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 0.0, 7.0, 8.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6585515966422213, 'mean_inference_ms': 1.055777711071856, 'mean_action_processing_ms': 0.12322258387694271, 'mean_env_wait_ms': 0.03422387236786434, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008939266204833984, 'ViewRequirementAgentConnector_ms': 0.12508893013000488}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800, 'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1400, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 2800, 'timers': {'training_iteration_time_ms': 385.937, 'load_time_ms': 0.43, 'load_throughput': 465332.087, 'learn_time_ms': 8.849, 'learn_throughput': 22602.622, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1400, 'num_env_steps_trained': 1400, 'num_agent_steps_sampled': 2800, 'num_agent_steps_trained': 2800}, 'done': False, 'episodes_total': 700, 'training_iteration': 7, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-58', 'timestamp': 1681973218, 'time_this_iter_s': 0.38883185386657715, 'time_total_s': 2.708134412765503, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB64D00>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 2.708134412765503, 'timesteps_since_restore': 0, 'iterations_since_restore': 7, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.85731041431427, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 15.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.07, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6657106886797587, 'mean_inference_ms': 1.0772254748466532, 'mean_action_processing_ms': 0.12328861505816983, 'mean_env_wait_ms': 0.03620254330751226, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.010513544082641602, 'ViewRequirementAgentConnector_ms': 0.12743854522705078}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.07, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 0.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6657106886797587, 'mean_inference_ms': 1.0772254748466532, 'mean_action_processing_ms': 0.12328861505816983, 'mean_env_wait_ms': 0.03620254330751226, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.010513544082641602, 'ViewRequirementAgentConnector_ms': 0.12743854522705078}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200, 'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1600, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3200, 'timers': {'training_iteration_time_ms': 393.099, 'load_time_ms': 0.376, 'load_throughput': 531808.099, 'learn_time_ms': 9.651, 'learn_throughput': 20723.678, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1600, 'num_env_steps_trained': 1600, 'num_agent_steps_sampled': 3200, 'num_agent_steps_trained': 3200}, 'done': False, 'episodes_total': 800, 'training_iteration': 8, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-58', 'timestamp': 1681973218, 'time_this_iter_s': 0.44522714614868164, 'time_total_s': 3.1533615589141846, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB53520>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.1533615589141846, 'timesteps_since_restore': 0, 'iterations_since_restore': 8, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 3.6, 'ram_util_percent': 40.6, 'gpu_util_percent0': 0.07, 'vram_util_percent0': 0.5296630859375}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9545664191246033, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 17.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.57, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6633380728917012, 'mean_inference_ms': 1.066249718208038, 'mean_action_processing_ms': 0.11748092032882122, 'mean_env_wait_ms': 0.038373476925457534, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008992910385131836, 'ViewRequirementAgentConnector_ms': 0.14661622047424316}}, 'episode_reward_max': 8.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 6.57, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6633380728917012, 'mean_inference_ms': 1.066249718208038, 'mean_action_processing_ms': 0.11748092032882122, 'mean_env_wait_ms': 0.038373476925457534, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.008992910385131836, 'ViewRequirementAgentConnector_ms': 0.14661622047424316}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600, 'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 1800, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 3600, 'timers': {'training_iteration_time_ms': 389.767, 'load_time_ms': 0.334, 'load_throughput': 598284.111, 'learn_time_ms': 9.587, 'learn_throughput': 20862.513, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 1800, 'num_env_steps_trained': 1800, 'num_agent_steps_sampled': 3600, 'num_agent_steps_trained': 3600}, 'done': False, 'episodes_total': 900, 'training_iteration': 9, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-59', 'timestamp': 1681973219, 'time_this_iter_s': 0.36411547660827637, 'time_total_s': 3.517477035522461, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F3CBB94910>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.517477035522461, 'timesteps_since_restore': 0, 'iterations_since_restore': 9, 'warmup_time': 8.846390724182129, 'perf': {}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.9812411069869995, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 19.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.62, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626733001144689, 'mean_inference_ms': 1.061902172502311, 'mean_action_processing_ms': 0.11869277553758524, 'mean_env_wait_ms': 0.039210503009603596, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0190579891204834, 'ViewRequirementAgentConnector_ms': 0.1209249496459961}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 6.62, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 8.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 0.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6626733001144689, 'mean_inference_ms': 1.061902172502311, 'mean_action_processing_ms': 0.11869277553758524, 'mean_env_wait_ms': 0.039210503009603596, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.0190579891204834, 'ViewRequirementAgentConnector_ms': 0.1209249496459961}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 2000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 389.165, 'load_time_ms': 0.301, 'load_throughput': 664760.124, 'learn_time_ms': 9.579, 'learn_throughput': 20878.714, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 1000, 'training_iteration': 10, 'trial_id': 'default', 'experiment_id': '726068d0a9d34a2cb8b6b9a060b810c4', 'date': '2023-04-19_23-46-59', 'timestamp': 1681973219, 'time_this_iter_s': 0.3837430477142334, 'time_total_s': 3.9012200832366943, 'pid': 1892, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001F0002B7280>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001F40C201C10>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 3.9012200832366943, 'timesteps_since_restore': 0, 'iterations_since_restore': 10, 'warmup_time': 8.846390724182129, 'perf': {'cpu_util_percent': 5.5, 'ram_util_percent': 40.4, 'gpu_util_percent0': 0.05, 'vram_util_percent0': 0.5296630859375}}\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(10):\n",
    "    print(algo.train())\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f957fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.models.torch.torch_action_dist.TorchCategorical'>\n"
     ]
    }
   ],
   "source": [
    "policy = algo.get_policy()\n",
    "print(policy.dist_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "184b73a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PG' object has no attribute 'observation_space'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_15792\\2622574447.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    prep = get_preprocessor(algo.observation_space)#(algo.observation_space)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m\u001b[1;31m:\u001b[0m 'PG' object has no attribute 'observation_space'\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "prep = get_preprocessor(algo.observation_space)#(algo.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6efb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
