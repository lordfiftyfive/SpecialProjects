{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 18:28:38.037534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.6.0, llvm 15.0.4, commit f1c6fbbd, linux, python 3.10.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 09/21/24 18:28:48.240 240089] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_probability.substrates import jax as tfp #this is tensorflow probability on jax \n",
    "import distrax\n",
    "\n",
    "from jaxmarl import *\n",
    "from jaxmarl.environments.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "from rapidae.models import Beta_VAE #rapidae is based on keras 3 which means a jax implementation is available\n",
    "from rapidae.models.base import VanillaEncoder, VanillaDecoder\n",
    "\n",
    "import numpyro\n",
    "from numpyro import *\n",
    "import numpyro.distributions as dist\n",
    "from numpyro import handlers\n",
    "from numpyro.infer import Predictive, SVI, Trace_ELBO\n",
    "from pyro.infer.abstract_infer import TracePosterior\n",
    "import pyro.poutine as poutine, queue\n",
    "from pyro.contrib.oed.eig import marginal_eig, posterior_eig\n",
    "#optimization\n",
    "import optax\n",
    "from optax.contrib import prodigy\n",
    "\n",
    "from gymnax import *\n",
    "import mo_gymnasium as mo_gym\n",
    "\n",
    "import dynamax\n",
    "\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "\n",
    "#bm4pml is for bayesian model reduction needed to simulate the way brain does structure learning for causal inference \n",
    "#from bmr4pml.models import SVIRegression, BMRRegression\n",
    "#from bmr4pml.nn import MLP, LeNet, VisionTransformer, resnet18, MlpMixer\n",
    "#from bmr4pml.datasets import load_data\n",
    "#from bmr4pml.inference import fit_and_test\n",
    "#from bmr4pml.nn.utils import PatchConvEmbed, PatchLinearEmbed\n",
    "\n",
    "#utilities\n",
    "import omegaconf\n",
    "from hydra import *\n",
    "\n",
    "from jax import *\n",
    "import functools\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import chex\n",
    "from flax import struct\n",
    "import flax.linen as nn\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from flax.training.train_state import TrainState\n",
    "import flashbax as fbx\n",
    "from flax.core import frozen_dict\n",
    "import wandb\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from safetensors.flax import save_file\n",
    "from flax.traverse_util import flatten_dict\n",
    "\n",
    "from jaxmarl import make\n",
    "from jaxmarl.wrappers.baselines import LogWrapper\n",
    "from jaxmarl.environments.smax import map_name_to_scenario\n",
    "from jaxmarl.environments.overcooked import overcooked_layouts\n",
    "from functools import partial\n",
    "\n",
    "# from gymnax.environments import environment, spaces\n",
    "from gymnax.environments.spaces import Box as BoxGymnax, Discrete as DiscreteGymnax\n",
    "from typing import Optional, List, Tuple, Union, NamedTuple, Dict\n",
    "from jaxmarl.environments.spaces import Box, Discrete, MultiDiscrete\n",
    "from jaxmarl.environments.multi_agent_env import MultiAgentEnv, State\n",
    "\n",
    "from flowjax.flows import block_neural_autoregressive_flow\n",
    "from flowjax.train import fit_to_data\n",
    "from flowjax.distributions import Normal\n",
    "\n",
    "import ivy\n",
    "\n",
    "import zuko#uses normalizing flows that is for ELBO then what we have implemented in \n",
    "#from pyro.contrib.zuko import ZukoToPyro\n",
    "from strnn.models.strNN import StrNN\n",
    "from torch2jax import j2t, t2j\n",
    "#hyperparameter optimization\n",
    "import sherpa\n",
    "\n",
    "#visualization\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "from ursina import *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import keras\n",
    "import keras as K\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "#from keras.layers.merge import concatenate as concat\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpyro.contrib.oed.eig import posterior_eig\n",
    "from numpyro.optim import Adam\n",
    "#from numpyro.infer.abstract_infer import TracePosterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overall objectives:\n",
    "\n",
    "perception\n",
    "learning\n",
    "action\n",
    "learning with full causal inference\n",
    "consciousness \n",
    "\n",
    "stage 1 objectives\n",
    "\n",
    "perception\n",
    "action\n",
    "\n",
    "achieved through implementation of standard EFE and VFE\n",
    "\n",
    "stage 2 objectives\n",
    "\n",
    "learning \n",
    "swap out all neural networks with either normalizing flows or tsetlin machines \n",
    "enhanced EFE and bayesian model reduction \n",
    "\n",
    "stage 3 objectives\n",
    "\n",
    "consciousness\n",
    "emotions\n",
    "\n",
    "\n",
    "part of qmix is going to be reversed with the mixer networks firing before the Emodels and then bmodels networks \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.arange(10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "brain has a energy budget of 20W per second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Ursina()\n",
    "ground = Entity(\n",
    "    model = 'cube',\n",
    "    color = color.magenta,\n",
    "    z = -.1,\n",
    "    y = -3,\n",
    "    origin = (0, .5),\n",
    "    scale = (50, 1, 10),\n",
    "    collider = 'box',\n",
    "    )\n",
    "\n",
    "#app.run()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we may need to move the entire agent and scanned rnn below where the q_tot is calculated or raise b and e models so that we can use the generated q_tot with obs to find z, the individual q values\n",
    "\n",
    "we do not need to raise b and emodels just add an extra argument representing q_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScannedRNN(nn.Module):\n",
    "\n",
    "    @partial(\n",
    "        nn.scan,\n",
    "        variable_broadcast=\"params\",\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        split_rngs={\"params\": False},\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):#keave scannedrnn the way it is \n",
    "        \"\"\"Applies the module.\"\"\"\n",
    "        rnn_state = carry\n",
    "        ins, resets = x\n",
    "        hidden_size = ins.shape[-1]\n",
    "        rnn_state = jnp.where(\n",
    "            resets[:, jnp.newaxis],\n",
    "            self.initialize_carry(hidden_size, *ins.shape[:-1]),\n",
    "            rnn_state,\n",
    "        )\n",
    "        new_rnn_state, y = nn.LSTMCell(hidden_size)(rnn_state, ins)\n",
    "\n",
    "        \n",
    "        return new_rnn_state, y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(hidden_size, *batch_size):\n",
    "        # Use a dummy key since the default state init fn is just zeros.\n",
    "        return nn.LSTMCell(hidden_size, parent=None).initialize_carry(\n",
    "            jax.random.PRNGKey(0), (*batch_size, hidden_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AgentRNN(nn.Module):\n",
    "    # homogenous agent for parameters sharing, assumes all agents have same obs and action dim\n",
    "\n",
    "    \"\"\"\"\n",
    "    specifications for either scanned rnn or agent rnn: this agent will ahve to accept: observations, and qtot values in order to generate a\n",
    "    individual q value \n",
    "\n",
    "    for the moment let us assume that obs will only be 1 row deep at each time step. Then all we need to do is determine which qvalues \n",
    "    corresponding to actions will allow us to get to qtot. This will need to be a variational autoencoder or we could use pyro or zuko.\n",
    "\n",
    "    THis is because we know x: the observations and y qtot. We need to find z: the latent space that will allow us to get from x to y.\n",
    "\n",
    "    We will for the moment be using a beta_vae. \n",
    "\n",
    "    we need a specific kind of variational autoencoder called a conditional variational autoencoder. we will use this https://github.com/nnormandin/Conditional_VAE/blob/master/Conditional_VAE.ipynb\n",
    "        \n",
    "    \n",
    "    we should assume that q_vals has a dimensionalty of 2. n_y will simply be the shape of q_tot\n",
    "\n",
    "\n",
    "    the decoding process is the reconstruction from z to X_hat.\n",
    "\n",
    "    we need to do this reconstruction because at no point can we ever see what z is. This reconstruction is used to validate z which we can never observe\n",
    "\n",
    "    x will be q_tot and y will be observations\n",
    "\n",
    "    during the predict stage x will be fully available byt only a subset of y will be available. \n",
    "    \"\"\"\n",
    "\n",
    "    action_dim: int\n",
    "    hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):#add extra argument y or labels which here will be q_tot\n",
    "        obs, dones = x\n",
    "        q_tot= 0 \n",
    "        label = q_tot\n",
    "\n",
    "        embedding = nn.Dense(self.hidden_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "\n",
    "        #we are going to feed both x and z into the decoder where z will either be the calcualted \n",
    "        \"\"\"\n",
    "        #put this in the scannedRnn\n",
    "\n",
    "        #m = 250 # batch size\n",
    "        n_z = 2 # latent space size\n",
    "        encoder_dim1 = 2 # dim of encoder hidden layer\n",
    "        decoder_dim = 2 # dim of decoder hidden layer\n",
    "        decoder_out_dim = 2#784 # dim of decoder output layer\n",
    "        activ = 'relu'\n",
    "\n",
    "        encoder_h = Dense(encoder_dim1, activation=activ)(inputs)\n",
    "        mu = Dense(n_z, activation='linear')(encoder_h)\n",
    "        l_sigma = Dense(n_z, activation='linear')(encoder_h)\n",
    "\n",
    "        def vae_loss(y_true, y_pred):\n",
    "            recon = K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "            kl = 0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=-1)\n",
    "            return recon + kl\n",
    "\n",
    "        def KL_loss(y_true, y_pred):\n",
    "            return(0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1))\n",
    "\n",
    "        def recon_loss(y_true, y_pred):\n",
    "            return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "        decoder_hidden = Dense(decoder_dim, activation=activ)\n",
    "        decoder_out = Dense(decoder_out_dim, activation='sigmoid')\n",
    "\n",
    "        #z is going to be individual q values \n",
    "\n",
    "        h_p = decoder_hidden(zc)\n",
    "        outputs = decoder_out(h_p)\n",
    "        \n",
    "        cvae = Model([x, label], outputs)\n",
    "        encoder = Model([x, label], mu)\n",
    "\n",
    "        d_in = Input(shape=(n_z+n_y,))\n",
    "        d_h = decoder_hidden(d_in)  \n",
    "        d_out = decoder_out(d_h)\n",
    "        decoder = Model(d_in, d_out)\n",
    "\n",
    "        #we need to return obs, qvalues reprsented by z. \n",
    "\n",
    "        #y or label is are the q_tot values\n",
    "        \"\"\"\n",
    "        q_vals = nn.Dense(self.action_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(embedding)#replace this with autoencoder \n",
    "\n",
    "        return hidden, q_vals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_dim(space):\n",
    "    # get the proper action/obs space from Discrete-MultiDiscrete-Box spaces\n",
    "    if isinstance(space, (DiscreteGymnax, Discrete)):\n",
    "        return space.n\n",
    "    elif isinstance(space, (BoxGymnax, Box, MultiDiscrete)):\n",
    "        return jnp.prod(space.shape)\n",
    "    else:\n",
    "        print(space)\n",
    "        raise NotImplementedError('Current wrapper works only with Discrete/MultiDiscrete/Box action and obs spaces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JaxMARLWrapper(object):\n",
    "    \"\"\"Base class for all jaxmarl wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env: MultiAgentEnv):\n",
    "        self._env = env\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    # def _batchify(self, x: dict):\n",
    "    #     x = jnp.stack([x[a] for a in self._env.agents])\n",
    "    #     return x.reshape((self._env.num_agents, -1))\n",
    "\n",
    "    def _batchify_floats(self, x: dict):\n",
    "        return jnp.stack([x[a] for a in self._env.agents])\n",
    "class CTRolloutManager(JaxMARLWrapper):\n",
    "    \"\"\"\n",
    "    Rollout Manager for Centralized Training of with Parameters Sharing. Used by JaxMARL Q-Learning Baselines.\n",
    "    - Batchify multiple environments (the number of parallel envs is defined by batch_size in __init__).\n",
    "    - Adds a global state (obs[\"__all__\"]) and a global reward (rewards[\"__all__\"]) in the env.step returns.\n",
    "    - Pads the observations of the agents in order to have all the same length.\n",
    "    - Adds an agent id (one hot encoded) to the observation vectors.\n",
    "\n",
    "    By default:\n",
    "    - global_state is the concatenation of all agents' observations.\n",
    "    - global_reward is the sum of all agents' rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: MultiAgentEnv, batch_size:int, training_agents:List=None, preprocess_obs:bool=True):\n",
    "        \n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # the agents to train could differ from the total trainable agents in the env (f.i. if using pretrained agents)\n",
    "        # it's important to know it in order to compute properly the default global rewards and state\n",
    "        self.training_agents = self.agents if training_agents is None else training_agents  \n",
    "        self.preprocess_obs = preprocess_obs  \n",
    "\n",
    "        # TOREMOVE: this is because overcooked doesn't follow other envs conventions\n",
    "        if len(env.observation_spaces) == 0:\n",
    "            self.observation_spaces = {agent:self.observation_space() for agent in self.agents}\n",
    "        if len(env.action_spaces) == 0:\n",
    "            self.action_spaces = {agent:env.action_space() for agent in self.agents}\n",
    "        \n",
    "        # batched action sampling\n",
    "        self.batch_samplers = {agent: jax.jit(jax.vmap(self.action_space(agent).sample, in_axes=0)) for agent in self.agents}\n",
    "\n",
    "        # assumes the observations are flattened vectors\n",
    "        self.max_obs_length = max(list(map(lambda x: get_space_dim(x), self.observation_spaces.values())))#\n",
    "        self.max_action_space = max(list(map(lambda x: get_space_dim(x), self.action_spaces.values())))#lambda x: get_space_dim(x)\n",
    "        self.obs_size = self.max_obs_length + len(self.agents)\n",
    "\n",
    "        # agents ids\n",
    "        self.agents_one_hot = {a:oh for a, oh in zip(self.agents, jnp.eye(len(self.agents)))}\n",
    "        # valid actions\n",
    "        self.valid_actions = {a:jnp.arange(u.n) for a, u in self.action_spaces.items()}\n",
    "        self.valid_actions_oh ={a:jnp.concatenate((jnp.ones(u.n), jnp.zeros(self.max_action_space - u.n))) for a, u in self.action_spaces.items()}\n",
    "\n",
    "        # custom global state and rewards for specific envs\n",
    "        if 'smax' in env.name.lower():\n",
    "            self.global_state = lambda obs, state: obs['world_state']\n",
    "            self.global_reward = lambda rewards: rewards[self.training_agents[0]]\n",
    "        elif 'hanabi' in env.name.lower():\n",
    "            self.global_state = self.hanabi_world_state\n",
    "        elif 'overcooked' in env.name.lower():\n",
    "            self.global_state = lambda obs, state:  jnp.concatenate([obs[agent].ravel() for agent in self.agents], axis=-1)\n",
    "            self.global_reward = lambda rewards: rewards[self.training_agents[0]]\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def batch_reset(self, key):\n",
    "        keys = jax.random.split(key, self.batch_size)\n",
    "        return jax.vmap(self.wrapped_reset, in_axes=0)(keys)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def batch_step(self, key, states, actions):\n",
    "        keys = jax.random.split(key, self.batch_size)\n",
    "        return jax.vmap(self.wrapped_step, in_axes=(0, 0, 0))(keys, states, actions)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def wrapped_reset(self, key):\n",
    "        obs_, state = self._env.reset(key)\n",
    "        if self.preprocess_obs:\n",
    "            obs = jax.tree_util.tree_map(self._preprocess_obs, {agent:obs_[agent] for agent in self.agents}, self.agents_one_hot)\n",
    "        else:\n",
    "            obs = obs_\n",
    "        obs[\"__all__\"] = self.global_state(obs_, state)\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def wrapped_step(self, key, state, actions):\n",
    "        if 'hanabi' in self._env.name.lower():\n",
    "            actions = jax.tree_util.tree_map(lambda x:jnp.expand_dims(x, 0), actions)\n",
    "        obs_, state, reward, done, infos = self._env.step(key, state, actions)\n",
    "        if self.preprocess_obs:\n",
    "            obs = jax.tree_util.tree_map(self._preprocess_obs, {agent:obs_[agent] for agent in self.agents}, self.agents_one_hot)\n",
    "            obs = jax.tree_util.tree_map(lambda d, o: jnp.where(d, 0., o), {agent:done[agent] for agent in self.agents}, obs) # ensure that the obs are 0s for done agents\n",
    "        else:\n",
    "            obs = obs_\n",
    "        obs[\"__all__\"] = self.global_state(obs_, state)\n",
    "        reward[\"__all__\"] = self.global_reward(reward)\n",
    "        return obs, state, reward, done, infos\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def global_state(self, obs, state):\n",
    "        return jnp.concatenate([obs[agent] for agent in self.agents], axis=-1)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def global_reward(self, reward):\n",
    "        return jnp.stack([reward[agent] for agent in self.training_agents]).sum(axis=0) \n",
    "    \n",
    "    def batch_sample(self, key, agent):\n",
    "        return self.batch_samplers[agent](jax.random.split(key, self.batch_size)).astype(int)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _preprocess_obs(self, arr, extra_features):\n",
    "        # flatten\n",
    "        arr = arr.flatten()\n",
    "        # pad the observation vectors to the maximum length\n",
    "        pad_width = [(0, 0)] * (arr.ndim - 1) + [(0, max(0, self.max_obs_length - arr.shape[-1]))]\n",
    "        arr = jnp.pad(arr, pad_width, mode='constant', constant_values=0)\n",
    "        # concatenate the extra features\n",
    "        arr = jnp.concatenate((arr, extra_features), axis=-1)\n",
    "        return arr\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def hanabi_world_state(self, obs, state):\n",
    "        \"\"\" \n",
    "        For each agent: [agent obs, own hand]\n",
    "        \"\"\"\n",
    "        all_obs = jnp.array([obs[agent] for agent in self._env.agents])\n",
    "        hands = state.env_state.player_hands.reshape((self._env.num_agents, -1))\n",
    "        return jnp.concatenate((all_obs, hands), axis=1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class MLP(nn.Module):                    # create a Flax Module dataclass\n",
    "  out_dims: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = nn.Dense(128)(x)                 # create inline Flax Module submodules\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(self.out_dims)(x)       # shape inference\n",
    "    return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refactor ESM completely in jax and numpyro. THis task has been completed \n",
    "\n",
    "TODO: replace neural networks with TMU rewritten in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create this like the outcome predictor. This is the expected free energy sub-module ESM\n",
    "this will essentially calculate the q(x|y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class OutcomePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#pragmatic value function below. Needs to be modified. \n",
    "\n",
    "#in order to actually be the pragmatic value we need to replace S with C where C is a prespecified prior. We will not start with a \n",
    "pragmatic function \n",
    "\n",
    "for now we will only use epsitemic value \n",
    "\n",
    "#P(O|S) is the first part of the epistemic value\n",
    "\n",
    "#so we have Entropy(RSA speaker P(O|S)) - Entropy(Bayesian optimal experimental design ) for epsistemic value \n",
    "\n",
    "BOED = q(y|policy) \n",
    "\n",
    "#speaker is P(O|S)\n",
    "\n",
    "#teh speaker is optimzing over p(observation given state) we do not need to consider listener right now\n",
    "\n",
    "#although listner could be used for inverse problems later \n",
    "\n",
    "#marginal guide = q(y|l)\n",
    "\n",
    "#y is data theta is hidden state and l is the policy\n",
    "\n",
    "Variational free energy = DKL term - surprise or to put it another way: divergence - evidence where evidence is ln(obs)\n",
    "\n",
    "surprise will be part of the reward for low level agents. DKL will be the loss in bmodels\n",
    "\n",
    "we should retask JEPPS for variational free energy in bmodels then use inverse maxentropy RL to create the individual q value in bmodel\n",
    "\n",
    "The reward will be staying upright for most agents and staying upright + surprise for low level agents/nl. where nl = number of low level agents \n",
    "\n",
    "the reward will eventually be the pragmatic value which is the E(ln(P(O|C))) where C is a prior\n",
    "\n",
    "\n",
    "Final reward: an independent reward for staying upright is redundant and can be folded into the pragmatic value.\n",
    "\n",
    "Therefore Final reward will be: pragmatic value - ln(obs) -ln(obs) is negative log evidence also known as surprise for data y which we are describing as obs. \n",
    "\n",
    "The first part of the reward will define the objective. The second part will ensure\n",
    "\n",
    "for the purposes of BOED and RSA components of epistemic value that pertain to us a latent \"state\" would correspond to an action or position where we could expect to receive a reward for keeping cartpole upright\n",
    "\n",
    "So the speaker is attempting to find the probability of an observation given an observed collective reward.Perhaps to be more precise the speaker is identifying the independence between observations and states. If states always generate the same observation this quantity would be 0. \n",
    "\n",
    "Can we do marginal here? We may need to use variatonal BOED here. \n",
    "\n",
    "Please note that P(theta|y,d) is proportional (essentially equivalent) to p(theta)*p(y|d, theta) where p(theta) is the prior over the hidden state\n",
    "\n",
    "furthermore the model likelhood is p(y|d, S)\n",
    "\n",
    "posterior APE = posterior predictive entropy\n",
    "\n",
    "posterior APE expectation subscript is p(y,s|d)\n",
    "\n",
    "\n",
    "final conclusion: \n",
    "q(s|d)-APE =  Epistemic value\n",
    "\n",
    "where q(s|d) = P(y|S)/Q(y|d)*q(s|d,y)\n",
    "\n",
    "q(s|d) = p(O|S)/q(y|d)*q(s|d,y) \n",
    "\n",
    "note: q(s|d,y) is simply APE without the expectation or entropy operation applied so q(s|d,y) is simply the posterior part of APE\n",
    "\n",
    "Enhanced expected free energy is \n",
    "\n",
    "-salience -novelty - pragmatic value\n",
    "\n",
    "we can ignore novelty for now and only focus on\n",
    "\n",
    "-salience since pragmatic value will most likely go to the reward function \n",
    "\n",
    "Note: adding the novelty term as part of our enhance expected free energy is crucial for learning. However for now we will just stick with classic EFE\n",
    "\n",
    "novelty optimizes theta which is just a list of parameters for the brains internal model. Novelty allows us to carry out parameter learning. For the full causal experience however\n",
    "\n",
    "Friston introduces his bayesian reduction scheme. The idea is that the brain also occasionally (during periods like sleep) does strucuture learning using bayesian model reduction \n",
    "\n",
    "we already know what both collecive qvalues will be. What we should consider doing is wondering: given first qvalue=APE and second one equals ELBO find the individual qvalue. \n",
    "\n",
    "one way we can do this is by restoring the agent nn and simply setting the loss (for the mixer network) to be the difference between the APE and the first collective qvalue and ELBO and the second collective qvalue respectively. We will attempt this later. \n",
    "\n",
    "for individual agent networks given q_tot and agent_actions during training and q_tot and subset(agent_actions) during execution generate an individual q-value .\n",
    "\n",
    "This should require nothing but a standard neural network with bottom most and top most neuronal agents doing a step before middle layer neuronal agents. \n",
    "\n",
    "epistemic value or salience = expected informationg gain. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the case of the working memory example y is observation, l is policy and theta is hidden state (true working memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpyro.handlers import scope, seed, trace, scale\n",
    "import torch#replace all stuff that depends on this torch import jax eventually\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose a person want to go to the cafe they like best but there is uncertainty as to whether the cafe is open. \n",
    "\n",
    "the person would first need to optimize for epestemic value to resolve the uncertainty about the state of the cafes and only then can the person maxmize pragmatic value by choosing the cafe they have the greatest prior preference for that is also open. \n",
    "\n",
    "Optimizing only over pragmatic value is only possible when there is no uncertainty in the enviroment and the person can know beforehand which policy will take them closest to their prior preference \n",
    "\n",
    "note: if we want to get rid of pre specified preference in our model we simply need to cut out pragmatic value and only optimize over \n",
    "\n",
    "epistemic value for expected free energy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "example of the kind of model that needs to go into SVI\n",
    "\n",
    "sensitivity = 1.0\n",
    "prior_mean = torch.tensor(7.0)\n",
    "prior_sd = torch.tensor(2.0)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "We can also run multiple ‘rounds’ or iterations of experiments. \n",
    "When doing this, we take the learned model from step 3 and use it as our prior in step 1 for the next round. \n",
    "This approach can be particularly useful because it allows us to design the next experiment based on what has already been learned: the experiments are adaptive.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def model(l):\n",
    "    # Dimension -1 of `l` represents the number of rounds\n",
    "    # Other dimensions are batch dimensions: we indicate this with a plate_stack\n",
    "    with pyro.plate_stack(\"plate\", l.shape[:-1]):\n",
    "        theta = pyro.sample(\"theta\", dist.Normal(prior_mean, prior_sd))\n",
    "        # Share theta across the number of rounds of the experiment\n",
    "        # This represents repeatedly testing the same participant\n",
    "        theta = theta.unsqueeze(-1)\n",
    "        # This define a *logistic regression* model for y\n",
    "        logit_p = sensitivity * (theta - l)\n",
    "        # The event shape represents responses from the same participant\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n",
    "        return y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal experimental design in this case uses a \"marginal guide\" which is q(y|l)\n",
    "\n",
    "q(y|l) is for approximating p(y|l). This marginal guide is not intended for inference since unlike the standard guide that approximates\n",
    "over p(theta|y,l) y in this case corresponds only to observed sample sites \n",
    "\n",
    "In the working memory tutorial, we used the ‘marginal’ estimator to find the EIG. This involved estimating the marginal density \n",
    ". In this experiment, that would be relatively difficult: \n",
    " is 51-dimensional with some rather tricky constraints that make modelling its density difficult. Furthermore, the marginal estimator requires us to know \n",
    " analytically, which we do not.\n",
    "\n",
    "Fortunately, other variational estimators of EIG exist: see [2] for more details. One such variational estimator is the ‘posterior’ estimator exist\n",
    "\n",
    "\n",
    "\n",
    "Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def posterior_eig(\n",
    "    model,\n",
    "    design,\n",
    "    observation_labels,\n",
    "    target_labels,\n",
    "    num_samples,\n",
    "    num_steps,\n",
    "    guide,\n",
    "    optim,\n",
    "    return_history=False,\n",
    "    final_design=None,\n",
    "    final_num_samples=None,\n",
    "    eig=True,\n",
    "    prior_entropy_kwargs={},\n",
    "    *args,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Posterior estimate of expected information gain (EIG) computed from the average posterior entropy (APE)\n",
    "    using :math:`EIG(d) = H[p(\\\\theta)] - APE(d)`. See [1] for full details.\n",
    "\n",
    "    The posterior representation of APE is\n",
    "\n",
    "        :math:`\\\\sup_{q}\\\\ E_{p(y, \\\\theta | d)}[\\\\log q(\\\\theta | y, d)]`\n",
    "\n",
    "    where :math:`q` is any distribution on :math:`\\\\theta`.\n",
    "\n",
    "    This method optimises the loss over a given `guide` family representing :math:`q`.\n",
    "\n",
    "    [1] Foster, Adam, et al. \"Variational Bayesian Optimal Experimental Design.\" arXiv preprint arXiv:1903.05480 (2019).\n",
    "\n",
    "    :param function model: A pyro model accepting `design` as only argument.\n",
    "    :param torch.Tensor design: Tensor representation of design\n",
    "    :param list observation_labels: A subset of the sample sites\n",
    "        present in `model`. These sites are regarded as future observations\n",
    "        and other sites are regarded as latent variables over which a\n",
    "        posterior is to be inferred.\n",
    "    :param list target_labels: A subset of the sample sites over which the posterior\n",
    "        entropy is to be measured.\n",
    "    :param int num_samples: Number of samples per iteration.\n",
    "    :param int num_steps: Number of optimization steps.\n",
    "    :param function guide: guide family for use in the (implicit) posterior estimation.\n",
    "        The parameters of `guide` are optimised to maximise the posterior\n",
    "        objective.\n",
    "    :param pyro.optim.Optim optim: Optimiser to use.\n",
    "    :param bool return_history: If `True`, also returns a tensor giving the loss function\n",
    "        at each step of the optimization.\n",
    "    :param torch.Tensor final_design: The final design tensor to evaluate at. If `None`, uses\n",
    "        `design`.\n",
    "    :param int final_num_samples: The number of samples to use at the final evaluation, If `None,\n",
    "        uses `num_samples`.\n",
    "    :param bool eig: Whether to compute the EIG or the average posterior entropy (APE). The EIG is given by\n",
    "        `EIG = prior entropy - APE`. If `True`, the prior entropy will be estimated analytically,\n",
    "        or by Monte Carlo as appropriate for the `model`. If `False` the APE is returned.\n",
    "    :param dict prior_entropy_kwargs: parameters for estimating the prior entropy: `num_prior_samples` indicating the\n",
    "        number of samples for a MC estimate of prior entropy, and `mean_field` indicating if an analytic form for\n",
    "        a mean-field prior should be tried.\n",
    "    :return: EIG estimate, optionally includes full optimization history\n",
    "    :rtype: torch.Tensor or tuple\n",
    "    \"\"\"\n",
    "    if isinstance(observation_labels, str):\n",
    "        observation_labels = [observation_labels]\n",
    "    if isinstance(target_labels, str):\n",
    "        target_labels = [target_labels]\n",
    "\n",
    "    ape = _posterior_ape(\n",
    "        model,\n",
    "        design,\n",
    "        observation_labels,\n",
    "        target_labels,\n",
    "        num_samples,\n",
    "        num_steps,\n",
    "        guide,\n",
    "        optim,\n",
    "        return_history=return_history,\n",
    "        final_design=final_design,\n",
    "        final_num_samples=final_num_samples,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = 0\n",
    "prior_covariance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the variational posterior estimator not the marginal one\n",
    "\n",
    "#out come predictor will be the guide \n",
    "\n",
    "#final note: we need to modify this to get rid of alpha. The reason is because we do not need any prior if we are only calculating APE \n",
    "\n",
    "def model(polling_allocation):\n",
    "    # This allows us to run many copies of the model in parallel\n",
    "    with numpyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "        # Begin by sampling alpha\n",
    "        alpha = numpyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "            prior_mean, covariance_matrix=prior_covariance))\n",
    "\n",
    "        # Sample y conditional on alpha\n",
    "        poll_results = numpyro.sample(\"y\", dist.Binomial(\n",
    "            polling_allocation, logits=alpha).to_event(1))\n",
    "\n",
    "        # Now compute w according to the (approximate) electoral college formula\n",
    "        dem_win = election_winner(alpha)\n",
    "        numpyro.sample(\"w\", dist.Delta(dem_win))\n",
    "\n",
    "        return poll_results, dem_win, alpha\n",
    "\n",
    "\n",
    "class OutcomePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        numpyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        numpyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))\n",
    "eigs = {}\n",
    "best_strategy, best_eig = None, 0\n",
    "aspace = []\n",
    "allocation = aspace\n",
    "\n",
    " \n",
    "for strategy in aspace:#poll_strategies needs to be replaced with action space\n",
    "    print(strategy, end=\" \")\n",
    "    guide = OutcomePredictor()\n",
    "    numpyro.clear_param_store()\n",
    "    # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "    # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "    # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "    ape = posterior_eig(model, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "        guide = ESM()# marginal_guide = posterior predictive entropy d can also be expressed as pi \n",
    "        pyro.clear_param_store()\n",
    "        # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "        # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "        # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)] where w = x  \n",
    "        with pyro.plate_stack(\"plate\", l.shape[:-1]):\n",
    "            theta = pyro.sample(\"theta\", dist.Dirichlet(1))#Normal(0, 0.8))# in our case this will represent the reward\n",
    "            # Share theta across the number of rounds of the experiment\n",
    "            # This represents repeatedly testing the same participant\n",
    "            theta = theta.unsqueeze(-1)\n",
    "            # This define a *logistic regression* model for y\n",
    "            #logit_p = sensitivity * (theta - l)\n",
    "            # The event shape represents responses from the same participant\n",
    "            y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n",
    "            return y\n",
    "            #ape = average posterior entropy\n",
    "        #we dont need tow wory about sample size or optimizer since here we are optimizing over psuedo observations\n",
    "        ape = posterior_eig(model, allocation, \"y\", \"theta\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "        \n",
    "        eigs[strategy] = prior_entropy - ape\n",
    "        print(eigs[strategy].item())\n",
    "\n",
    "        if eigs[strategy] > best_eig:\n",
    "            best_strategy, best_eig = strategy, eigs[strategy]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outstanding problem: we need to make sure that emodel is actually solving for collective expected free energy\n",
    "\n",
    "we know during training that the states of all the other agents and the outisde will be known to any given agent\n",
    "\n",
    "Possible solutions: \n",
    "\n",
    "If during training the states are all known then for the two networks that govern action selection and evaluation we can calculate the collective Expected free energy, and plug that into a standard neural network to recover the individual Qvalue (corresponding to how well it is believed an action the agent takes will fullfill collective free energy). then with the bmodel we can evaluate the collective variational free energy of our super agent and plug that into a standard neural network to recover the qvalue correpsonding to how the individual agents actions contributed to collective variational free energy \n",
    "\n",
    "alternative solution: \n",
    "\n",
    "We have to be careful not to conflate action selection of an individual agent with the action selection of our collectivized \"super-agent\". Our selection neural network will emit a qvalue that will correspond to a traditional RL selection with exploration for each individual agent which will be sent to the mixer network. \n",
    "\n",
    "our target network will also emit a traditional qvalue from a standard neural network. \n",
    "\n",
    "It is only at the level of the loss and reward that we will integrate expected free energy and variational free energy \n",
    "\n",
    "Note: we should probably make the reward expected free energy and the loss variational free energy for this alternative \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Master list of things left to do:\n",
    "\n",
    "1. complete bmodels and emodels. with emodels we need\n",
    "2. specify internal model of the world for emodels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function in the model that us fed into posterior eig needs to be the internal model of the cartpole enviroment. it needs to have a prior gravity parameter where new values for this parameter are continuously chosen by by posterior eig and then evaluated by variational free energy.\n",
    "\n",
    "we will be working backwards. First getting q_tot and then getting the individual q for each agent from q_tot. \n",
    "\n",
    "during training the VAEs will have access to both the q_tot and the actions of all of the other agents. during execution they will only have the q_tot and a small subset of actions during any given time step \n",
    "\n",
    "we should combine VFE and EFE into a single agent class but put them under different functions. \n",
    "\n",
    "emodels and bmodels are going to both be substituted for the mixer networks for selection and evaluation\n",
    "\n",
    "meanwhile we will have a single agent network whose job will be to take the obs and the q_tot and turn it into individual q values.\n",
    "\n",
    "observation space for bmodels and emodels will just be cartpole\n",
    "\n",
    "observation space for individual agents will be: all agents during training, some agents during execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEADLINES:\n",
    "\n",
    "stage 1 july 3rd 2024\n",
    "\n",
    "stage 2: november 2nd 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        numpyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        numpyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def model(polling_allocation):\n",
    "    # This allows us to run many copies of the model in parallel\n",
    "    with numpyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "        # Begin by sampling alpha\n",
    "        alpha = numpyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "            prior_mean, covariance_matrix=prior_covariance))\n",
    "\n",
    "        # Sample y conditional on alpha\n",
    "        poll_results = numpyro.sample(\"y\", dist.Binomial(\n",
    "            polling_allocation, logits=alpha).to_event(1))\n",
    "\n",
    "        # Now compute w according to the (approximate) electoral college formula\n",
    "        dem_win = election_winner(alpha)\n",
    "        numpyro.sample(\"w\", dist.Delta(dem_win))\n",
    "\n",
    "        return poll_results, dem_win, alpha\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the default implementation that we need to modify both target network and the other network are handled by agentRNN. Agentrnn in turn implements scannedRNN in itself\n",
    "\n",
    "It may not matter what we put down for the prior because we are only calculating APE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the prior we can actually choose a pretty informative prior for our cartpole specific simulation.\n",
    "\n",
    "the observation space in continuous which means we require a continuous distribution with limits at -4.8 and positive 4.8 as the bounds of the cartpole position \n",
    "\n",
    "however all the parameters in our cartpole sim that will be part of the win condition func will be a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will be our converter between position and velocity in the model. It will essentially be the simulated internal model of the cartpole env\n",
    "\n",
    "#more specifically q_tot for the inverted pendulum will be a continuous space from -3 to 3 this function will turn the velocity into a position.\n",
    "#it will then try to determine whether the cartpole falls or not.\n",
    "\n",
    "#it is possible that the function could just be a win condition.\n",
    "\n",
    "#the prior of the distribution will be mean 0 with variance 2\n",
    "\n",
    "#this function will be part of a larger model which is different from the guide. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This is the internal simulation of the cartpole env. \n",
    "\n",
    "However a different approach may be warranted\n",
    "\n",
    "for emodels we may need to express the simulated internal representation of the env as a bayesian network with some expressed beliefs about the env \n",
    "and some uncertainty. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "polemass_length=5\n",
    "\n",
    "def win_condition(x):\n",
    "    #pseudo code: angle < critical value \n",
    "    #\n",
    "    temp = (\n",
    "        force + polemass_length * theta_dot**2 * sintheta\n",
    "    ) /total_mass\n",
    "    thetaacc = (gravity * sintheta - costheta * temp) / (\n",
    "        length * (4.0 / 3.0 - masspole * costheta**2 / total_mass)\n",
    "    )\n",
    "    xacc = temp - self.polemass_length * thetaacc * costheta / total_mass\n",
    "    terminated = bool(\n",
    "        x < x_threshold\n",
    "        or x > x_threshold\n",
    "        or theta < -theta_threshold_radians\n",
    "        or theta > theta_threshold_radians\n",
    "    )\n",
    "    return w \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def election_winner(alpha):\n",
    "    dem_win_state = (alpha > 0.).float()\n",
    "    dem_electoral_college_votes = ec_votes_tensor * dem_win_state\n",
    "    w = (dem_electoral_college_votes.sum(-1) / ec_votes_tensor.sum(-1) > .5).float()\n",
    "    return w\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: we need an internal model for the Expected free energy. therefore should probably try to do expected free eneergy and ehance efe at the ssame time. \n",
    "\n",
    "the solution is to create a bayesian network (friston uses a factor graph which is similar). Number of variables in the bayesian network model of the world will depend on number of priors.\n",
    "\n",
    "Through bayesian model reduction one can compare a model with x priors with x+1 prios and x-1 priors. \n",
    "for structure learning\n",
    "1. intialize a random number which will be called x prior variables for two different models one with x+1 and x-1 prior variables \n",
    "2. create a likelihood from data\n",
    "3. determine using bayesian model reduction, whether x+1 or x-1 model is better and repeat\n",
    "4. termination will occur when only y priors are left or when adding more priors fails to improve the model\n",
    "\n",
    "for parameter learning: will be handled by a additional term added to EFE called the novelty term \n",
    "\n",
    "we will use a Conditional variational autoencoder to convert back and forth between the actions of the agents and the bayesian network. \n",
    "\n",
    "essentially the novelty term will be optimized to update beliefs about the model parameters\n",
    "\n",
    "itt is the expectation of the kullbacker divergence ebtween model parameers and model parameters given observations and states \n",
    "\n",
    "in other words there will be a seperate bayesian model. the bayesian model will not be in contact with the agents or their policies except through the novelty term. \n",
    "\n",
    "The novelty term will econmpass not just modifying parameters, but also model comparison where priors will be randomly removed or added and model comparison will be done. Another slight variation that could happen is add both novelty for the parameter learning and bayesian information criteria (for structure learning) as entities to be optimized. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "potential idea: what if for a given neural network, it was possible to convert between bayesian networks and neural networks?\n",
    "\n",
    "we could use a neural network to generate an output and then use a CVAE to to take the input and output and predict a given parameterization and structure. \n",
    "\n",
    "we could then use the bayesian network to adjust the neural network weights \n",
    "\n",
    "This conversion is not needed for the internal simulation itself\n",
    "\n",
    "\n",
    "create baeysian network such that when queried with an action it returns a CPD that tells us what the \n",
    "\n",
    "note: no pre-specified priors will be set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The novelty term calls for optimizing the negative expectation of the kullbacker leibler divergence between the model parameters given the observations and states and the model paramters\n",
    "\n",
    "in other words maximize the kullbacker liebler between the model parameters given the states and observations and the priors of the model parameters\n",
    "\n",
    "so the policy will have to optimize over this, while parameter learner will actually have to optmize over the parameters\n",
    "\n",
    "novelty can be done with vi_eig()\n",
    "\n",
    "novelty and salience are almost identical. \n",
    "\n",
    "basically novelty is first used to train a generative model. which is the expectation of the kullbacker leibler between model parameters given observations and hiddent staes and the prior of model parameters. Once the generative model is in place the EIG is applied normally to salience. The key modification is that we will substitute for a straight prior for the hidden state a probability in the generative model/simulation of the state given the policy/action\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "structure learning\n",
    "\n",
    "inputs: data from cartpole\n",
    "\n",
    "pr1 = 0.5\n",
    "pr2 = dir(1,2)\n",
    "model1 = pr1+pr2\n",
    "while improvement > 0:\n",
    "    prlist.append(new_pr)\n",
    "model2 = prlist + pr1 + pr2\n",
    "\n",
    "likelihoods = create_likelihoods(data)\n",
    "model1 = add_parameter_learning(data)\n",
    "model2 = add_parameter_learning(data)\n",
    "\n",
    "model2*likelihoods/model1*likelihoods\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emodel(nn.Module):    # homogenous agent for parameters sharing, assumes all agents have same obs and action dim\n",
    "    action_dim: int\n",
    "    hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        guide = ESM()# marginal_guide = posterior predictive entropy d can also be expressed as pi \n",
    "        numpyro.clear_param_store()\n",
    "        # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "        # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "        # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)] where w = x\n",
    "        \"\"\"\n",
    "        def model(polling_allocation):\n",
    "            # This allows us to run many copies of the model in parallel\n",
    "            with numpyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "                # We use a Normal prior for theta\n",
    "                #p\n",
    "                theta = numpyro.sample(\"theta\", dist.Normal(torch.tensor(0.0), torch.tensor(2.0)))\n",
    "                design = polling_allocation\n",
    "                # We use a simple logistic regression model for the likelihood\n",
    "                #we need to replace this with some kind of function that represents whether the cartpole will fall given action \n",
    "                #the gravity parameter will be one of the unknown parameters whose selection in addition to action we will be optimzing for\n",
    "                logit_p = theta - design\n",
    "                # Now compute w according to the (approximate) electoral college formula\n",
    "                #dem_win = election_winner(alpha)#where alpha is the hidden variable with the prior. It is essentially like theta\n",
    "                y = numpyro.sample(\"y\", dist.Bernoulli(logits=logit_p))\n",
    "\n",
    "                return y\n",
    "        \"\"\"\n",
    "\n",
    "        def model(polling_allocation):\n",
    "            # This allows us to run many copies of the model in parallel\n",
    "            with numpyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "                # Begin by sampling alpha\n",
    "                alpha = numpyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "                    prior_mean, covariance_matrix=prior_covariance))\n",
    "\n",
    "                # Sample y conditional on alpha\n",
    "                poll_results = numpyro.sample(\"y\", dist.Binomial(\n",
    "                    polling_allocation, logits=alpha).to_event(1))\n",
    "                \"\"\"\n",
    "                # Now compute w according to the (approximate) electoral college formula\n",
    "                dem_win = win_condition(alpha)\n",
    "\n",
    "\n",
    "                from pgmpy.models import BayesianNetwork\n",
    "                G = BayesianNetwork()\n",
    "                \"\"\"\n",
    "                #now compute win condition for cartpole\n",
    "                dem_win = 0\n",
    "\n",
    "                numpyro.sample(\"w\", dist.Delta(dem_win))\n",
    "\n",
    "                return poll_results, dem_win, alpha\n",
    "        #ape = average posterior entropy\n",
    "        #allocation here means experiment design\n",
    "        #we dont need to wory about sample size or optimizer since here we are optimizing over psuedo observations\n",
    "        ape = posterior_eig(model, allocation, \"y\", \"theta\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "        \"\"\"\n",
    "        eigs[strategy] = prior_entropy - ape\n",
    "        print(eigs[strategy].item())\n",
    "\n",
    "        if eigs[strategy] > best_eig:\n",
    "            best_strategy, best_eig = strategy, eigs[strategy]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        EFE + obs -> RNN -> IQValue +h\n",
    "\n",
    "        for now we should have this IQvalue equal ape/n and see what happens. \n",
    "\n",
    "        emodels and bmodels are going to both be substituted for the mixer networks for selection and evaluation\n",
    "\n",
    "        meanwhile we will have a single agent network whose job will be to take the obs and the q_tot and turn it into individual q values.\n",
    "\n",
    "        observation space for bmodels and emodels will just be cartpole\n",
    "\n",
    "        observation space for individual agents will be: all agents during training, some agents during execution\n",
    "\n",
    "        so ultimately we will have emodels and bmodels generate the q_tot\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "        \"\"\"\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        aspace=[0,1,2]\n",
    "        #the below will probably need to go into the main algorithm training loop\n",
    "        for strategy, allocation in aspace:#replace poll strategies with action space \n",
    "            guide = OutcomePredictor()\n",
    "            numpyro.clear_param_store()\n",
    "            # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "            # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "            # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "            y = input_dict[\"obs_flat\"].float()\n",
    "            ape = posterior_eig(model, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                                Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "            eigs[strategy] = -ape\n",
    "            print(eigs[strategy].item())\n",
    "            if eigs[strategy] > best_eig:\n",
    "                best_strategy, best_eig = strategy, eigs[strategy]\n",
    "        \n",
    "        #####################################################################################\n",
    "        \"\"\"\n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)   \n",
    "        q=0\n",
    "        return q, [h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch2jax import j2t, t2j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bmodel(nn.Module):\n",
    "    action_dim: int\n",
    "    hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "\n",
    "        \"\"\"\n",
    "        VFE = ELBO\n",
    "\n",
    "        we may need to call emodels within bmodels to furnish the prior\n",
    "        \n",
    "        \n",
    "        input: obs and q_tots\n",
    "        output: individual q values\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        flow = zuko.flows.NSF(features=2, transforms=3, hidden_features=(64, 64))\n",
    "        print(flow)\n",
    "        x = jnp.ones(30)\n",
    "        j2t(x)\n",
    "        trainset = data.TensorDataset(x)\n",
    "        trainloader = data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(flow.parameters(), lr=1e-3)\n",
    "        \"\"\"\n",
    "        for epoch in range(8):\n",
    "            losses = []\n",
    "\n",
    "            for x, label in trainloader:\n",
    "                loss = -flow().log_prob(x).mean()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                losses.append(loss.detach())\n",
    "\n",
    "            losses = torch.stack(losses)\n",
    "\n",
    "        \n",
    "        #alternatively we can use this\n",
    "\n",
    "        import torch\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=flow,\n",
    "            train_dataloader=train_dataloader,\n",
    "            eval_dataloader=eval_dataloader,\n",
    "            optimizers=torch.optim.Adam(lr=0.01),\n",
    "            max_duration=10,  # epochs\n",
    "            device='gpu'\n",
    "        )\n",
    "        #where trainer is simply our mosaiclml trainer. Our other solution would be to try to compile the model into jax\n",
    "        trainer.fit()\n",
    "                \n",
    "        samples = flow().sample((1,))\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        obs, dones = x\n",
    "        embedding = nn.Dense(self.hidden_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "        q_vals = nn.Dense(self.action_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(embedding)\n",
    "        \n",
    "        return hidden, q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of ivy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 3)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "input_array = tf.random.normal((1, 28, 28, 3))\n",
    "torch_model =  ivy.transpile(model, source=\"torch\", to=\"jax\")#ivy.transpile(model, to=\"torch\", args=(input_array,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetwork(nn.Module):\n",
    "    \"\"\"HyperNetwork for generating weights of QMix' mixing network.\"\"\"\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.hidden_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.))(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this module is slated for deletion and replacement with bmodels and emodels.\n",
    "\n",
    "class MixingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixing network for projecting individual agent Q-values into Q_tot. Follows the original QMix implementation.\n",
    "    \"\"\"\n",
    "    embedding_dim: int\n",
    "    hypernet_hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, q_vals, states):\n",
    "        \n",
    "        n_agents, time_steps, batch_size = q_vals.shape\n",
    "        q_vals = jnp.transpose(q_vals, (1, 2, 0)) # (time_steps, batch_size, n_agents)\n",
    "        \n",
    "        # hypernetwork\n",
    "        w_1 = HyperNetwork(hidden_dim=self.hypernet_hidden_dim, output_dim=self.embedding_dim*n_agents, init_scale=self.init_scale)(states)\n",
    "        b_1 = nn.Dense(self.embedding_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.))(states)\n",
    "        w_2 = HyperNetwork(hidden_dim=self.hypernet_hidden_dim, output_dim=self.embedding_dim, init_scale=self.init_scale)(states)\n",
    "        b_2 = HyperNetwork(hidden_dim=self.embedding_dim, output_dim=1, init_scale=self.init_scale)(states)\n",
    "        \n",
    "        # monotonicity and reshaping\n",
    "        w_1 = jnp.abs(w_1.reshape(time_steps, batch_size, n_agents, self.embedding_dim))\n",
    "        b_1 = b_1.reshape(time_steps, batch_size, 1, self.embedding_dim)\n",
    "        w_2 = jnp.abs(w_2.reshape(time_steps, batch_size, self.embedding_dim, 1))\n",
    "        b_2 = b_2.reshape(time_steps, batch_size, 1, 1)\n",
    "    \n",
    "        # mix\n",
    "        hidden = nn.elu(jnp.matmul(q_vals[:, :, None, :], w_1) + b_1)\n",
    "        q_tot  = jnp.matmul(hidden, w_2) + b_2\n",
    "        \n",
    "        return q_tot.squeeze() # (time_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpsilonGreedy:\n",
    "    \"\"\"Epsilon Greedy action selection\"\"\"\n",
    "\n",
    "    def __init__(self, start_e: float, end_e: float, duration: int):\n",
    "        self.start_e  = start_e\n",
    "        self.end_e    = end_e\n",
    "        self.duration = duration\n",
    "        self.slope    = (end_e - start_e) / duration\n",
    "        \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def get_epsilon(self, t: int):\n",
    "        e = self.slope*t + self.start_e\n",
    "        return jnp.clip(e, self.end_e)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def choose_actions(self, q_vals: dict, t: int, rng: chex.PRNGKey):\n",
    "        \n",
    "        def explore(q, eps, key):\n",
    "            key_a, key_e   = jax.random.split(key, 2) # a key for sampling random actions and one for picking\n",
    "            greedy_actions = jnp.argmax(q, axis=-1) # get the greedy actions \n",
    "            random_actions = jax.random.randint(key_a, shape=greedy_actions.shape, minval=0, maxval=q.shape[-1]) # sample random actions\n",
    "            pick_random    = jax.random.uniform(key_e, greedy_actions.shape)<eps # pick which actions should be random\n",
    "            chosed_actions = jnp.where(pick_random, random_actions, greedy_actions)\n",
    "            return chosed_actions\n",
    "        \n",
    "        eps = self.get_epsilon(t)\n",
    "        keys = dict(zip(q_vals.keys(), jax.random.split(rng, len(q_vals)))) # get a key for each agent\n",
    "        chosen_actions = jax.tree_map(lambda q, k: explore(q, eps, k), q_vals, keys)\n",
    "        return chosen_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    obs: dict\n",
    "    actions: dict\n",
    "    rewards: dict\n",
    "    dones: dict\n",
    "    infos: dict\n",
    "\n",
    "\n",
    "def make_train(config, env):\n",
    "\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    def train(rng):\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        wrapped_env = CTRolloutManager(env, batch_size=config[\"NUM_ENVS\"])\n",
    "        test_env = CTRolloutManager(env, batch_size=config[\"NUM_TEST_EPISODES\"]) # batched env for testing (has different batch size)\n",
    "        init_obs, env_state = wrapped_env.batch_reset(_rng)\n",
    "        init_dones = {agent:jnp.zeros((config[\"NUM_ENVS\"]), dtype=bool) for agent in env.agents+['__all__']}\n",
    "\n",
    "        # INIT BUFFER\n",
    "        # to initalize the buffer is necessary to sample a trajectory to know its strucutre\n",
    "        def _env_sample_step(env_state, unused):\n",
    "            rng, key_a, key_s = jax.random.split(jax.random.PRNGKey(0), 3) # use a dummy rng here\n",
    "            key_a = jax.random.split(key_a, env.num_agents)\n",
    "            actions = {agent: wrapped_env.batch_sample(key_a[i], agent) for i, agent in enumerate(env.agents)}\n",
    "            obs, env_state, rewards, dones, infos = wrapped_env.batch_step(key_s, env_state, actions)\n",
    "            transition = Transition(obs, actions, rewards, dones, infos)\n",
    "            return env_state, transition\n",
    "        _, sample_traj = jax.lax.scan(\n",
    "            _env_sample_step, env_state, None, config[\"NUM_STEPS\"]\n",
    "        )\n",
    "        sample_traj_unbatched = jax.tree_map(lambda x: x[:, 0], sample_traj) # remove the NUM_ENV dim\n",
    "        buffer = fbx.make_trajectory_buffer(\n",
    "            max_length_time_axis=config['BUFFER_SIZE']//config['NUM_ENVS'],\n",
    "            min_length_time_axis=config['BUFFER_BATCH_SIZE'],\n",
    "            sample_batch_size=config['BUFFER_BATCH_SIZE'],\n",
    "            add_batch_size=config['NUM_ENVS'],\n",
    "            sample_sequence_length=1,\n",
    "            period=1,\n",
    "        )\n",
    "        buffer_state = buffer.init(sample_traj_unbatched) \n",
    "\n",
    "        # INIT NETWORK\n",
    "        # init agent\n",
    "        agent = AgentRNN(action_dim=wrapped_env.max_action_space, hidden_dim=config[\"AGENT_HIDDEN_DIM\"], init_scale=config['AGENT_INIT_SCALE'])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        if config[\"PARAMETERS_SHARING\"]:\n",
    "            init_x = (\n",
    "                jnp.zeros((1, 1, wrapped_env.obs_size)), # (time_step, batch_size, obs_size)\n",
    "                jnp.zeros((1, 1)) # (time_step, batch size)\n",
    "            )\n",
    "            init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], 1) # (batch_size, hidden_dim)\n",
    "            agent_params = agent.init(_rng, init_hs, init_x)\n",
    "        else:\n",
    "            init_x = (\n",
    "                jnp.zeros((len(env.agents), 1, 1, wrapped_env.obs_size)), # (time_step, batch_size, obs_size)\n",
    "                jnp.zeros((len(env.agents), 1, 1)) # (time_step, batch size)\n",
    "            )\n",
    "            init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents),  1) # (n_agents, batch_size, hidden_dim)\n",
    "            rngs = jax.random.split(_rng, len(env.agents)) # a random init for each agent\n",
    "            agent_params = jax.vmap(agent.init, in_axes=(0, 0, 0))(rngs, init_hs, init_x)\n",
    "\n",
    "        # init mixer\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros((len(env.agents), 1, 1))\n",
    "        state_size = sample_traj.obs['__all__'].shape[-1]  # get the state shape from the buffer\n",
    "        init_state = jnp.zeros((1, 1, state_size))\n",
    "        mixer = MixingNetwork(config['MIXER_EMBEDDING_DIM'], config[\"MIXER_HYPERNET_HIDDEN_DIM\"], config['MIXER_INIT_SCALE'])\n",
    "        mixer_params = mixer.init(_rng, init_x, init_state)\n",
    "\n",
    "        # init optimizer\n",
    "        network_params = frozen_dict.freeze({'agent':agent_params, 'mixer':mixer_params})\n",
    "        def linear_schedule(count):\n",
    "            frac = 1.0 - (count / config[\"NUM_UPDATES\"])\n",
    "            return config[\"LR\"] * frac\n",
    "        lr = linear_schedule if config.get('LR_LINEAR_DECAY', False) else config['LR']\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adamw(learning_rate=lr, eps=config['EPS_ADAM'], weight_decay=config['WEIGHT_DECAY_ADAM']),#replace with prodigy\n",
    "        )\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=None,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "        # target network params\n",
    "        target_network_params = jax.tree_map(lambda x: jnp.copy(x), train_state.params)\n",
    "        \n",
    "        # INIT EXPLORATION STRATEGY\n",
    "        explorer = EpsilonGreedy(\n",
    "            start_e=config[\"EPSILON_START\"],\n",
    "            end_e=config[\"EPSILON_FINISH\"],\n",
    "            duration=config[\"EPSILON_ANNEAL_TIME\"]\n",
    "        )\n",
    "\n",
    "        # depending if using parameters sharing or not, q-values are computed using one or multiple parameters\n",
    "        if config[\"PARAMETERS_SHARING\"]:\n",
    "            def homogeneous_pass(params, hidden_state, obs, dones):\n",
    "                # concatenate agents and parallel envs to process them in one batch\n",
    "                agents, flatten_agents_obs = zip(*obs.items())\n",
    "                original_shape = flatten_agents_obs[0].shape # assumes obs shape is the same for all agents\n",
    "                batched_input = (\n",
    "                    jnp.concatenate(flatten_agents_obs, axis=1), # (time_step, n_agents*n_envs, obs_size)\n",
    "                    jnp.concatenate([dones[agent] for agent in agents], axis=1), # ensure to not pass other keys (like __all__)\n",
    "                )\n",
    "                hidden_state, q_vals = agent.apply(params, hidden_state, batched_input)\n",
    "                q_vals = q_vals.reshape(original_shape[0], len(agents), *original_shape[1:-1], -1) # (time_steps, n_agents, n_envs, action_dim)\n",
    "                q_vals = {a:q_vals[:,i] for i,a in enumerate(agents)}\n",
    "                return hidden_state, q_vals\n",
    "        else:\n",
    "            def homogeneous_pass(params, hidden_state, obs, dones):\n",
    "                # homogeneous pass vmapped in respect to the agents parameters (i.e., no parameter sharing)\n",
    "                agents, flatten_agents_obs = zip(*obs.items())\n",
    "                batched_input = (\n",
    "                    jnp.stack(flatten_agents_obs, axis=0), # (n_agents, time_step, n_envs, obs_size)\n",
    "                    jnp.stack([dones[agent] for agent in agents], axis=0), # ensure to not pass other keys (like __all__)\n",
    "                )\n",
    "                # computes the q_vals with the params of each agent separately by vmapping\n",
    "                hidden_state, q_vals = jax.vmap(agent.apply, in_axes=0)(params, hidden_state, batched_input)\n",
    "                q_vals = {a:q_vals[i] for i,a in enumerate(agents)}\n",
    "                return hidden_state, q_vals\n",
    "\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "\n",
    "            train_state, target_network_params, env_state, buffer_state, time_state, init_obs, init_dones, test_metrics, rng = runner_state\n",
    "\n",
    "            # EPISODE STEP\n",
    "            def _env_step(step_state, unused):\n",
    "\n",
    "                params, env_state, last_obs, last_dones, hstate, rng, t = step_state\n",
    "\n",
    "                # prepare rngs for actions and step\n",
    "                rng, key_a, key_s = jax.random.split(rng, 3)\n",
    "\n",
    "                # SELECT ACTION\n",
    "                # add a dummy time_step dimension to the agent input\n",
    "                obs_   = {a:last_obs[a] for a in env.agents} # ensure to not pass the global state (obs[\"__all__\"]) to the network\n",
    "                obs_   = jax.tree_map(lambda x: x[jnp.newaxis, :], obs_)\n",
    "                dones_ = jax.tree_map(lambda x: x[jnp.newaxis, :], last_dones)\n",
    "                # get the q_values from the agent network\n",
    "                hstate, q_vals = homogeneous_pass(params, hstate, obs_, dones_)#add in q_tot \n",
    "                # remove the dummy time_step dimension and index qs by the valid actions of each agent \n",
    "                valid_q_vals = jax.tree_util.tree_map(lambda q, valid_idx: q.squeeze(0)[..., valid_idx], q_vals, wrapped_env.valid_actions)\n",
    "                # explore with epsilon greedy_exploration\n",
    "                actions = explorer.choose_actions(valid_q_vals, t, key_a)# \n",
    "\n",
    "                # STEP ENV\n",
    "                obs, env_state, rewards, dones, infos = wrapped_env.batch_step(key_s, env_state, actions)\n",
    "                transition = Transition(last_obs, actions, rewards, dones, infos)\n",
    "\n",
    "                step_state = (params, env_state, obs, dones, hstate, rng, t+1)\n",
    "                return step_state, transition\n",
    "\n",
    "\n",
    "            # prepare the step state and collect the episode trajectory\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            if config[\"PARAMETERS_SHARING\"]:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents)*config[\"NUM_ENVS\"]) # (n_agents*n_envs, hs_size)\n",
    "            else:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents), config[\"NUM_ENVS\"]) # (n_agents, n_envs, hs_size)\n",
    "\n",
    "            step_state = (\n",
    "                train_state.params['agent'],\n",
    "                env_state,\n",
    "                init_obs,\n",
    "                init_dones,\n",
    "                hstate, \n",
    "                _rng,\n",
    "                time_state['timesteps'] # t is needed to compute epsilon\n",
    "            )\n",
    "\n",
    "            step_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, step_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # BUFFER UPDATE: save the collected trajectory in the buffer\n",
    "            buffer_traj_batch = jax.tree_util.tree_map(\n",
    "                lambda x:jnp.swapaxes(x, 0, 1)[:, jnp.newaxis], # put the batch dim first and add a dummy sequence dim\n",
    "                traj_batch\n",
    "            ) # (num_envs, 1, time_steps, ...)\n",
    "            buffer_state = buffer.add(buffer_state, buffer_traj_batch)\n",
    "\n",
    "            # LEARN PHASE\n",
    "            def q_of_action(q, u):\n",
    "                \"\"\"index the q_values with action indices\"\"\"\n",
    "                q_u = jnp.take_along_axis(q, jnp.expand_dims(u, axis=-1), axis=-1)\n",
    "                return jnp.squeeze(q_u, axis=-1)\n",
    "\n",
    "            def _loss_fn(params, target_network_params, init_hstate, learn_traj):\n",
    "\n",
    "                obs_ = {a:learn_traj.obs[a] for a in env.agents} # ensure to not pass the global state (obs[\"__all__\"]) to the network\n",
    "\n",
    "                # compute q_tot with the mixer network.we are reversing the order so that q_tot is computed first\n",
    "                chosen_action_qvals_mix = mixer.apply(\n",
    "                    params['mixer'], \n",
    "                    jnp.stack(list(chosen_action_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][:-1] # avoid last timestep #cartpole obs \n",
    "                )\n",
    "                target_max_qvals_mix = mixer.apply(#\n",
    "                    target_network_params['mixer'], \n",
    "                    jnp.stack(list(target_max_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][1:] # avoid first timestep #this may need to be modified to just cartpole obs\n",
    "                )\n",
    "\n",
    "                _, q_vals = homogeneous_pass(params['agent'], init_hstate, obs_, learn_traj.dones)#add q_tot as an argument \n",
    "                _, target_q_vals = homogeneous_pass(target_network_params['agent'], init_hstate, obs_, learn_traj.dones)# add tq_tot as an arg\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                under the standard approach it is computing individual q values followed by mixer network qvalues\n",
    "\n",
    "                we therefore have chosen to reverse the order \n",
    "\n",
    "                Before we proceed the following questions need to be asked:\n",
    "\n",
    "                Under the standard assumptions the agents need to do RL with exploration in order to find best actions that \n",
    "                will then be used to generate a q_TOT. However no exploration under this standard implementation is done. Q_tot soley\n",
    "                depends on qvalues that are generated through epsilon greedy and standard RL. Q_tot is simply a produced artifact\n",
    "                from the neural network given the individual qvalues. \n",
    "\n",
    "                In our modification both Q_tots are already known. However given the Q_tot, and obs we still need the individual agents \n",
    "                to do standard RL to select and evaluate actions that will give back the Q_tot. Hence epsilon-greedy will remain\n",
    "\n",
    "                bmodels -> first qtot; emodels -> second qtot;  Obs+ qtots+actions* -> \"unmixer\" network -> Individual q values\n",
    "\n",
    "                *full during training partial during execution \n",
    "\n",
    "                unmixer network = scannedrnn -> agentrnn\n",
    "\n",
    "                New arguments for agentrnn: Qtot, obs, other agent actions. \n",
    "\n",
    "                current arguments: obs, action space*\n",
    "\n",
    "                Last major outstanding question: how do we allow each agent to have different policies?\n",
    "\n",
    "                ANS: this implementation works with non-homogenous agents (different obs/action spaces). Also we can turn off parameter sharing. \n",
    "\n",
    "                for the moment this should be sufficient to create groups of different neuronal agents. \n",
    "\n",
    "                the preliminary groupings will be 1: excitatory pol1, 2: inhibitory pol1, 3: excitatory lowest \n",
    "\n",
    "                homogenous pass simply takes and transforms output of agent which in turn takes and transforms output of agentrnn\n",
    "\n",
    "                second note we will eliminate the mixer network and simply apply a minmax scalar transformation to the output of emodels and bmodels. \n",
    "\n",
    "                third note: from the point of view of every given neuronal agent every other agent will be treated as existing in the env. However in practice most\n",
    "                of the neuronal agents will only interact with one another and only the lowest level neuronal agents will get to interact with both the other neuronal agents and the env\n",
    "\n",
    "                Do not implment fourth note for now\n",
    "                fourth note: We need to limit the amount of domain knowledge we directly incorporate into our simulation so we can simulate the brain from first principles\n",
    "                and in particular from the free energy principle. That being said one rule that we will tack on is from the principles of neural design book which talks about \n",
    "                the scaling of the amount of information and the distance the information is sent \n",
    "\n",
    "                we will therefore tack on a regularizer of some kind that will penalize each action taken by the neuronal agents as well as the distance the information is sent. \n",
    "\n",
    "                Final note: We most likely cannot use the trainers withen the emodels, bmodels etc. All of the trainers and such will need to go into this training loop\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "                # get the q_vals of the taken actions (with exploration) for each agent\n",
    "                chosen_action_qvals = jax.tree_map(\n",
    "                    lambda q, u: q_of_action(q, u)[:-1], # avoid last timestep\n",
    "                    q_vals,\n",
    "                    learn_traj.actions\n",
    "                )\n",
    "\n",
    "                # get the target q value of the greedy actions for each agent\n",
    "                valid_q_vals = jax.tree_util.tree_map(lambda q, valid_idx: q[..., valid_idx], q_vals, wrapped_env.valid_actions)\n",
    "                target_max_qvals = jax.tree_map(\n",
    "                    lambda t_q, q: q_of_action(t_q, jnp.argmax(q, axis=-1))[1:], # avoid first timestep\n",
    "                    target_q_vals,\n",
    "                    jax.lax.stop_gradient(valid_q_vals)\n",
    "                )\n",
    "\n",
    "\n",
    "                # compute target\n",
    "                if config.get('TD_LAMBDA_LOSS', True):\n",
    "                    # time difference loss\n",
    "                    def _td_lambda_target(ret, values):\n",
    "                        reward, done, target_qs = values\n",
    "                        ret = jnp.where(\n",
    "                            done,\n",
    "                            target_qs,\n",
    "                            ret*config['TD_LAMBDA']*config['GAMMA']\n",
    "                            + reward\n",
    "                            + (1-config['TD_LAMBDA'])*config['GAMMA']*(1-done)*target_qs\n",
    "                        )\n",
    "                        return ret, ret\n",
    "\n",
    "                    ret = target_max_qvals_mix[-1] * (1-learn_traj.dones['__all__'][-1])\n",
    "                    ret, td_targets = jax.lax.scan(\n",
    "                        _td_lambda_target,\n",
    "                        ret,\n",
    "                        (learn_traj.rewards['__all__'][-2::-1], learn_traj.dones['__all__'][-2::-1], target_max_qvals_mix[-1::-1])\n",
    "                    )\n",
    "                    targets = td_targets[::-1]\n",
    "                    loss = jnp.mean(0.5*((chosen_action_qvals_mix - jax.lax.stop_gradient(targets))**2))\n",
    "                else:\n",
    "                    # standard DQN loss\n",
    "                    targets = (\n",
    "                        learn_traj.rewards['__all__'][:-1]\n",
    "                        + config['GAMMA']*(1-learn_traj.dones['__all__'][:-1])*target_max_qvals_mix\n",
    "                    )\n",
    "                    loss = jnp.mean((chosen_action_qvals_mix - jax.lax.stop_gradient(targets))**2)\n",
    "                \n",
    "                return loss\n",
    "\n",
    "\n",
    "            # sample a batched trajectory from the buffer and set the time step dim in first axis\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            learn_traj = buffer.sample(buffer_state, _rng).experience # (batch_size, 1, max_time_steps, ...)\n",
    "            learn_traj = jax.tree_map(\n",
    "                lambda x: jnp.swapaxes(x[:, 0], 0, 1), # remove the dummy sequence dim (1) and swap batch and temporal dims\n",
    "                learn_traj\n",
    "            ) # (max_time_steps, batch_size, ...)\n",
    "            if config[\"PARAMETERS_SHARING\"]:\n",
    "                init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents)*config[\"BUFFER_BATCH_SIZE\"]) # (n_agents*batch_size, hs_size)\n",
    "            else:\n",
    "                init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents), config[\"BUFFER_BATCH_SIZE\"]) # (n_agents, batch_size, hs_size)\n",
    "\n",
    "            # compute loss and optimize grad\n",
    "            grad_fn = jax.value_and_grad(_loss_fn, has_aux=False)\n",
    "            loss, grads = grad_fn(train_state.params, target_network_params, init_hs, learn_traj)\n",
    "            train_state = train_state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "            # UPDATE THE VARIABLES AND RETURN\n",
    "            # reset the environment\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            init_obs, env_state = wrapped_env.batch_reset(_rng)\n",
    "            init_dones = {agent:jnp.zeros((config[\"NUM_ENVS\"]), dtype=bool) for agent in env.agents+['__all__']}\n",
    "\n",
    "            # update the states\n",
    "            time_state['timesteps'] = step_state[-1]\n",
    "            time_state['updates']   = time_state['updates'] + 1\n",
    "\n",
    "            # update the target network if necessary\n",
    "            target_network_params = jax.lax.cond(\n",
    "                time_state['updates'] % config['TARGET_UPDATE_INTERVAL'] == 0,\n",
    "                lambda _: jax.tree_map(lambda x: jnp.copy(x), train_state.params),\n",
    "                lambda _: target_network_params,\n",
    "                operand=None\n",
    "            )\n",
    "\n",
    "            # update the greedy rewards\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            test_metrics = jax.lax.cond(\n",
    "                time_state['updates'] % (config[\"TEST_INTERVAL\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]) == 0,\n",
    "                lambda _: get_greedy_metrics(_rng, train_state.params['agent'], time_state),\n",
    "                lambda _: test_metrics,\n",
    "                operand=None\n",
    "            )\n",
    "\n",
    "            # update the returning metrics\n",
    "            metrics = {\n",
    "                'timesteps': time_state['timesteps']*config['NUM_ENVS'],\n",
    "                'updates' : time_state['updates'],\n",
    "                'loss': loss,\n",
    "                'rewards': jax.tree_util.tree_map(lambda x: jnp.sum(x, axis=0).mean(), traj_batch.rewards),\n",
    "                #'eps': explorer.get_epsilon(time_state['timesteps'])\n",
    "            }\n",
    "            metrics['test_metrics'] = test_metrics # add the test metrics dictionary\n",
    "\n",
    "            if config.get('WANDB_ONLINE_REPORT', False):\n",
    "                def callback(metrics, infos):\n",
    "                    info_metrics = {\n",
    "                        k:v[...,0][infos[\"returned_episode\"][..., 0]].mean()\n",
    "                        for k,v in infos.items() if k!=\"returned_episode\"\n",
    "                    }\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"returns\": metrics['rewards']['__all__'].mean(),\n",
    "                            \"timestep\": metrics['timesteps'],\n",
    "                            \"updates\": metrics['updates'],\n",
    "                            \"loss\": metrics['loss'],\n",
    "                            'epsilon': metrics['eps'],\n",
    "                            **info_metrics,\n",
    "                            **{k:v.mean() for k, v in metrics['test_metrics'].items()}\n",
    "                        }\n",
    "                    )\n",
    "                jax.debug.callback(callback, metrics, traj_batch.infos)\n",
    "\n",
    "            runner_state = (\n",
    "                train_state,\n",
    "                target_network_params,\n",
    "                env_state,\n",
    "                buffer_state,\n",
    "                time_state,\n",
    "                init_obs,\n",
    "                init_dones,\n",
    "                test_metrics,\n",
    "                rng\n",
    "            )\n",
    "\n",
    "            return runner_state, metrics\n",
    "\n",
    "        def get_greedy_metrics(rng, params, time_state):\n",
    "            \"\"\"Help function to test greedy policy during training\"\"\"\n",
    "            def _greedy_env_step(step_state, unused):\n",
    "                params, env_state, last_obs, last_dones, hstate, rng = step_state\n",
    "                rng, key_s = jax.random.split(rng)\n",
    "                obs_   = {a:last_obs[a] for a in env.agents}\n",
    "                obs_   = jax.tree_map(lambda x: x[jnp.newaxis, :], obs_)\n",
    "                dones_ = jax.tree_map(lambda x: x[jnp.newaxis, :], last_dones)\n",
    "                hstate, q_vals = homogeneous_pass(params, hstate, obs_, dones_)\n",
    "                actions = jax.tree_util.tree_map(lambda q, valid_idx: jnp.argmax(q.squeeze(0)[..., valid_idx], axis=-1), q_vals, test_env.valid_actions)\n",
    "                obs, env_state, rewards, dones, infos = test_env.batch_step(key_s, env_state, actions)\n",
    "                step_state = (params, env_state, obs, dones, hstate, rng)\n",
    "                return step_state, (rewards, dones, infos)\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            init_obs, env_state = test_env.batch_reset(_rng)\n",
    "            init_dones = {agent:jnp.zeros((config[\"NUM_TEST_EPISODES\"]), dtype=bool) for agent in env.agents+['__all__']}\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            if config[\"PARAMETERS_SHARING\"]:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents)*config[\"NUM_TEST_EPISODES\"]) # (n_agents*n_envs, hs_size)\n",
    "            else:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents), config[\"NUM_TEST_EPISODES\"]) # (n_agents, n_envs, hs_size)\n",
    "            step_state = (\n",
    "                params,\n",
    "                env_state,\n",
    "                init_obs,\n",
    "                init_dones,\n",
    "                hstate, \n",
    "                _rng,\n",
    "            )\n",
    "            step_state, (rewards, dones, infos) = jax.lax.scan(\n",
    "                _greedy_env_step, step_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            # compute the metrics of the first episode that is done for each parallel env\n",
    "            def first_episode_returns(rewards, dones):\n",
    "                first_done = jax.lax.select(jnp.argmax(dones)==0., dones.size, jnp.argmax(dones))\n",
    "                first_episode_mask = jnp.where(jnp.arange(dones.size) <= first_done, True, False)\n",
    "                return jnp.where(first_episode_mask, rewards, 0.).sum()\n",
    "            all_dones = dones['__all__']\n",
    "            first_returns = jax.tree_map(lambda r: jax.vmap(first_episode_returns, in_axes=1)(r, all_dones), rewards)\n",
    "            first_infos   = jax.tree_map(lambda i: jax.vmap(first_episode_returns, in_axes=1)(i[..., 0], all_dones), infos)\n",
    "            metrics = {\n",
    "                'test_returns': first_returns['__all__'],# episode returns\n",
    "                **{'test_'+k:v for k,v in first_infos.items()}\n",
    "            }\n",
    "            if config.get('VERBOSE', False):\n",
    "                def callback(timestep, val):\n",
    "                    print(f\"Timestep: {timestep}, return: {val}\")\n",
    "                jax.debug.callback(callback, time_state['timesteps']*config['NUM_ENVS'], first_returns['__all__'].mean())\n",
    "            return metrics\n",
    "        \n",
    "        time_state = {\n",
    "            'timesteps':jnp.array(0),\n",
    "            'updates':  jnp.array(0)\n",
    "        }\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        test_metrics = get_greedy_metrics(_rng, train_state.params['agent'],time_state) # initial greedy metrics\n",
    "\n",
    "        # train\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (\n",
    "            train_state,\n",
    "            target_network_params,\n",
    "            env_state,\n",
    "            buffer_state,\n",
    "            time_state,\n",
    "            init_obs,\n",
    "            init_dones,\n",
    "            test_metrics,\n",
    "            _rng\n",
    "        )\n",
    "        runner_state, metrics = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {'runner_state':runner_state, 'metrics':metrics}\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified env with cartpole and agent goes here \n",
    "\n",
    "import chex\n",
    "from flax import struct\n",
    "import jax\n",
    "from jax import lax\n",
    "import jax.numpy as jnp\n",
    "from gymnax.environments import environment\n",
    "from gymnax.environments import spaces\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class EnvState(environment.EnvState):\n",
    "    x: jnp.ndarray\n",
    "    x_dot: jnp.ndarray\n",
    "    theta: jnp.ndarray\n",
    "    theta_dot: jnp.ndarray\n",
    "    time: int\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class EnvParams(environment.EnvParams):\n",
    "    gravity: float = 9.8\n",
    "    masscart: float = 1.0\n",
    "    masspole: float = 0.1\n",
    "    total_mass: float = 1.0 + 0.1  # (masscart + masspole)\n",
    "    length: float = 0.5\n",
    "    polemass_length: float = 0.05  # (masspole * length)\n",
    "    force_mag: float = 10.0\n",
    "    tau: float = 0.02\n",
    "    theta_threshold_radians: float = 12 * 2 * jnp.pi / 360\n",
    "    x_threshold: float = 2.4\n",
    "    max_steps_in_episode: int = 500  # v0 had only 200 steps!\n",
    "\n",
    "\n",
    "class CartPole(environment.Environment[EnvState, EnvParams]):\n",
    "    \"\"\"JAX Compatible version of CartPole-v1 OpenAI gym environment.\n",
    "\n",
    "\n",
    "    Source: github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.obs_shape = (4,)\n",
    "\n",
    "    @property\n",
    "    def default_params(self) -> EnvParams:\n",
    "        # Default environment parameters for CartPole-v1\n",
    "        return EnvParams()\n",
    "\n",
    "    def step_env(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: EnvState,\n",
    "        action: Union[int, float, chex.Array],\n",
    "        params: EnvParams,\n",
    "    ) -> Tuple[chex.Array, EnvState, jnp.ndarray, jnp.ndarray, Dict[Any, Any]]:\n",
    "        \"\"\"Performs step transitions in the environment.\"\"\"\n",
    "        prev_terminal = self.is_terminal(state, params)\n",
    "        force = params.force_mag * action - params.force_mag * (1 - action)\n",
    "        costheta = jnp.cos(state.theta)\n",
    "        sintheta = jnp.sin(state.theta)\n",
    "\n",
    "        temp = (\n",
    "            force + params.polemass_length * state.theta_dot**2 * sintheta\n",
    "        ) / params.total_mass\n",
    "        thetaacc = (params.gravity * sintheta - costheta * temp) / (\n",
    "            params.length\n",
    "            * (4.0 / 3.0 - params.masspole * costheta**2 / params.total_mass)\n",
    "        )\n",
    "        xacc = temp - params.polemass_length * thetaacc * costheta / params.total_mass\n",
    "\n",
    "        # Only default Euler integration option available here!\n",
    "        x = state.x + params.tau * state.x_dot\n",
    "        x_dot = state.x_dot + params.tau * xacc\n",
    "        theta = state.theta + params.tau * state.theta_dot\n",
    "        theta_dot = state.theta_dot + params.tau * thetaacc\n",
    "\n",
    "        # Important: Reward is based on termination is previous step transition\n",
    "        reward = 1.0 - prev_terminal\n",
    "\n",
    "        # Update state dict and evaluate termination conditions\n",
    "        state = EnvState(\n",
    "            x=x,\n",
    "            x_dot=x_dot,\n",
    "            theta=theta,\n",
    "            theta_dot=theta_dot,\n",
    "            time=state.time + 1,\n",
    "        )\n",
    "        done = self.is_terminal(state, params)\n",
    "\n",
    "        return (\n",
    "            lax.stop_gradient(self.get_obs(state)),\n",
    "            lax.stop_gradient(state),\n",
    "            jnp.array(reward),\n",
    "            done,\n",
    "            {\"discount\": self.discount(state, params)},\n",
    "        )\n",
    "\n",
    "    def reset_env(\n",
    "        self, key: chex.PRNGKey, params: EnvParams\n",
    "    ) -> Tuple[chex.Array, EnvState]:\n",
    "        \"\"\"Performs resetting of environment.\"\"\"\n",
    "        init_state = jax.random.uniform(key, minval=-0.05, maxval=0.05, shape=(4,))\n",
    "        state = EnvState(\n",
    "            x=init_state[0],\n",
    "            x_dot=init_state[1],\n",
    "            theta=init_state[2],\n",
    "            theta_dot=init_state[3],\n",
    "            time=0,\n",
    "        )\n",
    "        return self.get_obs(state), state\n",
    "\n",
    "    def get_obs(self, state: EnvState, params=None, key=None) -> chex.Array:\n",
    "        \"\"\"Applies observation function to state.\"\"\"\n",
    "        return jnp.array([state.x, state.x_dot, state.theta, state.theta_dot])\n",
    "\n",
    "    def is_terminal(self, state: EnvState, params: EnvParams) -> jnp.ndarray:\n",
    "        \"\"\"Check whether state is terminal.\"\"\"\n",
    "        # Check termination criteria\n",
    "        done1 = jnp.logical_or(\n",
    "            state.x < -params.x_threshold,\n",
    "            state.x > params.x_threshold,\n",
    "        )\n",
    "        done2 = jnp.logical_or(\n",
    "            state.theta < -params.theta_threshold_radians,\n",
    "            state.theta > params.theta_threshold_radians,\n",
    "        )\n",
    "\n",
    "        # Check number of steps in episode termination condition\n",
    "        done_steps = state.time >= params.max_steps_in_episode\n",
    "        done = jnp.logical_or(jnp.logical_or(done1, done2), done_steps)\n",
    "        return done\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Environment name.\"\"\"\n",
    "        return \"CartPole-v1\"\n",
    "\n",
    "    @property\n",
    "    def num_actions(self) -> int:\n",
    "        \"\"\"Number of actions possible in environment.\"\"\"\n",
    "        return 2\n",
    "\n",
    "    def action_space(self, params: Optional[EnvParams] = None) -> spaces.Discrete:\n",
    "        \"\"\"Action space of the environment.\"\"\"\n",
    "        return spaces.Discrete(2)\n",
    "\n",
    "    def observation_space(self, params: EnvParams) -> spaces.Box:\n",
    "        \"\"\"Observation space of the environment.\"\"\"\n",
    "        high = jnp.array(\n",
    "            [\n",
    "                params.x_threshold * 2,\n",
    "                jnp.finfo(jnp.float32).max,\n",
    "                params.theta_threshold_radians * 2,\n",
    "                jnp.finfo(jnp.float32).max,\n",
    "            ]\n",
    "        )\n",
    "        return spaces.Box(-high, high, (4,), dtype=jnp.float32)\n",
    "\n",
    "    def state_space(self, params: EnvParams) -> spaces.Dict:\n",
    "        \"\"\"State space of the environment.\"\"\"\n",
    "        high = jnp.array(\n",
    "            [\n",
    "                params.x_threshold * 2,\n",
    "                jnp.finfo(jnp.float32).max,\n",
    "                params.theta_threshold_radians * 2,\n",
    "                jnp.finfo(jnp.float32).max,\n",
    "            ]\n",
    "        )\n",
    "        return spaces.Dict(\n",
    "            {\n",
    "                \"x\": spaces.Box(-high[0], high[0], (), jnp.float32),\n",
    "                \"x_dot\": spaces.Box(-high[1], high[1], (), jnp.float32),\n",
    "                \"theta\": spaces.Box(-high[2], high[2], (), jnp.float32),\n",
    "                \"theta_dot\": spaces.Box(-high[3], high[3], (), jnp.float32),\n",
    "                \"time\": spaces.Discrete(params.max_steps_in_episode),\n",
    "            }\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify later \n",
    "config = {\n",
    "    \"NUM_ENVS\": 8,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"BUFFER_SIZE\": 3000,\n",
    "    \"BUFFER_BATCH_SIZE\": 32,\n",
    "    \"TOTAL_TIMESTEPS\": 20000000,\n",
    "    \"AGENT_HIDDEN_DIM\": 256,\n",
    "    \"AGENT_INIT_SCALE\": 1.,\n",
    "    \"PARAMETERS_SHARING\": True,\n",
    "    \"EPSILON_START\": 1.0,\n",
    "    \"EPSILON_FINISH\": 0.05,\n",
    "    \"EPSILON_ANNEAL_TIME\": 100000,\n",
    "    \"MIXER_EMBEDDING_DIM\": 64,\n",
    "    \"MIXER_HYPERNET_HIDDEN_DIM\": 256,\n",
    "    \"MIXER_INIT_SCALE\": 0.001,\n",
    "    \"MAX_GRAD_NORM\": 10,\n",
    "    \"TARGET_UPDATE_INTERVAL\": 200,\n",
    "    #\"LR\": 0.001\n",
    "    #\"LR_LINEAR_DECAY\": False #the primary optmizer will not use a learning rate\n",
    "    #\"EPS_ADAM\": 0.00001\n",
    "    #\"WEIGHT_DECAY_ADAM\": 0.000001\n",
    "    \"TD_LAMBDA_LOSS\": False,\n",
    "    \"TD_LAMBDA\": 0.6,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"VERBOSE\": False,\n",
    "    \"WANDB_ONLINE_REPORT\": True,\n",
    "    \"NUM_TEST_EPISODES\": 32,\n",
    "    \"TEST_INTERVAL\": 100000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(version_base=None, config_path=\"./config\", config_name=\"config\")\n",
    "def main(config):\n",
    "    config = OmegaConf.to_container(config)\n",
    "\n",
    "    print('Config:\\n', OmegaConf.to_yaml(config))\n",
    "\n",
    "    env_name = config[\"env\"][\"ENV_NAME\"]\n",
    "    alg_name = f'qmix_{\"ps\" if config[\"alg\"].get(\"PARAMETERS_SHARING\", True) else \"ns\"}'\n",
    "\n",
    "    env = make(config[\"env\"][\"ENV_NAME\"], **config['env']['ENV_KWARGS'])\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    \n",
    "    # smac init neeeds a scenario\n",
    "    \"\"\"\n",
    "    if 'smax' in env_name.lower():\n",
    "        config['env']['ENV_KWARGS']['scenario'] = map_name_to_scenario(config['env']['MAP_NAME'])\n",
    "        env_name = f\"{config['env']['ENV_NAME']}_{config['env']['MAP_NAME']}\"\n",
    "        env = make(config[\"env\"][\"ENV_NAME\"], **config['env']['ENV_KWARGS'])\n",
    "        env = SMAXLogWrapper(env)\n",
    "    else:\n",
    "        env = make(config[\"env\"][\"ENV_NAME\"], **config['env']['ENV_KWARGS'])\n",
    "        env = LogWrapper(env)\n",
    "\n",
    "    config[\"alg\"][\"NUM_STEPS\"] = config[\"alg\"].get(\"NUM_STEPS\", env.max_steps) # default steps defined by the env\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    we also need to implement a sherpa-parameter hyperparameter optimization with population based search and eliminate wanb.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    wandb.init(\n",
    "        entity=config[\"ENTITY\"],\n",
    "        project=config[\"PROJECT\"],\n",
    "        tags=[\n",
    "            alg_name.upper(),\n",
    "            env_name.upper(),\n",
    "            \"RNN\",\n",
    "            \"TD_LOSS\" if config[\"alg\"].get(\"TD_LAMBDA_LOSS\", True) else \"DQN_LOSS\",\n",
    "            f\"jax_{jax.__version__}\",\n",
    "        ],\n",
    "        name=f'{alg_name}_{env_name}',\n",
    "        config=config,\n",
    "        mode=config[\"WANDB_MODE\"],\n",
    "    )\n",
    "    \n",
    "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "    rngs = jax.random.split(rng, config[\"NUM_SEEDS\"])\n",
    "    train_vjit = jax.jit(jax.vmap(make_train(config[\"alg\"], env)))\n",
    "    outs = jax.block_until_ready(train_vjit(rngs))\n",
    "    \n",
    "    # save params\n",
    "    if config['SAVE_PATH'] is not None:\n",
    "\n",
    "        def save_params(params: Dict, filename: Union[str, os.PathLike]) -> None:\n",
    "            flattened_dict = flatten_dict(params, sep=',')\n",
    "            save_file(flattened_dict, filename)\n",
    "\n",
    "        model_state = outs['runner_state'][0]\n",
    "        params = jax.tree_map(lambda x: x[0], model_state.params) # save only params of the firt run\n",
    "        save_dir = os.path.join(config['SAVE_PATH'], env_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_params(params, f'{save_dir}/{alg_name}.safetensors')\n",
    "        print(f'Parameters of first batch saved in {save_dir}/{alg_name}.safetensors')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the def _loss_fn from the jaxmarl qmix from the class make_train. We have to fogure out how to get the losses of swap out agent and target networks for bmodels and emodels\n",
    "\n",
    "also in pegasus we do not use epsilon greedy or any other explorer because it is automatically handled by bmodels and optimization of expected free energy. we will have to figure out how to modify\n",
    "the jaxmarl implementation and remove the epsiolon greedy explorer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "            def _loss_fn(params, target_network_params, init_hstate, learn_traj):\n",
    "\n",
    "                obs_ = {a:learn_traj.obs[a] for a in env.agents} # ensure to not pass the global state (obs[\"__all__\"]) to the network\n",
    "                _, q_vals = homogeneous_pass(params['agent'], init_hstate, obs_, learn_traj.dones)\n",
    "                _, target_q_vals = homogeneous_pass(target_network_params['agent'], init_hstate, obs_, learn_traj.dones)\n",
    "\n",
    "                # get the q_vals of the taken actions (with exploration) for each agent\n",
    "                chosen_action_qvals = jax.tree_map(\n",
    "                    lambda q, u: q_of_action(q, u)[:-1], # avoid last timestep\n",
    "                    q_vals,\n",
    "                    learn_traj.actions\n",
    "                )\n",
    "\n",
    "                # get the target q value of the greedy actions for each agent\n",
    "                valid_q_vals = jax.tree_util.tree_map(lambda q, valid_idx: q[..., valid_idx], q_vals, wrapped_env.valid_actions)\n",
    "                target_max_qvals = jax.tree_map(\n",
    "                    lambda t_q, q: q_of_action(t_q, jnp.argmax(q, axis=-1))[1:], # avoid first timestep\n",
    "                    target_q_vals,\n",
    "                    jax.lax.stop_gradient(valid_q_vals)\n",
    "                )\n",
    "\n",
    "                # compute q_tot with the mixer network\n",
    "                chosen_action_qvals_mix = mixer.apply(\n",
    "                    params['mixer'], \n",
    "                    jnp.stack(list(chosen_action_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][:-1] # avoid last timestep\n",
    "                )\n",
    "                target_max_qvals_mix = mixer.apply(\n",
    "                    target_network_params['mixer'], \n",
    "                    jnp.stack(list(target_max_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][1:] # avoid first timestep\n",
    "                )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reinforcement learning is computationally very intensive. We are therefore going to carry out a partial simulation with a smaller number of agents and then use maximum entropy maximization to infer the behavior of the rest of the agents. \n",
    "\n",
    "we can use another aspect of max entropy to test various hypotheses and yet another aspect to detect motifs or patterns in the simulation\n",
    "\n",
    "This will be implemented in phase 2 when we increase the definition of our simulation\n",
    "\n",
    "each resolution level will have a small number of rl agents. We will then use max entropy to infer the full graph strucutre including the other neuronal agents, their actions and the \n",
    "which agents recieve the sent action potentials \n",
    "\n",
    "question: is it possible to infer the optimal number of agents needed to infer the full graph structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will be particulary useful in downscaling: creating agents in finer and smaller resolutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one of the constraints we can impose is one of self-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from NEMtropy import UndirectedGraph, matrix_generator\n",
    "from NEMtropy.network_functions import build_adjacency_from_edgelist\n",
    "\n",
    "import networkx as nx\n",
    "from NEMtropy import UndirectedGraph\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "adj_kar = nx.to_numpy_array(G)\n",
    "graph = UndirectedGraph(adj_kar)\n",
    "\n",
    "#generating null models\n",
    "graph.solve_tool(model=\"cm_exp\",\n",
    "                 method=\"fixed_point\",\n",
    "                 initial_guess=\"random\")\n",
    "\n",
    "# After that we have solved the model we can use it to generate random versions of zachary karate club.\n",
    "# The ensemble sampler function generates \"n\" random versions of zachary karate club using the model parameters computed by solve_tool.\n",
    "\n",
    "graph.ensemble_sampler(10, cpu_n=12, output_dir=\"sample/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "\n",
    "#policy proposer algorithm\n",
    "\n",
    "#the simplest way to do this is with some kind of imitation learning of fmri data \n",
    "\n",
    "#two reinforcement learning algorithms will be stacked on top of each other:\n",
    "\n",
    "#the first one is going to decide whether to create a new agent that are dictated by a policy and how many of those agents to create\n",
    "\n",
    "#note: the agent can choose to create a negative number of agents consisting of an existing class of agents. Basically allowing for reducing number of agents\n",
    "\n",
    "#the second is going to train the agents of each policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from juliacall import Main as jl, convert as jlconvert\n",
    "#julia_module1 = pyjulia.Pyjulia(\"C://Users//subar//.julia//packages//CUDA//35NC6//lib//cutensor//test//base.jl\")\n",
    "#jul2 = pyjulia.Pyjulia(\"C://Users//subar//.julia//packages//NeuralPDE//OPjbo//src//NeuralPDE.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl.seval(\"using Pkg\")\n",
    "jl.seval('Pkg.add(\"Starlight\")')\n",
    "#jl.seval(\"using Starlight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jl.seval(\"using Starlight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl.seval(\"using Flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Info: Precompiling TMLE [8afdd2fb-6e73-43df-8b62-b1650cd9c8cf]\n",
      "[ Info: Precompiling CategoricalArraysStructTypesExt [e0b1e8f7-f9d4-55ed-aa4c-3cb4a66b5cfe]\n"
     ]
    }
   ],
   "source": [
    "jl.seval(\"using TMLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = jl.Chain(\n",
    "    jl.Dense(1, 10, jl.relu),\n",
    "    jl.Dense(10, 10, jl.relu),\n",
    "    jl.Dense(10, 10, jl.relu),\n",
    "    jl.Dense(10, 1),\n",
    ")\n",
    "loss = jl.seval(\"m -> (x, y) -> Flux.Losses.mse(m(x), y)\")(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "#microbial ecologies section This is the section that will be used to establish a lower bound on how many subtypes there are"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
