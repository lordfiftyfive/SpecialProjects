{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d047f18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#from ray.rllib.utils.torch_utils import \n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8aa2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "#from nebulgym.decorators.torch_decorators import accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa29d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431cb273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "import logging\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b86103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nitime\n",
    "from deeptime.sindy import SINDy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed=111)\n",
    "#causal inference libraries\n",
    "from timeawarepc.tpc import cfc_tpc, cfc_pc, cfc_gc\n",
    "#from timeawarepc.simulate_data import *\n",
    "#from timeawarepc.find_cfc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c11181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf\n"
     ]
    }
   ],
   "source": [
    "print(\"asdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd6fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "        # Place hidden states on same device as model.\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "    #accelerate_model()# this will accelerate the model \n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, hidden_state, seq_lens):\n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)\n",
    "        return q, [h]\n",
    "\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n",
    "#model = RNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ab65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4f9c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "class bmodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "        # Place hidden states on same device as model.\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    #@override(ModelV2)\n",
    "    def _forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)\n",
    "        return q, [h]\n",
    "\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cc27053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CModel(torch.nn.Module,ivy.Module):\n",
    "    def __init__(self):#,obs_space,action_space,num_outputs,model_config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        \"\"\"\n",
    "        self._avg_pool = torch.nn.AvgPool2d(4)\n",
    "        self._linear = torch.nn.Linear(3136, 1024)\n",
    "        self._relu = torch.nn.ReLU()\n",
    "        #self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        #self.obs_size = _get_size(obs_space)\n",
    "        #self.n_agents = model_config[\"n_agents\"]\n",
    "        \n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "    #@override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "        # Place hidden states on same device as model.\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "            ]\n",
    "    #@override(ModelV2)\n",
    "    def _forward(self, x):\n",
    "\n",
    "        x = self._avg_pool(x).mean(dim=-3).view(-1, 3136)\n",
    "        x = self._relu(self._linear(x))\n",
    "        return self._linears(x)\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1155747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ivy\\framework_handler.py:115: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if isinstance(specific_v, collections.Hashable):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class RNNModel with abstract method _forward",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11908\\2657232723.py\"\u001b[1;36m, line \u001b[1;32m167\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    model = RNNModel()#note that this particular line compiles if we get rid of parameters. at initialization we will call this with parameters\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m\u001b[1;31m:\u001b[0m Can't instantiate abstract class RNNModel with abstract method _forward\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow.compat.v2 as tf\n",
    "#tf.enable_v2_behavior()\n",
    "num_inducing_points=1\n",
    "#this is the agent network that will be used \n",
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "input_shape = [0,0]\n",
    "from numba import jit\n",
    "#@accelerate_model()\n",
    "\n",
    "class RNNModel(TorchModelV2,nn.Module,ivy.Module):\n",
    "  \n",
    "  def __init__(self):#,obs_space, action_space, num_outputs, model_config, name): #add this in later\n",
    "    \n",
    "    #self.obs_size = _get_size(obs_space)\n",
    "    #self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "\n",
    "    \"\"\"\n",
    "    what do we want our VAE to do?\n",
    "\n",
    "    our vae should: \n",
    "    1. infer a hidden state and \n",
    "    2. output an action   `\n",
    "\n",
    "    \"\"\"\n",
    "    tfk = tf.keras\n",
    "    tfkl = tf.keras.layers\n",
    "    \n",
    "    tfpl = tfp.layers\n",
    "    tfd = tfp.distributions\n",
    "\n",
    "    tfd = tfp.distributions\n",
    "    encoded_size = 16\n",
    "    \"\"\"\n",
    "    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),#This scale variable cannot be fixed but instead must be allowed to vary \n",
    "                                reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    The prior should be some kind of composite or dict type distribution with multiple dirchlet distributions. \n",
    "\n",
    "\n",
    "    \n",
    "    #note: the tfd Independent is for assumptions which are independent and unaffected or related to other assumptions. These distribtutions may need to be hand tuned based on domain knowledge and data analysis\n",
    "    #second note: this will be the top level prior. we will have to figure out a way to ensure that this prior is only inherited from posteriors of above signals\n",
    "\n",
    "    #solution: create an algorithm that tunes parameters of distribution and have it default to hand-written prior parameters if upper level observation space is empty and epoch is 0. \n",
    "    \n",
    "    the last layer needs to be a discrete (bernoulli) distribution layer for the high and midlevel layers and a continuous distribution for the lowest layer \n",
    "\n",
    "    dist = tfd.Normal(loc=0.5, scale=0.25). Note: we may want to consider using the normal for all agents and just discretize for higher level agents. \n",
    "\n",
    "    this is going to be where we consider things like lateral disinhibition where the prior pertaining to the probability carrying out an action is going to decrease if it has already carried out an action under certain circumstances\n",
    "    \n",
    "    Note: we are going to replace almost all the code in the init with the compiled model laplace CModel we have above\n",
    "    \"\"\"\n",
    "    #encoded_shape = 2\n",
    "    num_schools=2\n",
    "    \"\"\"\n",
    "    prior  = tfd.JointDistributionSequential([\n",
    "        tfd.Normal(loc=0., scale=10., name=\"avg_effect\"),  # `mu` above\n",
    "        tfd.Dirichlet(2),\n",
    "        tfd.Normal(loc=5., scale=1., name=\"avg_stddev\"),  # `log(tau)` above\n",
    "        tfd.Independent(tfd.Normal(loc=tf.zeros(num_schools),\n",
    "                                  scale=tf.ones(num_schools),\n",
    "                                  name=\"school_effects_standard\"),  # `theta_prime` \n",
    "                        reinterpreted_batch_ndims=1)\n",
    "    ])\n",
    "    \"\"\"\n",
    "    #prior= tfd.Normal(loc=0., scale=10., name=\"avg_effect\")\n",
    "    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                        reinterpreted_batch_ndims=1)\n",
    "    tfpl = tfp.layers\n",
    "    encoder = tfk.Sequential([\n",
    "        tfkl.InputLayer(input_shape=input_shape),\n",
    "        #tfkl.Dense(8)\n",
    "        tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                      activation=None),\n",
    "\n",
    "        tfpl.MultivariateNormalTriL(\n",
    "                encoded_size,\n",
    "                activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "    ])\n",
    "    decoder = tfk.Sequential([\n",
    "        tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "        tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "        #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "    ])\n",
    "    negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "    \"\"\"\n",
    "    #working theory as of 11/26/2022\n",
    "    \n",
    "    in order to convert from posterior -> action potential -> prior we need information theory specifically \n",
    "\n",
    "    probability of excitatory action potential = -log2(p(w)) if P(w) = 0.5 probability of excitatory action potential is 1\n",
    "\n",
    "    furthermore a prediction of w being 0.5 also yields a prediction of not(w) also being 0.5 \n",
    "\n",
    "    this is where I suspect inhibitory potentials come in since not(w) is expressed as log2(p(w)) which of 0.5 is -1 \n",
    "\n",
    "    we will probably do this translation from the observation space and action space\n",
    "\n",
    "    each action will actually correspond to a series of a 100 steps\n",
    "\n",
    "    if the algorithm tries to send a signal of 0.2 bits to a particular neuronal agent we will send \n",
    "    20 excitatory action potentials and 80 0s over 100 steps  or to put it another way we will say that 0.2 \n",
    "\n",
    "    is just the probability of it sending a excitatory potential at any given time \n",
    "\n",
    "    we will need to do an if else thing\n",
    "\n",
    "    if neurons is top neuron: \n",
    "      prior = <hand coded prior>\n",
    "    else: \n",
    "      if obs == 1:\n",
    "        prob = obs**2 #where obs will be the observed frequency over n previously observed values\n",
    "      if obs == -1:\n",
    "        prob = obs**2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    uncomment out later\n",
    "\n",
    "    loss = lambda y, rv_y: rv_y.variational_loss(\n",
    "        y, kl_weight=np.array(batch_size, tf.float64) / x.shape[0])\n",
    "    #tf.keras.optimizers.Adam(1e-4) tf.optimizers.Adam(learning_rate=0.011)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.011), loss=loss)#tf.optimizers.Adam(learning_rate=0.01)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    we are choosing to use a variational autoencoder because there are two things that need to be done: make predictions and select action that minimizes free energy \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    decoder = tfk.Sequential([\n",
    "        tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "        #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits),\n",
    "        ])\n",
    "    vae = tfk.Model(inputs=encoder.inputs,\n",
    "                        outputs=decoder(encoder.outputs[0]))\n",
    "    \"\"\"\n",
    "    ivy.Module.__init__(self)\n",
    "    \n",
    "  #@override(ModelV2)\n",
    "    @jit(nopython=True)\n",
    "    def get_initial_state(self):\n",
    "        # Place hidden states on same device as model.\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "            ]\n",
    "    \n",
    "    #\n",
    "    #@jit(nopython=True)\n",
    "    #@override(ModelV2)\n",
    "    def _forward(self, input_dict, hidden_state, seq_lens):# _forward\n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)#this may be the q-value \n",
    "        return q, [h]\n",
    "\n",
    "  \n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n",
    "\n",
    "ivy.set_framework('torch') \n",
    "model = RNNModel()#note that this particular line compiles if we get rid of parameters. at initialization we will call this with parameters \n",
    "#model = Laplace(model, 'regression', subset_of_weights='all', hessian_structure='full')\n",
    "\n",
    "\"\"\"\n",
    "optimized_model = optimize_model(\n",
    "    model, input_data=input_data, optimization_time=\"constrained\"\n",
    ")\n",
    "note: under current code minimization of kullbacker-leibler divergence is already part of the code in the form the KLD divergence regularizer which means the loss simply needs to be the \"suprise\" \n",
    "\n",
    "w\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847bf1b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'obs_space', 'action_space', 'num_outputs', 'model_config', and 'name'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11908\\3601144890.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    model = bmodel()#CModel()#CModel()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m\u001b[1;31m:\u001b[0m __init__() missing 5 required positional arguments: 'obs_space', 'action_space', 'num_outputs', 'model_config', and 'name'\n"
     ]
    }
   ],
   "source": [
    "#ivy.set_framework('torch') \n",
    "model = bmodel()#CModel()#CModel()\n",
    "cmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#all\n",
    "\n",
    "#this compiles under if modelV2 is not passed wiht any arguments. It demands arguments when modelV2 is passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a34b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<laplace.lllaplace.FullLLLaplace object at 0x0000018D5F20B1C0>\n"
     ]
    }
   ],
   "source": [
    "print(cmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021fca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put this code into \n",
    "\n",
    "model = optimize_model(\n",
    "  cmodel, input_data=input_data, device=\"gpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3739ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b004e9c",
   "metadata": {},
   "source": [
    "As of january 13 2022 we have performed a sucessful partial execution test of the project pegasus agent \n",
    "network\n",
    "\n",
    "We still have to determine the following:\n",
    "1. how to create heterogenuous agents\n",
    "2. how to visualize using pysurfer\n",
    "3. figure out what if any alterations will be needed for the mixer network \n",
    "\n",
    "ANS for 3: we should perhaps think of the mixer network as maximizing the overall free energy and \n",
    "acting on the level of active inference instead of predictive coding. \n",
    "\n",
    "we are going to use laplace-torch for among other things model selection\n",
    "\n",
    "This means that after we finish pretraining our RL algorithm on normal conditions we will fine tune and select simulated models generated by our RL algorithm by how well they fit the frieburg data. \n",
    "\n",
    "we are actually only going to have 1 env. I dont think we need the brain env only the humanoid env\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44b2b8de",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<class 'gymnasium.spaces.dict.Dict'> is not a generic class",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11908\\1207131118.py\"\u001b[0m, line \u001b[0;32m141\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    class QMixTorchPolicy(TorchPolicy):\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11908\\1207131118.py\"\u001b[0m, line \u001b[0;32m267\u001b[0m, in \u001b[0;35mQMixTorchPolicy\u001b[0m\n    input_dict: Dict[str, TensorType],\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\typing.py\"\u001b[0m, line \u001b[0;32m277\u001b[0m, in \u001b[0;35minner\u001b[0m\n    return func(*args, **kwds)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\typing.py\"\u001b[0m, line \u001b[0;32m1004\u001b[0m, in \u001b[0;35m__class_getitem__\u001b[0m\n    _check_generic(cls, params, len(cls.__parameters__))\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\typing_extensions.py\"\u001b[1;36m, line \u001b[1;32m94\u001b[1;36m, in \u001b[1;35m_check_generic\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise TypeError(f\"{cls} is not a generic class\")\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m\u001b[1;31m:\u001b[0m <class 'gymnasium.spaces.dict.Dict'> is not a generic class\n"
     ]
    }
   ],
   "source": [
    "# Torch must be installed.\n",
    "#torch, nn = try_import_torch(error=True)\n",
    "\n",
    "#logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class QMixLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        target_model,\n",
    "        mixer,\n",
    "        target_mixer,\n",
    "        n_agents,\n",
    "        n_actions,\n",
    "        double_q=True,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.mixer = mixer\n",
    "        self.target_mixer = target_mixer\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        self.double_q = double_q\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rewards,\n",
    "        actions,\n",
    "        terminated,\n",
    "        mask,\n",
    "        obs,\n",
    "        next_obs,\n",
    "        action_mask,\n",
    "        next_action_mask,\n",
    "        state=None,\n",
    "        next_state=None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the loss.\n",
    "        Args:\n",
    "            rewards: Tensor of shape [B, T, n_agents]\n",
    "            actions: Tensor of shape [B, T, n_agents]\n",
    "            terminated: Tensor of shape [B, T, n_agents]\n",
    "            mask: Tensor of shape [B, T, n_agents]\n",
    "            obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            next_obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            next_action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            state: Tensor of shape [B, T, state_dim] (optional)\n",
    "            next_state: Tensor of shape [B, T, state_dim] (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert either none or both of state and next_state are given\n",
    "        if state is None and next_state is None:\n",
    "            state = obs  # default to state being all agents' observations\n",
    "            next_state = next_obs\n",
    "        elif (state is None) != (next_state is None):\n",
    "            raise ValueError(\n",
    "                \"Expected either neither or both of `state` and \"\n",
    "                \"`next_state` to be given. Got: \"\n",
    "                \"\\n`state` = {}\\n`next_state` = {}\".format(state, next_state)\n",
    "            )\n",
    "\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = _unroll_mac(self.model, obs)\n",
    "\n",
    "        # Pick the Q-Values for the actions taken -> [B * n_agents, T]\n",
    "        chosen_action_qvals = torch.gather(\n",
    "            mac_out, dim=3, index=actions.unsqueeze(3)\n",
    "        ).squeeze(3)\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = _unroll_mac(self.target_model, next_obs)\n",
    "\n",
    "        # Mask out unavailable actions for the t+1 step\n",
    "        ignore_action_tp1 = (next_action_mask == 0) & (mask == 1).unsqueeze(-1)\n",
    "        target_mac_out[ignore_action_tp1] = -np.inf\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.double_q:\n",
    "            # Double Q learning computes the target Q values by selecting the\n",
    "            # t+1 timestep action according to the \"policy\" neural network and\n",
    "            # then estimating the Q-value of that action with the \"target\"\n",
    "            # neural network\n",
    "            \"\"\"\n",
    "            note that the target neural network uses the RNN network defined above and that \n",
    "\n",
    "            policy neural network also appears to be the mixer network \n",
    "\n",
    "            double q-learning will find 2 q values. One will be used in our case to identify an action that minimizes free energy while another \n",
    "            will be used to evaluate that action \n",
    "            \"\"\"\n",
    "\n",
    "            # Compute the t+1 Q-values to be used in action selection\n",
    "            # using next_obs\n",
    "            mac_out_tp1 = _unroll_mac(self.model, next_obs)\n",
    "\n",
    "            # mask out unallowed actions\n",
    "            mac_out_tp1[ignore_action_tp1] = -np.inf\n",
    "\n",
    "            # obtain best actions at t+1 according to policy NN\n",
    "            cur_max_actions = mac_out_tp1.argmax(dim=3, keepdim=True)\n",
    "\n",
    "            # use the target network to estimate the Q-values of policy\n",
    "            # network's selected actions\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(\n",
    "                3\n",
    "            )\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "        assert (\n",
    "            target_max_qvals.min().item() != -np.inf\n",
    "        ), \"target_max_qvals contains a masked action; \\\n",
    "            there may be a state with no valid actions.\"\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state)\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, next_state)\n",
    "\n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n",
    "\n",
    "        # Td-error\n",
    "        td_error = chosen_action_qvals - targets.detach()\n",
    "\n",
    "        mask = mask.expand_as(td_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = td_error * mask\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        loss = (masked_td_error ** 2).sum() / mask.sum()\n",
    "        return loss, mask, masked_td_error, chosen_action_qvals, targets\n",
    "\n",
    "\n",
    "class QMixTorchPolicy(TorchPolicy):\n",
    "    \"\"\"QMix impl. Assumes homogeneous agents for now.\n",
    "    You must use MultiAgentEnv.with_agent_groups() to group agents\n",
    "    together for QMix. This creates the proper Tuple obs/action spaces and\n",
    "    populates the '_group_rewards' info field.\n",
    "    Action masking: to specify an action mask for individual agents, use a\n",
    "    dict space with an action_mask key, e.g. {\"obs\": ob, \"action_mask\": mask}.\n",
    "    The mask space must be `Box(0, 1, (n_actions,))`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        _validate(obs_space, action_space)\n",
    "        config = dict(ray.rllib.algorithms.qmix.qmix.DEFAULT_CONFIG, **config)\n",
    "        self.framework = \"torch\"\n",
    "\n",
    "        self.n_agents = len(obs_space.original_space.spaces)\n",
    "        config[\"model\"][\"n_agents\"] = self.n_agents\n",
    "        self.n_actions = action_space.spaces[0].n\n",
    "        self.h_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "        self.has_env_global_state = False\n",
    "        self.has_action_mask = False\n",
    "\n",
    "        agent_obs_space = obs_space.original_space.spaces[0]\n",
    "        if isinstance(agent_obs_space, gym.spaces.Dict):\n",
    "            space_keys = set(agent_obs_space.spaces.keys())\n",
    "            if \"obs\" not in space_keys:\n",
    "                raise ValueError(\"Dict obs space must have subspace labeled `obs`\")\n",
    "            self.obs_size = _get_size(agent_obs_space.spaces[\"obs\"])\n",
    "            if \"action_mask\" in space_keys:\n",
    "                mask_shape = tuple(agent_obs_space.spaces[\"action_mask\"].shape)\n",
    "                if mask_shape != (self.n_actions,):\n",
    "                    raise ValueError(\n",
    "                        \"Action mask shape must be {}, got {}\".format(\n",
    "                            (self.n_actions,), mask_shape\n",
    "                        )\n",
    "                    )\n",
    "                self.has_action_mask = True\n",
    "            if ENV_STATE in space_keys:\n",
    "                self.env_global_state_shape = _get_size(\n",
    "                    agent_obs_space.spaces[ENV_STATE]\n",
    "                )\n",
    "                self.has_env_global_state = True\n",
    "            else:\n",
    "                self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            # The real agent obs space is nested inside the dict\n",
    "            config[\"model\"][\"full_obs_space\"] = agent_obs_space\n",
    "            agent_obs_space = agent_obs_space.spaces[\"obs\"]\n",
    "        else:\n",
    "            self.obs_size = _get_size(agent_obs_space)\n",
    "            self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            \n",
    "            \n",
    "        ivy.set_framework('torch') \n",
    "        model = bmodel()#CModel()#CModel()\n",
    "        cmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#all\n",
    "        \n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"model\",\n",
    "            default_model=cmodel#RNNModel,\n",
    "        )\n",
    "\n",
    "        super().__init__(obs_space, action_space, config, model=self.model)\n",
    "\n",
    "        self.target_model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"target_model\",\n",
    "            default_model=cmodel#CModel#RNNModel,#this target model is the agent model \n",
    "        ).to(self.device)\n",
    "\n",
    "        self.exploration = self._create_exploration()\n",
    "\n",
    "        # Setup the mixer network.\n",
    "        if config[\"mixer\"] is None:\n",
    "            self.mixer = None\n",
    "            self.target_mixer = None\n",
    "        elif config[\"mixer\"] == \"qmix\":\n",
    "            self.mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "            self.target_mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "        elif config[\"mixer\"] == \"vdn\":\n",
    "            self.mixer = VDNMixer().to(self.device)\n",
    "            self.target_mixer = VDNMixer().to(self.device)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown mixer type {}\".format(config[\"mixer\"]))\n",
    "\n",
    "        self.cur_epsilon = 1.0\n",
    "        self.update_target()  # initial sync\n",
    "\n",
    "        # Setup optimizer\n",
    "        self.params = list(self.model.parameters())\n",
    "        if self.mixer:\n",
    "            self.params += list(self.mixer.parameters())\n",
    "        self.loss = QMixLoss(\n",
    "            self.model,\n",
    "            self.target_model,\n",
    "            self.mixer,\n",
    "            self.target_mixer,\n",
    "            self.n_agents,\n",
    "            self.n_actions,\n",
    "            self.config[\"double_q\"],\n",
    "            self.config[\"gamma\"],\n",
    "        )\n",
    "        from torch.optim import RMSprop\n",
    "        #it appears that this RMSprop is optimizing the mixer network and not the agent network\n",
    "        self.rmsprop_optimizer = RMSprop(\n",
    "            params=self.params,\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"optim_alpha\"],\n",
    "            eps=config[\"optim_eps\"],\n",
    "        )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions_from_input_dict(\n",
    "        self,\n",
    "        input_dict: Dict[str, TensorType],\n",
    "        explore: bool = None,\n",
    "        timestep: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:\n",
    "\n",
    "        obs_batch = input_dict[SampleBatch.OBS]\n",
    "        state_batches = []\n",
    "        i = 0\n",
    "        while f\"state_in_{i}\" in input_dict:\n",
    "            state_batches.append(input_dict[f\"state_in_{i}\"])\n",
    "            i += 1\n",
    "\n",
    "        explore = explore if explore is not None else self.config[\"explore\"]\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        # We need to ensure we do not use the env global state\n",
    "        # to compute actions\n",
    "\n",
    "        # Compute actions\n",
    "        with torch.no_grad():\n",
    "            q_values, hiddens = _mac(\n",
    "                self.model,\n",
    "                torch.as_tensor(obs_batch, dtype=torch.float, device=self.device),\n",
    "                [\n",
    "                    torch.as_tensor(np.array(s), dtype=torch.float, device=self.device)\n",
    "                    for s in state_batches\n",
    "                ],\n",
    "            )\n",
    "            avail = torch.as_tensor(action_mask, dtype=torch.float, device=self.device)\n",
    "            masked_q_values = q_values.clone()\n",
    "            masked_q_values[avail == 0.0] = -float(\"inf\")\n",
    "            masked_q_values_folded = torch.reshape(\n",
    "                masked_q_values, [-1] + list(masked_q_values.shape)[2:]\n",
    "            )\n",
    "            actions, _ = self.exploration.get_exploration_action(\n",
    "                action_distribution=TorchCategorical(masked_q_values_folded),\n",
    "                timestep=timestep,\n",
    "                explore=explore,\n",
    "            )\n",
    "            actions = (\n",
    "                torch.reshape(actions, list(masked_q_values.shape)[:-1]).cpu().numpy()\n",
    "            )\n",
    "            hiddens = [s.cpu().numpy() for s in hiddens]\n",
    "\n",
    "        return tuple(actions.transpose([1, 0])), hiddens, {}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions(self, *args, **kwargs):\n",
    "        return self.compute_actions_from_input_dict(*args, **kwargs)\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "    ):\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        return np.zeros(obs_batch.size()[0])\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        obs_batch, action_mask, env_global_state = self._unpack_observation(\n",
    "            samples[SampleBatch.CUR_OBS]\n",
    "        )\n",
    "        (\n",
    "            next_obs_batch,\n",
    "            next_action_mask,\n",
    "            next_env_global_state,\n",
    "        ) = self._unpack_observation(samples[SampleBatch.NEXT_OBS])\n",
    "        group_rewards = self._get_group_rewards(samples[SampleBatch.INFOS])\n",
    "\n",
    "        input_list = [\n",
    "            group_rewards,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            samples[SampleBatch.ACTIONS],\n",
    "            samples[SampleBatch.DONES],\n",
    "            obs_batch,\n",
    "            next_obs_batch,\n",
    "        ]\n",
    "        if self.has_env_global_state:\n",
    "            input_list.extend([env_global_state, next_env_global_state])\n",
    "\n",
    "        output_list, _, seq_lens = chop_into_sequences(\n",
    "            episode_ids=samples[SampleBatch.EPS_ID],\n",
    "            unroll_ids=samples[SampleBatch.UNROLL_ID],\n",
    "            agent_indices=samples[SampleBatch.AGENT_INDEX],\n",
    "            feature_columns=input_list,\n",
    "            state_columns=[],  # RNN states not used here\n",
    "            max_seq_len=self.config[\"model\"][\"max_seq_len\"],\n",
    "            dynamic_max=True,\n",
    "        )\n",
    "        # These will be padded to shape [B * T, ...]\n",
    "        if self.has_env_global_state:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                dones,\n",
    "                obs,\n",
    "                next_obs,\n",
    "                env_global_state,\n",
    "                next_env_global_state,\n",
    "            ) = output_list\n",
    "        else:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                dones,\n",
    "                obs,\n",
    "                next_obs,\n",
    "            ) = output_list\n",
    "        B, T = len(seq_lens), max(seq_lens)\n",
    "\n",
    "        def to_batches(arr, dtype):\n",
    "            new_shape = [B, T] + list(arr.shape[1:])\n",
    "            return torch.as_tensor(\n",
    "                np.reshape(arr, new_shape), dtype=dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        rewards = to_batches(rew, torch.float)\n",
    "        actions = to_batches(act, torch.long)\n",
    "        obs = to_batches(obs, torch.float).reshape([B, T, self.n_agents, self.obs_size])\n",
    "        action_mask = to_batches(action_mask, torch.float)\n",
    "        next_obs = to_batches(next_obs, torch.float).reshape(\n",
    "            [B, T, self.n_agents, self.obs_size]\n",
    "        )\n",
    "        next_action_mask = to_batches(next_action_mask, torch.float)\n",
    "        if self.has_env_global_state:\n",
    "            env_global_state = to_batches(env_global_state, torch.float)\n",
    "            next_env_global_state = to_batches(next_env_global_state, torch.float)\n",
    "\n",
    "        # TODO(ekl) this treats group termination as individual termination\n",
    "        terminated = (\n",
    "            to_batches(dones, torch.float).unsqueeze(2).expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Create mask for where index is < unpadded sequence length\n",
    "        filled = np.reshape(\n",
    "            np.tile(np.arange(T, dtype=np.float32), B), [B, T]\n",
    "        ) < np.expand_dims(seq_lens, 1)\n",
    "        mask = (\n",
    "            torch.as_tensor(filled, dtype=torch.float, device=self.device)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss_out, mask, masked_td_error, chosen_action_qvals, targets = self.loss(\n",
    "            rewards,\n",
    "            actions,\n",
    "            terminated,\n",
    "            mask,\n",
    "            obs,\n",
    "            next_obs,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            env_global_state,\n",
    "            next_env_global_state,\n",
    "        )\n",
    "\n",
    "        # Optimise\n",
    "        self.rmsprop_optimizer.zero_grad()\n",
    "        loss_out.backward()\n",
    "        grad_norm_info = apply_grad_clipping(self, self.rmsprop_optimizer, loss_out)\n",
    "        self.rmsprop_optimizer.step()\n",
    "\n",
    "        mask_elems = mask.sum().item()\n",
    "        stats = {\n",
    "            \"loss\": loss_out.item(),\n",
    "            \"td_error_abs\": masked_td_error.abs().sum().item() / mask_elems,\n",
    "            \"q_taken_mean\": (chosen_action_qvals * mask).sum().item() / mask_elems,\n",
    "            \"target_mean\": (targets * mask).sum().item() / mask_elems,\n",
    "        }\n",
    "        stats.update(grad_norm_info)\n",
    "\n",
    "        return {LEARNER_STATS_KEY: stats}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_initial_state(self):  # initial RNN state\n",
    "        return [\n",
    "            s.expand([self.n_agents, -1]).cpu().numpy()\n",
    "            for s in self.model.get_initial_state()\n",
    "        ]\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            \"model\": self._cpu_dict(self.model.state_dict()),\n",
    "            \"target_model\": self._cpu_dict(self.target_model.state_dict()),\n",
    "            \"mixer\": self._cpu_dict(self.mixer.state_dict()) if self.mixer else None,\n",
    "            \"target_mixer\": self._cpu_dict(self.target_mixer.state_dict())\n",
    "            if self.mixer\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(self._device_dict(weights[\"model\"]))\n",
    "        self.target_model.load_state_dict(self._device_dict(weights[\"target_model\"]))\n",
    "        if weights[\"mixer\"] is not None:\n",
    "            self.mixer.load_state_dict(self._device_dict(weights[\"mixer\"]))\n",
    "            self.target_mixer.load_state_dict(\n",
    "                self._device_dict(weights[\"target_mixer\"])\n",
    "            )\n",
    "\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_state(self):\n",
    "        state = self.get_weights()\n",
    "        state[\"cur_epsilon\"] = self.cur_epsilon\n",
    "        return state\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_state(self, state):\n",
    "        self.set_weights(state)\n",
    "        self.set_epsilon(state[\"cur_epsilon\"])\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        logger.debug(\"Updated target networks\")\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cur_epsilon = epsilon\n",
    "\n",
    "    def _get_group_rewards(self, info_batch):\n",
    "        group_rewards = np.array(\n",
    "            [info.get(GROUP_REWARDS, [0.0] * self.n_agents) for info in info_batch]\n",
    "        )\n",
    "        return group_rewards\n",
    "\n",
    "    def _device_dict(self, state_dict):\n",
    "        return {\n",
    "            k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _cpu_dict(state_dict):\n",
    "        return {k: v.cpu().detach().numpy() for k, v in state_dict.items()}\n",
    "\n",
    "    def _unpack_observation(self, obs_batch):\n",
    "        \"\"\"Unpacks the observation, action mask, and state (if present)\n",
    "        from agent grouping.\n",
    "        Returns:\n",
    "            obs (np.ndarray): obs tensor of shape [B, n_agents, obs_size]\n",
    "            mask (np.ndarray): action mask, if any\n",
    "            state (np.ndarray or None): state tensor of shape [B, state_size]\n",
    "                or None if it is not in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        unpacked = _unpack_obs(\n",
    "            np.array(obs_batch, dtype=np.float32),\n",
    "            self.observation_space.original_space,\n",
    "            tensorlib=np,\n",
    "        )\n",
    "\n",
    "        if isinstance(unpacked[0], dict):\n",
    "            assert \"obs\" in unpacked[0]\n",
    "            unpacked_obs = [np.concatenate(tree.flatten(u[\"obs\"]), 1) for u in unpacked]\n",
    "        else:\n",
    "            unpacked_obs = unpacked\n",
    "\n",
    "        obs = np.concatenate(unpacked_obs, axis=1).reshape(\n",
    "            [len(obs_batch), self.n_agents, self.obs_size]\n",
    "        )\n",
    "\n",
    "        if self.has_action_mask:\n",
    "            action_mask = np.concatenate(\n",
    "                [o[\"action_mask\"] for o in unpacked], axis=1\n",
    "            ).reshape([len(obs_batch), self.n_agents, self.n_actions])\n",
    "        else:\n",
    "            action_mask = np.ones(\n",
    "                [len(obs_batch), self.n_agents, self.n_actions], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.has_env_global_state:\n",
    "            state = np.concatenate(tree.flatten(unpacked[0][ENV_STATE]), 1)\n",
    "        else:\n",
    "            state = None\n",
    "        return obs, action_mask, state\n",
    "\n",
    "\n",
    "def _validate(obs_space, action_space):\n",
    "    if not hasattr(obs_space, \"original_space\") or not isinstance(\n",
    "        obs_space.original_space, gym.spaces.Tuple\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Obs space must be a Tuple, got {}. Use \".format(obs_space)\n",
    "            + \"MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space, gym.spaces.Tuple):\n",
    "        raise ValueError(\n",
    "            \"Action space must be a Tuple, got {}. \".format(action_space)\n",
    "            + \"Use MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space.spaces[0], gym.spaces.Discrete):\n",
    "        raise ValueError(\n",
    "            \"QMix requires a discrete action space, got {}\".format(\n",
    "                action_space.spaces[0]\n",
    "            )\n",
    "        )\n",
    "    if len({str(x) for x in obs_space.original_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: observations of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(obs_space.original_space.spaces)\n",
    "        )\n",
    "    if len({str(x) for x in action_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: action space of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(action_space.spaces)\n",
    "        )\n",
    "\n",
    "\n",
    "def _mac(model, obs, h):\n",
    "    \"\"\"Forward pass of the multi-agent controller.\n",
    "    Args:\n",
    "        model: TorchModelV2 class\n",
    "        obs: Tensor of shape [B, n_agents, obs_size]\n",
    "        h: List of tensors of shape [B, n_agents, h_size]\n",
    "    Returns:\n",
    "        q_vals: Tensor of shape [B, n_agents, n_actions]\n",
    "        h: Tensor of shape [B, n_agents, h_size]\n",
    "    \"\"\"\n",
    "    B, n_agents = obs.size(0), obs.size(1)\n",
    "    if not isinstance(obs, dict):\n",
    "        obs = {\"obs\": obs}\n",
    "    obs_agents_as_batches = {k: _drop_agent_dim(v) for k, v in obs.items()}\n",
    "    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n",
    "    \n",
    "    model = optimize_model(\n",
    "      model, input_data=obs, device=\"gpu\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    q_flat, h_flat = model(obs_agents_as_batches, h_flat, None)\n",
    "    return q_flat.reshape([B, n_agents, -1]), [\n",
    "        s.reshape([B, n_agents, -1]) for s in h_flat\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _unroll_mac(model, obs_tensor):\n",
    "    \"\"\"Computes the estimated Q values for an entire trajectory batch\"\"\"\n",
    "    B = obs_tensor.size(0)\n",
    "    T = obs_tensor.size(1)\n",
    "    n_agents = obs_tensor.size(2)\n",
    "    \"\"\"\n",
    "    model = optimize_model(\n",
    "      cmodel, input_data=input_data, device=\"gpu\"\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    mac_out = []\n",
    "    h = [s.expand([B, n_agents, -1]) for s in model.get_initial_state()]\n",
    "    for t in range(T):\n",
    "        q, h = _mac(model, obs_tensor[:, t], h)\n",
    "        mac_out.append(q)\n",
    "    mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "    return mac_out\n",
    "\n",
    "\n",
    "def _drop_agent_dim(T):\n",
    "    shape = list(T.shape)\n",
    "    B, n_agents = shape[0], shape[1]\n",
    "    return T.reshape([B * n_agents] + shape[2:])\n",
    "\n",
    "\n",
    "def _add_agent_dim(T, n_agents):\n",
    "    shape = list(T.shape)\n",
    "    B = shape[0] // n_agents\n",
    "    assert shape[0] % n_agents == 0\n",
    "    return T.reshape([B, n_agents] + shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53706fc1",
   "metadata": {},
   "source": [
    "Everything up to this point has sucessfully executed\n",
    "\n",
    "if prediction errors are what it is sending up and predictions are what it is sending down what is it sending to neurons\n",
    "to its side?\n",
    "\n",
    "ANS: most likely predictions but we may need to check this\n",
    "\n",
    "In the active inference book it shows that the highest level neurons and their expectations conditioned on policy has no \n",
    "lateral connections while the lower level neurons which are represented with average under policy do have lateral connections\n",
    "\n",
    "The lowest level neurons are represented just as average under policies. \n",
    "\n",
    "New hypothesis: it may just be excitatory or inhibitory signals that are being sent laterally. (see phenomena such as lateral inhibition. \n",
    "\n",
    "In addition we should consider bringing back at this stage temporal and spatial summation. We know now that the upper level neuronal agents form discrete models and only the lowest level neurons send and recieve continuous signals. \n",
    "\n",
    "Question: Could lateral inhibition be related to that sampling method in QUASI? \n",
    "\n",
    "Note: empirical priors from high level neuronal agents drive habits (which can be seen as analogous to model-free RL) and influence policy selection independently of expected free energy. while The expected free energy of a policy that comes from predicted outcomes and prediction error drives goal directed behavior and is analogous to model-based RL. Low dopamine favors context sensitive priors in the indirect pathways whose role is to suppress implausible policies. (it favors habits over goal directed behavior). Complete depletion fo dopamine leads to an inability to enact specific policies.\n",
    "\n",
    "we can hold beliefs not just about states of the world but also about the fixed or slowly varying parameters that determine dependencies between variables. The substrate of these beliefs is the efficacy of synaptic connections representing time-varying variables. When we observe an outcome that we believe was generated by a given state we can update beliefs about the parameter connecting the two, reflecting an increase in the probability of them occuring in the future \n",
    "\n",
    "Translation: we will also need to infer not just states of the world but also parameters that determine dependenceis between variables (which in this case are different neurons or populations of neurons)\n",
    "\n",
    "where teh lowest level might deal with the requisite changes in muscle length descending input is based on decisions about which movement to make\n",
    "\n",
    "connections entering and leaving a cotrical column relate to likelhood distributions wheras transition probabilities and continuous dynamics depend on connections within a microcircuit\n",
    "\n",
    "This suggests that learning dynamics should lead to changes in intrninsic connectivity while learning observations should modify extrinsic connectivity. \n",
    "\n",
    "question for the future: Does lateral inhibition play an analogous role as the XOR parity constraints do in the WISH scheme? If does a WISH like mechanism play a role in creating the low quality approximate attention schema that creates consciousness?\n",
    "\n",
    "parameters to of interest: prior precision (already built in to and taken care of by laplace torch, connection parameter will need to figure out how to handle this. \n",
    "\n",
    "what if we put the other parameters like the connection parameter into the local reward function? \n",
    "\n",
    "our agent network needs to:\n",
    "1. infer the hidden state \n",
    "2. generate a prediction to send down the hierarchy or an action to send laterally to another neuronal agent\n",
    "3. also needs to infer the parameterization that determines dependencies between neuronal agents\n",
    "\n",
    "I dont believe that our variational autoencoder in its present state can do all three things. we need to find some way to figure these things out sequentially by first inferring hiddens tates then finding the parameterization and finally generating the predictions and actions. Our variational autoencoder is only configured to 1 and 3 and partially 2 if combined with laplace-torch\n",
    "\n",
    "Apparently a hidden markov model is a special case of POMDP in which choices and and behavior are ignored. The first half of our agent autoencoder may need to act like a hidden markov model. \n",
    "\n",
    "For first term of the global reward function see the optimal experimental design in pyro. Apparently the first term of minimzing free energy has the same exact form as this. The second term is expected ambiguity where \n",
    "\n",
    "qmix uses double q learning where it learns 2 q values: one for choosing the action and a second for evaluating the action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0653cd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AgentID' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11908\\2580833365.py\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    class brain(MultiAgentEnv):\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11908\\2580833365.py\"\u001b[1;36m, line \u001b[1;32m146\u001b[1;36m, in \u001b[1;35mbrain\u001b[1;36m\u001b[0m\n\u001b[1;33m    groups: Dict[str, List[AgentID]],\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'AgentID' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GroupAgentsWrapper\n",
    "class brain(MultiAgentEnv):    \n",
    "    def __init__(self, config):\n",
    "        self.num_agents = 3\n",
    "        \"\"\"\n",
    "        this is how the actions and actions space will work: neurons do either spatial or temporal summation depending on\n",
    "        whether they are getting repeated inputs from on other neuron or inputs from multiple neurons.\n",
    "\n",
    "        The rate of fire of the neuron for specific impulses to the external agent will determine how much to slow\n",
    "        things down in the acceleration game or how much to speed things up. \n",
    "\n",
    "        There will be a baseline probability of firing is 0.5 \n",
    "        and in order to collectively minimize Free energy the neuronal agents will have to increase or decrease \n",
    "        the probability of firing \n",
    "\n",
    "        so the action space cannot be encoded as just one discrete thing. It will have to be a dict composed of:\n",
    "        box(probability between -1 ans 1 of firing) and discrete(num_neuronal_agents)\n",
    "\n",
    "        \"\"\"\n",
    "        self._spaces_in_preferred_format = True\n",
    "        self.a = self.num_agents-1#Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
    "        #the two kind of actions any agent can do are action potentials and post-synaptic potentials\n",
    "        #in addition the agent neuron can direct their action to any one of the neurons that exist\n",
    "        self.action_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent0\": Dict(\n",
    "                    {\n",
    "                        #\"prob\": Discrete(100), \n",
    "                        \"action_potential\": Discrete(1) #external agent neurons\n",
    "                    },dtype=np.float32),\n",
    "                \"agent1\": Dict(\n",
    "                    {\n",
    "                        #\"prob\": Discrete(100),\n",
    "                        \"action_potential\": Discrete(1) #excitatory neurons\n",
    "                    } ,dtype=np.float32),\n",
    "                \"agent2\": Dict(\n",
    "                    {\n",
    "                        #\"prob\": Discrete(100),\n",
    "                        \"outsideAction\":gym.spaces.Box(low=-1.0, high=1.0, shape=(10,)),\n",
    "                        \"action_potential\": Discrete(1) #inhibitory neurons\n",
    "                    }, dtype=np.float32)\n",
    "            }\n",
    "        )\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "                {\n",
    "                    \"agent0\": self.action_space,#agent 0 will be our external agent/motor neurons\n",
    "                    \"agent1\": self.action_space,#Discrete(1), #excitatory neurons\n",
    "                    \"agent2\": gym.spaces.Box(low=0, high=1.0, shape=(10,))#Discrete(1)#inhibitory neurons\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "     #config.get(Dict({\"prob\": Discrete(200),\"pos\": Discrete(self.a),\"action_potential\": Discrete(2),\"ps_potential\":Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32))}))\n",
    "\n",
    "        #external agent action space needs to involve observation = accesible_state*d where d is the distortion variable \n",
    "\n",
    "        #the observation space is only what the neuronal agent recievs from the other neuronal agent\n",
    "        #self.observation_space = config.get(Discrete(self.a))\n",
    "        self.timestep_limit = config.get(\"ts\", 1000)\n",
    "\n",
    "        \"\"\" Create the A matrix  \"\"\"\n",
    "        \"\"\"\n",
    "        A = np.zeros( (n_states, n_observations))\n",
    "        np.fill_diagonal(A, 0.5) \n",
    "        log_likelihood = log_stable(A[observation_index,:])\n",
    "\n",
    "        log_prior = log_stable(prior)\n",
    "\n",
    "        qs = softmax(log_likelihood + log_prior)\n",
    "\n",
    "        qs_past = utils.onehot(4, n_states) # agent believes they were at location 4 -- i.e. (1,1) one timestep ag\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        A-matrix here is about the conditional distribution between observations and the hidden state. The hidden state being\n",
    "        the action corresponding to a change in the acceleration number needed for the acceleration number to become 0 \n",
    "\n",
    "        The b-matrix is probability of current state given past state and past action. \n",
    "\n",
    "        if state is synapse strength then what we need to do is say, synapse strength depends on past synapse strength \n",
    "        and whether or not the neuron fired a positive signal at t-1, an inhibitory signal or did not fire at all.\n",
    "\n",
    "        THe second factor which will go into state is the optimal acceleration number. \n",
    "\n",
    "        c-matrix for the external hidden state factor will be a uniform prior across all possible acceleration numbers. Furthermore the \n",
    "        d-matrix for the external state factor will be a uniform prior across all possible acceleration numbers. \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the episode and returns the initial observation of the new one.\n",
    "        # Reset the episode len.\n",
    "        self.episode_len = 0\n",
    "        # Sample a random number from our observation space. in the line directly below is where we should do A \n",
    "        self.cur_obs = self.observation_space.sample()#qo_u_left = get_expected_observations(A, qs_u_left)\n",
    "        # Return initial observation.\n",
    "        return self.cur_obs\n",
    "        \"\"\"\n",
    "        self.dones = set()\n",
    "        return {i: self.observation_space[i].sample() for i in self.agents}\n",
    "\n",
    "    def step(self, action):        \n",
    "        obs, rew, done, info = {}, {}, {}, {}\n",
    "\n",
    "        if self.actions_are_logits:\n",
    "            action_dict = {\n",
    "                k: np.random.choice([0, 1], p=v) for k, v in action_dict.items()\n",
    "            }\n",
    "\n",
    "            state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "        global_rew = 1 #put in overall active inference reward here \n",
    "        rewards = {self.agent_0: global_rew /2, self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0} #modify global rew for each group of agents\n",
    "\n",
    "        for i, action in action_dict.items():\n",
    "            obs[i] = self.observation_space[i].sample()\n",
    "            rew[i] = 0.0\n",
    "            done[i] = False\n",
    "            info[i] = {}\n",
    "            done[\"__all__\"] = len(self.dones) == len(self.agents)\n",
    "        return obs, rew, done, info\n",
    "        #this ensures that the agents are not all homogenous \n",
    "    def with_agent_groups(\n",
    "        self,\n",
    "        groups: Dict[str, List[AgentID]],\n",
    "        obs_space: gym.Space = None,\n",
    "            act_space: gym.Space = None) -> \"MultiAgentEnv\":\n",
    "        \"\"\"Convenience method for grouping together agents in this env.\n",
    "\n",
    "        An agent group is a list of agent IDs that are mapped to a single\n",
    "        logical agent. All agents of the group must act at the same time in the\n",
    "        environment. The grouped agent exposes Tuple action and observation\n",
    "        spaces that are the concatenated action and obs spaces of the\n",
    "        individual agents.\n",
    "\n",
    "        The rewards of all the agents in a group are summed. The individual\n",
    "        agent rewards are available under the \"individual_rewards\" key of the\n",
    "        group info return.\n",
    "\n",
    "        Agent grouping is required to leverage algorithms such as Q-Mix.\n",
    "\n",
    "        Args:\n",
    "            groups: Mapping from group id to a list of the agent ids\n",
    "                of group members. If an agent id is not present in any group\n",
    "                value, it will be left ungrouped. The group id becomes a new agent ID\n",
    "                in the final environment.\n",
    "            obs_space: Optional observation space for the grouped\n",
    "                env. Must be a tuple space. If not provided, will infer this to be a\n",
    "                Tuple of n individual agents spaces (n=num agents in a group).\n",
    "            act_space: Optional action space for the grouped env.\n",
    "                Must be a tuple space. If not provided, will infer this to be a Tuple\n",
    "                of n individual agents spaces (n=num agents in a group).\n",
    "\n",
    "        Examples:\n",
    "            >>> from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "            >>> class MyMultiAgentEnv(MultiAgentEnv): # doctest: +SKIP\n",
    "            ...     # define your env here\n",
    "            ...     ... # doctest: +SKIP\n",
    "            >>> env = MyMultiAgentEnv(...) # doctest: +SKIP\n",
    "            >>> grouped_env = env.with_agent_groups(env, { # doctest: +SKIP\n",
    "            ...   \"group1\": [\"agent1\", \"agent2\", \"agent3\"], # doctest: +SKIP\n",
    "            ...   \"group2\": [\"agent4\", \"agent5\"], # doctest: +SKIP\n",
    "            ... }) # doctest: +SKIP\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        return GroupAgentsWrapper(self, groups, obs_space, act_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2d1aa",
   "metadata": {},
   "source": [
    "We are going to have two parallel vectorized parallel enviroments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"    \n",
    "    config = (\n",
    "            PPOConfig()\n",
    "            .environment(HierarchicalWindyMazeEnv)\n",
    "            .framework(args.framework)\n",
    "            .rollouts(num_rollout_workers=0)\n",
    "            .training(entropy_coeff=0.01)\n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        maze.observation_space,\n",
    "                        Discrete(4),\n",
    "                        PPOConfig.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        Tuple([maze.observation_space, Discrete(4)]),\n",
    "                        maze.action_space,\n",
    "                        PPOConfig.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn,\n",
    "            )\n",
    "            # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "            .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "            \n",
    "        )\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941b73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMixConfig(SimpleQConfig):\n",
    "    \"\"\"Defines a configuration class from which QMix can be built.\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> config = QMixConfig().training(gamma=0.9, lr=0.01, kl_coeff=0.3)\\\n",
    "        ...             .resources(num_gpus=0)\\\n",
    "        ...             .rollouts(num_workers=4)\n",
    "        >>> print(config.to_dict())\n",
    "        >>> # Build an Algorithm object from the config and run 1 training iteration.\n",
    "        >>> algo = config.build(env=TwoStepGame)\n",
    "        >>> algo.train()\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> from ray import tune\n",
    "        >>> config = QMixConfig()\n",
    "        >>> # Print out some default values.\n",
    "        >>> print(config.optim_alpha)\n",
    "        >>> # Update the config object.\n",
    "        >>> config.training(lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97)\n",
    "        >>> # Set the config object's env.\n",
    "        >>> config.environment(env=TwoStepGame)\n",
    "        >>> # Use to_dict() to get the old-style python config dict\n",
    "        >>> # when running with tune.\n",
    "        >>> tune.run(\n",
    "        ...     \"QMix\",\n",
    "        ...     stop={\"episode_reward_mean\": 200},\n",
    "        ...     config=config.to_dict(),\n",
    "        ... )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PPOConfig instance.\"\"\"\n",
    "        super().__init__(algo_class=QMix)\n",
    "\n",
    "        # fmt: off\n",
    "        # __sphinx_doc_begin__\n",
    "        # QMix specific settings:\n",
    "        self.mixer = \"qmix\"\n",
    "        self.mixing_embed_dim = 32\n",
    "        self.double_q = True\n",
    "        self.optim_alpha = 0.99\n",
    "        self.optim_eps = 0.00001\n",
    "        self.grad_clip = 10\n",
    "\n",
    "        # Override some of AlgorithmConfig's default values with QMix-specific values.\n",
    "        # .training()\n",
    "        self.lr = 0.0005\n",
    "        self.train_batch_size = 32\n",
    "        self.target_network_update_freq = 500\n",
    "        # Number of timesteps to collect from rollout workers before we start\n",
    "        # sampling from replay buffers for learning. Whether we count this in agent\n",
    "        # steps  or environment steps depends on config[\"multiagent\"][\"count_steps_by\"].\n",
    "        self.num_steps_sampled_before_learning_starts = 1000\n",
    "        self.replay_buffer_config = {\n",
    "            \"type\": \"ReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "            \"prioritized_replay\": DEPRECATED_VALUE,\n",
    "            # Size of the replay buffer in batches (not timesteps!).\n",
    "            \"capacity\": 1000,\n",
    "            \n",
    "            # Choosing `fragments` here makes it so that the buffer stores entire\n",
    "            # batches, instead of sequences, episodes or timesteps.\n",
    "            \"storage_unit\": \"fragments\",\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        }\n",
    "        self.model = {\n",
    "            \"lstm_cell_size\": 64,\n",
    "            \"max_seq_len\": 999999,\n",
    "        }\n",
    "\n",
    "        # .framework()\n",
    "        self.framework_str = \"torch\"\n",
    "\n",
    "        # .rollouts()\n",
    "        self.num_workers = 3\n",
    "        self.rollout_fragment_length = 4\n",
    "        self.batch_mode = \"complete_episodes\"\n",
    "\n",
    "        # .reporting()\n",
    "        self.min_time_s_per_iteration = 1\n",
    "        self.min_sample_timesteps_per_iteration = 1000\n",
    "\n",
    "        # .exploration()\n",
    "        self.exploration_config = {\n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\",\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.01,\n",
    "            # Timesteps over which to anneal epsilon.\n",
    "            \"epsilon_timesteps\": 40000,\n",
    "\n",
    "            # For soft_q, use:\n",
    "            # \"exploration_config\" = {\n",
    "            #   \"type\": \"SoftQ\"\n",
    "            #   \"temperature\": [float, e.g. 1.0]\n",
    "            # }\n",
    "        }\n",
    "\n",
    "        # .evaluation()\n",
    "        # Evaluate with epsilon=0 every `evaluation_interval` training iterations.\n",
    "        # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "        # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "        # metrics are already only reported for the lowest epsilon workers.\n",
    "        self.evaluation_interval = None\n",
    "        self.evaluation_duration = 10\n",
    "        self.evaluation_config = {\n",
    "            \"explore\": False,\n",
    "        }\n",
    "        # __sphinx_doc_end__\n",
    "        # fmt: on\n",
    "\n",
    "        self.worker_side_prioritization = DEPRECATED_VALUE\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        mixer: Optional[str] = None,\n",
    "        mixing_embed_dim: Optional[int] = None,\n",
    "        double_q: Optional[bool] = None,\n",
    "        target_network_update_freq: Optional[int] = None,\n",
    "        replay_buffer_config: Optional[dict] = None,\n",
    "        optim_alpha: Optional[float] = None,\n",
    "        optim_eps: Optional[float] = None,\n",
    "        grad_norm_clipping: Optional[float] = None,\n",
    "        grad_clip: Optional[float] = None,\n",
    "        **kwargs,\n",
    "    ) -> \"QMixConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            mixer: Mixing network. Either \"qmix\", \"vdn\", or None.\n",
    "            mixing_embed_dim: Size of the mixing network embedding.\n",
    "            double_q: Whether to use Double_Q learning.\n",
    "            target_network_update_freq: Update the target network every\n",
    "                `target_network_update_freq` sample steps.\n",
    "            replay_buffer_config:\n",
    "            optim_alpha: RMSProp alpha.\n",
    "            optim_eps: RMSProp epsilon.\n",
    "            grad_clip: If not None, clip gradients during optimization at\n",
    "                this value.\n",
    "            grad_norm_clipping: Depcrecated in favor of grad_clip\n",
    "        Returns:\n",
    "            This updated AlgorithmConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if grad_norm_clipping is not None:\n",
    "            deprecation_warning(\n",
    "                old=\"grad_norm_clipping\",\n",
    "                new=\"grad_clip\",\n",
    "                help=\"Parameter `grad_norm_clipping` has been \"\n",
    "                \"deprecated in favor of grad_clip in QMix. \"\n",
    "                \"This is now the same parameter as in other \"\n",
    "                \"algorithms. `grad_clip` will be overwritten by \"\n",
    "                \"`grad_norm_clipping={}`\".format(grad_norm_clipping),\n",
    "                error=False,\n",
    "            )\n",
    "            grad_clip = grad_norm_clipping\n",
    "\n",
    "        if mixer is not None:\n",
    "            self.mixer = mixer\n",
    "        if mixing_embed_dim is not None:\n",
    "            self.mixing_embed_dim = mixing_embed_dim\n",
    "        if double_q is not None:\n",
    "            self.double_q = double_q\n",
    "        if target_network_update_freq is not None:\n",
    "            self.target_network_update_freq = target_network_update_freq\n",
    "        if replay_buffer_config is not None:\n",
    "            self.replay_buffer_config = replay_buffer_config\n",
    "        if optim_alpha is not None:\n",
    "            self.optim_alpha = optim_alpha\n",
    "        if optim_eps is not None:\n",
    "            self.optim_eps = optim_eps\n",
    "        if grad_clip is not None:\n",
    "            self.grad_clip = grad_clip\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class QMix(SimpleQ):\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_config(cls) -> AlgorithmConfigDict:\n",
    "        return QMixConfig().to_dict()\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def validate_config(self, config: AlgorithmConfigDict) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate_config(config)\n",
    "\n",
    "        if config[\"framework\"] != \"torch\":\n",
    "            raise ValueError(\"Only `framework=torch` supported so far for QMix!\")\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def get_default_policy_class(self, config: AlgorithmConfigDict) -> Type[Policy]:\n",
    "        return QMixTorchPolicy\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def training_step(self) -> ResultDict:\n",
    "        \"\"\"QMIX training iteration function.\n",
    "        - Sample n MultiAgentBatches from n workers synchronously.\n",
    "        - Store new samples in the replay buffer.\n",
    "        - Sample one training MultiAgentBatch from the replay buffer.\n",
    "        - Learn on the training batch.\n",
    "        - Update the target network every `target_network_update_freq` sample steps.\n",
    "        - Return all collected training metrics for the iteration.\n",
    "        Returns:\n",
    "            The results dict from executing the training iteration.\n",
    "        \"\"\"\n",
    "        # Sample n batches from n workers.\n",
    "        new_sample_batches = synchronous_parallel_sample(\n",
    "            worker_set=self.workers, concat=False\n",
    "        )\n",
    "\n",
    "        for batch in new_sample_batches:\n",
    "            # Update counters.\n",
    "            self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n",
    "            self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n",
    "            # Store new samples in the replay buffer.\n",
    "            self.local_replay_buffer.add(batch)\n",
    "\n",
    "        # Update target network every `target_network_update_freq` sample steps.\n",
    "        cur_ts = self._counters[\n",
    "            NUM_AGENT_STEPS_SAMPLED if self._by_agent_steps else NUM_ENV_STEPS_SAMPLED\n",
    "        ]\n",
    "\n",
    "        train_results = {}\n",
    "\n",
    "        if cur_ts > self.config[\"num_steps_sampled_before_learning_starts\"]:\n",
    "            # Sample n batches from replay buffer until the total number of timesteps\n",
    "            # reaches `train_batch_size`.\n",
    "            train_batch = sample_min_n_steps_from_buffer(\n",
    "                replay_buffer=self.local_replay_buffer,\n",
    "                min_steps=self.config[\"train_batch_size\"],\n",
    "                count_by_agent_steps=self._by_agent_steps,\n",
    "            )\n",
    "\n",
    "            # Learn on the training batch.\n",
    "            # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
    "            # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
    "            if self.config.get(\"simple_optimizer\") is True:\n",
    "                train_results = train_one_step(self, train_batch)\n",
    "            else:\n",
    "                train_results = multi_gpu_train_one_step(self, train_batch)\n",
    "\n",
    "            # Update target network every `target_network_update_freq` sample steps.\n",
    "            last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
    "            if cur_ts - last_update >= self.config[\"target_network_update_freq\"]:\n",
    "                to_update = self.workers.local_worker().get_policies_to_train()\n",
    "                self.workers.local_worker().foreach_policy_to_train(\n",
    "                    lambda p, pid: pid in to_update and p.update_target()\n",
    "                )\n",
    "                self._counters[NUM_TARGET_UPDATES] += 1\n",
    "                self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
    "\n",
    "            update_priorities_in_replay_buffer(\n",
    "                self.local_replay_buffer, self.config, train_batch, train_results\n",
    "            )\n",
    "\n",
    "            # Update weights and global_vars - after learning on the local worker -\n",
    "            # on all remote workers.\n",
    "            global_vars = {\n",
    "                \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
    "            }\n",
    "            # Update remote workers' weights and global vars after learning on local\n",
    "            # worker.\n",
    "            with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
    "                self.workers.sync_weights(global_vars=global_vars)\n",
    "\n",
    "        # Return all collected metrics for the iteration.\n",
    "        return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee8cbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import get_trainable_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4915c17b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11908\\2059511265.py\"\u001b[1;36m, line \u001b[1;32m58\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    get_trainable_cls(args.run)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m\u001b[1;31m:\u001b[0m 'Namespace' object has no attribute 'run'\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\"\"\"\n",
    "                {\n",
    "                    \"agent0\": Discrete(1),#agent 0 will be sour external agent/motor neuron\n",
    "                    \"agent1\": Discrete(1),#Discrete(1), #excitatory neurons\n",
    "                    \"agent2\": gym.spaces.Box(low=0, high=1.0, shape=(10,))#Discrete(1)#inhibitory neurons\n",
    "                }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #ray.init(num_cpus=args.num_cpus or None, local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0, 1,2],\n",
    "    }\n",
    "    obs_space = gym.spaces.Tuple(\n",
    "        [\n",
    "            gym.spaces.Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "            gym.spaces.Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    act_space = gym.spaces.Tuple(\n",
    "        [\n",
    "            gym.spaces.Dict(\n",
    "                {\n",
    "                    \"agent0\": Discrete(1),#agent 0 will be sour external agent/motor neuron\n",
    "                }\n",
    "            ),\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=obs_space, act_space=act_space\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(TwoStepGame)\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "    (\n",
    "    config.framework(\"torch\")\n",
    "        .training(mixer=args.mixer, train_batch_size=32)\n",
    "        .rollouts(num_rollout_workers=0, rollout_fragment_length=4)\n",
    "        .exploration(\n",
    "            exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        .environment(\n",
    "            env=\"grouped_twostep\",\n",
    "            env_config={\n",
    "                \"separate_state_space\": True,\n",
    "                \"one_hot_state_encoding\": True,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f150c4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.97\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "The Ray Tune run failed. Please inspect the previous error messages for a cause. After fixing the issue, you can restart the run from scratch or continue this run. To continue this run, you can use `tuner = Tuner.restore(\"C:\\Users\\subar\\ray_results\\QMix\")`.",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\tuner.py\"\u001b[0m, line \u001b[0;32m272\u001b[0m, in \u001b[0;35mfit\u001b[0m\n    return self._local_tuner.fit()\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py\"\u001b[0m, line \u001b[0;32m420\u001b[0m, in \u001b[0;35mfit\u001b[0m\n    analysis = self._fit_internal(trainable, param_space)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py\"\u001b[0m, line \u001b[0;32m520\u001b[0m, in \u001b[0;35m_fit_internal\u001b[0m\n    **self._get_tune_run_arguments(trainable),\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py\"\u001b[0m, line \u001b[0;32m483\u001b[0m, in \u001b[0;35m_get_tune_run_arguments\u001b[0m\n    if is_function_trainable(trainable):\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\registry.py\"\u001b[0m, line \u001b[0;32m64\u001b[0m, in \u001b[0;35mis_function_trainable\u001b[0m\n    trainable = get_trainable_cls(trainable)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\registry.py\"\u001b[0m, line \u001b[0;32m45\u001b[0m, in \u001b[0;35mget_trainable_cls\u001b[0m\n    validate_trainable(trainable_name)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\registry.py\"\u001b[1;36m, line \u001b[1;32m57\u001b[1;36m, in \u001b[1;35mvalidate_trainable\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise TuneError(\"Unknown trainable: \" + trainable_name)\u001b[0m\n",
      "\u001b[1;31mTuneError\u001b[0m\u001b[1;31m:\u001b[0m Unknown trainable: QMix\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11908\\675260110.py\"\u001b[0m, line \u001b[0;32m20\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    tune.Tuner(\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\tuner.py\"\u001b[1;36m, line \u001b[1;32m274\u001b[1;36m, in \u001b[1;35mfit\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise TuneError(\u001b[0m\n",
      "\u001b[1;31mTuneError\u001b[0m\u001b[1;31m:\u001b[0m The Ray Tune run failed. Please inspect the previous error messages for a cause. After fixing the issue, you can restart the run from scratch or continue this run. To continue this run, you can use `tuner = Tuner.restore(\"C:\\Users\\subar\\ray_results\\QMix\")`.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = QMixConfig()\n",
    "# Print out some default values.\n",
    "print(config.optim_alpha)  \n",
    "# Update the config object.\n",
    "config.training(  \n",
    "    lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97\n",
    ")\n",
    "# Set the config object's env.\n",
    "#print(config.optim_alpha) \n",
    "config.environment(env=TwoStepGame)  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "print(config.optim_alpha) \n",
    "\n",
    "#only the tune.tuner code is breaking. Everything up to this point is compiling\n",
    "\n",
    "tune.Tuner(  \n",
    "    \"QMix\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6c57ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adf\n",
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': None, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 4, 'batch_mode': 'complete_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9, 'lr': 0.01, 'train_batch_size': 32, 'model': {'lstm_cell_size': 64, 'max_seq_len': 999999}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.01, 'epsilon_timesteps': 40000}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'export_native_model_files': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'replay_sequence_length': None, 'target_network_update_freq': 500, 'replay_buffer_config': {'type': 'ReplayBuffer', 'prioritized_replay': -1, 'capacity': 1000, 'storage_unit': 'fragments', 'worker_side_prioritization': False}, 'num_steps_sampled_before_learning_starts': 1000, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 10, 'tau': 1.0, 'mixer': 'qmix', 'mixing_embed_dim': 32, 'double_q': True, 'optim_alpha': 0.99, 'optim_eps': 1e-05, 'worker_side_prioritization': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x0000023A1E3A4340>}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x0000023A1C610430>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': None, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 21:22:08,776\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=17056)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=18376)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9928)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19828)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=4756)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=24096)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19996)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=10220)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19824)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=21936)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=18500)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=2648)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m 2023-01-30 21:22:22,364\tWARNING env.py:51 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m 2023-01-30 21:22:22,561\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=17056, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000021316517A90>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17056)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m 2023-01-30 21:22:22,559\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=18376, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022295667AC0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18376)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m 2023-01-30 21:22:22,619\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=19828, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022796517AC0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19828)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m 2023-01-30 21:22:22,620\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24096, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000015416516AC0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24096)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,754\tERROR actor_manager.py:486 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4756, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000024C96516AF0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,755\tERROR actor_manager.py:486 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=17056, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000021316517A90>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,757\tERROR actor_manager.py:486 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=18500, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000019015667A90>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m 2023-01-30 21:22:22,718\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=19996, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022216517AC0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19996)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,759\tERROR actor_manager.py:486 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=18376, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022295667AC0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m 2023-01-30 21:22:22,715\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9928, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000025A16517AC0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9928)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m 2023-01-30 21:22:22,718\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10220, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001EC96516AF0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10220)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m 2023-01-30 21:22:22,743\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=21936, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001D196527AC0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21936)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,761\tERROR actor_manager.py:486 -- Ray error, taking actor 5 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=19828, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022796517AC0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m 2023-01-30 21:22:22,720\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4756, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000024C96516AF0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4756)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m 2023-01-30 21:22:22,748\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=18500, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000019015667A90>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18500)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m 2023-01-30 21:22:22,741\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2648, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C316516B20>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m 2023-01-30 21:22:22,706\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=19824, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001E816527A60>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m     policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m     _validate(obs_space, action_space)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19824)\u001b[0m ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,765\tERROR actor_manager.py:486 -- Ray error, taking actor 6 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24096, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000015416516AC0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,767\tERROR actor_manager.py:486 -- Ray error, taking actor 7 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=19824, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001E816527A60>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,768\tERROR actor_manager.py:486 -- Ray error, taking actor 8 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=19996, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022216517AC0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,769\tERROR actor_manager.py:486 -- Ray error, taking actor 9 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9928, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000025A16517AC0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,771\tERROR actor_manager.py:486 -- Ray error, taking actor 10 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10220, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001EC96516AF0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,772\tERROR actor_manager.py:486 -- Ray error, taking actor 11 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2648, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C316516B20>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n",
      "2023-01-30 21:22:22,773\tERROR actor_manager.py:486 -- Ray error, taking actor 12 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=21936, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001D196527AC0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 712, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1970, in _build_policy_map\n",
      "    self.policy_map.create_policy(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 146, in create_policy\n",
      "    policy = create_policy_for_framework(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 126, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 170, in __init__\n",
      "    _validate(obs_space, action_space)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\", line 574, in _validate\n",
      "    raise ValueError(\n",
      "ValueError: Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11908\\3695984586.py\"\u001b[0m, line \u001b[0;32m12\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    algo = config.build(env=TwoStepGame)  # doctest: +SKIP\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py\"\u001b[0m, line \u001b[0;32m746\u001b[0m, in \u001b[0;35mbuild\u001b[0m\n    return algo_class(\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\"\u001b[0m, line \u001b[0;32m441\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    super().__init__(\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\"\u001b[0m, line \u001b[0;32m169\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    self.setup(copy.deepcopy(self.config))\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\"\u001b[0m, line \u001b[0;32m566\u001b[0m, in \u001b[0;35msetup\u001b[0m\n    self.workers = WorkerSet(\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\"\u001b[0m, line \u001b[0;32m191\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    raise e.args[0].args[2]\n",
      "  File \u001b[0;32m\"python\\ray\\_raylet.pyx\"\u001b[0m, line \u001b[0;32m823\u001b[0m, in \u001b[0;35mray._raylet.execute_task\u001b[0m\n",
      "  File \u001b[0;32m\"python\\ray\\_raylet.pyx\"\u001b[0m, line \u001b[0;32m875\u001b[0m, in \u001b[0;35mray._raylet.execute_task\u001b[0m\n",
      "  File \u001b[0;32m\"python\\ray\\_raylet.pyx\"\u001b[0m, line \u001b[0;32m830\u001b[0m, in \u001b[0;35mray._raylet.execute_task\u001b[0m\n",
      "  File \u001b[0;32m\"python\\ray\\_raylet.pyx\"\u001b[0m, line \u001b[0;32m834\u001b[0m, in \u001b[0;35mray._raylet.execute_task\u001b[0m\n",
      "  File \u001b[0;32m\"python\\ray\\_raylet.pyx\"\u001b[0m, line \u001b[0;32m780\u001b[0m, in \u001b[0;35mray._raylet.execute_task.function_executor\u001b[0m\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\"\u001b[0m, line \u001b[0;32m674\u001b[0m, in \u001b[0;35mactor_method_executor\u001b[0m\n    return method(__ray_actor, *args, **kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\"\u001b[0m, line \u001b[0;32m466\u001b[0m, in \u001b[0;35m_resume_span\u001b[0m\n    return method(self, *_args, **_kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\"\u001b[0m, line \u001b[0;32m712\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    self._build_policy_map(\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\"\u001b[0m, line \u001b[0;32m466\u001b[0m, in \u001b[0;35m_resume_span\u001b[0m\n    return method(self, *_args, **_kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\"\u001b[0m, line \u001b[0;32m1970\u001b[0m, in \u001b[0;35m_build_policy_map\u001b[0m\n    self.policy_map.create_policy(\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\"\u001b[0m, line \u001b[0;32m146\u001b[0m, in \u001b[0;35mcreate_policy\u001b[0m\n    policy = create_policy_for_framework(\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\"\u001b[0m, line \u001b[0;32m126\u001b[0m, in \u001b[0;35mcreate_policy_for_framework\u001b[0m\n    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\"\u001b[0m, line \u001b[0;32m170\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    _validate(obs_space, action_space)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py\"\u001b[1;36m, line \u001b[1;32m574\u001b[1;36m, in \u001b[1;35m_validate\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise ValueError(\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m\u001b[1;31m:\u001b[0m Obs space must be a Tuple, got Box(-1.0, 1.0, (6,), float32). Use MultiAgentEnv.with_agent_groups() to group related agents for QMix.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "config = QMixConfig()  # doctest: +SKIP\n",
    "print(\"adf\")\n",
    "config = config.training(gamma=0.9, lr=0.01)#, kl_coeff=0.3)  # doctest: +SKIP\n",
    "\n",
    "config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "config = config.rollouts(num_rollout_workers=12)  # doctest: +SKIP\n",
    "print(config.to_dict())  # doctest: +SKIP\n",
    "\n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=TwoStepGame)  # doctest: +SKIP\n",
    "\n",
    "algo.train()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640a98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#sys.path.append(\"../src\")\n",
    "\n",
    "import os\n",
    "#os.environ['EEG_FMRI_DATASETS']=\"/home/david/eeg_to_fmri/datasets\"\n",
    "#os.environ['EEG_FMRI']=\"/home/david/eeg_to_fmri\"\n",
    "\n",
    "import eeg_to_fmri\n",
    "from eeg_to_fmri.utils import tf_config\n",
    "\n",
    "#dataset=\"01\"\n",
    "tf_config.set_seed(seed=2)\n",
    "#tf_config.setup_tensorflow(device=\"GPU\", memory_limit=1500, run_eagerly=True)\n",
    "\n",
    "from eeg_to_fmri.models.synthesizers import EEG_to_fMRI\n",
    "from eeg_to_fmri.data import preprocess_data, eeg_utils, data_utils\n",
    "from eeg_to_fmri.learning import train, losses\n",
    "from eeg_to_fmri import metrics\n",
    "from eeg_to_fmri.utils import viz_utils\n",
    "from eeg_to_fmri.models.synthesizers import na_specification_eeg\n",
    "from eeg_to_fmri.models.fmri_ae import na_specification_fmri\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae25365",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    model = EEG_to_fMRI(latent_dimension, eeg_train.shape[1:], na_specification_eeg, n_channels,\n",
    "                        weight_decay=weight_decay, skip_connections=True, batch_norm=True, fourier_features=True,\n",
    "                        random_fourier=True, topographical_attention=True, conditional_attention_style=True,\n",
    "                        conditional_attention_style_prior=False, local=True, seed=None, \n",
    "                        fmri_args = (latent_dimension, fmri_train.shape[1:], kernel_size, stride_size, n_channels, \n",
    "                        max_pool, batch_norm, weight_decay, skip_connections,\n",
    "                        n_stacks, True, False, outfilter, dropout, None, False, na_specification_fmri))\n",
    "    model.build(eeg_train.shape, fmri_train.shape)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    loss_fn = losses.mae_cosine\n",
    "    train_set = tf.data.Dataset.from_tensor_slices((eeg_train, fmri_train)).batch(batch_size)\n",
    "    test_set= tf.data.Dataset.from_tensor_slices((eeg_test, fmri_test)).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "451d8dc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The subjects directory has to be specified using the subjects_dir parameter or the SUBJECTS_DIR environment variable.",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11620\\3846482204.py\"\u001b[0m, line \u001b[0;32m7\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    brain = Brain(sub, hemi, surf)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\surfer\\viz.py\"\u001b[0m, line \u001b[0;32m419\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    subjects_dir = _get_subjects_dir(subjects_dir=subjects_dir)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\surfer\\utils.py\"\u001b[1;36m, line \u001b[1;32m723\u001b[1;36m, in \u001b[1;35m_get_subjects_dir\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise ValueError('The subjects directory has to be specified '\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m\u001b[1;31m:\u001b[0m The subjects directory has to be specified using the subjects_dir parameter or the SUBJECTS_DIR environment variable.\n"
     ]
    }
   ],
   "source": [
    "from surfer import Brain\n",
    "\n",
    "sub = 'fsaverage'\n",
    "hemi = 'lh'\n",
    "surf = 'inflated'\n",
    "\n",
    "brain = Brain(sub, hemi, surf)\n",
    "\n",
    "brain.animate(['l', 'c'])\n",
    "\n",
    "# control number of steps\n",
    "brain.animate(['l', 'm'], n_steps=30)\n",
    "\n",
    "# any path you can think of\n",
    "brain.animate(['l', 'c', 'm', 'r', 'c', 'r', 'l'], n_steps=45)\n",
    "\n",
    "# full turns\n",
    "brain.animate([\"m\"] * 3)\n",
    "\n",
    "# movies\n",
    "brain.animate(['l', 'l'], n_steps=10, fname='simple_animation.avi')\n",
    "\n",
    "# however, rotating out of the axial plane is not allowed\n",
    "try:\n",
    "    brain.animate(['l', 'd'])\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4a29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
