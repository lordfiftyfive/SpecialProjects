{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f67e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "if we do this carefully we can use taichi to carry out speedup\n",
    "\n",
    "The mixer and the bmodel would be ti.funcs\n",
    "\n",
    "qmixpolicy would also be a ti.func\n",
    "\n",
    "qmix would be the ti.kernel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import torch\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "#from nebulgym.decorators.torch_decorators import accel\n",
    "\n",
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "#from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE, make_multi_agent\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "\n",
    "import nitime\n",
    "from deeptime.sindy import SINDy\n",
    "\n",
    "#data visualization\n",
    "import pygwalker as pyg\n",
    "\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmodel(design):\n",
    "\n",
    "    # This line allows batching of designs, treating all batch dimensions as independent\n",
    "    with pyro.plate_stack(\"plate_stack\", design.shape):\n",
    "\n",
    "        # We use a dirchlet prior for theta\n",
    "        theta = pyro.sample(\"theta\", dist.Normal(torch.tensor(0.0), torch.tensor(1.0)))\n",
    "        #theta = pyro.sample(\"theta\", dist.Dirichlet())\n",
    "        # We use a simple logistic regression model for the likelihood\n",
    "        logit_p = theta - design\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p))\n",
    "\n",
    "        return y\n",
    "\n",
    "eig = nmc_eig(pmodel, design, observation_labels=[\"y\"], target_labels=[\"theta\"], N=2500, M=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ba0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pmodel(polling_allocation):\n",
    "    # This allows us to run many copies of the model in parallel\n",
    "    with pyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "        # Begin by sampling alpha\n",
    "        alpha = pyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "            prior_mean, covariance_matrix=prior_covariance))\n",
    "\n",
    "        # Sample y conditional on alpha\n",
    "        poll_results = pyro.sample(\"y\", dist.Binomial(\n",
    "            polling_allocation, logits=alpha).to_event(1))\n",
    "\n",
    "        # Now compute w according to the (approximate) electoral college formula\n",
    "        dem_win = election_winner(alpha)\n",
    "        pyro.sample(\"w\", dist.Delta(dem_win))\n",
    "\n",
    "        return poll_results, dem_win, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31139061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutcomePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a576975",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_entropy = dist.Bernoulli(prior_w_prob).entropy()\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "poll_in_florida = torch.zeros(51)\n",
    "poll_in_florida[9] = 1000\n",
    "\n",
    "poll_in_dc = torch.zeros(51)\n",
    "poll_in_dc[8] = 1000\n",
    "\n",
    "uniform_poll = (1000 // 51) * torch.ones(51)\n",
    "\n",
    "# The swing score measures how close the state is to 50/50\n",
    "swing_score = 1. / (.5 - torch.tensor(prior_prob_dem.sort_values(\"State\").values).squeeze()).abs()\n",
    "swing_poll = 1000 * swing_score / swing_score.sum()\n",
    "swing_poll = swing_poll.round()\n",
    "\n",
    "poll_strategies = OrderedDict([(\"Florida\", poll_in_florida),\n",
    "                               (\"DC\", poll_in_dc),\n",
    "                               (\"Uniform\", uniform_poll),\n",
    "                               (\"Swing\", swing_poll)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89bc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "\n",
    "eigs = {}\n",
    "best_strategy, best_eig = None, 0\n",
    "\n",
    "for strategy, allocation in poll_strategies.items():\n",
    "    print(strategy, end=\" \")\n",
    "    guide = OutcomePredictor()\n",
    "    pyro.clear_param_store()\n",
    "    # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "    # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "    # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "    ape = posterior_eig(pmodel, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "    eigs[strategy] = prior_entropy - ape\n",
    "    print(eigs[strategy].item())\n",
    "    if eigs[strategy] > best_eig:\n",
    "        best_strategy, best_eig = strategy, eigs[strategy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create this like the outcome predictor. This is the expected free energy sub-module ESM\n",
    "#this will essentially calculate the  \n",
    "class ESM(nn.Module,ivy.Module):\n",
    "     def __init__(self):\n",
    "        super().__init__()\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        #model = tf.keras.Sequential([\n",
    "        u = tfkl.InputLayer(input_shape=input_shape),\n",
    "        \n",
    "        u = tf.keras.layers.LSTM(25,kernel_initializer='zeros',activation='tanh', dtype = x.dtype, use_bias=True)(u),\n",
    "        u = tfp.layers.VariationalGaussianProcess(\n",
    "                num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\n",
    "                inducing_index_points_initializer=tf.compat.v1.constant_initializer(\n",
    "                    np.linspace(0,x_range, num=1125,\n",
    "                                dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))), mean_fn=None,\n",
    "                jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)(u)\n",
    "\n",
    "  \n",
    "    #in unconstrained thing replace astype with tf.dtype thing.    #tf.initializers.constant(-10.0)\n",
    "    #])\n",
    "    def compute_dem_probability(self, y):\n",
    "        #fk.Model()\n",
    "        o = tf.nn.relu(u)\n",
    "        return o \n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        \n",
    "        #bmodel = FullLaplace(bmodel,'regression',prior_precision=2)\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class emodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        guide = ESM()\n",
    "        pyro.clear_param_store()\n",
    "        # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "        # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "        # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "        \n",
    "        #ape = average posterior entropy\n",
    "        ape = posterior_eig(model, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                            Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "        eigs[strategy] = prior_entropy - ape\n",
    "        print(eigs[strategy].item())\n",
    "        if eigs[strategy] > best_eig:\n",
    "            best_strategy, best_eig = strategy, eigs[strategy]\n",
    "            \n",
    "        pragmatic_value = 0\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Next objective: determine the probaility of hidden state given observation\n",
    "        \n",
    "        we should use a neural network to calculate the probability of reward given action \n",
    "        \n",
    "        if there is 0 correlation, that is to say reward does not change given a certain action then p(s|y) = 0\n",
    "        \n",
    "        this is the pragmatic value\n",
    "        \n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        #q = self.fc2(h)\n",
    "        vae = tfk.Model(inputs=input_dict[\"obs_flat\"],\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n",
    "ivy.set_framework('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4147ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "#@ti.func\n",
    "class bmodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):#(self, obs_space, action_space, num_outputs, model_config):\n",
    "        TorchModelV2.__init__(\n",
    "             self, obs_space, action_space, num_outputs, model_config, name#self, obs_space, action_space, num_outputs, model_config\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        \n",
    "        \"\"\"\n",
    "        pseudo code\n",
    "        \n",
    "        assign a prior to be the posterior of the output of previous agent\n",
    "        \n",
    "        if no output of agent is found we have a preset prior. Prior is going to be a dirchlet\n",
    "        \n",
    "        \"\"\"\n",
    "        prior = tfp.distributions.Dirichlet(concentration)\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(priora, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        #q = self.fc2(h)\n",
    "        vae = tfk.Model(inputs=input_dict[\"obs_flat\"],\n",
    "                outputs=decoder(input_dict[\"obs_flat\"][0]))\n",
    "        kl = tf.keras.losses.KLDivergence()\n",
    "        model.add_loss(kl)\n",
    "        return q, [h]\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d547074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\"\"\"\n",
    "next objective for april 8th 2023: we are going to have to modify the mixer network \n",
    "\n",
    "The mixer network is going to be a bayesian tensorflow network with a -ELBO builtin loss and a kullbacker leibeler loss\n",
    "that will be seen in the QMIX loss function\n",
    "\n",
    "We will have a configurable parameter that depending on whether the policy is high or low will have either -elbo or \n",
    "kullbacker leibler as builtin losses with a generic bayesian neural network . \n",
    "\n",
    "finally we will have emodel which will have a neural network guide\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#@ti.func\n",
    "class QMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape, mixing_embed_dim):\n",
    "        super(QMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.embed_dim = mixing_embed_dim\n",
    "        self.state_dim = int(np.prod(state_shape))\n",
    "\n",
    "        self.hyper_w_1 = nn.Linear(self.state_dim, self.embed_dim * self.n_agents)\n",
    "        self.hyper_w_final = nn.Linear(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # State dependent bias for hidden layer\n",
    "        self.hyper_b_1 = nn.Linear(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # V(s) instead of a bias for the last layers\n",
    "        self.V = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embed_dim, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, agent_qs, states):\n",
    "        \"\"\"Forward pass for the mixer.\n",
    "        Args:\n",
    "            agent_qs: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            states: Tensor of shape [B, T, state_dim]\n",
    "        \"\"\"\n",
    "        bs = agent_qs.size(0)\n",
    "        states = states.reshape(-1, self.state_dim)\n",
    "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
    "        # First layer\n",
    "        w1 = torch.abs(self.hyper_w_1(states))\n",
    "        b1 = self.hyper_b_1(states)\n",
    "        w1 = w1.view(-1, self.n_agents, self.embed_dim)\n",
    "        b1 = b1.view(-1, 1, self.embed_dim)\n",
    "        hidden = nn.functional.elu(torch.bmm(agent_qs, w1) + b1)\n",
    "        # Second layer\n",
    "        w_final = torch.abs(self.hyper_w_final(states))\n",
    "        w_final = w_final.view(-1, self.embed_dim, 1)\n",
    "        # State-dependent bias\n",
    "        v = self.V(states).view(-1, 1, 1)\n",
    "        # Compute final output\n",
    "        y = torch.bmm(hidden, w_final) + v\n",
    "        # Reshape and return\n",
    "        q_tot = y.view(bs, -1, 1)\n",
    "        return q_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52143953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMixLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        target_model,\n",
    "        mixer,\n",
    "        target_mixer,\n",
    "        n_agents,\n",
    "        n_actions,\n",
    "        double_q=True,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.mixer = mixer\n",
    "        self.target_mixer = target_mixer\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        self.double_q = double_q\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rewards,\n",
    "        actions,\n",
    "        terminated,\n",
    "        mask,\n",
    "        obs,\n",
    "        next_obs,\n",
    "        action_mask,\n",
    "        next_action_mask,\n",
    "        state=None,\n",
    "        next_state=None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the loss.\n",
    "        Args:\n",
    "            rewards: Tensor of shape [B, T, n_agents]\n",
    "            actions: Tensor of shape [B, T, n_agents]\n",
    "            terminated: Tensor of shape [B, T, n_agents]\n",
    "            mask: Tensor of shape [B, T, n_agents]\n",
    "            obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            next_obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            next_action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            state: Tensor of shape [B, T, state_dim] (optional)\n",
    "            next_state: Tensor of shape [B, T, state_dim] (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert either none or both of state and next_state are given\n",
    "        if state is None and next_state is None:\n",
    "            state = obs  # default to state being all agents' observations\n",
    "            next_state = next_obs\n",
    "        elif (state is None) != (next_state is None):\n",
    "            raise ValueError(\n",
    "                \"Expected either neither or both of `state` and \"\n",
    "                \"`next_state` to be given. Got: \"\n",
    "                \"\\n`state` = {}\\n`next_state` = {}\".format(state, next_state)\n",
    "            )\n",
    "\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = _unroll_mac(self.model, obs)\n",
    "\n",
    "        # Pick the Q-Values for the actions taken -> [B * n_agents, T]\n",
    "        chosen_action_qvals = torch.gather(\n",
    "            mac_out, dim=3, index=actions.unsqueeze(3)\n",
    "        ).squeeze(3)\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = _unroll_mac(self.target_model, next_obs)\n",
    "\n",
    "        # Mask out unavailable actions for the t+1 step\n",
    "        ignore_action_tp1 = (next_action_mask == 0) & (mask == 1).unsqueeze(-1)\n",
    "        target_mac_out[ignore_action_tp1] = -np.inf\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.double_q:\n",
    "            # Double Q learning computes the target Q values by selecting the\n",
    "            # t+1 timestep action according to the \"policy\" neural network and\n",
    "            # then estimating the Q-value of that action with the \"target\"\n",
    "            # neural network\n",
    "            \n",
    "            #target neural network does expected free energy while policy\n",
    "            #neural network will be variational free energy\n",
    "\n",
    "            # Compute the t+1 Q-values to be used in action selection\n",
    "            # using next_obs\n",
    "            mac_out_tp1 = _unroll_mac(self.model, next_obs)\n",
    "\n",
    "            # mask out unallowed actions\n",
    "            mac_out_tp1[ignore_action_tp1] = -np.inf\n",
    "\n",
    "            # obtain best actions at t+1 according to policy NN\n",
    "            cur_max_actions = mac_out_tp1.argmax(dim=3, keepdim=True)\n",
    "\n",
    "            # use the target network to estimate the Q-values of policy\n",
    "            # network's selected actions\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(\n",
    "                3\n",
    "            )\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "        assert (\n",
    "            target_max_qvals.min().item() != -np.inf\n",
    "        ), \"target_max_qvals contains a masked action; \\\n",
    "            there may be a state with no valid actions.\"\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state)\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, next_state)\n",
    "\n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n",
    "        \"\"\"\n",
    "        \n",
    "        guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        elbo = elbo_(model, guide)\n",
    "        \n",
    "        el = elbo(data)\n",
    "        \n",
    "        #rewards = {self.agent_1:kullbacker, self.agent_2: el}\n",
    "        #loss = lambda y, rv_y: rv_y.variational_loss(y, kl_weight=np.array(batch_size, x.dtype) / x.shape[0])\n",
    "                \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Td-error\n",
    "        #we need to replace this with a variational free energy error\n",
    "        td_error = chosen_action_qvals - targets.detach()\n",
    "        te_error= tf.keras.losses.KLDivergence(chosen_action_qvals - targets.detach()).numpy()\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)#this is ELBO \n",
    "        \n",
    "        #we are going to ahve a kldivergence loss and a -ELBO regularizer for the mixer network \n",
    "        \n",
    "        mask = mask.expand_as(td_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = td_error * mask\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        \n",
    "        #guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "        #elbo_ = pyro.infer.Trace_ELBO(num_particles=1)\n",
    "\n",
    "        # Fix the model/guide pair\n",
    "        #elbo = elbo_(model, guide)        \n",
    "        \n",
    "        #data = obs + preds \n",
    "        #loss = -ln()\n",
    "        #loss = lambda y, rv_y: -rv_y.log_prob(y)\n",
    "        loss = (masked_td_error**2).sum() / mask.sum()\n",
    "        return loss, mask, masked_td_error, chosen_action_qvals, targets\n",
    "\n",
    "    \n",
    "#this part just above is what we need to revise\n",
    "    \n",
    "#@ti.func\n",
    "class QMixTorchPolicy(TorchPolicy):\n",
    "    \"\"\"QMix impl. Assumes homogeneous agents for now.\n",
    "    You must use MultiAgentEnv.with_agent_groups() to group agents\n",
    "    together for QMix. This creates the proper Tuple obs/action spaces and\n",
    "    populates the '_group_rewards' info field.\n",
    "    Action masking: to specify an action mask for individual agents, use a\n",
    "    dict space with an action_mask key, e.g. {\"obs\": ob, \"action_mask\": mask}.\n",
    "    The mask space must be `Box(0, 1, (n_actions,))`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        # We want to error out on instantiation and not on import, because tune\n",
    "        # imports all RLlib algorithms when registering them\n",
    "        # TODO (Artur): Find a way to only import algorithms when needed\n",
    "        if not torch:\n",
    "            raise ImportError(\"Could not import PyTorch, which QMix requires.\")\n",
    "\n",
    "        _validate(obs_space, action_space)\n",
    "        config = dict(ray.rllib.algorithms.qmix.qmix.DEFAULT_CONFIG, **config)\n",
    "        self.framework = \"torch\"\n",
    "\n",
    "        self.n_agents = 3000#len(obs_space.original_space.spaces)\n",
    "        config[\"model\"][\"n_agents\"] = self.n_agents\n",
    "        self.n_actions = action_space.spaces[0].n\n",
    "        self.h_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "        self.has_env_global_state = False\n",
    "        self.has_action_mask = False\n",
    "\n",
    "        agent_obs_space = obs_space.original_space.spaces[0]\n",
    "        if isinstance(agent_obs_space, gym.spaces.Dict):\n",
    "            space_keys = set(agent_obs_space.spaces.keys())\n",
    "            if \"obs\" not in space_keys:\n",
    "                raise ValueError(\"Dict obs space must have subspace labeled `obs`\")\n",
    "            self.obs_size = _get_size(agent_obs_space.spaces[\"obs\"])\n",
    "            if \"action_mask\" in space_keys:\n",
    "                mask_shape = tuple(agent_obs_space.spaces[\"action_mask\"].shape)\n",
    "                if mask_shape != (self.n_actions,):\n",
    "                    raise ValueError(\n",
    "                        \"Action mask shape must be {}, got {}\".format(\n",
    "                            (self.n_actions,), mask_shape\n",
    "                        )\n",
    "                    )\n",
    "                self.has_action_mask = True\n",
    "            if ENV_STATE in space_keys:\n",
    "                self.env_global_state_shape = _get_size(\n",
    "                    agent_obs_space.spaces[ENV_STATE]\n",
    "                )\n",
    "                self.has_env_global_state = True\n",
    "            else:\n",
    "                self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            # The real agent obs space is nested inside the dict\n",
    "            config[\"model\"][\"full_obs_space\"] = agent_obs_space\n",
    "            agent_obs_space = agent_obs_space.spaces[\"obs\"]\n",
    "        else:\n",
    "            self.obs_size = _get_size(agent_obs_space)\n",
    "            self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "        #model = bmodel()#CModel()#CModel()\n",
    "        #bmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#\n",
    "        ivy.set_framework('torch')\n",
    "        #model = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')\n",
    "        bmodel = bmodel()\n",
    "        bmodel = FullLaplace(bmodel,'regression',prior_precision=2)#0.0000000000000000000001)\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"model\",\n",
    "            default_model=bmodel#a#RNNModel#bmodel()#RNNModel,\n",
    "        )\n",
    "\n",
    "        super().__init__(obs_space, action_space, config, model=self.model)\n",
    "\n",
    "        self.target_model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"target_model\",\n",
    "            default_model=bmodel#bmodel()#RNNModel\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.exploration = self._create_exploration()\n",
    "\n",
    "        # Setup the mixer network.\n",
    "        if config[\"mixer\"] is None:\n",
    "            self.mixer = None\n",
    "            self.target_mixer = None\n",
    "        elif config[\"mixer\"] == \"qmix\":\n",
    "            self.mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "            self.target_mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.cur_epsilon = 1.0\n",
    "        self.update_target()  # initial sync\n",
    "\n",
    "        # Setup optimizer\n",
    "        self.params = list(self.model.parameters())\n",
    "        if self.mixer:\n",
    "            self.params += list(self.mixer.parameters())\n",
    "        self.loss = QMixLoss(\n",
    "            self.model,\n",
    "            self.target_model,\n",
    "            self.mixer,\n",
    "            self.target_mixer,\n",
    "            self.n_agents,\n",
    "            self.n_actions,\n",
    "            self.config[\"double_q\"],\n",
    "            self.config[\"gamma\"],\n",
    "        )\n",
    "        from torch.optim import RMSprop\n",
    "\n",
    "        self.rmsprop_optimizer = RMSprop(\n",
    "            params=self.params,\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"optim_alpha\"],\n",
    "            eps=config[\"optim_eps\"],\n",
    "        )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions_from_input_dict(\n",
    "        self,\n",
    "        input_dict: Dict[str, TensorType],\n",
    "        explore: bool = None,\n",
    "        timestep: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:\n",
    "\n",
    "        obs_batch = input_dict[SampleBatch.OBS]\n",
    "        state_batches = []\n",
    "        i = 0\n",
    "        while f\"state_in_{i}\" in input_dict:\n",
    "            state_batches.append(input_dict[f\"state_in_{i}\"])\n",
    "            i += 1\n",
    "\n",
    "        explore = explore if explore is not None else self.config[\"explore\"]\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        # We need to ensure we do not use the env global state\n",
    "        # to compute actions\n",
    "\n",
    "        # Compute actions\n",
    "        with torch.no_grad():\n",
    "            q_values, hiddens = _mac(\n",
    "                self.model,\n",
    "                torch.as_tensor(obs_batch, dtype=torch.float, device=self.device),\n",
    "                [\n",
    "                    torch.as_tensor(np.array(s), dtype=torch.float, device=self.device)\n",
    "                    for s in state_batches\n",
    "                ],\n",
    "            )\n",
    "            avail = torch.as_tensor(action_mask, dtype=torch.float, device=self.device)\n",
    "            masked_q_values = q_values.clone()\n",
    "            masked_q_values[avail == 0.0] = -float(\"inf\")\n",
    "            masked_q_values_folded = torch.reshape(\n",
    "                masked_q_values, [-1] + list(masked_q_values.shape)[2:]\n",
    "            )\n",
    "            actions, _ = self.exploration.get_exploration_action(\n",
    "                action_distribution=TorchCategorical(masked_q_values_folded),\n",
    "                timestep=timestep,\n",
    "                explore=explore,\n",
    "            )\n",
    "            actions = (\n",
    "                torch.reshape(actions, list(masked_q_values.shape)[:-1]).cpu().numpy()\n",
    "            )\n",
    "            hiddens = [s.cpu().numpy() for s in hiddens]\n",
    "\n",
    "        return tuple(actions.transpose([1, 0])), hiddens, {}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions(self, *args, **kwargs):\n",
    "        return self.compute_actions_from_input_dict(*args, **kwargs)\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "    ):\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        return np.zeros(obs_batch.size()[0])\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        obs_batch, action_mask, env_global_state = self._unpack_observation(\n",
    "            samples[SampleBatch.CUR_OBS]\n",
    "        )\n",
    "        (\n",
    "            next_obs_batch,\n",
    "            next_action_mask,\n",
    "            next_env_global_state,\n",
    "        ) = self._unpack_observation(samples[SampleBatch.NEXT_OBS])\n",
    "        group_rewards = self._get_group_rewards(samples[SampleBatch.INFOS])\n",
    "\n",
    "        input_list = [\n",
    "            group_rewards,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            samples[SampleBatch.ACTIONS],\n",
    "            samples[SampleBatch.TERMINATEDS],\n",
    "            obs_batch,\n",
    "            next_obs_batch,\n",
    "        ]\n",
    "        if self.has_env_global_state:\n",
    "            input_list.extend([env_global_state, next_env_global_state])\n",
    "\n",
    "        output_list, _, seq_lens = chop_into_sequences(\n",
    "            episode_ids=samples[SampleBatch.EPS_ID],\n",
    "            unroll_ids=samples[SampleBatch.UNROLL_ID],\n",
    "            agent_indices=samples[SampleBatch.AGENT_INDEX],\n",
    "            feature_columns=input_list,\n",
    "            state_columns=[],  # RNN states not used here\n",
    "            max_seq_len=self.config[\"model\"][\"max_seq_len\"],\n",
    "            dynamic_max=True,\n",
    "        )\n",
    "        # These will be padded to shape [B * T, ...]\n",
    "        if self.has_env_global_state:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "                env_global_state,\n",
    "                next_env_global_state,\n",
    "            ) = output_list\n",
    "        else:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                terminateds,\n",
    "                obs,\n",
    "                next_obs,\n",
    "            ) = output_list\n",
    "        B, T = len(seq_lens), max(seq_lens)\n",
    "\n",
    "        def to_batches(arr, dtype):\n",
    "            new_shape = [B, T] + list(arr.shape[1:])\n",
    "            return torch.as_tensor(\n",
    "                np.reshape(arr, new_shape), dtype=dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        rewards = to_batches(rew, torch.float)\n",
    "        actions = to_batches(act, torch.long)\n",
    "        obs = to_batches(obs, torch.float).reshape([B, T, self.n_agents, self.obs_size])\n",
    "        action_mask = to_batches(action_mask, torch.float)\n",
    "        next_obs = to_batches(next_obs, torch.float).reshape(\n",
    "            [B, T, self.n_agents, self.obs_size]\n",
    "        )\n",
    "        next_action_mask = to_batches(next_action_mask, torch.float)\n",
    "        if self.has_env_global_state:\n",
    "            env_global_state = to_batches(env_global_state, torch.float)\n",
    "            next_env_global_state = to_batches(next_env_global_state, torch.float)\n",
    "\n",
    "        # TODO(ekl) this treats group termination as individual termination\n",
    "        terminated = (\n",
    "            to_batches(terminateds, torch.float)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Create mask for where index is < unpadded sequence length\n",
    "        filled = np.reshape(\n",
    "            np.tile(np.arange(T, dtype=np.float32), B), [B, T]\n",
    "        ) < np.expand_dims(seq_lens, 1)\n",
    "        mask = (\n",
    "            torch.as_tensor(filled, dtype=torch.float, device=self.device)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss_out, mask, masked_td_error, chosen_action_qvals, targets = self.loss(\n",
    "            rewards,\n",
    "            actions,\n",
    "            terminated,\n",
    "            mask,\n",
    "            obs,\n",
    "            next_obs,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            env_global_state,\n",
    "            next_env_global_state,\n",
    "        )\n",
    "\n",
    "        # Optimise\n",
    "        self.rmsprop_optimizer.zero_grad()\n",
    "\n",
    "        loss_out.backward()\n",
    "        grad_norm_info = apply_grad_clipping(self, self.rmsprop_optimizer, loss_out)\n",
    "        self.rmsprop_optimizer.step()\n",
    "\n",
    "        mask_elems = mask.sum().item()\n",
    "        stats = {\n",
    "            \"loss\": loss_out.item(),\n",
    "            \"td_error_abs\": masked_td_error.abs().sum().item() / mask_elems,\n",
    "            \"q_taken_mean\": (chosen_action_qvals * mask).sum().item() / mask_elems,\n",
    "            \"target_mean\": (targets * mask).sum().item() / mask_elems,\n",
    "        }\n",
    "        stats.update(grad_norm_info)\n",
    "\n",
    "        return {LEARNER_STATS_KEY: stats}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_initial_state(self):  # initial RNN state\n",
    "        return [\n",
    "            s.expand([self.n_agents, -1]).cpu().numpy()\n",
    "            for s in self.model.get_initial_state()\n",
    "        ]\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            \"model\": self._cpu_dict(self.model.state_dict()),\n",
    "            \"target_model\": self._cpu_dict(self.target_model.state_dict()),\n",
    "            \"mixer\": self._cpu_dict(self.mixer.state_dict()) if self.mixer else None,\n",
    "            \"target_mixer\": self._cpu_dict(self.target_mixer.state_dict())\n",
    "            if self.mixer\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(self._device_dict(weights[\"model\"]))\n",
    "        self.target_model.load_state_dict(self._device_dict(weights[\"target_model\"]))\n",
    "        if weights[\"mixer\"] is not None:\n",
    "            self.mixer.load_state_dict(self._device_dict(weights[\"mixer\"]))\n",
    "            self.target_mixer.load_state_dict(\n",
    "                self._device_dict(weights[\"target_mixer\"])\n",
    "            )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_state(self):\n",
    "        state = self.get_weights()\n",
    "        state[\"cur_epsilon\"] = self.cur_epsilon\n",
    "        return state\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_state(self, state):\n",
    "        self.set_weights(state)\n",
    "        self.set_epsilon(state[\"cur_epsilon\"])\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        logger.debug(\"Updated target networks\")\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cur_epsilon = epsilon\n",
    "\n",
    "    def _get_group_rewards(self, info_batch):\n",
    "        group_rewards = np.array(\n",
    "            [info.get(GROUP_REWARDS, [0.0] * self.n_agents) for info in info_batch]\n",
    "        )\n",
    "        return group_rewards\n",
    "\n",
    "    def _device_dict(self, state_dict):\n",
    "        return {\n",
    "            k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _cpu_dict(state_dict):\n",
    "        return {k: v.cpu().detach().numpy() for k, v in state_dict.items()}\n",
    "\n",
    "    def _unpack_observation(self, obs_batch):\n",
    "        \"\"\"Unpacks the observation, action mask, and state (if present)\n",
    "        from agent grouping.\n",
    "        Returns:\n",
    "            obs (np.ndarray): obs tensor of shape [B, n_agents, obs_size]\n",
    "            mask (np.ndarray): action mask, if any\n",
    "            state (np.ndarray or None): state tensor of shape [B, state_size]\n",
    "                or None if it is not in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        unpacked = _unpack_obs(\n",
    "            np.array(obs_batch, dtype=np.float32),\n",
    "            self.observation_space.original_space,\n",
    "            tensorlib=np,\n",
    "        )\n",
    "\n",
    "        if isinstance(unpacked[0], dict):\n",
    "            assert \"obs\" in unpacked[0]\n",
    "            unpacked_obs = [np.concatenate(tree.flatten(u[\"obs\"]), 1) for u in unpacked]\n",
    "        else:\n",
    "            unpacked_obs = unpacked\n",
    "\n",
    "        obs = np.concatenate(unpacked_obs, axis=1).reshape(\n",
    "            [len(obs_batch), self.n_agents, self.obs_size]\n",
    "        )\n",
    "\n",
    "        if self.has_action_mask:\n",
    "            action_mask = np.concatenate(\n",
    "                [o[\"action_mask\"] for o in unpacked], axis=1\n",
    "            ).reshape([len(obs_batch), self.n_agents, self.n_actions])\n",
    "        else:\n",
    "            action_mask = np.ones(\n",
    "                [len(obs_batch), self.n_agents, self.n_actions], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.has_env_global_state:\n",
    "            state = np.concatenate(tree.flatten(unpacked[0][ENV_STATE]), 1)\n",
    "        else:\n",
    "            state = None\n",
    "        return obs, action_mask, state\n",
    "\n",
    "#@ti.func\n",
    "def _validate(obs_space, action_space):\n",
    "    if not hasattr(obs_space, \"original_space\") or not isinstance(\n",
    "        obs_space.original_space, gym.spaces.Tuple\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Obs space must be a Tuple, got {}. Use \".format(obs_space)\n",
    "            + \"MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space, gym.spaces.Tuple):\n",
    "        raise ValueError(\n",
    "            \"Action space must be a Tuple, got {}. \".format(action_space)\n",
    "            + \"Use MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space.spaces[0], gym.spaces.Discrete):\n",
    "        raise ValueError(\n",
    "            \"QMix requires a discrete action space, got {}\".format(\n",
    "                action_space.spaces[0]\n",
    "            )\n",
    "        )\n",
    "    if len({str(x) for x in obs_space.original_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: observations of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(obs_space.original_space.spaces)\n",
    "        )\n",
    "    if len({str(x) for x in action_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: action space of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(action_space.spaces)\n",
    "        )\n",
    "\n",
    "#@ti.func\n",
    "def _mac(model, obs, h):\n",
    "    \"\"\"Forward pass of the multi-agent controller.\n",
    "    Args:\n",
    "        model: TorchModelV2 class\n",
    "        obs: Tensor of shape [B, n_agents, obs_size]\n",
    "        h: List of tensors of shape [B, n_agents, h_size]\n",
    "    Returns:\n",
    "        q_vals: Tensor of shape [B, n_agents, n_actions]\n",
    "        h: Tensor of shape [B, n_agents, h_size]\n",
    "    \"\"\"\n",
    "    B, n_agents = obs.size(0), obs.size(1)\n",
    "    if not isinstance(obs, dict):\n",
    "        obs = {\"obs\": obs}\n",
    "    obs_agents_as_batches = {k: _drop_agent_dim(v) for k, v in obs.items()}\n",
    "    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n",
    "    q_flat, h_flat = model(obs_agents_as_batches, h_flat, None)\n",
    "    return q_flat.reshape([B, n_agents, -1]), [\n",
    "        s.reshape([B, n_agents, -1]) for s in h_flat\n",
    "    ]\n",
    "#@ti.func\n",
    "def _unroll_mac(model, obs_tensor):\n",
    "    \"\"\"Computes the estimated Q values for an entire trajectory batch\"\"\"\n",
    "    B = obs_tensor.size(0)\n",
    "    T = obs_tensor.size(1)\n",
    "    n_agents = obs_tensor.size(2)\n",
    "\n",
    "    mac_out = []\n",
    "    h = [s.expand([B, n_agents, -1]) for s in model.get_initial_state()]\n",
    "    for t in range(T):\n",
    "        q, h = _mac(model, obs_tensor[:, t], h)\n",
    "        mac_out.append(q)\n",
    "    mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "    return mac_out\n",
    "#@ti.func\n",
    "def _drop_agent_dim(T):\n",
    "    shape = list(T.shape)\n",
    "    B, n_agents = shape[0], shape[1]\n",
    "    return T.reshape([B * n_agents] + shape[2:])\n",
    "#@ti.func\n",
    "def _add_agent_dim(T, n_agents):\n",
    "    shape = list(T.shape)\n",
    "    B = shape[0] // n_agents\n",
    "    assert shape[0] % n_agents == 0\n",
    "    return T.reshape([B, n_agents] + shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c072aa2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass TwoStepGameWithGroupedAgents(MultiAgentEnv):\\n    def __init__(self, env_config):\\n        super().__init__()\\n        env = TwoStepGame(env_config)\\n        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\\n        tuple_act_space = Tuple([env.action_space, env.action_space])\\n\\n        self.env = env.with_agent_groups(\\n            groups={\"agents\": [0, 1]},\\n            obs_space=tuple_obs_space,\\n            act_space=tuple_act_space,\\n        )\\n        self.observation_space = self.env.observation_space\\n        self.action_space = self.env.action_space\\n        self._agent_ids = {\"agents\"}\\n\\n    def reset(self, *, seed=None, options=None):\\n        return self.env.reset(seed=seed, options=options)\\n\\n    def step(self, actions):\\n        return self.env.step(actions)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TwoStepGame(MultiAgentEnv):\n",
    "    action_space = Discrete(2)\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(2)\n",
    "        self.state = None\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        #self.agent_3=2\n",
    "        self._skip_env_checking = True\n",
    "        # MADDPG emits action logits instead of actual discrete actions\n",
    "        self.actions_are_logits = env_config.get(\"actions_are_logits\", False)\n",
    "        self.one_hot_state_encoding = env_config.get(\"one_hot_state_encoding\", False)\n",
    "        self.with_state = env_config.get(\"separate_state_space\", False)\n",
    "        self._agent_ids = {0, 1}\n",
    "        if not self.one_hot_state_encoding:\n",
    "            self.observation_space = Discrete(6)\n",
    "            self.with_state = False\n",
    "        else:\n",
    "            # Each agent gets the full state (one-hot encoding of which of the\n",
    "            # three states are active) as input with the receiving agent's\n",
    "            # ID (1 or 2) concatenated onto the end.\n",
    "            if self.with_state:\n",
    "                self.observation_space = Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.observation_space = MultiDiscrete([2, 2, 2, 3])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = np.array([1, 0, 0])\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "\n",
    "        state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "        \n",
    "\n",
    "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
    "        obs = self._obs()\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        truncateds = {\"__all__\": False}\n",
    "        infos = {\n",
    "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
    "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "        }\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "\n",
    "    def _obs(self):\n",
    "        if self.with_state:\n",
    "            return {\n",
    "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
    "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
    "            }\n",
    "        else:\n",
    "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
    "\n",
    "    def agent_1_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [1]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0]\n",
    "\n",
    "    def agent_2_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [2]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0] + 3\n",
    "\n",
    "        #if self.render_mode == \"rgb_array\":\n",
    "            #return self._render_frame()\n",
    "\n",
    "\"\"\"\n",
    "class TwoStepGameWithGroupedAgents(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        env = TwoStepGame(env_config)\n",
    "        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\n",
    "        tuple_act_space = Tuple([env.action_space, env.action_space])\n",
    "\n",
    "        self.env = env.with_agent_groups(\n",
    "            groups={\"agents\": [0, 1]},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self._agent_ids = {\"agents\"}\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        return self.env.reset(seed=seed, options=options)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f964642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class EFE(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a SoftQ Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        #self.model  is the first agent model we use to get the first q value used for model selection\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0cf4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our custom replacement for softq in explore config\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Space\n",
    "from typing import Union, Optional\n",
    "\n",
    "from ray.rllib.utils.annotations import PublicAPI\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "@PublicAPI\n",
    "class SoftQ(StochasticSampling):\n",
    "    \"\"\"Special case of StochasticSampling w/ Categorical and temperature param.\n",
    "    Returns a stochastic sample from a Categorical parameterized by the model\n",
    "    output divided by the temperature. Returns the argmax iff explore=False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        *,\n",
    "        framework: Optional[str],\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes a SoftQ Exploration object.\n",
    "        Args:\n",
    "            action_space: The gym action space used by the environment.\n",
    "            temperature: The temperature to divide model outputs by\n",
    "                before creating the Categorical distribution to sample from.\n",
    "            framework: One of None, \"tf\", \"torch\".\n",
    "        \"\"\"\n",
    "        assert isinstance(action_space, (Discrete, MultiDiscrete))\n",
    "        super().__init__(action_space, framework=framework, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @override(StochasticSampling)\n",
    "    def get_exploration_action(\n",
    "        self,\n",
    "        action_distribution: ActionDistribution,\n",
    "        timestep: Union[int, TensorType],\n",
    "        explore: bool = True,\n",
    "    ):\n",
    "        cls = type(action_distribution)\n",
    "        assert issubclass(cls, (Categorical, TorchCategorical))\n",
    "        # Re-create the action distribution with the correct temperature\n",
    "        # applied.\n",
    "        dist = cls(action_distribution.inputs, self.model, temperature=self.temperature)\n",
    "        # Delegate to super method.\n",
    "        return super().get_exploration_action(\n",
    "            action_distribution=dist, timestep=timestep, explore=explore\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f8e1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig, NotProvided\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    "    SAMPLE_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "#@ti.kernel\n",
    "class QMixConfig(SimpleQConfig):\n",
    "    \"\"\"Defines a configuration class from which QMix can be built.\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> config = QMixConfig()  # doctest: +SKIP\n",
    "        >>> config = config.training(gamma=0.9, lr=0.01, kl_coeff=0.3)  # doctest: +SKIP\n",
    "        >>> config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "        >>> config = config.rollouts(num_rollout_workers=4)  # doctest: +SKIP\n",
    "        >>> print(config.to_dict())  # doctest: +SKIP\n",
    "        >>> # Build an Algorithm object from the config and run 1 training iteration.\n",
    "        >>> algo = config.build(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> algo.train()  # doctest: +SKIP\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> from ray import air\n",
    "        >>> from ray import tune\n",
    "        >>> config = QMixConfig()\n",
    "        >>> # Print out some default values.\n",
    "        >>> print(config.optim_alpha)  # doctest: +SKIP\n",
    "        >>> # Update the config object.\n",
    "        >>> config.training(  # doctest: +SKIP\n",
    "        ...     lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97\n",
    "        ... )\n",
    "        >>> # Set the config object's env.\n",
    "        >>> config.environment(env=TwoStepGame)  # doctest: +SKIP\n",
    "        >>> # Use to_dict() to get the old-style python config dict\n",
    "        >>> # when running with tune.\n",
    "        >>> tune.Tuner(  # doctest: +SKIP\n",
    "        ...     \"QMix\",\n",
    "        ...     run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "        ...     param_space=config.to_dict(),\n",
    "        ... ).fit()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PPOConfig instance.\"\"\"\n",
    "        super().__init__(algo_class=QMix)\n",
    "\n",
    "        # fmt: off\n",
    "        # __sphinx_doc_begin__\n",
    "        # QMix specific settings:\n",
    "        self.mixer = \"qmix\"\n",
    "        self.mixing_embed_dim = 32\n",
    "        self.double_q = True\n",
    "        self.optim_alpha = 0.99\n",
    "        self.optim_eps = 0.00001\n",
    "        self.grad_clip = 10\n",
    "        #self.render_mode = 'rgb_array'\n",
    "        # QMix-torch overrides the TorchPolicy's learn_on_batch w/o specifying a\n",
    "        # alternative `learn_on_loaded_batch` alternative for the GPU.\n",
    "        # TODO: This hack will be resolved once we move all algorithms to the new\n",
    "        #  RLModule/Learner APIs.\n",
    "        self.simple_optimizer = True\n",
    "\n",
    "        # Override some of AlgorithmConfig's default values with QMix-specific values.\n",
    "        # .training()\n",
    "        self.lr = 0.0005\n",
    "        self.train_batch_size = 32\n",
    "        self.target_network_update_freq = 500\n",
    "        self.num_steps_sampled_before_learning_starts = 1000\n",
    "        self.replay_buffer_config = {\n",
    "            \"type\": \"ReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "            \"prioritized_replay\": DEPRECATED_VALUE,\n",
    "            # Size of the replay buffer in batches (not timesteps!).\n",
    "            \"capacity\": 1000,\n",
    "            # Choosing `fragments` here makes it so that the buffer stores entire\n",
    "            # batches, instead of sequences, episodes or timesteps.\n",
    "            \"storage_unit\": \"fragments\",\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.model = {\n",
    "            \"lstm_cell_size\": 64,\n",
    "            \"max_seq_len\": 999999,\n",
    "        }\n",
    "        \"\"\"\n",
    "        # .framework()\n",
    "        self.framework_str = \"torch\"\n",
    "\n",
    "        # .rollouts()\n",
    "        self.rollout_fragment_length = 4\n",
    "        self.batch_mode = \"complete_episodes\"\n",
    "\n",
    "        # .reporting()\n",
    "        self.min_time_s_per_iteration = 1\n",
    "        self.min_sample_timesteps_per_iteration = 1000\n",
    "\n",
    "        # .exploration()\n",
    "        self.exploration_config = {\n",
    "            \"\"\"\n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\", #replace this with SoftQ\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.01,\n",
    "            # Timesteps over which to anneal epsilon.\n",
    "            \"epsilon_timesteps\": 40000,\n",
    "            \"\"\"\n",
    "            \"type\": \"SoftQ\",\n",
    "            \"temperature\": 1.0\n",
    "            # For soft_q, use:\n",
    "            # \"exploration_config\" = {\n",
    "            #   \"type\": \"SoftQ\"\n",
    "            #   \"temperature\": [float, e.g. 1.0]\n",
    "            # }\n",
    "        }\n",
    "\n",
    "        # .evaluation()\n",
    "        # Evaluate with epsilon=0 every `evaluation_interval` training iterations.\n",
    "        # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "        self.evaluation(\n",
    "            evaluation_config=AlgorithmConfig.overrides(explore=False)\n",
    "        )\n",
    "        # __sphinx_doc_end__\n",
    "        # fmt: on\n",
    "\n",
    "        self.worker_side_prioritization = DEPRECATED_VALUE\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        mixer: Optional[str] = NotProvided,\n",
    "        mixing_embed_dim: Optional[int] = NotProvided,\n",
    "        double_q: Optional[bool] = NotProvided,\n",
    "        target_network_update_freq: Optional[int] = NotProvided,\n",
    "        replay_buffer_config: Optional[dict] = NotProvided,\n",
    "        optim_alpha: Optional[float] = NotProvided,\n",
    "        optim_eps: Optional[float] = NotProvided,\n",
    "        grad_clip: Optional[float] = NotProvided,\n",
    "        # Deprecated args.\n",
    "        grad_norm_clipping=DEPRECATED_VALUE,\n",
    "        **kwargs,\n",
    "    ) -> \"QMixConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            mixer: Mixing network. Either \"qmix\", \"vdn\", or None.\n",
    "            mixing_embed_dim: Size of the mixing network embedding.\n",
    "            double_q: Whether to use Double_Q learning.\n",
    "            target_network_update_freq: Update the target network every\n",
    "                `target_network_update_freq` sample steps.\n",
    "            replay_buffer_config:\n",
    "            optim_alpha: RMSProp alpha.\n",
    "            optim_eps: RMSProp epsilon.\n",
    "            grad_clip: If not None, clip gradients during optimization at\n",
    "                this value.\n",
    "            grad_norm_clipping: Depcrecated in favor of grad_clip\n",
    "        Returns:\n",
    "            This updated AlgorithmConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if grad_norm_clipping != DEPRECATED_VALUE:\n",
    "            deprecation_warning(\n",
    "                old=\"grad_norm_clipping\",\n",
    "                new=\"grad_clip\",\n",
    "                help=\"Parameter `grad_norm_clipping` has been \"\n",
    "                \"deprecated in favor of grad_clip in QMix. \"\n",
    "                \"This is now the same parameter as in other \"\n",
    "                \"algorithms. `grad_clip` will be overwritten by \"\n",
    "                \"`grad_norm_clipping={}`\".format(grad_norm_clipping),\n",
    "                error=True,\n",
    "            )\n",
    "            grad_clip = grad_norm_clipping\n",
    "\n",
    "        if mixer is not NotProvided:\n",
    "            self.mixer = mixer\n",
    "        if mixing_embed_dim is not NotProvided:\n",
    "            self.mixing_embed_dim = mixing_embed_dim\n",
    "        if double_q is not NotProvided:\n",
    "            self.double_q = double_q\n",
    "        if target_network_update_freq is not NotProvided:\n",
    "            self.target_network_update_freq = target_network_update_freq\n",
    "        if replay_buffer_config is not NotProvided:\n",
    "            self.replay_buffer_config = replay_buffer_config\n",
    "        if optim_alpha is not NotProvided:\n",
    "            self.optim_alpha = optim_alpha\n",
    "        if optim_eps is not NotProvided:\n",
    "            self.optim_eps = optim_eps\n",
    "        if grad_clip is not NotProvided:\n",
    "            self.grad_clip = grad_clip\n",
    "\n",
    "        return self\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def validate(self) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate()\n",
    "\n",
    "        if self.framework_str != \"torch\":\n",
    "            raise ValueError(\n",
    "                \"Only `config.framework('torch')` supported so far for QMix!\"\n",
    "            )\n",
    "#@ti.kernel\n",
    "class QMix(SimpleQ):\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_config(cls) -> AlgorithmConfig:\n",
    "        return QMixConfig()\n",
    "\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_policy_class(\n",
    "        cls, config: AlgorithmConfig\n",
    "    ) -> Optional[Type[Policy]]:\n",
    "        return QMixTorchPolicy\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def training_step(self) -> ResultDict:\n",
    "        \"\"\"QMIX training iteration function.\n",
    "        - Sample n MultiAgentBatches from n workers synchronously.\n",
    "        - Store new samples in the replay buffer.\n",
    "        - Sample one training MultiAgentBatch from the replay buffer.\n",
    "        - Learn on the training batch.\n",
    "        - Update the target network every `target_network_update_freq` sample steps.\n",
    "        - Return all collected training metrics for the iteration.\n",
    "        Returns:\n",
    "            The results dict from executing the training iteration.\n",
    "        \"\"\"\n",
    "        # Sample n batches from n workers.\n",
    "        with self._timers[SAMPLE_TIMER]:\n",
    "            new_sample_batches = synchronous_parallel_sample(\n",
    "                worker_set=self.workers, concat=False\n",
    "            )\n",
    "\n",
    "        for batch in new_sample_batches:\n",
    "            # Update counters.\n",
    "            self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n",
    "            self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n",
    "            # Store new samples in the replay buffer.\n",
    "            self.local_replay_buffer.add(batch)\n",
    "\n",
    "        # Update target network every `target_network_update_freq` sample steps.\n",
    "        cur_ts = self._counters[\n",
    "            NUM_AGENT_STEPS_SAMPLED\n",
    "            if self.config.count_steps_by == \"agent_steps\"\n",
    "            else NUM_ENV_STEPS_SAMPLED\n",
    "        ]\n",
    "\n",
    "        train_results = {}\n",
    "\n",
    "        if cur_ts > self.config.num_steps_sampled_before_learning_starts:\n",
    "            # Sample n batches from replay buffer until the total number of timesteps\n",
    "            # reaches `train_batch_size`.\n",
    "            train_batch = sample_min_n_steps_from_buffer(\n",
    "                replay_buffer=self.local_replay_buffer,\n",
    "                min_steps=self.config.train_batch_size,\n",
    "                count_by_agent_steps=self.config.count_steps_by == \"agent_steps\",\n",
    "            )\n",
    "\n",
    "            # Learn on the training batch.\n",
    "            # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
    "            # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
    "            if self.config.get(\"simple_optimizer\") is True:\n",
    "                train_results = train_one_step(self, train_batch)\n",
    "            else:\n",
    "                train_results = multi_gpu_train_one_step(self, train_batch)\n",
    "\n",
    "            # Update target network every `target_network_update_freq` sample steps.\n",
    "            last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
    "            if cur_ts - last_update >= self.config.target_network_update_freq:\n",
    "                to_update = self.workers.local_worker().get_policies_to_train()\n",
    "                self.workers.local_worker().foreach_policy_to_train(\n",
    "                    lambda p, pid: pid in to_update and p.update_target()\n",
    "                )\n",
    "                self._counters[NUM_TARGET_UPDATES] += 1\n",
    "                self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
    "\n",
    "            update_priorities_in_replay_buffer(\n",
    "                self.local_replay_buffer, self.config, train_batch, train_results\n",
    "            )\n",
    "\n",
    "            # Update weights and global_vars - after learning on the local worker -\n",
    "            # on all remote workers.\n",
    "            global_vars = {\n",
    "                \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
    "            }\n",
    "            # Update remote workers' weights and global vars after learning on local\n",
    "            # worker.\n",
    "            with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
    "                self.workers.sync_weights(global_vars=global_vars)\n",
    "\n",
    "        # Return all collected metrics for the iteration.\n",
    "        return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4256f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "#b = np.ones(3000).tolist()\n",
    "#obs_space= Tuple({MultiDiscrete([])})\n",
    "\n",
    "observation_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]), \n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,3]),\n",
    "                ENV_STATE: MultiDiscrete([3,3]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "action_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([1,3]), \n",
    "                ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([3,1]),\n",
    "                ENV_STATE: MultiDiscrete([3,1]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "obs_space = Tuple([\n",
    "    Dict({\n",
    "        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    \n",
    "    }\n",
    "    )\n",
    "]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24e9afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 01:22:10,991\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\tune.py:562: UserWarning: Consider boosting PBT performance by enabling `reuse_actors` as well as implementing `reset_config` for Trainable.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-04-09 01:27:43</td></tr>\n",
       "<tr><td>Running for: </td><td>00:05:27.38        </td></tr>\n",
       "<tr><td>Memory:      </td><td>26.8/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 312 checkpoints, 40 perturbs<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">    alpha</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_a32d4_00000</td><td>TERMINATED</td><td>127.0.0.1:46608</td><td style=\"text-align: right;\">0.443618 </td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         89.9028</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_a32d4_00001</td><td>TERMINATED</td><td>127.0.0.1:47940</td><td style=\"text-align: right;\">0.369682 </td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         89.8274</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_a32d4_00002</td><td>TERMINATED</td><td>127.0.0.1:46840</td><td style=\"text-align: right;\">0.0190511</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         86.6093</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "<tr><td>PG_TwoStepGame_a32d4_00003</td><td>TERMINATED</td><td>127.0.0.1:35536</td><td style=\"text-align: right;\">0.0152409</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         91.7184</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35704)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=35704)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=35704)\u001b[0m 2023-04-09 01:22:24,135\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=35704)\u001b[0m 2023-04-09 01:22:24,490\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=35704)\u001b[0m 2023-04-09 01:22:24,510\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=17984)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=17984)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=17984)\u001b[0m 2023-04-09 01:22:31,942\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=17984)\u001b[0m 2023-04-09 01:22:32,292\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=17984)\u001b[0m 2023-04-09 01:22:32,313\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=45488)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45488)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=45488)\u001b[0m 2023-04-09 01:22:39,866\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45488)\u001b[0m 2023-04-09 01:22:40,222\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45488)\u001b[0m 2023-04-09 01:22:40,243\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=41752)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=41752)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=41752)\u001b[0m 2023-04-09 01:22:47,805\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=41752)\u001b[0m 2023-04-09 01:22:48,165\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=41752)\u001b[0m 2023-04-09 01:22:48,186\tWARNING env.py:53 -- Skipping env checking for this experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                         </th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                             </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                 </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                    </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </th><th>timers                                                                                                                                                                            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_TwoStepGame_a32d4_00000</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.022972822189331055, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.14131927490234375}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 1.2454390798666282e-06, &#x27;cur_lr&#x27;: 5e-05}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 7.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 1.4, &#x27;ram_util_percent&#x27;: 42.1, &#x27;gpu_util_percent0&#x27;: 0.05, &#x27;vram_util_percent0&#x27;: 0.6712646484375}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.651175609688634, &#x27;mean_inference_ms&#x27;: 0.8780798513195788, &#x27;mean_action_processing_ms&#x27;: 0.1055988926119572, &#x27;mean_env_wait_ms&#x27;: 0.036365530464086644, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.651175609688634, &#x27;mean_inference_ms&#x27;: 0.8780798513195788, &#x27;mean_action_processing_ms&#x27;: 0.1055988926119572, &#x27;mean_env_wait_ms&#x27;: 0.036365530464086644, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.022972822189331055, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.14131927490234375}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 344.682, &#x27;load_time_ms&#x27;: 0.249, &#x27;load_throughput&#x27;: 801970.172, &#x27;learn_time_ms&#x27;: 6.996, &#x27;learn_throughput&#x27;: 28585.914, &#x27;synch_weights_time_ms&#x27;: 0.0}</td></tr>\n",
       "<tr><td>PG_TwoStepGame_a32d4_00001</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.019071102142333984, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12917685508728027}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 1.4519691831083037e-06, &#x27;cur_lr&#x27;: 1e-05}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 9.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 4.9, &#x27;ram_util_percent&#x27;: 42.1, &#x27;gpu_util_percent0&#x27;: 0.04, &#x27;vram_util_percent0&#x27;: 0.6712646484375}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6248758032129004, &#x27;mean_inference_ms&#x27;: 0.7975146725223019, &#x27;mean_action_processing_ms&#x27;: 0.12221560254320878, &#x27;mean_env_wait_ms&#x27;: 0.0323086947232455, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6248758032129004, &#x27;mean_inference_ms&#x27;: 0.7975146725223019, &#x27;mean_action_processing_ms&#x27;: 0.12221560254320878, &#x27;mean_env_wait_ms&#x27;: 0.0323086947232455, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.019071102142333984, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.12917685508728027}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 325.804, &#x27;load_time_ms&#x27;: 0.2, &#x27;load_throughput&#x27;: 999119.581, &#x27;learn_time_ms&#x27;: 7.404, &#x27;learn_throughput&#x27;: 27011.231, &#x27;synch_weights_time_ms&#x27;: 0.0}  </td></tr>\n",
       "<tr><td>PG_TwoStepGame_a32d4_00002</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.009001493453979492, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.15443873405456543}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.17529889196157455, &#x27;cur_lr&#x27;: 0.0001}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 73.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000} </td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 6.9, &#x27;ram_util_percent&#x27;: 44.5, &#x27;gpu_util_percent0&#x27;: 0.02, &#x27;vram_util_percent0&#x27;: 0.6712646484375}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.7909964973935242, &#x27;mean_inference_ms&#x27;: 1.036981137567816, &#x27;mean_action_processing_ms&#x27;: 0.14470848034014813, &#x27;mean_env_wait_ms&#x27;: 0.04936353433230423, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.7909964973935242, &#x27;mean_inference_ms&#x27;: 1.036981137567816, &#x27;mean_action_processing_ms&#x27;: 0.14470848034014813, &#x27;mean_env_wait_ms&#x27;: 0.04936353433230423, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.009001493453979492, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.15443873405456543}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 408.289, &#x27;load_time_ms&#x27;: 0.0, &#x27;load_throughput&#x27;: 0.0, &#x27;learn_time_ms&#x27;: 10.054, &#x27;learn_throughput&#x27;: 19892.595, &#x27;synch_weights_time_ms&#x27;: 0.0}        </td></tr>\n",
       "<tr><td>PG_TwoStepGame_a32d4_00003</td><td style=\"text-align: right;\">                  80000</td><td>{&#x27;StateBufferConnector_ms&#x27;: 0.021004915237426758, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1459050178527832} </td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td>{}              </td><td style=\"text-align: right;\">                 2</td><td>{}             </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                    7</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                 100</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;policy_loss&#x27;: 0.060302183497697115, &#x27;cur_lr&#x27;: 5e-05}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 200.0, &#x27;num_grad_updates_lifetime&#x27;: 111.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 80000, &#x27;num_agent_steps_trained&#x27;: 80000}</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                    80000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 7.3, &#x27;ram_util_percent&#x27;: 42.8, &#x27;gpu_util_percent0&#x27;: 0.0, &#x27;vram_util_percent0&#x27;: 0.6712646484375} </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 1.009382293560858, &#x27;mean_inference_ms&#x27;: 1.2602559392680632, &#x27;mean_action_processing_ms&#x27;: 0.17006398737297548, &#x27;mean_env_wait_ms&#x27;: 0.05309371668976784, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 7.0, &#x27;episode_reward_min&#x27;: 7.0, &#x27;episode_reward_mean&#x27;: 7.0, &#x27;episode_len_mean&#x27;: 2.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0], &#x27;episode_lengths&#x27;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 1.009382293560858, &#x27;mean_inference_ms&#x27;: 1.2602559392680632, &#x27;mean_action_processing_ms&#x27;: 0.17006398737297548, &#x27;mean_env_wait_ms&#x27;: 0.05309371668976784, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;StateBufferConnector_ms&#x27;: 0.021004915237426758, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1459050178527832}} </td><td>{&#x27;training_iteration_time_ms&#x27;: 413.552, &#x27;load_time_ms&#x27;: 0.2, &#x27;load_throughput&#x27;: 1000668.973, &#x27;learn_time_ms&#x27;: 8.823, &#x27;learn_throughput&#x27;: 22667.87, &#x27;synch_weights_time_ms&#x27;: 0.1}  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 01:22:48,901\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:22:49,356\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -4.550000) into trial a32d4_00001 (score = -5.120000)\n",
      "\n",
      "2023-04-09 01:22:49,357\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 0.0004 --- (resample) --> 1e-05\n",
      "alpha : 0.8882664055617858 --- (* 1.2) --> 1.065919686674143\n",
      "\n",
      "2023-04-09 01:22:49,565\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -4.550000) into trial a32d4_00002 (score = -5.610000)\n",
      "\n",
      "2023-04-09 01:22:49,566\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00002:\n",
      "lr : 0.0004 --- (resample) --> 0.001\n",
      "alpha : 0.8882664055617858 --- (* 1.2) --> 1.065919686674143\n",
      "\n",
      "2023-04-09 01:22:50,261\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:22:51,426\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:22:51,831\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:22:51,862\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:22:52,288\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:22:52,635\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:22:53,065\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:22:53,556\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:22:54,290\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:22:54,736\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:22:55,122\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:22:55,201\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:22:55,580\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:22:56,013\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:22:56,425\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:22:56,907\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=46960)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46960)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-09 01:22:57,645\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:22:58,021\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:22:58,386\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "\u001b[2m\u001b[36m(PG pid=33464)\u001b[0m 2023-04-09 01:22:58,374\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46960)\u001b[0m 2023-04-09 01:22:58,365\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=33464)\u001b[0m 2023-04-09 01:22:58,775\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=33464)\u001b[0m 2023-04-09 01:22:58,788\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=46960)\u001b[0m 2023-04-09 01:22:58,767\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46960)\u001b[0m 2023-04-09 01:22:58,782\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-04-09 01:22:58,957\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "\u001b[2m\u001b[36m(PG pid=33464)\u001b[0m 2023-04-09 01:22:58,917\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_bfd7f212ed41481e84b8391c866a3f0c\n",
      "\u001b[2m\u001b[36m(PG pid=33464)\u001b[0m 2023-04-09 01:22:58,917\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 0.8293404579162598, '_episodes_total': 200}\n",
      "\u001b[2m\u001b[36m(PG pid=46960)\u001b[0m 2023-04-09 01:22:58,914\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_00d84757ecc940afb3c709266cdf2cb8\n",
      "\u001b[2m\u001b[36m(PG pid=46960)\u001b[0m 2023-04-09 01:22:58,914\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 0.8293404579162598, '_episodes_total': 200}\n",
      "2023-04-09 01:22:59,291\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:23:00,083\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:23:00,254\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:23:00,679\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00002 (score = -5.260000) into trial a32d4_00003 (score = -7.040000)\n",
      "\n",
      "2023-04-09 01:23:00,680\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00003:\n",
      "lr : 0.001 --- (resample) --> 5e-05\n",
      "alpha : 1.065919686674143 --- (* 1.2) --> 1.2791036240089715\n",
      "\n",
      "2023-04-09 01:23:04,151\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -6.600000) into trial a32d4_00000 (score = -7.040000)\n",
      "\n",
      "2023-04-09 01:23:04,152\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 1.065919686674143 --- (* 0.8) --> 0.8527357493393144\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6984)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=6984)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-09 01:23:08,963\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00002 (score = -6.510000) into trial a32d4_00001 (score = -7.100000)\n",
      "\n",
      "2023-04-09 01:23:08,964\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 0.001 --- (shift right) --> 0.0005\n",
      "alpha : 1.065919686674143 --- (* 0.8) --> 0.8527357493393144\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=6984)\u001b[0m 2023-04-09 01:23:09,420\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=6984)\u001b[0m 2023-04-09 01:23:09,794\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=6984)\u001b[0m 2023-04-09 01:23:09,815\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=6984)\u001b[0m 2023-04-09 01:23:09,903\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3eb182fa5f574f99adc47b34ce8ed754\n",
      "\u001b[2m\u001b[36m(PG pid=6984)\u001b[0m 2023-04-09 01:23:09,903\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 2.123417377471924, '_episodes_total': 500}\n",
      "\u001b[2m\u001b[36m(pid=32552)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=32552)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=44696)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=44696)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=32552)\u001b[0m 2023-04-09 01:23:13,794\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=44696)\u001b[0m 2023-04-09 01:23:13,808\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=32552)\u001b[0m 2023-04-09 01:23:14,236\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=32552)\u001b[0m 2023-04-09 01:23:14,251\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=44696)\u001b[0m 2023-04-09 01:23:14,243\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=44696)\u001b[0m 2023-04-09 01:23:14,258\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=32552)\u001b[0m 2023-04-09 01:23:14,381\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_88af4a35ecf34dab8d61c7813be5ce16\n",
      "\u001b[2m\u001b[36m(PG pid=32552)\u001b[0m 2023-04-09 01:23:14,381\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 13, '_timesteps_total': None, '_time_total': 5.219691753387451, '_episodes_total': 1300}\n",
      "\u001b[2m\u001b[36m(PG pid=44696)\u001b[0m 2023-04-09 01:23:14,438\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f2250b5b7b8741139639794e3643beae\n",
      "\u001b[2m\u001b[36m(PG pid=44696)\u001b[0m 2023-04-09 01:23:14,438\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 12, '_timesteps_total': None, '_time_total': 4.792490482330322, '_episodes_total': 1200}\n",
      "\u001b[2m\u001b[36m(pid=47672)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47672)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=47672)\u001b[0m 2023-04-09 01:23:19,143\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47672)\u001b[0m 2023-04-09 01:23:19,621\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47672)\u001b[0m 2023-04-09 01:23:19,640\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47672)\u001b[0m 2023-04-09 01:23:19,770\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_36b6f4fb7ef34bb39b6e15f62e4ec7ad\n",
      "\u001b[2m\u001b[36m(PG pid=47672)\u001b[0m 2023-04-09 01:23:19,770\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 13, '_timesteps_total': None, '_time_total': 5.219691753387451, '_episodes_total': 1300}\n",
      "2023-04-09 01:23:20,527\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -6.870000) into trial a32d4_00000 (score = -7.050000)\n",
      "\n",
      "2023-04-09 01:23:20,528\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 0.0005 --- (resample) --> 1e-05\n",
      "alpha : 0.8527357493393144 --- (resample) --> 0.21863683180641824\n",
      "\n",
      "2023-04-09 01:23:20,836\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "\u001b[2m\u001b[36m(pid=12776)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=12776)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=12776)\u001b[0m 2023-04-09 01:23:30,672\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=12776)\u001b[0m 2023-04-09 01:23:31,150\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=12776)\u001b[0m 2023-04-09 01:23:31,173\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=12776)\u001b[0m 2023-04-09 01:23:31,372\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_1548b5c127be43b3bbf7859ff48b9587\n",
      "\u001b[2m\u001b[36m(PG pid=12776)\u001b[0m 2023-04-09 01:23:31,372\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 14, '_timesteps_total': None, '_time_total': 5.605109453201294, '_episodes_total': 1400}\n",
      "2023-04-09 01:23:32,436\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00000 (score = -6.940000) into trial a32d4_00001 (score = -7.020000)\n",
      "\n",
      "2023-04-09 01:23:32,438\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 1e-05 --- (shift left) --> 5e-05\n",
      "alpha : 0.21863683180641824 --- (resample) --> 0.41078627094742637\n",
      "\n",
      "2023-04-09 01:23:32,666\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00000 (score = -6.940000) into trial a32d4_00002 (score = -7.030000)\n",
      "\n",
      "2023-04-09 01:23:32,667\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00002:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.21863683180641824 --- (* 0.8) --> 0.1749094654451346\n",
      "\n",
      "2023-04-09 01:23:39,088\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -7.010000) into trial a32d4_00000 (score = -7.030000)\n",
      "\n",
      "2023-04-09 01:23:39,088\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 5e-05 --- (shift left) --> 0.0001\n",
      "alpha : 1.2791036240089715 --- (* 1.2) --> 1.5349243488107658\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=41616)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=41616)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=41192)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=41192)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=41192)\u001b[0m 2023-04-09 01:23:42,491\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=41616)\u001b[0m 2023-04-09 01:23:42,454\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=41192)\u001b[0m 2023-04-09 01:23:42,910\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=41192)\u001b[0m 2023-04-09 01:23:42,926\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=41616)\u001b[0m 2023-04-09 01:23:42,868\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=41616)\u001b[0m 2023-04-09 01:23:42,892\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=41192)\u001b[0m 2023-04-09 01:23:43,060\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_e0b8ceedbf1140a3847b8ab39891bc9e\n",
      "\u001b[2m\u001b[36m(PG pid=41192)\u001b[0m 2023-04-09 01:23:43,060\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 15, '_timesteps_total': None, '_time_total': 6.0892627239227295, '_episodes_total': 1500}\n",
      "\u001b[2m\u001b[36m(PG pid=41616)\u001b[0m 2023-04-09 01:23:43,047\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_d933244845214cc685bd782a84e1cbe2\n",
      "\u001b[2m\u001b[36m(PG pid=41616)\u001b[0m 2023-04-09 01:23:43,047\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 15, '_timesteps_total': None, '_time_total': 6.0892627239227295, '_episodes_total': 1500}\n",
      "2023-04-09 01:23:45,171\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -6.750000) into trial a32d4_00002 (score = -7.070000)\n",
      "\n",
      "2023-04-09 01:23:45,172\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00002:\n",
      "lr : 5e-05 --- (resample) --> 5e-05\n",
      "alpha : 0.41078627094742637 --- (resample) --> 0.013229944917218295\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=45700)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45700)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=41696)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=41696)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=45700)\u001b[0m 2023-04-09 01:23:49,517\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=41696)\u001b[0m 2023-04-09 01:23:49,509\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45700)\u001b[0m 2023-04-09 01:23:49,921\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45700)\u001b[0m 2023-04-09 01:23:49,937\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=41696)\u001b[0m 2023-04-09 01:23:49,916\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=41696)\u001b[0m 2023-04-09 01:23:49,932\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=41696)\u001b[0m 2023-04-09 01:23:50,053\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_62d96a6ef8b54da1aa0864bd3c01d262\n",
      "\u001b[2m\u001b[36m(PG pid=41696)\u001b[0m 2023-04-09 01:23:50,053\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 54, '_timesteps_total': None, '_time_total': 23.70386266708374, '_episodes_total': 5400}\n",
      "\u001b[2m\u001b[36m(PG pid=45700)\u001b[0m 2023-04-09 01:23:50,084\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f6379b78cde94ba7a7c7a59a99016f14\n",
      "\u001b[2m\u001b[36m(PG pid=45700)\u001b[0m 2023-04-09 01:23:50,084\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 55, '_timesteps_total': None, '_time_total': 24.127998113632202, '_episodes_total': 5500}\n",
      "\u001b[2m\u001b[36m(pid=45340)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45340)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=45340)\u001b[0m 2023-04-09 01:23:55,094\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45340)\u001b[0m 2023-04-09 01:23:55,499\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45340)\u001b[0m 2023-04-09 01:23:55,521\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=45340)\u001b[0m 2023-04-09 01:23:55,626\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_bc21bb47d72243f084124d73584e4b63\n",
      "\u001b[2m\u001b[36m(PG pid=45340)\u001b[0m 2023-04-09 01:23:55,626\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 18, '_timesteps_total': None, '_time_total': 7.524239778518677, '_episodes_total': 1800}\n",
      "2023-04-09 01:23:56,402\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00002 (score = -6.840000) into trial a32d4_00003 (score = -7.010000)\n",
      "\n",
      "2023-04-09 01:23:56,403\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00003:\n",
      "lr : 5e-05 --- (shift right) --> 1e-05\n",
      "alpha : 0.013229944917218295 --- (* 1.2) --> 0.015875933900661953\n",
      "\n",
      "2023-04-09 01:23:56,642\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00002 (score = -6.840000) into trial a32d4_00000 (score = -7.010000)\n",
      "\n",
      "2023-04-09 01:23:56,643\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 5e-05 --- (resample) --> 5e-05\n",
      "alpha : 0.013229944917218295 --- (* 1.2) --> 0.015875933900661953\n",
      "\n",
      "2023-04-09 01:23:56,918\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:23:58,029\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00002\n",
      "2023-04-09 01:24:00,145\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00002 (score = -6.970000) into trial a32d4_00001 (score = -7.040000)\n",
      "\n",
      "2023-04-09 01:24:00,146\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 5e-05 --- (shift left) --> 0.0001\n",
      "alpha : 0.013229944917218295 --- (* 1.2) --> 0.015875933900661953\n",
      "\n",
      "2023-04-09 01:24:04,256\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00002\n",
      "\u001b[2m\u001b[36m(pid=10628)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=10628)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=45784)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45784)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=10628)\u001b[0m 2023-04-09 01:24:07,224\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45784)\u001b[0m 2023-04-09 01:24:07,240\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=10628)\u001b[0m 2023-04-09 01:24:07,630\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=10628)\u001b[0m 2023-04-09 01:24:07,652\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=45784)\u001b[0m 2023-04-09 01:24:07,643\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45784)\u001b[0m 2023-04-09 01:24:07,658\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=10628)\u001b[0m 2023-04-09 01:24:07,790\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_52f747d0a5464738878fd6fb51b6c1d5\n",
      "\u001b[2m\u001b[36m(PG pid=10628)\u001b[0m 2023-04-09 01:24:07,790\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 19, '_timesteps_total': None, '_time_total': 7.9588868618011475, '_episodes_total': 1900}\n",
      "\u001b[2m\u001b[36m(PG pid=45784)\u001b[0m 2023-04-09 01:24:07,800\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_53df9875ad994a529f220c6baec4e20d\n",
      "\u001b[2m\u001b[36m(PG pid=45784)\u001b[0m 2023-04-09 01:24:07,800\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 19, '_timesteps_total': None, '_time_total': 7.9588868618011475, '_episodes_total': 1900}\n",
      "\u001b[2m\u001b[36m(pid=47348)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47348)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=47348)\u001b[0m 2023-04-09 01:24:10,355\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47348)\u001b[0m 2023-04-09 01:24:10,784\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47348)\u001b[0m 2023-04-09 01:24:10,802\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47348)\u001b[0m 2023-04-09 01:24:10,970\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ab4a8a47837948b08d5da3eecd392981\n",
      "\u001b[2m\u001b[36m(PG pid=47348)\u001b[0m 2023-04-09 01:24:10,970\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 25, '_timesteps_total': None, '_time_total': 10.767222881317139, '_episodes_total': 2500}\n",
      "2023-04-09 01:24:11,444\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -6.990000) into trial a32d4_00001 (score = -7.020000)\n",
      "\n",
      "2023-04-09 01:24:11,445\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 1e-05 --- (resample) --> 1e-05\n",
      "alpha : 0.015875933900661953 --- (* 1.2) --> 0.019051120680794342\n",
      "\n",
      "2023-04-09 01:24:12,614\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -6.900000) into trial a32d4_00002 (score = -7.050000)\n",
      "\n",
      "2023-04-09 01:24:12,616\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00002:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.015875933900661953 --- (* 0.8) --> 0.012700747120529564\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=46128)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46128)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=10920)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=10920)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=46128)\u001b[0m 2023-04-09 01:24:21,513\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(pid=47984)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47984)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=10920)\u001b[0m 2023-04-09 01:24:21,535\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46128)\u001b[0m 2023-04-09 01:24:21,912\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46128)\u001b[0m 2023-04-09 01:24:21,927\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=10920)\u001b[0m 2023-04-09 01:24:21,933\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=10920)\u001b[0m 2023-04-09 01:24:21,951\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=46128)\u001b[0m 2023-04-09 01:24:22,037\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_7a573ccdd65d4b6b8f834d085f978740\n",
      "\u001b[2m\u001b[36m(PG pid=46128)\u001b[0m 2023-04-09 01:24:22,037\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 24, '_timesteps_total': None, '_time_total': 10.155311584472656, '_episodes_total': 2400}\n",
      "\u001b[2m\u001b[36m(PG pid=10920)\u001b[0m 2023-04-09 01:24:22,115\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f9cf97c685de4cc28b9894ba8c7ed29f\n",
      "\u001b[2m\u001b[36m(PG pid=10920)\u001b[0m 2023-04-09 01:24:22,115\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 25, '_timesteps_total': None, '_time_total': 10.63399863243103, '_episodes_total': 2500}\n",
      "\u001b[2m\u001b[36m(PG pid=47984)\u001b[0m 2023-04-09 01:24:22,366\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47984)\u001b[0m 2023-04-09 01:24:22,841\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47984)\u001b[0m 2023-04-09 01:24:22,860\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47984)\u001b[0m 2023-04-09 01:24:23,013\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_cf5e96a0fdd0405f9a907fba95a44b46\n",
      "\u001b[2m\u001b[36m(PG pid=47984)\u001b[0m 2023-04-09 01:24:23,013\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 25, '_timesteps_total': None, '_time_total': 10.63399863243103, '_episodes_total': 2500}\n",
      "2023-04-09 01:24:23,403\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:24:23,929\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -6.950000) into trial a32d4_00003 (score = -7.030000)\n",
      "\n",
      "2023-04-09 01:24:23,930\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00003:\n",
      "lr : 1e-05 --- (shift left) --> 5e-05\n",
      "alpha : 0.019051120680794342 --- (* 1.2) --> 0.02286134481695321\n",
      "\n",
      "2023-04-09 01:24:25,034\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00002 (score = -6.900000) into trial a32d4_00001 (score = -7.030000)\n",
      "\n",
      "2023-04-09 01:24:25,036\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.012700747120529564 --- (resample) --> 0.5626827534701431\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=35148)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=35148)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=41196)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=41196)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=36500)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=36500)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=35148)\u001b[0m 2023-04-09 01:24:33,871\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=41196)\u001b[0m 2023-04-09 01:24:33,873\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=35148)\u001b[0m 2023-04-09 01:24:34,307\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=35148)\u001b[0m 2023-04-09 01:24:34,323\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=41196)\u001b[0m 2023-04-09 01:24:34,312\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=41196)\u001b[0m 2023-04-09 01:24:34,327\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=35148)\u001b[0m 2023-04-09 01:24:34,512\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_6416e947b5c348d98db55ce3c522c2ea\n",
      "\u001b[2m\u001b[36m(PG pid=35148)\u001b[0m 2023-04-09 01:24:34,512\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 27, '_timesteps_total': None, '_time_total': 11.538078546524048, '_episodes_total': 2700}\n",
      "\u001b[2m\u001b[36m(PG pid=41196)\u001b[0m 2023-04-09 01:24:34,496\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ce7bd73940ec4e3cb8c2e939cb123704\n",
      "\u001b[2m\u001b[36m(PG pid=41196)\u001b[0m 2023-04-09 01:24:34,496\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 27, '_timesteps_total': None, '_time_total': 11.50898265838623, '_episodes_total': 2700}\n",
      "\u001b[2m\u001b[36m(PG pid=36500)\u001b[0m 2023-04-09 01:24:34,649\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "2023-04-09 01:24:35,189\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00002 (score = -7.000000) into trial a32d4_00003 (score = -7.040000)\n",
      "\n",
      "2023-04-09 01:24:35,190\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00003:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.012700747120529564 --- (resample) --> 0.08364520585857382\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=36500)\u001b[0m 2023-04-09 01:24:35,103\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=36500)\u001b[0m 2023-04-09 01:24:35,117\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=36500)\u001b[0m 2023-04-09 01:24:35,230\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ab9d78594534477fbe209692121f55de\n",
      "\u001b[2m\u001b[36m(PG pid=36500)\u001b[0m 2023-04-09 01:24:35,230\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 27, '_timesteps_total': None, '_time_total': 11.50898265838623, '_episodes_total': 2700}\n",
      "\u001b[2m\u001b[36m(pid=43848)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43848)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-09 01:24:44,037\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00002 (score = -7.000000) into trial a32d4_00001 (score = -7.050000)\n",
      "\n",
      "2023-04-09 01:24:44,039\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 1e-05 --- (resample) --> 0.0001\n",
      "alpha : 0.012700747120529564 --- (* 1.2) --> 0.015240896544635476\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=43848)\u001b[0m 2023-04-09 01:24:44,724\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43848)\u001b[0m 2023-04-09 01:24:45,165\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43848)\u001b[0m 2023-04-09 01:24:45,185\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=43848)\u001b[0m 2023-04-09 01:24:45,321\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_79fae51b69544d8f9990fec9ed1ede19\n",
      "\u001b[2m\u001b[36m(PG pid=43848)\u001b[0m 2023-04-09 01:24:45,321\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 11.96222186088562, '_episodes_total': 2800}\n",
      "\u001b[2m\u001b[36m(pid=46980)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46980)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=27472)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=27472)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=46980)\u001b[0m 2023-04-09 01:24:53,983\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=27472)\u001b[0m 2023-04-09 01:24:53,983\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46980)\u001b[0m 2023-04-09 01:24:54,386\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46980)\u001b[0m 2023-04-09 01:24:54,401\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=27472)\u001b[0m 2023-04-09 01:24:54,391\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=27472)\u001b[0m 2023-04-09 01:24:54,405\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=27472)\u001b[0m 2023-04-09 01:24:54,520\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_3440e4c409424f2da1f539737ddede20\n",
      "\u001b[2m\u001b[36m(PG pid=27472)\u001b[0m 2023-04-09 01:24:54,520\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 92, '_timesteps_total': None, '_time_total': 38.71715593338013, '_episodes_total': 9200}\n",
      "\u001b[2m\u001b[36m(PG pid=46980)\u001b[0m 2023-04-09 01:24:54,578\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_5697b8579c6e433382324b58945b46cb\n",
      "\u001b[2m\u001b[36m(PG pid=46980)\u001b[0m 2023-04-09 01:24:54,578\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 44, '_timesteps_total': None, '_time_total': 18.39702606201172, '_episodes_total': 4400}\n",
      "2023-04-09 01:24:56,239\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -6.960000) into trial a32d4_00001 (score = -7.010000)\n",
      "\n",
      "2023-04-09 01:24:56,240\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.08364520585857382 --- (* 1.2) --> 0.10037424703028859\n",
      "\n",
      "2023-04-09 01:24:56,478\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:25:00,921\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00002\n",
      "2023-04-09 01:25:05,526\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00002\n",
      "2023-04-09 01:25:06,063\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00000 (score = -7.000000) into trial a32d4_00002 (score = -7.020000)\n",
      "\n",
      "2023-04-09 01:25:06,064\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00002:\n",
      "lr : 5e-05 --- (shift left) --> 0.0001\n",
      "alpha : 0.015875933900661953 --- (* 1.2) --> 0.019051120680794342\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=36316)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=36316)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=18508)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=18508)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=36316)\u001b[0m 2023-04-09 01:25:07,819\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=18508)\u001b[0m 2023-04-09 01:25:07,839\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=36316)\u001b[0m 2023-04-09 01:25:08,279\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=36316)\u001b[0m 2023-04-09 01:25:08,296\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=18508)\u001b[0m 2023-04-09 01:25:08,308\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=18508)\u001b[0m 2023-04-09 01:25:08,326\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=36316)\u001b[0m 2023-04-09 01:25:08,455\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ec90a5aae7f041acae7ed530b16491e7\n",
      "\u001b[2m\u001b[36m(PG pid=36316)\u001b[0m 2023-04-09 01:25:08,455\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 48, '_timesteps_total': None, '_time_total': 20.33593988418579, '_episodes_total': 4800}\n",
      "\u001b[2m\u001b[36m(PG pid=18508)\u001b[0m 2023-04-09 01:25:08,458\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_80f1fcb949aa4b5b8f7bf1d7bced87ad\n",
      "\u001b[2m\u001b[36m(PG pid=18508)\u001b[0m 2023-04-09 01:25:08,458\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 47, '_timesteps_total': None, '_time_total': 19.923572778701782, '_episodes_total': 4700}\n",
      "2023-04-09 01:25:09,716\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:10,545\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00000 (score = -7.000000) into trial a32d4_00003 (score = -7.030000)\n",
      "\n",
      "2023-04-09 01:25:10,547\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00003:\n",
      "lr : 5e-05 --- (shift right) --> 1e-05\n",
      "alpha : 0.015875933900661953 --- (* 1.2) --> 0.019051120680794342\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3120)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=3120)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=3120)\u001b[0m 2023-04-09 01:25:16,339\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=3120)\u001b[0m 2023-04-09 01:25:16,760\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=3120)\u001b[0m 2023-04-09 01:25:16,777\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=3120)\u001b[0m 2023-04-09 01:25:16,887\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_e7b1c267fb4c47389b71f6ea17dc903f\n",
      "\u001b[2m\u001b[36m(PG pid=3120)\u001b[0m 2023-04-09 01:25:16,887\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 111, '_timesteps_total': None, '_time_total': 47.488476037979126, '_episodes_total': 11100}\n",
      "\u001b[2m\u001b[36m(pid=42352)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=42352)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=29600)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=29600)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=42352)\u001b[0m 2023-04-09 01:25:20,462\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=29600)\u001b[0m 2023-04-09 01:25:20,479\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=42352)\u001b[0m 2023-04-09 01:25:20,917\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=42352)\u001b[0m 2023-04-09 01:25:20,939\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=29600)\u001b[0m 2023-04-09 01:25:20,932\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=29600)\u001b[0m 2023-04-09 01:25:20,950\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=29600)\u001b[0m 2023-04-09 01:25:21,093\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_eed2a37d194740d7a204f129820ce8a1\n",
      "\u001b[2m\u001b[36m(PG pid=29600)\u001b[0m 2023-04-09 01:25:21,093\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 118, '_timesteps_total': None, '_time_total': 50.82426357269287, '_episodes_total': 11800}\n",
      "\u001b[2m\u001b[36m(PG pid=42352)\u001b[0m 2023-04-09 01:25:21,135\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_75ae1b5e9a9f4ed99a51e5ba7ee6a9a3\n",
      "\u001b[2m\u001b[36m(PG pid=42352)\u001b[0m 2023-04-09 01:25:21,135\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 51, '_timesteps_total': None, '_time_total': 21.69231629371643, '_episodes_total': 5100}\n",
      "2023-04-09 01:25:22,088\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -6.950000) into trial a32d4_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-09 01:25:22,089\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.10037424703028859 --- (* 0.8) --> 0.08029939762423088\n",
      "\n",
      "2023-04-09 01:25:23,010\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:23,546\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:24,750\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:28,021\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:28,820\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -6.890000) into trial a32d4_00003 (score = -7.010000)\n",
      "\n",
      "2023-04-09 01:25:28,820\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00003:\n",
      "lr : 1e-05 --- (resample) --> 1e-05\n",
      "alpha : 0.10037424703028859 --- (* 1.2) --> 0.1204490964363463\n",
      "\n",
      "2023-04-09 01:25:29,306\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:29,834\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:30,317\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:31,506\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:32,078\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "\u001b[2m\u001b[36m(pid=47064)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47064)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-09 01:25:32,570\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "\u001b[2m\u001b[36m(PG pid=47064)\u001b[0m 2023-04-09 01:25:33,077\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47064)\u001b[0m 2023-04-09 01:25:33,481\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47064)\u001b[0m 2023-04-09 01:25:33,498\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47064)\u001b[0m 2023-04-09 01:25:33,654\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ed1124b97eca4fac918618cd8c069d87\n",
      "\u001b[2m\u001b[36m(PG pid=47064)\u001b[0m 2023-04-09 01:25:33,654\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 52, '_timesteps_total': None, '_time_total': 22.111260175704956, '_episodes_total': 5200}\n",
      "2023-04-09 01:25:34,096\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -6.960000) into trial a32d4_00000 (score = -7.010000)\n",
      "\n",
      "2023-04-09 01:25:34,097\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.10037424703028859 --- (* 0.8) --> 0.08029939762423088\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=44436)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=44436)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=42676)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=42676)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=44436)\u001b[0m 2023-04-09 01:25:39,079\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=42676)\u001b[0m 2023-04-09 01:25:39,107\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=44436)\u001b[0m 2023-04-09 01:25:39,487\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=44436)\u001b[0m 2023-04-09 01:25:39,501\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=42676)\u001b[0m 2023-04-09 01:25:39,513\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=44436)\u001b[0m 2023-04-09 01:25:39,600\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_6755757318b24f3eac6170b582ba5ab4\n",
      "\u001b[2m\u001b[36m(PG pid=44436)\u001b[0m 2023-04-09 01:25:39,600\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 63, '_timesteps_total': None, '_time_total': 27.33842706680298, '_episodes_total': 6300}\n",
      "\u001b[2m\u001b[36m(PG pid=42676)\u001b[0m 2023-04-09 01:25:39,531\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=42676)\u001b[0m 2023-04-09 01:25:39,698\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_27780711f6d94a1895b3efe338c5f578\n",
      "\u001b[2m\u001b[36m(PG pid=42676)\u001b[0m 2023-04-09 01:25:39,698\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 130, '_timesteps_total': None, '_time_total': 56.32498002052307, '_episodes_total': 13000}\n",
      "2023-04-09 01:25:40,045\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -6.990000) into trial a32d4_00003 (score = -7.030000)\n",
      "\n",
      "2023-04-09 01:25:40,046\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00003:\n",
      "lr : 1e-05 --- (shift left) --> 5e-05\n",
      "alpha : 0.10037424703028859 --- (* 1.2) --> 0.1204490964363463\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=45012)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45012)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=45216)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45216)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-04-09 01:25:44,665\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45216)\u001b[0m 2023-04-09 01:25:44,658\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-04-09 01:25:45,053\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-04-09 01:25:45,069\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=45216)\u001b[0m 2023-04-09 01:25:45,052\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45216)\u001b[0m 2023-04-09 01:25:45,068\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-04-09 01:25:45,189\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f27ad96686414efbbd9412d75775bf37\n",
      "\u001b[2m\u001b[36m(PG pid=45012)\u001b[0m 2023-04-09 01:25:45,189\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 72, '_timesteps_total': None, '_time_total': 31.485403776168823, '_episodes_total': 7200}\n",
      "\u001b[2m\u001b[36m(PG pid=45216)\u001b[0m 2023-04-09 01:25:45,239\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_5022a55bbfba4662aaeabfc6211835f6\n",
      "\u001b[2m\u001b[36m(PG pid=45216)\u001b[0m 2023-04-09 01:25:45,239\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 73, '_timesteps_total': None, '_time_total': 31.893115758895874, '_episodes_total': 7300}\n",
      "2023-04-09 01:25:46,282\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:25:47,158\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "\u001b[2m\u001b[36m(pid=19560)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=19560)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=25104)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=25104)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-04-09 01:25:49,255\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00001\n",
      "2023-04-09 01:25:49,733\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00000 (score = -7.000000) into trial a32d4_00001 (score = -7.050000)\n",
      "\n",
      "2023-04-09 01:25:49,734\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.08029939762423088 --- (resample) --> 0.9777412290789299\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=19560)\u001b[0m 2023-04-09 01:25:49,825\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=25104)\u001b[0m 2023-04-09 01:25:49,809\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=19560)\u001b[0m 2023-04-09 01:25:50,276\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=19560)\u001b[0m 2023-04-09 01:25:50,294\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=25104)\u001b[0m 2023-04-09 01:25:50,257\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=25104)\u001b[0m 2023-04-09 01:25:50,283\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=19560)\u001b[0m 2023-04-09 01:25:50,442\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_81334366dc7f42d48b2e42409fcbc962\n",
      "\u001b[2m\u001b[36m(PG pid=19560)\u001b[0m 2023-04-09 01:25:50,442\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 131, '_timesteps_total': None, '_time_total': 56.768640995025635, '_episodes_total': 13100}\n",
      "\u001b[2m\u001b[36m(PG pid=25104)\u001b[0m 2023-04-09 01:25:50,476\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_7021f998c0f749e6aabd59e412078d7f\n",
      "\u001b[2m\u001b[36m(PG pid=25104)\u001b[0m 2023-04-09 01:25:50,476\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 73, '_timesteps_total': None, '_time_total': 31.893115758895874, '_episodes_total': 7300}\n",
      "2023-04-09 01:25:53,183\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:25:55,730\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -6.980000) into trial a32d4_00000 (score = -7.070000)\n",
      "\n",
      "2023-04-09 01:25:55,731\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 5e-05 --- (shift left) --> 0.0001\n",
      "alpha : 0.1204490964363463 --- (* 1.2) --> 0.14453891572361555\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=37004)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=37004)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=37004)\u001b[0m 2023-04-09 01:25:59,856\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=37004)\u001b[0m 2023-04-09 01:26:00,326\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=37004)\u001b[0m 2023-04-09 01:26:00,343\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=37004)\u001b[0m 2023-04-09 01:26:00,468\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_2cdf6a36dc234f73901a6c5eadc055f6\n",
      "\u001b[2m\u001b[36m(PG pid=37004)\u001b[0m 2023-04-09 01:26:00,468\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 80, '_timesteps_total': None, '_time_total': 34.76921820640564, '_episodes_total': 8000}\n",
      "\u001b[2m\u001b[36m(pid=30356)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=30356)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=8316)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=8316)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=30356)\u001b[0m 2023-04-09 01:26:06,087\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=8316)\u001b[0m 2023-04-09 01:26:06,105\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=30356)\u001b[0m 2023-04-09 01:26:06,540\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=30356)\u001b[0m 2023-04-09 01:26:06,557\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=8316)\u001b[0m 2023-04-09 01:26:06,568\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=8316)\u001b[0m 2023-04-09 01:26:06,584\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=30356)\u001b[0m 2023-04-09 01:26:06,693\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_8650c621291d48d793af2d3d651d8a1f\n",
      "\u001b[2m\u001b[36m(PG pid=30356)\u001b[0m 2023-04-09 01:26:06,693\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 81, '_timesteps_total': None, '_time_total': 35.422144174575806, '_episodes_total': 8100}\n",
      "\u001b[2m\u001b[36m(PG pid=8316)\u001b[0m 2023-04-09 01:26:06,757\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_eff1a5ffcf5d4a35ad412457c26fb672\n",
      "\u001b[2m\u001b[36m(PG pid=8316)\u001b[0m 2023-04-09 01:26:06,757\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 141, '_timesteps_total': None, '_time_total': 61.21249580383301, '_episodes_total': 14100}\n",
      "2023-04-09 01:26:07,215\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:26:07,451\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "2023-04-09 01:26:08,366\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00000 (score = -6.960000) into trial a32d4_00001 (score = -7.020000)\n",
      "\n",
      "2023-04-09 01:26:08,367\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 0.0001 --- (resample) --> 0.001\n",
      "alpha : 0.14453891572361555 --- (* 0.8) --> 0.11563113257889245\n",
      "\n",
      "2023-04-09 01:26:08,625\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:26:09,108\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00002 (score = -7.000000) into trial a32d4_00003 (score = -7.100000)\n",
      "\n",
      "2023-04-09 01:26:09,109\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00003:\n",
      "lr : 0.0001 --- (shift right) --> 5e-05\n",
      "alpha : 0.019051120680794342 --- (* 0.8) --> 0.015240896544635474\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=44772)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=44772)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=45904)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45904)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=35536)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=35536)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:18,227\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45904)\u001b[0m 2023-04-09 01:26:18,227\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:18,614\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:18,630\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=45904)\u001b[0m 2023-04-09 01:26:18,614\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45904)\u001b[0m 2023-04-09 01:26:18,629\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:18,771\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_0c555d19233640dc8fc0c4af15e06535\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:18,771\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 83, '_timesteps_total': None, '_time_total': 36.46069574356079, '_episodes_total': 8300}\n",
      "\u001b[2m\u001b[36m(PG pid=45904)\u001b[0m 2023-04-09 01:26:18,768\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_28f6022970174dc6b256b6dc6730057f\n",
      "\u001b[2m\u001b[36m(PG pid=45904)\u001b[0m 2023-04-09 01:26:18,768\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 84, '_timesteps_total': None, '_time_total': 37.00155544281006, '_episodes_total': 8400}\n",
      "\u001b[2m\u001b[36m(PG pid=35536)\u001b[0m 2023-04-09 01:26:18,930\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=35536)\u001b[0m 2023-04-09 01:26:19,377\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=35536)\u001b[0m 2023-04-09 01:26:19,392\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=35536)\u001b[0m 2023-04-09 01:26:19,508\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_c6461bf2b2964dbfb30917225a1348b5\n",
      "\u001b[2m\u001b[36m(PG pid=35536)\u001b[0m 2023-04-09 01:26:19,509\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 144, '_timesteps_total': None, '_time_total': 62.792868852615356, '_episodes_total': 14400}\n",
      "2023-04-09 01:26:20,669\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -7.000000) into trial a32d4_00001 (score = -7.030000)\n",
      "\n",
      "2023-04-09 01:26:20,670\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 5e-05 --- (resample) --> 0.001\n",
      "alpha : 0.015240896544635474 --- (resample) --> 0.38605107481938994\n",
      "\n",
      "2023-04-09 01:26:21,302\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -7.000000) into trial a32d4_00000 (score = -7.040000)\n",
      "\n",
      "2023-04-09 01:26:21,306\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 5e-05 --- (shift left) --> 0.0001\n",
      "alpha : 0.015240896544635474 --- (* 1.2) --> 0.01828907585356257\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=47936)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47936)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=47936)\u001b[0m 2023-04-09 01:26:30,322\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47936)\u001b[0m 2023-04-09 01:26:30,674\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47936)\u001b[0m 2023-04-09 01:26:30,697\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(pid=41760)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=41760)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=46840)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46840)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=47936)\u001b[0m 2023-04-09 01:26:30,819\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_258289c1cfab415985124ce0de8cad58\n",
      "\u001b[2m\u001b[36m(PG pid=47936)\u001b[0m 2023-04-09 01:26:30,819\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 145, '_timesteps_total': None, '_time_total': 63.21511793136597, '_episodes_total': 14500}\n",
      "\u001b[2m\u001b[36m(PG pid=46840)\u001b[0m 2023-04-09 01:26:31,594\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=41760)\u001b[0m 2023-04-09 01:26:31,589\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46840)\u001b[0m 2023-04-09 01:26:31,975\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46840)\u001b[0m 2023-04-09 01:26:31,995\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=41760)\u001b[0m 2023-04-09 01:26:31,968\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=41760)\u001b[0m 2023-04-09 01:26:31,990\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=46840)\u001b[0m 2023-04-09 01:26:32,118\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_bf8a3f14bb4b4e1a9416d6f55ba44a5e\n",
      "\u001b[2m\u001b[36m(PG pid=46840)\u001b[0m 2023-04-09 01:26:32,119\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 163, '_timesteps_total': None, '_time_total': 71.06272053718567, '_episodes_total': 16300}\n",
      "\u001b[2m\u001b[36m(PG pid=41760)\u001b[0m 2023-04-09 01:26:32,145\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_679eba8aec774af49a8cf180d2dfe9b3\n",
      "\u001b[2m\u001b[36m(PG pid=41760)\u001b[0m 2023-04-09 01:26:32,145\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 146, '_timesteps_total': None, '_time_total': 64.05291604995728, '_episodes_total': 14600}\n",
      "2023-04-09 01:26:32,657\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -7.000000) into trial a32d4_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-09 01:26:32,660\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 5e-05 --- (shift left) --> 0.0001\n",
      "alpha : 0.015240896544635474 --- (* 0.8) --> 0.01219271723570838\n",
      "\n",
      "2023-04-09 01:26:39,818\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "\u001b[2m\u001b[36m(pid=45160)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45160)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=45160)\u001b[0m 2023-04-09 01:26:42,670\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45160)\u001b[0m 2023-04-09 01:26:43,074\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45160)\u001b[0m 2023-04-09 01:26:43,097\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=45160)\u001b[0m 2023-04-09 01:26:43,188\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_e8b03d3948bb4062a4a1002beabf6915\n",
      "\u001b[2m\u001b[36m(PG pid=45160)\u001b[0m 2023-04-09 01:26:43,188\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 155, '_timesteps_total': None, '_time_total': 72.60689163208008, '_episodes_total': 15500}\n",
      "2023-04-09 01:26:43,678\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -7.000000) into trial a32d4_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-09 01:26:43,679\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 5e-05 --- (shift left) --> 0.0001\n",
      "alpha : 0.015240896544635474 --- (resample) --> 0.421639586763047\n",
      "\n",
      "2023-04-09 01:26:44,290\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -7.000000) into trial a32d4_00001 (score = -7.010000)\n",
      "\n",
      "2023-04-09 01:26:44,291\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00001:\n",
      "lr : 5e-05 --- (shift right) --> 1e-05\n",
      "alpha : 0.015240896544635474 --- (resample) --> 0.36968175747629106\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=43108)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43108)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=44772)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=44772)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=43108)\u001b[0m 2023-04-09 01:26:53,093\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43108)\u001b[0m 2023-04-09 01:26:53,530\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43108)\u001b[0m 2023-04-09 01:26:53,554\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=43108)\u001b[0m 2023-04-09 01:26:53,691\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_c4b2e421891c4dfabb3e9aa689799971\n",
      "\u001b[2m\u001b[36m(PG pid=43108)\u001b[0m 2023-04-09 01:26:53,692\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 172, '_timesteps_total': None, '_time_total': 80.24286246299744, '_episodes_total': 17200}\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:53,658\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:54,056\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:54,072\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:54,219\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_b45e2c3f328a4072acbdd2803662ca8e\n",
      "\u001b[2m\u001b[36m(PG pid=44772)\u001b[0m 2023-04-09 01:26:54,219\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 173, '_timesteps_total': None, '_time_total': 80.64518618583679, '_episodes_total': 17300}\n",
      "2023-04-09 01:26:55,273\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00003 (score = -7.000000) into trial a32d4_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-09 01:26:55,274\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 5e-05 --- (shift left) --> 0.0001\n",
      "alpha : 0.015240896544635474 --- (* 1.2) --> 0.01828907585356257\n",
      "\n",
      "2023-04-09 01:26:57,626\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00003\n",
      "\u001b[2m\u001b[36m(pid=31888)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=31888)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=43052)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43052)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=31888)\u001b[0m 2023-04-09 01:27:04,192\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43052)\u001b[0m 2023-04-09 01:27:04,192\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=31888)\u001b[0m 2023-04-09 01:27:04,555\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=31888)\u001b[0m 2023-04-09 01:27:04,576\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=43052)\u001b[0m 2023-04-09 01:27:04,554\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43052)\u001b[0m 2023-04-09 01:27:04,576\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=31888)\u001b[0m 2023-04-09 01:27:04,697\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_fcaf4be3cde643e2933edb8c7c666738\n",
      "\u001b[2m\u001b[36m(PG pid=31888)\u001b[0m 2023-04-09 01:27:04,697\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 191, '_timesteps_total': None, '_time_total': 88.08768939971924, '_episodes_total': 19100}\n",
      "\u001b[2m\u001b[36m(PG pid=43052)\u001b[0m 2023-04-09 01:27:04,702\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_4f2af1352e374c03b765c64f27eba45d\n",
      "\u001b[2m\u001b[36m(PG pid=43052)\u001b[0m 2023-04-09 01:27:04,702\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 175, '_timesteps_total': None, '_time_total': 81.60794186592102, '_episodes_total': 17500}\n",
      "2023-04-09 01:27:05,030\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial PG_TwoStepGame_a32d4_00000\n",
      "2023-04-09 01:27:05,404\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -7.000000) into trial a32d4_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-09 01:27:05,405\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 1e-05 --- (shift left) --> 5e-05\n",
      "alpha : 0.36968175747629106 --- (resample) --> 0.4106295656382982\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=44868)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=44868)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=44868)\u001b[0m 2023-04-09 01:27:13,420\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=44868)\u001b[0m 2023-04-09 01:27:13,804\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=44868)\u001b[0m 2023-04-09 01:27:13,824\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=44868)\u001b[0m 2023-04-09 01:27:13,957\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_f66c9557cfb64d6a8e5ff70aebf3cea7\n",
      "\u001b[2m\u001b[36m(PG pid=44868)\u001b[0m 2023-04-09 01:27:13,957\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 176, '_timesteps_total': None, '_time_total': 81.93553256988525, '_episodes_total': 17600}\n",
      "2023-04-09 01:27:14,324\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -7.000000) into trial a32d4_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-09 01:27:14,325\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.36968175747629106 --- (* 1.2) --> 0.44361810897154924\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=43388)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=43388)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=46608)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46608)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=43388)\u001b[0m 2023-04-09 01:27:22,798\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:22,795\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=43388)\u001b[0m 2023-04-09 01:27:23,184\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=43388)\u001b[0m 2023-04-09 01:27:23,198\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:23,184\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:23,198\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=43388)\u001b[0m 2023-04-09 01:27:23,372\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_9ab009ead427471281a2f9b3cf177d95\n",
      "\u001b[2m\u001b[36m(PG pid=43388)\u001b[0m 2023-04-09 01:27:23,372\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 194, '_timesteps_total': None, '_time_total': 87.8522732257843, '_episodes_total': 19400}\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:23,323\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_ebb0419adfe34a0898daedfb6e0804d3\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:23,323\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 193, '_timesteps_total': None, '_time_total': 87.51710271835327, '_episodes_total': 19300}\n",
      "2023-04-09 01:27:23,687\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -7.000000) into trial a32d4_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-09 01:27:23,687\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 1e-05 --- (shift right (noop)) --> 1e-05\n",
      "alpha : 0.36968175747629106 --- (* 1.2) --> 0.44361810897154924\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=45656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=45656)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=47940)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=47940)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=45656)\u001b[0m 2023-04-09 01:27:32,500\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=47940)\u001b[0m 2023-04-09 01:27:32,487\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=45656)\u001b[0m 2023-04-09 01:27:32,855\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=45656)\u001b[0m 2023-04-09 01:27:32,868\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47940)\u001b[0m 2023-04-09 01:27:32,848\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=47940)\u001b[0m 2023-04-09 01:27:32,862\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=47940)\u001b[0m 2023-04-09 01:27:32,990\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_dc95c5cc873a433aa9988e7e234bcefb\n",
      "\u001b[2m\u001b[36m(PG pid=47940)\u001b[0m 2023-04-09 01:27:32,990\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 195, '_timesteps_total': None, '_time_total': 88.19341683387756, '_episodes_total': 19500}\n",
      "\u001b[2m\u001b[36m(PG pid=45656)\u001b[0m 2023-04-09 01:27:33,042\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_d077cbbddeb74f14b5022fdd667a13b2\n",
      "\u001b[2m\u001b[36m(PG pid=45656)\u001b[0m 2023-04-09 01:27:33,042\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 194, '_timesteps_total': None, '_time_total': 87.8522732257843, '_episodes_total': 19400}\n",
      "2023-04-09 01:27:33,585\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial a32d4_00001 (score = -7.000000) into trial a32d4_00000 (score = -7.000000)\n",
      "\n",
      "2023-04-09 01:27:33,586\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triala32d4_00000:\n",
      "lr : 1e-05 --- (shift left) --> 5e-05\n",
      "alpha : 0.36968175747629106 --- (* 1.2) --> 0.44361810897154924\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=46608)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=46608)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:41,563\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property alpha not supported.\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:41,929\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:41,944\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:42,098\tINFO trainable.py:791 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\subar\\AppData\\Local\\Temp\\checkpoint_tmp_5241089ec50f4a10b379666a7e1892af\n",
      "\u001b[2m\u001b[36m(PG pid=46608)\u001b[0m 2023-04-09 01:27:42,098\tINFO trainable.py:800 -- Current state after restoring: {'_iteration': 196, '_timesteps_total': None, '_time_total': 88.52110266685486, '_episodes_total': 19600}\n",
      "2023-04-09 01:27:44,317\tINFO tune.py:798 -- Total run time: 327.96 seconds (327.34 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "#from gym.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "from gymnasium.spaces import Dict, Discrete, MultiDiscrete, Tuple,Box\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"PG\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=16)\n",
    "#parser.add_argument(\"--num-gpus\", type=int, default=1)\n",
    "parser.add_argument(\"--num-gpus\", type=float, default=0.5)\n",
    "#parser.add_argument(\"render_mode\", type=int, default=1)\n",
    "parser.add_argument(\n",
    "    \"--mixer\",\n",
    "    type=str,\n",
    "    default=\"qmix\",\n",
    "    choices=[\"qmix\", \"vdn\", \"none\"],\n",
    "    help=\"The mixer model to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=200, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=70000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=8.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "            #if agent_id.startswith(\"low_level_\"):\n",
    "                #return \"low_level_policy\"\n",
    "            #else:\n",
    "                #return \"high_level_policy\"\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ray.init(num_cpus=16, num_gpus=1,local_mode=args.local_mode)\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [0],\n",
    "        \"group_2\": [0,1],\n",
    "        \"group_3\": [0]\n",
    "    }\n",
    "\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=observation_space, act_space=action_space\n",
    "        ),\n",
    "    )\n",
    "    \"\"\"\n",
    "    from ray.tune import register_env\n",
    "    from ray.rllib.algorithms.dqn import DQN \n",
    "    YourExternalEnv = ... \n",
    "    register_env(\"my_env\", \n",
    "        lambda config: YourExternalEnv(config))\n",
    "    trainer = DQN(env=\"my_env\") \n",
    "    while True: \n",
    "        print(trainer.train()) \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(TwoStepGame)\n",
    "        .framework(args.framework)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "    def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "        #if agent_id.startswith(\"low_level_\"):\n",
    "        if agent_id.startswith(\"group_1\"):\n",
    "            return \"low_level_policy\"\n",
    "        else:\n",
    "            return \"high_level_policy\"\n",
    "\n",
    "    if args.run == \"QMIX\":\n",
    "        \n",
    "        (\n",
    "            config.framework(\"torch\")\n",
    "\n",
    "            .training(mixer=args.mixer, train_batch_size=32)\n",
    "            .resources(\n",
    "                # How many GPUs does the local worker (driver) need? For most algos,\n",
    "                # this is where the learning updates happen.\n",
    "                # Set this to > 1 for multi-GPU learning.\n",
    "                num_gpus=1,\n",
    "                # How many GPUs does each RolloutWorker (`num_workers`) need?\n",
    "                num_gpus_per_worker=0.25,\n",
    "        )           \n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        observation_space,\n",
    "                        action_space,\n",
    "                        config.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        #Tuple([observation_space,Box(-inf, inf, (1,), float64)]),#,Box()\n",
    "                        Tuple([observation_space,obs_space]),\n",
    "                        Tuple([action_space,obs_space]),#action_space,\n",
    "                        config.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn#lambda agent_id, episode, worker, **kwargs: \"pol2\"\n",
    "                #policy_mapping_fn=(lambda agent_id, episode, worker, **kw: (\"pol1\" if agent_id == \"agent1\" else \"pol2\")\n",
    "    #)#policy_mapping_fn,\n",
    "            )\n",
    "            .rollouts(num_rollout_workers=16, rollout_fragment_length=4)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"final_epsilon\": 0.0,\n",
    "                }\n",
    "            )\n",
    "            .environment(\n",
    "                env=\"grouped_twostep\",\n",
    "                env_config={\n",
    "                    \"separate_state_space\": True,\n",
    "                    \"one_hot_state_encoding\": True,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "    pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr='training_iteration',\n",
    "        metric='episode_reward_mean',#'loss',\n",
    "        mode='min',\n",
    "        perturbation_interval=1,\n",
    "        hyperparam_mutations={\n",
    "            \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"alpha\": tune.uniform(0.0, 1.0),\n",
    "        }\n",
    "    )\n",
    "    results = tune.Tuner(\n",
    "        args.run,\n",
    "        run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "        \n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=4,\n",
    "            scheduler=pbt_scheduler,\n",
    "        ),\n",
    "        \n",
    "        param_space=config,\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61234407",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebcf5d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 01:17:50,076\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2023-04-09 01:17:53,593\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-04-09 01:18:00,099\tWARNING env.py:53 -- Skipping env checking for this experiment\n",
      "2023-04-09 01:18:00,101\tWARNING algorithm_config.py:596 -- Cannot create PGConfig from given `config_dict`! Property worker_index not supported.\n",
      "2023-04-09 01:18:00,121\tINFO trainable.py:172 -- Trainable.setup took 10.044 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': 1.734908401966095, 'cur_lr': 0.0004}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 200.0, 'num_grad_updates_lifetime': 1.5, 'diff_num_grad_updates_vs_sampler_policy': 0.5}}, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'sampler_results': {'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.04, 'episode_len_mean': 2.0, 'episode_media': {}, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 1.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 0.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 0.0, 8.0, 7.0, 8.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 8.0, 1.0, 1.0, 8.0, 1.0, 8.0, 7.0, 7.0, 0.0, 8.0, 7.0, 0.0, 1.0, 8.0, 8.0, 0.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.722238673499567, 'mean_inference_ms': 0.9728213447836502, 'mean_action_processing_ms': 0.1196446110360065, 'mean_env_wait_ms': 0.009950713731756254, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021090136633978948, 'ViewRequirementAgentConnector_ms': 0.1826595730251736}}, 'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.04, 'episode_len_mean': 2.0, 'episodes_this_iter': 100, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [7.0, 1.0, 1.0, 1.0, 7.0, 8.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 1.0, 7.0, 7.0, 8.0, 1.0, 7.0, 1.0, 7.0, 1.0, 7.0, 0.0, 1.0, 1.0, 1.0, 8.0, 7.0, 1.0, 7.0, 7.0, 1.0, 7.0, 7.0, 0.0, 7.0, 1.0, 0.0, 7.0, 1.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 7.0, 8.0, 0.0, 8.0, 7.0, 8.0, 8.0, 7.0, 0.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 1.0, 7.0, 1.0, 1.0, 7.0, 8.0, 1.0, 1.0, 8.0, 1.0, 8.0, 7.0, 7.0, 0.0, 8.0, 7.0, 0.0, 1.0, 8.0, 8.0, 0.0, 7.0, 1.0, 7.0, 8.0, 7.0, 1.0, 7.0], 'episode_lengths': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.722238673499567, 'mean_inference_ms': 0.9728213447836502, 'mean_action_processing_ms': 0.1196446110360065, 'mean_env_wait_ms': 0.009950713731756254, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'StateBufferConnector_ms': 0.021090136633978948, 'ViewRequirementAgentConnector_ms': 0.1826595730251736}, 'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400, 'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'timesteps_total': 200, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 400, 'timers': {'training_iteration_time_ms': 382.005, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 11.246, 'learn_throughput': 17783.401, 'synch_weights_time_ms': 0.0}, 'counters': {'num_env_steps_sampled': 200, 'num_env_steps_trained': 200, 'num_agent_steps_sampled': 400, 'num_agent_steps_trained': 400}, 'done': False, 'episodes_total': 100, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': 'fe7f6d8b5a114a758a22eeeef73ef676', 'date': '2023-04-09_01-18-00', 'timestamp': 1681028280, 'time_this_iter_s': 0.38300538063049316, 'time_total_s': 0.38300538063049316, 'pid': 7968, 'hostname': 'Gipsy', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.TwoStepGame'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': False, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x000001E0074024F0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x000001E006BFECA0>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 0}, 'time_since_restore': 0.38300538063049316, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 10.046207427978516, 'perf': {'cpu_util_percent': 3.2, 'ram_util_percent': 41.3, 'gpu_util_percent0': 0.05, 'vram_util_percent0': 0.6751708984375}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorboardX\\summary.py:234: DeprecationWarning: using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "  cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "for i in range(1):\n",
    "    print(algo.train())\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd49a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
