{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c25af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#core enviroment libraries for RL \n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "#utilities \n",
    "import numpy as np\n",
    "import random\n",
    "#these libraries have to do with the agents \n",
    "import ray\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    LAST_TARGET_UPDATE_TS,\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_TARGET_UPDATES,\n",
    "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
    ")\n",
    "from ray.rllib.utils.replay_buffers.utils import sample_min_n_steps_from_buffer\n",
    "from ray.rllib.utils.typing import ResultDict, AlgorithmConfigDict\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import air, tune\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "#import pathpy as pp\n",
    "\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "import mne# preprocessing and brain importation and utilities library including acessing and preprocessing the EEG data\n",
    "#these libraries have to do with the free energy principle\n",
    "#import pymdp\n",
    "#from pymdp import utils\n",
    "#from pymdp.agent import Agent\n",
    "#from gym.spaces import \n",
    "\n",
    "#from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "\n",
    "#optimization of deep learning and RL aspects of algorithm these will allow the algorithm to run faster with less memory \n",
    "#from composer import Trainer\n",
    "#from nebullvm.api.functions import optimize_model \n",
    "from numba import jit\n",
    "\n",
    "\"\"\"\n",
    "dependency network\n",
    "\n",
    "Qmix.py - has qmixpolicy.py as a dependency \n",
    "Qmixpolicy.py has  mixers.py and Model.py dependencies\n",
    "Model.py -base\n",
    "mixers.py -base\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#from ray.rllib.utils.torch_utils import \n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d242ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import ivy# library for interoperable across all deep learning frameworks \n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import torch\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "#from nebulgym.decorators.torch_decorators import accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91837aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset\n",
    "\n",
    "#below libraries are core libraries for q-mix Rllib algorithm\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from torch import nn\n",
    "\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "\n",
    "from typing import Optional, Type,  Dict, List, Tuple\n",
    "\n",
    "from ray.rllib.algorithms.simple_q.simple_q import SimpleQ, SimpleQConfig\n",
    "from ray.rllib.algorithms.qmix.qmix_policy import QMixTorchPolicy\n",
    "from ray.rllib.utils.replay_buffers.utils import update_priorities_in_replay_buffer\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae64b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.execution.train_ops import (\n",
    "    multi_gpu_train_one_step,\n",
    "    train_one_step,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "import logging\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "from ray.rllib.algorithms.qmix.mixers import VDNMixer, QMixer\n",
    "from ray.rllib.algorithms.qmix.model import RNNModel, _get_size\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GROUP_REWARDS\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import _unpack_obs\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.policy.rnn_sequencing import chop_into_sequences\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
    "from ray.rllib.utils.typing import TensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829fdbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nitime\n",
    "from deeptime.sindy import SINDy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f84644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed=111)\n",
    "#causal inference libraries\n",
    "#from timeawarepc.tpc import cfc_tpc, cfc_pc, cfc_gc\n",
    "#from timeawarepc.simulate_data import *\n",
    "#from timeawarepc.find_cfc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6fca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "        # Place hidden states on same device as model.\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "    #accelerate_model()# this will accelerate the model \n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, hidden_state, seq_lens):\n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)\n",
    "        return q, [h]\n",
    "\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size\n",
    "#model = RNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d18160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "class bmodel(TorchModelV2, nn.Module,ivy.Module):\n",
    "    \"\"\"The default RNN model for QMIX.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "        self.obs_size = _get_size(obs_space)\n",
    "        self.rnn_hidden_dim = model_config[\"lstm_cell_size\"]\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "        # Place hidden states on same device as model.\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    #@override(ModelV2)\n",
    "    def _forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)\n",
    "        return q, [h]\n",
    "\n",
    "\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b8c918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CModel(torch.nn.Module,ivy.Module):\n",
    "    def __init__(self):#,obs_space,action_space,num_outputs,model_config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        \"\"\"\n",
    "        self._avg_pool = torch.nn.AvgPool2d(4)\n",
    "        self._linear = torch.nn.Linear(3136, 1024)\n",
    "        self._relu = torch.nn.ReLU()\n",
    "        #self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        #self.obs_size = _get_size(obs_space)\n",
    "        #self.n_agents = model_config[\"n_agents\"]\n",
    "        \n",
    "        tfk = tf.keras\n",
    "        tfkl = tf.keras.layers\n",
    "        input_shape=2\n",
    "        tfpl = tfp.layers\n",
    "        tfd = tfp.distributions\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        encoded_size = 16\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                            reinterpreted_batch_ndims=1)\n",
    "        tfpl = tfp.layers\n",
    "        encoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=input_shape),\n",
    "            #tfkl.Dense(8)\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "                          activation=None),\n",
    "\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                    encoded_size,\n",
    "                    activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.2)),\n",
    "        ])\n",
    "        decoder = tfk.Sequential([\n",
    "            tfkl.InputLayer(input_shape=[encoded_size]),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_size))\n",
    "            #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits)\n",
    "\n",
    "        ])\n",
    "        negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "    #@override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "        # Place hidden states on same device as model.\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "            ]\n",
    "    #@override(ModelV2)\n",
    "    def _forward(self, x):\n",
    "\n",
    "        x = self._avg_pool(x).mean(dim=-3).view(-1, 3136)\n",
    "        x = self._relu(self._linear(x))\n",
    "        return self._linears(x)\n",
    "def _get_size(obs_space):\n",
    "    return get_preprocessor(obs_space)(obs_space).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12b6641",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'obs_space', 'action_space', 'num_outputs', 'model_config', and 'name'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_20936\\3601144890.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    model = bmodel()#CModel()#CModel()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m\u001b[1;31m:\u001b[0m __init__() missing 5 required positional arguments: 'obs_space', 'action_space', 'num_outputs', 'model_config', and 'name'\n"
     ]
    }
   ],
   "source": [
    "#ivy.set_framework('torch') \n",
    "model = bmodel()#CModel()#CModel()\n",
    "cmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#all\n",
    "\n",
    "#this compiles under if modelV2 is not passed wiht any arguments. It demands arguments when modelV2 is passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f716e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<laplace.lllaplace.FullLLLaplace object at 0x0000018D5F20B1C0>\n"
     ]
    }
   ],
   "source": [
    "print(cmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43509e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put this code into \n",
    "\n",
    "model = optimize_model(\n",
    "  cmodel, input_data=input_data, device=\"gpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b934dc",
   "metadata": {},
   "source": [
    "As of january 13 2022 we have performed a sucessful partial execution test of the project pegasus agent \n",
    "network\n",
    "\n",
    "We still have to determine the following:\n",
    "1. how to create heterogenuous agents\n",
    "2. how to visualize using pysurfer\n",
    "3. figure out what if any alterations will be needed for the mixer network \n",
    "\n",
    "ANS for 3: we should perhaps think of the mixer network as maximizing the overall free energy and \n",
    "acting on the level of active inference instead of predictive coding. \n",
    "\n",
    "we are going to use laplace-torch for among other things model selection\n",
    "\n",
    "This means that after we finish pretraining our RL algorithm on normal conditions we will fine tune and select simulated models generated by our RL algorithm by how well they fit the frieburg data. \n",
    "\n",
    "an Evidence lower bound of F and a posterior of Q or F[Q,y] = -ELBO of - H[q(x)] <- entropy term which can ebe rewritten as kullbacker divergence [Q(x) || P(x|y)] -ln(P(y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a978f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch must be installed.\n",
    "#torch, nn = try_import_torch(error=True)\n",
    "\n",
    "#logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class QMixLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        target_model,\n",
    "        mixer,\n",
    "        target_mixer,\n",
    "        n_agents,\n",
    "        n_actions,\n",
    "        double_q=True,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.mixer = mixer\n",
    "        self.target_mixer = target_mixer\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        self.double_q = double_q\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rewards,\n",
    "        actions,\n",
    "        terminated,\n",
    "        mask,\n",
    "        obs,\n",
    "        next_obs,\n",
    "        action_mask,\n",
    "        next_action_mask,\n",
    "        state=None,\n",
    "        next_state=None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the loss.\n",
    "        Args:\n",
    "            rewards: Tensor of shape [B, T, n_agents]\n",
    "            actions: Tensor of shape [B, T, n_agents]\n",
    "            terminated: Tensor of shape [B, T, n_agents]\n",
    "            mask: Tensor of shape [B, T, n_agents]\n",
    "            obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            next_obs: Tensor of shape [B, T, n_agents, obs_size]\n",
    "            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            next_action_mask: Tensor of shape [B, T, n_agents, n_actions]\n",
    "            state: Tensor of shape [B, T, state_dim] (optional)\n",
    "            next_state: Tensor of shape [B, T, state_dim] (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert either none or both of state and next_state are given\n",
    "        if state is None and next_state is None:\n",
    "            state = obs  # default to state being all agents' observations\n",
    "            next_state = next_obs\n",
    "        elif (state is None) != (next_state is None):\n",
    "            raise ValueError(\n",
    "                \"Expected either neither or both of `state` and \"\n",
    "                \"`next_state` to be given. Got: \"\n",
    "                \"\\n`state` = {}\\n`next_state` = {}\".format(state, next_state)\n",
    "            )\n",
    "\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = _unroll_mac(self.model, obs)\n",
    "\n",
    "        # Pick the Q-Values for the actions taken -> [B * n_agents, T]\n",
    "        chosen_action_qvals = torch.gather(\n",
    "            mac_out, dim=3, index=actions.unsqueeze(3)\n",
    "        ).squeeze(3)\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = _unroll_mac(self.target_model, next_obs)\n",
    "\n",
    "        # Mask out unavailable actions for the t+1 step\n",
    "        ignore_action_tp1 = (next_action_mask == 0) & (mask == 1).unsqueeze(-1)\n",
    "        target_mac_out[ignore_action_tp1] = -np.inf\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.double_q:\n",
    "            # Double Q learning computes the target Q values by selecting the\n",
    "            # t+1 timestep action according to the \"policy\" neural network and\n",
    "            # then estimating the Q-value of that action with the \"target\"\n",
    "            # neural network\n",
    "            \"\"\"\n",
    "            note that the target neural network uses the RNN network defined above and that \n",
    "\n",
    "            policy neural network also appears to be the mixer network \n",
    "\n",
    "            double q-learning will find 2 q values. One will be used in our case to identify an action that minimizes free energy while another \n",
    "            will be used to evaluate that action \n",
    "            \"\"\"\n",
    "\n",
    "            # Compute the t+1 Q-values to be used in action selection\n",
    "            # using next_obs\n",
    "            mac_out_tp1 = _unroll_mac(self.model, next_obs)\n",
    "\n",
    "            # mask out unallowed actions\n",
    "            mac_out_tp1[ignore_action_tp1] = -np.inf\n",
    "\n",
    "            # obtain best actions at t+1 according to policy NN\n",
    "            cur_max_actions = mac_out_tp1.argmax(dim=3, keepdim=True)\n",
    "\n",
    "            # use the target network to estimate the Q-values of policy\n",
    "            # network's selected actions\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(\n",
    "                3\n",
    "            )\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "        assert (\n",
    "            target_max_qvals.min().item() != -np.inf\n",
    "        ), \"target_max_qvals contains a masked action; \\\n",
    "            there may be a state with no valid actions.\"\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state)\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, next_state)\n",
    "\n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n",
    "\n",
    "        # Td-error\n",
    "        td_error = chosen_action_qvals - targets.detach()\n",
    "\n",
    "        mask = mask.expand_as(td_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = td_error * mask\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        loss = (masked_td_error ** 2).sum() / mask.sum()\n",
    "        return loss, mask, masked_td_error, chosen_action_qvals, targets\n",
    "\n",
    "\n",
    "class QMixTorchPolicy(TorchPolicy):\n",
    "    \"\"\"QMix impl. Assumes homogeneous agents for now.\n",
    "    You must use MultiAgentEnv.with_agent_groups() to group agents\n",
    "    together for QMix. This creates the proper Tuple obs/action spaces and\n",
    "    populates the '_group_rewards' info field.\n",
    "    Action masking: to specify an action mask for individual agents, use a\n",
    "    dict space with an action_mask key, e.g. {\"obs\": ob, \"action_mask\": mask}.\n",
    "    The mask space must be `Box(0, 1, (n_actions,))`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        _validate(obs_space, action_space)\n",
    "        config = dict(ray.rllib.algorithms.qmix.qmix.DEFAULT_CONFIG, **config)\n",
    "        self.framework = \"torch\"\n",
    "\n",
    "        self.n_agents = len(obs_space.original_space.spaces)\n",
    "        config[\"model\"][\"n_agents\"] = self.n_agents\n",
    "        self.n_actions = action_space.spaces[0].n\n",
    "        self.h_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "        self.has_env_global_state = False\n",
    "        self.has_action_mask = False\n",
    "\n",
    "        agent_obs_space = obs_space.original_space.spaces[0]\n",
    "        if isinstance(agent_obs_space, gym.spaces.Dict):\n",
    "            space_keys = set(agent_obs_space.spaces.keys())\n",
    "            if \"obs\" not in space_keys:\n",
    "                raise ValueError(\"Dict obs space must have subspace labeled `obs`\")\n",
    "            self.obs_size = _get_size(agent_obs_space.spaces[\"obs\"])\n",
    "            if \"action_mask\" in space_keys:\n",
    "                mask_shape = tuple(agent_obs_space.spaces[\"action_mask\"].shape)\n",
    "                if mask_shape != (self.n_actions,):\n",
    "                    raise ValueError(\n",
    "                        \"Action mask shape must be {}, got {}\".format(\n",
    "                            (self.n_actions,), mask_shape\n",
    "                        )\n",
    "                    )\n",
    "                self.has_action_mask = True\n",
    "            if ENV_STATE in space_keys:\n",
    "                self.env_global_state_shape = _get_size(\n",
    "                    agent_obs_space.spaces[ENV_STATE]\n",
    "                )\n",
    "                self.has_env_global_state = True\n",
    "            else:\n",
    "                self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            # The real agent obs space is nested inside the dict\n",
    "            config[\"model\"][\"full_obs_space\"] = agent_obs_space\n",
    "            agent_obs_space = agent_obs_space.spaces[\"obs\"]\n",
    "        else:\n",
    "            self.obs_size = _get_size(agent_obs_space)\n",
    "            self.env_global_state_shape = (self.obs_size, self.n_agents)\n",
    "            \n",
    "            \n",
    "        ivy.set_framework('torch') \n",
    "        model = bmodel()#CModel()#CModel()\n",
    "        cmodel = Laplace(model,'regression', subset_of_weights='last_layer', hessian_structure='full')#all\n",
    "        \n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"model\",\n",
    "            default_model=cmodel#RNNModel,\n",
    "        )\n",
    "\n",
    "        super().__init__(obs_space, action_space, config, model=self.model)\n",
    "\n",
    "        self.target_model = ModelCatalog.get_model_v2(\n",
    "            agent_obs_space,\n",
    "            action_space.spaces[0],\n",
    "            self.n_actions,\n",
    "            config[\"model\"],\n",
    "            framework=\"torch\",\n",
    "            name=\"target_model\",\n",
    "            default_model=cmodel#CModel#RNNModel,#this target model is the agent model \n",
    "        ).to(self.device)\n",
    "\n",
    "        self.exploration = self._create_exploration()\n",
    "\n",
    "        # Setup the mixer network.\n",
    "        if config[\"mixer\"] is None:\n",
    "            self.mixer = None\n",
    "            self.target_mixer = None\n",
    "        elif config[\"mixer\"] == \"qmix\":\n",
    "            self.mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "            self.target_mixer = QMixer(\n",
    "                self.n_agents, self.env_global_state_shape, config[\"mixing_embed_dim\"]\n",
    "            ).to(self.device)\n",
    "        elif config[\"mixer\"] == \"vdn\":\n",
    "            self.mixer = VDNMixer().to(self.device)\n",
    "            self.target_mixer = VDNMixer().to(self.device)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown mixer type {}\".format(config[\"mixer\"]))\n",
    "\n",
    "        self.cur_epsilon = 1.0\n",
    "        self.update_target()  # initial sync\n",
    "\n",
    "        # Setup optimizer\n",
    "        self.params = list(self.model.parameters())\n",
    "        if self.mixer:\n",
    "            self.params += list(self.mixer.parameters())\n",
    "        self.loss = QMixLoss(\n",
    "            self.model,\n",
    "            self.target_model,\n",
    "            self.mixer,\n",
    "            self.target_mixer,\n",
    "            self.n_agents,\n",
    "            self.n_actions,\n",
    "            self.config[\"double_q\"],\n",
    "            self.config[\"gamma\"],\n",
    "        )\n",
    "        from torch.optim import RMSprop\n",
    "        #it appears that this RMSprop is optimizing the mixer network and not the agent network\n",
    "        self.rmsprop_optimizer = RMSprop(\n",
    "            params=self.params,\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"optim_alpha\"],\n",
    "            eps=config[\"optim_eps\"],\n",
    "        )\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions_from_input_dict(\n",
    "        self,\n",
    "        input_dict: Dict[str, TensorType],\n",
    "        explore: bool = None,\n",
    "        timestep: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:\n",
    "\n",
    "        obs_batch = input_dict[SampleBatch.OBS]\n",
    "        state_batches = []\n",
    "        i = 0\n",
    "        while f\"state_in_{i}\" in input_dict:\n",
    "            state_batches.append(input_dict[f\"state_in_{i}\"])\n",
    "            i += 1\n",
    "\n",
    "        explore = explore if explore is not None else self.config[\"explore\"]\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        # We need to ensure we do not use the env global state\n",
    "        # to compute actions\n",
    "\n",
    "        # Compute actions\n",
    "        with torch.no_grad():\n",
    "            q_values, hiddens = _mac(\n",
    "                self.model,\n",
    "                torch.as_tensor(obs_batch, dtype=torch.float, device=self.device),\n",
    "                [\n",
    "                    torch.as_tensor(np.array(s), dtype=torch.float, device=self.device)\n",
    "                    for s in state_batches\n",
    "                ],\n",
    "            )\n",
    "            avail = torch.as_tensor(action_mask, dtype=torch.float, device=self.device)\n",
    "            masked_q_values = q_values.clone()\n",
    "            masked_q_values[avail == 0.0] = -float(\"inf\")\n",
    "            masked_q_values_folded = torch.reshape(\n",
    "                masked_q_values, [-1] + list(masked_q_values.shape)[2:]\n",
    "            )\n",
    "            actions, _ = self.exploration.get_exploration_action(\n",
    "                action_distribution=TorchCategorical(masked_q_values_folded),\n",
    "                timestep=timestep,\n",
    "                explore=explore,\n",
    "            )\n",
    "            actions = (\n",
    "                torch.reshape(actions, list(masked_q_values.shape)[:-1]).cpu().numpy()\n",
    "            )\n",
    "            hiddens = [s.cpu().numpy() for s in hiddens]\n",
    "\n",
    "        return tuple(actions.transpose([1, 0])), hiddens, {}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_actions(self, *args, **kwargs):\n",
    "        return self.compute_actions_from_input_dict(*args, **kwargs)\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def compute_log_likelihoods(\n",
    "        self,\n",
    "        actions,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "    ):\n",
    "        obs_batch, action_mask, _ = self._unpack_observation(obs_batch)\n",
    "        return np.zeros(obs_batch.size()[0])\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        obs_batch, action_mask, env_global_state = self._unpack_observation(\n",
    "            samples[SampleBatch.CUR_OBS]\n",
    "        )\n",
    "        (\n",
    "            next_obs_batch,\n",
    "            next_action_mask,\n",
    "            next_env_global_state,\n",
    "        ) = self._unpack_observation(samples[SampleBatch.NEXT_OBS])\n",
    "        group_rewards = self._get_group_rewards(samples[SampleBatch.INFOS])\n",
    "\n",
    "        input_list = [\n",
    "            group_rewards,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            samples[SampleBatch.ACTIONS],\n",
    "            samples[SampleBatch.DONES],\n",
    "            obs_batch,\n",
    "            next_obs_batch,\n",
    "        ]\n",
    "        if self.has_env_global_state:\n",
    "            input_list.extend([env_global_state, next_env_global_state])\n",
    "\n",
    "        output_list, _, seq_lens = chop_into_sequences(\n",
    "            episode_ids=samples[SampleBatch.EPS_ID],\n",
    "            unroll_ids=samples[SampleBatch.UNROLL_ID],\n",
    "            agent_indices=samples[SampleBatch.AGENT_INDEX],\n",
    "            feature_columns=input_list,\n",
    "            state_columns=[],  # RNN states not used here\n",
    "            max_seq_len=self.config[\"model\"][\"max_seq_len\"],\n",
    "            dynamic_max=True,\n",
    "        )\n",
    "        # These will be padded to shape [B * T, ...]\n",
    "        if self.has_env_global_state:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                dones,\n",
    "                obs,\n",
    "                next_obs,\n",
    "                env_global_state,\n",
    "                next_env_global_state,\n",
    "            ) = output_list\n",
    "        else:\n",
    "            (\n",
    "                rew,\n",
    "                action_mask,\n",
    "                next_action_mask,\n",
    "                act,\n",
    "                dones,\n",
    "                obs,\n",
    "                next_obs,\n",
    "            ) = output_list\n",
    "        B, T = len(seq_lens), max(seq_lens)\n",
    "\n",
    "        def to_batches(arr, dtype):\n",
    "            new_shape = [B, T] + list(arr.shape[1:])\n",
    "            return torch.as_tensor(\n",
    "                np.reshape(arr, new_shape), dtype=dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        rewards = to_batches(rew, torch.float)\n",
    "        actions = to_batches(act, torch.long)\n",
    "        obs = to_batches(obs, torch.float).reshape([B, T, self.n_agents, self.obs_size])\n",
    "        action_mask = to_batches(action_mask, torch.float)\n",
    "        next_obs = to_batches(next_obs, torch.float).reshape(\n",
    "            [B, T, self.n_agents, self.obs_size]\n",
    "        )\n",
    "        next_action_mask = to_batches(next_action_mask, torch.float)\n",
    "        if self.has_env_global_state:\n",
    "            env_global_state = to_batches(env_global_state, torch.float)\n",
    "            next_env_global_state = to_batches(next_env_global_state, torch.float)\n",
    "\n",
    "        # TODO(ekl) this treats group termination as individual termination\n",
    "        terminated = (\n",
    "            to_batches(dones, torch.float).unsqueeze(2).expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Create mask for where index is < unpadded sequence length\n",
    "        filled = np.reshape(\n",
    "            np.tile(np.arange(T, dtype=np.float32), B), [B, T]\n",
    "        ) < np.expand_dims(seq_lens, 1)\n",
    "        mask = (\n",
    "            torch.as_tensor(filled, dtype=torch.float, device=self.device)\n",
    "            .unsqueeze(2)\n",
    "            .expand(B, T, self.n_agents)\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss_out, mask, masked_td_error, chosen_action_qvals, targets = self.loss(\n",
    "            rewards,\n",
    "            actions,\n",
    "            terminated,\n",
    "            mask,\n",
    "            obs,\n",
    "            next_obs,\n",
    "            action_mask,\n",
    "            next_action_mask,\n",
    "            env_global_state,\n",
    "            next_env_global_state,\n",
    "        )\n",
    "\n",
    "        # Optimise\n",
    "        self.rmsprop_optimizer.zero_grad()\n",
    "        loss_out.backward()\n",
    "        grad_norm_info = apply_grad_clipping(self, self.rmsprop_optimizer, loss_out)\n",
    "        self.rmsprop_optimizer.step()\n",
    "\n",
    "        mask_elems = mask.sum().item()\n",
    "        stats = {\n",
    "            \"loss\": loss_out.item(),\n",
    "            \"td_error_abs\": masked_td_error.abs().sum().item() / mask_elems,\n",
    "            \"q_taken_mean\": (chosen_action_qvals * mask).sum().item() / mask_elems,\n",
    "            \"target_mean\": (targets * mask).sum().item() / mask_elems,\n",
    "        }\n",
    "        stats.update(grad_norm_info)\n",
    "\n",
    "        return {LEARNER_STATS_KEY: stats}\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_initial_state(self):  # initial RNN state\n",
    "        return [\n",
    "            s.expand([self.n_agents, -1]).cpu().numpy()\n",
    "            for s in self.model.get_initial_state()\n",
    "        ]\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            \"model\": self._cpu_dict(self.model.state_dict()),\n",
    "            \"target_model\": self._cpu_dict(self.target_model.state_dict()),\n",
    "            \"mixer\": self._cpu_dict(self.mixer.state_dict()) if self.mixer else None,\n",
    "            \"target_mixer\": self._cpu_dict(self.target_mixer.state_dict())\n",
    "            if self.mixer\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(self._device_dict(weights[\"model\"]))\n",
    "        self.target_model.load_state_dict(self._device_dict(weights[\"target_model\"]))\n",
    "        if weights[\"mixer\"] is not None:\n",
    "            self.mixer.load_state_dict(self._device_dict(weights[\"mixer\"]))\n",
    "            self.target_mixer.load_state_dict(\n",
    "                self._device_dict(weights[\"target_mixer\"])\n",
    "            )\n",
    "\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def get_state(self):\n",
    "        state = self.get_weights()\n",
    "        state[\"cur_epsilon\"] = self.cur_epsilon\n",
    "        return state\n",
    "\n",
    "    @override(TorchPolicy)\n",
    "    def set_state(self, state):\n",
    "        self.set_weights(state)\n",
    "        self.set_epsilon(state[\"cur_epsilon\"])\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        logger.debug(\"Updated target networks\")\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cur_epsilon = epsilon\n",
    "\n",
    "    def _get_group_rewards(self, info_batch):\n",
    "        group_rewards = np.array(\n",
    "            [info.get(GROUP_REWARDS, [0.0] * self.n_agents) for info in info_batch]\n",
    "        )\n",
    "        return group_rewards\n",
    "\n",
    "    def _device_dict(self, state_dict):\n",
    "        return {\n",
    "            k: torch.as_tensor(v, device=self.device) for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _cpu_dict(state_dict):\n",
    "        return {k: v.cpu().detach().numpy() for k, v in state_dict.items()}\n",
    "\n",
    "    def _unpack_observation(self, obs_batch):\n",
    "        \"\"\"Unpacks the observation, action mask, and state (if present)\n",
    "        from agent grouping.\n",
    "        Returns:\n",
    "            obs (np.ndarray): obs tensor of shape [B, n_agents, obs_size]\n",
    "            mask (np.ndarray): action mask, if any\n",
    "            state (np.ndarray or None): state tensor of shape [B, state_size]\n",
    "                or None if it is not in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        unpacked = _unpack_obs(\n",
    "            np.array(obs_batch, dtype=np.float32),\n",
    "            self.observation_space.original_space,\n",
    "            tensorlib=np,\n",
    "        )\n",
    "\n",
    "        if isinstance(unpacked[0], dict):\n",
    "            assert \"obs\" in unpacked[0]\n",
    "            unpacked_obs = [np.concatenate(tree.flatten(u[\"obs\"]), 1) for u in unpacked]\n",
    "        else:\n",
    "            unpacked_obs = unpacked\n",
    "\n",
    "        obs = np.concatenate(unpacked_obs, axis=1).reshape(\n",
    "            [len(obs_batch), self.n_agents, self.obs_size]\n",
    "        )\n",
    "\n",
    "        if self.has_action_mask:\n",
    "            action_mask = np.concatenate(\n",
    "                [o[\"action_mask\"] for o in unpacked], axis=1\n",
    "            ).reshape([len(obs_batch), self.n_agents, self.n_actions])\n",
    "        else:\n",
    "            action_mask = np.ones(\n",
    "                [len(obs_batch), self.n_agents, self.n_actions], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.has_env_global_state:\n",
    "            state = np.concatenate(tree.flatten(unpacked[0][ENV_STATE]), 1)\n",
    "        else:\n",
    "            state = None\n",
    "        return obs, action_mask, state\n",
    "\n",
    "\n",
    "def _validate(obs_space, action_space):\n",
    "    if not hasattr(obs_space, \"original_space\") or not isinstance(\n",
    "        obs_space.original_space, gym.spaces.Tuple\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Obs space must be a Tuple, got {}. Use \".format(obs_space)\n",
    "            + \"MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space, gym.spaces.Tuple):\n",
    "        raise ValueError(\n",
    "            \"Action space must be a Tuple, got {}. \".format(action_space)\n",
    "            + \"Use MultiAgentEnv.with_agent_groups() to group related \"\n",
    "            \"agents for QMix.\"\n",
    "        )\n",
    "    if not isinstance(action_space.spaces[0], gym.spaces.Discrete):\n",
    "        raise ValueError(\n",
    "            \"QMix requires a discrete action space, got {}\".format(\n",
    "                action_space.spaces[0]\n",
    "            )\n",
    "        )\n",
    "    if len({str(x) for x in obs_space.original_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: observations of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(obs_space.original_space.spaces)\n",
    "        )\n",
    "    if len({str(x) for x in action_space.spaces}) > 1:\n",
    "        raise ValueError(\n",
    "            \"Implementation limitation: action space of grouped agents \"\n",
    "            \"must be homogeneous, got {}\".format(action_space.spaces)\n",
    "        )\n",
    "\n",
    "\n",
    "def _mac(model, obs, h):\n",
    "    \"\"\"Forward pass of the multi-agent controller.\n",
    "    Args:\n",
    "        model: TorchModelV2 class\n",
    "        obs: Tensor of shape [B, n_agents, obs_size]\n",
    "        h: List of tensors of shape [B, n_agents, h_size]\n",
    "    Returns:\n",
    "        q_vals: Tensor of shape [B, n_agents, n_actions]\n",
    "        h: Tensor of shape [B, n_agents, h_size]\n",
    "    \"\"\"\n",
    "    B, n_agents = obs.size(0), obs.size(1)\n",
    "    if not isinstance(obs, dict):\n",
    "        obs = {\"obs\": obs}\n",
    "    obs_agents_as_batches = {k: _drop_agent_dim(v) for k, v in obs.items()}\n",
    "    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n",
    "    \n",
    "    model = optimize_model(\n",
    "      model, input_data=obs, device=\"gpu\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    q_flat, h_flat = model(obs_agents_as_batches, h_flat, None)\n",
    "    return q_flat.reshape([B, n_agents, -1]), [\n",
    "        s.reshape([B, n_agents, -1]) for s in h_flat\n",
    "    ]\n",
    "\n",
    "\n",
    "def _unroll_mac(model, obs_tensor):\n",
    "    \"\"\"Computes the estimated Q values for an entire trajectory batch\"\"\"\n",
    "    B = obs_tensor.size(0)\n",
    "    T = obs_tensor.size(1)\n",
    "    n_agents = obs_tensor.size(2)\n",
    "    \"\"\"\n",
    "    model = optimize_model(\n",
    "      cmodel, input_data=input_data, device=\"gpu\"\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    mac_out = []\n",
    "    h = [s.expand([B, n_agents, -1]) for s in model.get_initial_state()]\n",
    "    for t in range(T):\n",
    "        q, h = _mac(model, obs_tensor[:, t], h)\n",
    "        mac_out.append(q)\n",
    "    mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "    return mac_out\n",
    "\n",
    "\n",
    "def _drop_agent_dim(T):\n",
    "    shape = list(T.shape)\n",
    "    B, n_agents = shape[0], shape[1]\n",
    "    return T.reshape([B * n_agents] + shape[2:])\n",
    "\n",
    "\n",
    "def _add_agent_dim(T, n_agents):\n",
    "    shape = list(T.shape)\n",
    "    B = shape[0] // n_agents\n",
    "    assert shape[0] % n_agents == 0\n",
    "    return T.reshape([B, n_agents] + shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = Dict(\n",
    "    {\n",
    "        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = Discrete(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = Tuple({\n",
    "    \"agent1\": Dict(\n",
    "        {\n",
    "            #\"prob\": Discrete(100),\n",
    "            \"action_potential\": Discrete(1) #excitatory neurons\n",
    "        } ,dtype=np.float32),\n",
    "    \"agent2\": Dict(\n",
    "        {\n",
    "            #\"prob\": Discrete(100),\n",
    "            \"outsideAction\":gym.spaces.Box(low=-1.0, high=1.0, shape=(10,)),\n",
    "            \"action_potential\": Discrete(1) #inhibitory neurons\n",
    "        }, dtype=np.float32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3981ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStepGameWithGroupedAgents(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        env = brain(env_config)\n",
    "        tuple_obs_space = Tuple([observation_space, observation_space])\n",
    "        tuple_act_space = Tuple([action_space, action_space])\n",
    "\n",
    "        self.env = env.with_agent_groups(\n",
    "            groups={\"agents\": [0, 1]},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self._agent_ids = {\"agents\"}\n",
    "\n",
    "    def reset(self):#, *, seed=None, options=None):\n",
    "        return self.env.reset()#(seed=seed, options=options)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84263d3e",
   "metadata": {},
   "source": [
    "Everything up to this point has sucessfully executed\n",
    "\n",
    "if prediction errors are what it is sending up and predictions are what it is sending down what is it sending to neurons\n",
    "to its side?\n",
    "\n",
    "ANS: most likely predictions but we may need to check this\n",
    "\n",
    "In the active inference book it shows that the highest level neurons and their expectations conditioned on policy has no \n",
    "lateral connections while the lower level neurons which are represented with average under policy do have lateral connections\n",
    "\n",
    "The lowest level neurons are represented just as average under policies. \n",
    "\n",
    "New hypothesis: it may just be excitatory or inhibitory signals that are being sent laterally. (see phenomena such as lateral inhibition. \n",
    "\n",
    "In addition we should consider bringing back at this stage temporal and spatial summation. We know now that the upper level neuronal agents form discrete models and only the lowest level neurons send and recieve continuous signals. \n",
    "\n",
    "Question: Could lateral inhibition be related to that sampling method in QUASI? \n",
    "\n",
    "Note: empirical priors from high level neuronal agents drive habits (which can be seen as analogous to model-free RL) and influence policy selection independently of expected free energy. while The expected free energy of a policy that comes from predicted outcomes and prediction error drives goal directed behavior and is analogous to model-based RL. Low dopamine favors context sensitive priors in the indirect pathways whose role is to suppress implausible policies. (it favors habits over goal directed behavior). Complete depletion fo dopamine leads to an inability to enact specific policies.\n",
    "\n",
    "we can hold beliefs not just about states of the world but also about the fixed or slowly varying parameters that determine dependencies between variables. The substrate of these beliefs is the efficacy of synaptic connections representing time-varying variables. When we observe an outcome that we believe was generated by a given state we can update beliefs about the parameter connecting the two, reflecting an increase in the probability of them occuring in the future \n",
    "\n",
    "Translation: we will also need to infer not just states of the world but also parameters that determine dependenceis between variables (which in this case are different neurons or populations of neurons)\n",
    "\n",
    "where teh lowest level might deal with the requisite changes in muscle length descending input is based on decisions about which movement to make\n",
    "\n",
    "connections entering and leaving a cotrical column relate to likelhood distributions wheras transition probabilities and continuous dynamics depend on connections within a microcircuit\n",
    "\n",
    "This suggests that learning dynamics should lead to changes in intrninsic connectivity while learning observations should modify extrinsic connectivity. \n",
    "\n",
    "question for the future: Does lateral inhibition play an analogous role as the XOR parity constraints do in the WISH scheme? If does a WISH like mechanism play a role in creating the low quality approximate attention schema that creates consciousness?\n",
    "\n",
    "parameters to of interest: prior precision (already built in to and taken care of by laplace torch, connection parameter will need to figure out how to handle this. \n",
    "\n",
    "what if we put the other parameters like the connection parameter into the local reward function? \n",
    "\n",
    "our agent network needs to:\n",
    "1. infer the hidden state \n",
    "2. generate a prediction to send down the hierarchy or an action to send laterally to another neuronal agent\n",
    "3. also needs to infer the parameterization that determines dependencies between neuronal agents\n",
    "\n",
    "I dont believe that our variational autoencoder in its present state can do all three things. we need to find some way to figure these things out sequentially by first inferring hiddens tates then finding the parameterization and finally generating the predictions and actions. Our variational autoencoder is only configured to 1 and 3 and partially 2 if combined with laplace-torch\n",
    "\n",
    "Apparently a hidden markov model is a special case of POMDP in which choices and and behavior are ignored. The first half of our agent autoencoder may need to act like a hidden markov model. \n",
    "\n",
    "For first term of the global reward function see the optimal experimental design in pyro. Apparently the first term of minimzing free energy has the same exact form as this. The second term is expected ambiguity where \n",
    "\n",
    "qmix uses double q learning where it learns 2 q values: one for choosing the action and a second for evaluating the action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e0c7c",
   "metadata": {},
   "source": [
    "########################################\n",
    "\n",
    "we could in theory use WISH rather then the described message passing scheme or using maximum a posteriori estimates for each state as described in active inference as predicitive coding \n",
    "\n",
    "psuedo code for global reward function\n",
    "\n",
    "global_rew = \n",
    "\n",
    "def entropy(A):\n",
    "  \"\"\" Compute the entropy of a set of conditional distributions, i.e. one entropy value per column \"\"\"\n",
    "\n",
    "  H_A = - (A * log_stable(A)).sum(axis=0)\n",
    "\n",
    "  return H_A\n",
    "\n",
    "def kl_divergence(qo_u, C):\n",
    "  \"\"\" Compute the Kullback-Leibler divergence between two 1-D categorical distributions\"\"\"\n",
    "  \n",
    "  return (log_stable(qo_u) - log_stable(C)).dot(qo_u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f376856",
   "metadata": {},
   "source": [
    "For the first phase we can do on policy training to simulate a normal brain but for the second phase we should do strictly off-line training or even just parameter tuning to simulate the parkinsons brain. \n",
    "\n",
    "Off-line training is when data is used to train the RL algorithm\n",
    "\n",
    "if we imitate the maxq hierarchical algorithm then correspondence between the local and global reward function should just be each agent generating a prediction given a prediction error that ends up being a sub task. These sub tasks are divisble decomposed components of our overall goal of our brain\n",
    "\n",
    "The way the subtasks are divided will be influenced by the fact that our lowest level neuronal agents will have continuous action and observation spaces while our higher level neuronal agents will be discrete and only be able to act in a categorical way\n",
    "\n",
    "one possible way to deal with this is to create 2 enviroments: one for the upper level brain and one for the humanoid enviroment which the lowest level neuronal agents will act on. \n",
    "\n",
    "psuedo code for rew of lowest level neuronal agents: \n",
    "\n",
    "rew = difference between predictions and observation\n",
    "\n",
    "psuedo code for higher level neuronal agents\n",
    "\n",
    "rew = -ELBO\n",
    "\n",
    "to calculate suprise we can do 0.1*0.81+0.9*0.01 = 0.09\n",
    "\n",
    "-ln(0.09) = 2.4 units of surprise where 0.1 is unconditional prior, 0.9 is unconditional posterior, 0.01 is conditional prior and 0.81 is conditional posterior. If we update the unconditional prior to 1.0 then we get\n",
    "1*0.81 + 0.9*0.01 = 0.9 -> -ln(0.9) = 0.1 units of surprise. conlcusion a 1.0 prior is better then a 0.1 prior because it led to lower surprise\n",
    "\n",
    "\n",
    "Above is the first notion of surprise. the second notion of surprise is confusingly called bayesian surprise. It is closely related and measures how much we have to update our beliefs following an observation. This is measured by taking the kullbacker leibler divergence between our prior and posterior\n",
    "\n",
    "a prior of 1.0 that second variable is true will lead to a bayesian surprise of 0. However if the actual likelihood of second variable being true is low this will lead to a very high surprise. \n",
    "\n",
    "free energy = surprise or -ELBO + KL of what i belive is predicted vs observed states\n",
    "\n",
    "We will be using variational free energy not free energy which substitutes above terms for \n",
    "\n",
    "variational free energy = -ELBO\n",
    "\n",
    "we are going to have our B-VAE use LSTM layers since there is evidence that memory can help with partial information\n",
    "\n",
    "This is what we are going to use for our reward since we already are using a Kl-divergence regularizer\n",
    "\n",
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x). We are going to use this for the global reward of the mixer network. That still leaves what the reward should be for the neuronal agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ccd08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ray.rllib.env.wrappers.group_agents_wrapper import GroupAgentsWrapper\n",
    "class brain(MultiAgentEnv):    \n",
    "    def __init__(self, env_config):\n",
    "        self.num_agents = 3\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        \"\"\"\n",
    "        this is how the actions and actions space will work: neurons do either spatial or temporal summation depending on\n",
    "        whether they are getting repeated inputs from on other neuron or inputs from multiple neurons.\n",
    "\n",
    "        The rate of fire of the neuron for specific impulses to the external agent will determine how much to slow\n",
    "        things down in the acceleration game or how much to speed things up. \n",
    "\n",
    "        There will be a baseline probability of firing is 0.5 \n",
    "        and in order to collectively minimize Free energy the neuronal agents will have to increase or decrease \n",
    "        the probability of firing \n",
    "\n",
    "        so the action space cannot be encoded as just one discrete thing. It will have to be a dict composed of:\n",
    "        box(probability between -1 ans 1 of firing) and discrete(num_neuronal_agents)\n",
    "\n",
    "        \"\"\"\n",
    "        self._spaces_in_preferred_format = True\n",
    "        self.a = self.num_agents-1#Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
    "        #the two kind of actions any agent can do are action potentials and post-synaptic potentials\n",
    "        #in addition the agent neuron can direct their action to any one of the neurons that exist\n",
    "        \"\"\"\n",
    "        self.action_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"action_potential\": Discrete(1) #external agent neurons,dtype=np.float32),\n",
    "                \n",
    "\n",
    "            }\n",
    "        )\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "                {\n",
    "                    \"agent0\": self.action_space,#agent 0 will be our external agent/motor neurons\n",
    "                    \"agent1\": self.action_space,#Discrete(1), #excitatory neurons\n",
    "                    \"agent2\": gym.spaces.Box(low=0, high=1.0, shape=(10,))#Discrete(1)#inhibitory neurons\n",
    "                }\n",
    "            )\n",
    "\n",
    "        \"\"\"\n",
    "        #config.get(Dict({\"prob\": Discrete(200),\"pos\": Discrete(self.a),\"action_potential\": Discrete(2),\"ps_potential\":Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32))}))\n",
    "\n",
    "        #external agent action space needs to involve observation = accesible_state*d where d is the distortion variable \n",
    "\n",
    "        #the observation space is only what the neuronal agent recievs from the other neuronal agent\n",
    "        #self.observation_space = config.get(Discrete(self.a))\n",
    "        self.timestep_limit = config.get(\"ts\", 1000)\n",
    "\n",
    "        \"\"\" Create the A matrix  \"\"\"\n",
    "        \"\"\"\n",
    "        A = np.zeros( (n_states, n_observations))\n",
    "        np.fill_diagonal(A, 0.5) \n",
    "        log_likelihood = log_stable(A[observation_index,:])\n",
    "\n",
    "        log_prior = log_stable(prior)\n",
    "\n",
    "        qs = softmax(log_likelihood + log_prior)\n",
    "\n",
    "        qs_past = utils.onehot(4, n_states) # agent believes they were at location 4 -- i.e. (1,1) one timestep ag\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        A-matrix here is about the conditional distribution between observations and the hidden state. The hidden state being\n",
    "        the action corresponding to a change in the acceleration number needed for the acceleration number to become 0 \n",
    "\n",
    "        The b-matrix is probability of current state given past state and past action. \n",
    "\n",
    "        if state is synapse strength then what we need to do is say, synapse strength depends on past synapse strength \n",
    "        and whether or not the neuron fired a positive signal at t-1, an inhibitory signal or did not fire at all.\n",
    "\n",
    "        THe second factor which will go into state is the optimal acceleration number. \n",
    "\n",
    "        c-matrix for the external hidden state factor will be a uniform prior across all possible acceleration numbers. Furthermore the \n",
    "        d-matrix for the external state factor will be a uniform prior across all possible acceleration numbers. \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def reset(self,*,seed=None):\n",
    "        \"\"\"Resets the episode and returns the initial observation of the new one.\n",
    "        # Reset the episode len.\n",
    "        self.episode_len = 0\n",
    "        # Sample a random number from our observation space. in the line directly below is where we should do A \n",
    "        self.cur_obs = self.observation_space.sample()#qo_u_left = get_expected_observations(A, qs_u_left)\n",
    "        # Return initial observation.\n",
    "        return self.cur_obs\n",
    "        \"\"\"\n",
    "        self.dones = set()\n",
    "        return self._obs(), {}#{i: self.observation_space[i].sample() for i in self.agents}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        global_rew = 0\n",
    "        if self.actions_are_logits:\n",
    "            action_dict = {\n",
    "                k: np.random.choice([0, 1], p=v) for k, v in action_dict.items()\n",
    "            }\n",
    "\n",
    "        state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "        \n",
    "\n",
    "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
    "        obs = self._obs()\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        truncateds = {\"__all__\": False}\n",
    "        infos = {\n",
    "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
    "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "        }\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "        #this ensures that the agents are not all homogenous\n",
    "    def _obs(self):\n",
    "        if self.with_state:\n",
    "            return {\n",
    "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
    "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
    "            }\n",
    "        else:\n",
    "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
    "    def agent_1_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [1]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0]\n",
    "    def agent_2_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [2]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0] + 3  \n",
    "\n",
    "        \"\"\"Convenience method for grouping together agents in this env.\n",
    "\n",
    "        An agent group is a list of agent IDs that are mapped to a single\n",
    "        logical agent. All agents of the group must act at the same time in the\n",
    "        environment. The grouped agent exposes Tuple action and observation\n",
    "        spaces that are the concatenated action and obs spaces of the\n",
    "        individual agents.\n",
    "\n",
    "        The rewards of all the agents in a group are summed. The individual\n",
    "        agent rewards are available under the \"individual_rewards\" key of the\n",
    "        group info return.\n",
    "\n",
    "        Agent grouping is required to leverage algorithms such as Q-Mix.\n",
    "\n",
    "        Args:\n",
    "            groups: Mapping from group id to a list of the agent ids\n",
    "                of group members. If an agent id is not present in any group\n",
    "                value, it will be left ungrouped. The group id becomes a new agent ID\n",
    "                in the final environment.\n",
    "            obs_space: Optional observation space for the grouped\n",
    "                env. Must be a tuple space. If not provided, will infer this to be a\n",
    "                Tuple of n individual agents spaces (n=num agents in a group).\n",
    "            act_space: Optional action space for the grouped env.\n",
    "                Must be a tuple space. If not provided, will infer this to be a Tuple\n",
    "                of n individual agents spaces (n=num agents in a group).\n",
    "\n",
    "        Examples:\n",
    "            >>> from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "            >>> class MyMultiAgentEnv(MultiAgentEnv): # doctest: +SKIP\n",
    "            ...     # define your env here\n",
    "            ...     ... # doctest: +SKIP\n",
    "            >>> env = MyMultiAgentEnv(...) # doctest: +SKIP\n",
    "            >>> grouped_env = env.with_agent_groups(env, { # doctest: +SKIP\n",
    "            ...   \"group1\": [\"agent1\", \"agent2\", \"agent3\"], # doctest: +SKIP\n",
    "            ...   \"group2\": [\"agent4\", \"agent5\"], # doctest: +SKIP\n",
    "            ... }) # doctest: +SKIP\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #return GroupAgentsWrapper(self, groups, obs_space, act_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6091a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (996245450.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_6312\\996245450.py\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    ]\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "grouping = {\n",
    "    \"group_1\": [0, 1],\n",
    "}\n",
    "obs_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "            Dict(\n",
    "                {\n",
    "                    \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                    ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                }\n",
    "            ),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStepGame(MultiAgentEnv):\n",
    "    action_space = Discrete(2)\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(2)\n",
    "        self.state = None\n",
    "        self.agent_1 = 0\n",
    "        self.agent_2 = 1\n",
    "        self._skip_env_checking = True\n",
    "        # MADDPG emits action logits instead of actual discrete actions\n",
    "        self.actions_are_logits = env_config.get(\"actions_are_logits\", False)\n",
    "        self.one_hot_state_encoding = env_config.get(\"one_hot_state_encoding\", False)\n",
    "        self.with_state = env_config.get(\"separate_state_space\", False)\n",
    "        self._agent_ids = {0, 1}\n",
    "        if not self.one_hot_state_encoding:\n",
    "            self.observation_space = Discrete(6)\n",
    "            self.with_state = False\n",
    "        else:\n",
    "            # Each agent gets the full state (one-hot encoding of which of the\n",
    "            # three states are active) as input with the receiving agent's\n",
    "            # ID (1 or 2) concatenated onto the end.\n",
    "            if self.with_state:\n",
    "                self.observation_space = Dict(\n",
    "                    {\n",
    "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.observation_space = MultiDiscrete([2, 2, 2, 3])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = np.array([1, 0, 0])\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        if self.actions_are_logits:\n",
    "            action_dict = {\n",
    "                k: np.random.choice([0, 1], p=v) for k, v in action_dict.items()\n",
    "            }\n",
    "\n",
    "        state_index = np.flatnonzero(self.state)\n",
    "        if state_index == 0:\n",
    "            action = action_dict[self.agent_1]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = np.array([0, 1, 0])\n",
    "            else:\n",
    "                self.state = np.array([0, 0, 1])\n",
    "            global_rew = 0\n",
    "            terminated = False\n",
    "        elif state_index == 1:\n",
    "            global_rew = 7\n",
    "            terminated = True\n",
    "        else:\n",
    "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            terminated = True\n",
    "\n",
    "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
    "        obs = self._obs()\n",
    "        terminateds = {\"__all__\": terminated}\n",
    "        truncateds = {\"__all__\": False}\n",
    "        infos = {\n",
    "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
    "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
    "        }\n",
    "        return obs, rewards, terminateds, truncateds, infos\n",
    "\n",
    "    def _obs(self):\n",
    "        if self.with_state:\n",
    "            return {\n",
    "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
    "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
    "            }\n",
    "        else:\n",
    "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
    "\n",
    "    def agent_1_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [1]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0]\n",
    "\n",
    "    def agent_2_obs(self):\n",
    "        if self.one_hot_state_encoding:\n",
    "            return np.concatenate([self.state, [2]])\n",
    "        else:\n",
    "            return np.flatnonzero(self.state)[0] + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da59abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TwoStepGameWithGroupedAgents(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        env = brain(env_config)\n",
    "        tuple_obs_space = Tuple([env.observation_space, env.observation_space])\n",
    "        tuple_act_space = Tuple([env.action_space, env.action_space])\n",
    "\n",
    "        self.env = env.with_agent_groups(\n",
    "            groups={\"agents\": [0, 1]},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self._agent_ids = {\"agents\"}\n",
    "    \n",
    "    def reset(self):#, *, seed=None, options=None):\n",
    "        return self.env.reset()#seed=seed, options=options)\n",
    "    \n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f1c132",
   "metadata": {},
   "source": [
    "We are going to have two parallel vectorized parallel enviroments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b985561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMixConfig(SimpleQConfig):\n",
    "    \"\"\"Defines a configuration class from which QMix can be built.\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> config = QMixConfig().training(gamma=0.9, lr=0.01, kl_coeff=0.3)\\\n",
    "        ...             .resources(num_gpus=0)\\\n",
    "        ...             .rollouts(num_workers=4)\n",
    "        >>> print(config.to_dict())\n",
    "        >>> # Build an Algorithm object from the config and run 1 training iteration.\n",
    "        >>> algo = config.build(env=TwoStepGame)\n",
    "        >>> algo.train()\n",
    "    Example:\n",
    "        >>> from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "        >>> from ray.rllib.algorithms.qmix import QMixConfig\n",
    "        >>> from ray import tune\n",
    "        >>> config = QMixConfig()\n",
    "        >>> # Print out some default values.\n",
    "        >>> print(config.optim_alpha)\n",
    "        >>> # Update the config object.\n",
    "        >>> config.training(lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97)\n",
    "        >>> # Set the config object's env.\n",
    "        >>> config.environment(env=TwoStepGame)\n",
    "        >>> # Use to_dict() to get the old-style python config dict\n",
    "        >>> # when running with tune.\n",
    "        >>> tune.run(\n",
    "        ...     \"QMix\",\n",
    "        ...     stop={\"episode_reward_mean\": 200},\n",
    "        ...     config=config.to_dict(),\n",
    "        ... )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PPOConfig instance.\"\"\"\n",
    "        super().__init__(algo_class=QMix)\n",
    "\n",
    "        # fmt: off\n",
    "        # __sphinx_doc_begin__\n",
    "        # QMix specific settings:\n",
    "        self.mixer = \"qmix\"\n",
    "        self.mixing_embed_dim = 32\n",
    "        self.double_q = True\n",
    "        self.optim_alpha = 0.99\n",
    "        self.optim_eps = 0.00001\n",
    "        self.grad_clip = 10\n",
    "\n",
    "        # Override some of AlgorithmConfig's default values with QMix-specific values.\n",
    "        # .training()\n",
    "        self.lr = 0.0005\n",
    "        self.train_batch_size = 32\n",
    "        self.target_network_update_freq = 500\n",
    "        # Number of timesteps to collect from rollout workers before we start\n",
    "        # sampling from replay buffers for learning. Whether we count this in agent\n",
    "        # steps  or environment steps depends on config[\"multiagent\"][\"count_steps_by\"].\n",
    "        self.num_steps_sampled_before_learning_starts = 1000\n",
    "        self.replay_buffer_config = {\n",
    "            \"type\": \"ReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "            \"prioritized_replay\": DEPRECATED_VALUE,\n",
    "            # Size of the replay buffer in batches (not timesteps!).\n",
    "            \"capacity\": 1000,\n",
    "            \n",
    "            # Choosing `fragments` here makes it so that the buffer stores entire\n",
    "            # batches, instead of sequences, episodes or timesteps.\n",
    "            \"storage_unit\": \"fragments\",\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        }\n",
    "        self.model = {\n",
    "            \"lstm_cell_size\": 64,\n",
    "            \"max_seq_len\": 999999,\n",
    "        }\n",
    "\n",
    "        # .framework()\n",
    "        self.framework_str = \"torch\"\n",
    "\n",
    "        # .rollouts()\n",
    "        self.num_workers = 12\n",
    "        self.rollout_fragment_length = 4\n",
    "        self.batch_mode = \"complete_episodes\"\n",
    "\n",
    "        # .reporting()\n",
    "        self.min_time_s_per_iteration = 1\n",
    "        self.min_sample_timesteps_per_iteration = 1000\n",
    "\n",
    "        # .exploration()\n",
    "        self.exploration_config = {\n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\",\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.01,\n",
    "            # Timesteps over which to anneal epsilon.\n",
    "            \"epsilon_timesteps\": 40000,\n",
    "\n",
    "            # For soft_q, use:\n",
    "            # \"exploration_config\" = {\n",
    "            #   \"type\": \"SoftQ\"\n",
    "            #   \"temperature\": [float, e.g. 1.0]\n",
    "            # }\n",
    "        }\n",
    "\n",
    "        # .evaluation()\n",
    "        # Evaluate with epsilon=0 every `evaluation_interval` training iterations.\n",
    "        # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "        # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "        # metrics are already only reported for the lowest epsilon workers.\n",
    "        self.evaluation_interval = None\n",
    "        self.evaluation_duration = 10\n",
    "        self.evaluation_config = {\n",
    "            \"explore\": False,\n",
    "        }\n",
    "        # __sphinx_doc_end__\n",
    "        # fmt: on\n",
    "\n",
    "        self.worker_side_prioritization = DEPRECATED_VALUE\n",
    "\n",
    "    @override(SimpleQConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        mixer: Optional[str] = None,\n",
    "        mixing_embed_dim: Optional[int] = None,\n",
    "        double_q: Optional[bool] = None,\n",
    "        target_network_update_freq: Optional[int] = None,\n",
    "        replay_buffer_config: Optional[dict] = None,\n",
    "        optim_alpha: Optional[float] = None,\n",
    "        optim_eps: Optional[float] = None,\n",
    "        grad_norm_clipping: Optional[float] = None,\n",
    "        grad_clip: Optional[float] = None,\n",
    "        **kwargs,\n",
    "    ) -> \"QMixConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            mixer: Mixing network. Either \"qmix\", \"vdn\", or None.\n",
    "            mixing_embed_dim: Size of the mixing network embedding.\n",
    "            double_q: Whether to use Double_Q learning.\n",
    "            target_network_update_freq: Update the target network every\n",
    "                `target_network_update_freq` sample steps.\n",
    "            replay_buffer_config:\n",
    "            optim_alpha: RMSProp alpha.\n",
    "            optim_eps: RMSProp epsilon.\n",
    "            grad_clip: If not None, clip gradients during optimization at\n",
    "                this value.\n",
    "            grad_norm_clipping: Depcrecated in favor of grad_clip\n",
    "        Returns:\n",
    "            This updated AlgorithmConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if grad_norm_clipping is not None:\n",
    "            deprecation_warning(\n",
    "                old=\"grad_norm_clipping\",\n",
    "                new=\"grad_clip\",\n",
    "                help=\"Parameter `grad_norm_clipping` has been \"\n",
    "                \"deprecated in favor of grad_clip in QMix. \"\n",
    "                \"This is now the same parameter as in other \"\n",
    "                \"algorithms. `grad_clip` will be overwritten by \"\n",
    "                \"`grad_norm_clipping={}`\".format(grad_norm_clipping),\n",
    "                error=False,\n",
    "            )\n",
    "            grad_clip = grad_norm_clipping\n",
    "\n",
    "        if mixer is not None:\n",
    "            self.mixer = mixer\n",
    "        if mixing_embed_dim is not None:\n",
    "            self.mixing_embed_dim = mixing_embed_dim\n",
    "        if double_q is not None:\n",
    "            self.double_q = double_q\n",
    "        if target_network_update_freq is not None:\n",
    "            self.target_network_update_freq = target_network_update_freq\n",
    "        if replay_buffer_config is not None:\n",
    "            self.replay_buffer_config = replay_buffer_config\n",
    "        if optim_alpha is not None:\n",
    "            self.optim_alpha = optim_alpha\n",
    "        if optim_eps is not None:\n",
    "            self.optim_eps = optim_eps\n",
    "        if grad_clip is not None:\n",
    "            self.grad_clip = grad_clip\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class QMix(SimpleQ):\n",
    "    @classmethod\n",
    "    @override(SimpleQ)\n",
    "    def get_default_config(cls) -> AlgorithmConfigDict:\n",
    "        return QMixConfig().to_dict()\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def validate_config(self, config: AlgorithmConfigDict) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate_config(config)\n",
    "\n",
    "        if config[\"framework\"] != \"torch\":\n",
    "            raise ValueError(\"Only `framework=torch` supported so far for QMix!\")\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def get_default_policy_class(self, config: AlgorithmConfigDict) -> Type[Policy]:\n",
    "        return QMixTorchPolicy\n",
    "\n",
    "    @override(SimpleQ)\n",
    "    def training_step(self) -> ResultDict:\n",
    "        \"\"\"QMIX training iteration function.\n",
    "        - Sample n MultiAgentBatches from n workers synchronously.\n",
    "        - Store new samples in the replay buffer.\n",
    "        - Sample one training MultiAgentBatch from the replay buffer.\n",
    "        - Learn on the training batch.\n",
    "        - Update the target network every `target_network_update_freq` sample steps.\n",
    "        - Return all collected training metrics for the iteration.\n",
    "        Returns:\n",
    "            The results dict from executing the training iteration.\n",
    "        \"\"\"\n",
    "        # Sample n batches from n workers.\n",
    "        new_sample_batches = synchronous_parallel_sample(\n",
    "            worker_set=self.workers, concat=False\n",
    "        )\n",
    "\n",
    "        for batch in new_sample_batches:\n",
    "            # Update counters.\n",
    "            self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n",
    "            self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n",
    "            # Store new samples in the replay buffer.\n",
    "            self.local_replay_buffer.add(batch)\n",
    "\n",
    "        # Update target network every `target_network_update_freq` sample steps.\n",
    "        cur_ts = self._counters[\n",
    "            NUM_AGENT_STEPS_SAMPLED if self._by_agent_steps else NUM_ENV_STEPS_SAMPLED\n",
    "        ]\n",
    "\n",
    "        train_results = {}\n",
    "\n",
    "        if cur_ts > self.config[\"num_steps_sampled_before_learning_starts\"]:\n",
    "            # Sample n batches from replay buffer until the total number of timesteps\n",
    "            # reaches `train_batch_size`.\n",
    "            train_batch = sample_min_n_steps_from_buffer(\n",
    "                replay_buffer=self.local_replay_buffer,\n",
    "                min_steps=self.config[\"train_batch_size\"],\n",
    "                count_by_agent_steps=self._by_agent_steps,\n",
    "            )\n",
    "\n",
    "            # Learn on the training batch.\n",
    "            # Use simple optimizer (only for multi-agent or tf-eager; all other\n",
    "            # cases should use the multi-GPU optimizer, even if only using 1 GPU)\n",
    "            if self.config.get(\"simple_optimizer\") is True:\n",
    "                train_results = train_one_step(self, train_batch)\n",
    "            else:\n",
    "                train_results = multi_gpu_train_one_step(self, train_batch)\n",
    "\n",
    "            # Update target network every `target_network_update_freq` sample steps.\n",
    "            last_update = self._counters[LAST_TARGET_UPDATE_TS]\n",
    "            if cur_ts - last_update >= self.config[\"target_network_update_freq\"]:\n",
    "                to_update = self.workers.local_worker().get_policies_to_train()\n",
    "                self.workers.local_worker().foreach_policy_to_train(\n",
    "                    lambda p, pid: pid in to_update and p.update_target()\n",
    "                )\n",
    "                self._counters[NUM_TARGET_UPDATES] += 1\n",
    "                self._counters[LAST_TARGET_UPDATE_TS] = cur_ts\n",
    "\n",
    "            update_priorities_in_replay_buffer(\n",
    "                self.local_replay_buffer, self.config, train_batch, train_results\n",
    "            )\n",
    "\n",
    "            # Update weights and global_vars - after learning on the local worker -\n",
    "            # on all remote workers.\n",
    "            global_vars = {\n",
    "                \"timestep\": self._counters[NUM_ENV_STEPS_SAMPLED],\n",
    "            }\n",
    "            # Update remote workers' weights and global vars after learning on local\n",
    "            # worker.\n",
    "            with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:\n",
    "                self.workers.sync_weights(global_vars=global_vars)\n",
    "\n",
    "        # Return all collected metrics for the iteration.\n",
    "        return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf217b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete,Box, Dict, MultiBinary,Tuple\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import get_trainable_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ebbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "initial_sparsity = 0.0\n",
    "final_sparsity = 0.75\n",
    "begin_step = 1000\n",
    "end_step = 5000\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=initial_sparsity,\n",
    "            final_sparsity=final_sparsity,\n",
    "            begin_step=begin_step,\n",
    "            end_step=end_step)\n",
    "    }\n",
    "model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "pruning_callback = tfmot.sparsity.keras.UpdatePruningStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d53f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = QMixConfig()\n",
    "# Print out some default values.\n",
    "print(config.optim_alpha)  \n",
    "# Update the config object.\n",
    "config.training(  \n",
    "    lr=tune.grid_search([0.001, 0.0001]), optim_alpha=0.97\n",
    ")\n",
    "# Set the config object's env.\n",
    "#print(config.optim_alpha) \n",
    "config.environment(env=TwoStepGame)  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "print(config.optim_alpha) \n",
    "\n",
    "#only the tune.tuner code is breaking. Everything up to this point is compiling\n",
    "\n",
    "tune.Tuner(  \n",
    "    \"QMix\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663afa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ffbce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "config = QMixConfig()  # doctest: +SKIP\n",
    "\n",
    "config = config.training(gamma=0.9, lr=0.01)#, kl_coeff=0.3)  # doctest: +SKIP\n",
    "\n",
    "config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "config = config.rollouts(num_rollout_workers=12)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.algorithms.qmix import QMixConfig\n",
    "config = QMixConfig()  # doctest: +SKIP\n",
    "print(\"adf\")\n",
    "config = config.training(gamma=0.9, lr=0.01)#, kl_coeff=0.3)  # doctest: +SKIP\n",
    "\n",
    "config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "config = config.rollouts(num_rollout_workers=12)  # doctest: +SKIP\n",
    "print(config.to_dict())  # doctest: +SKIP\n",
    "\n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=TwoStepGameWithGroupedAgents)  # doctest: +SKIP\n",
    "\n",
    "algo.train()  # doctest: +SKIP\n",
    "\n",
    "#current error: problem with dict and agent 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ecc444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from gymnasium.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.tune import register_env\n",
    "from ray.rllib.env.multi_agent_env import ENV_STATE\n",
    "from ray.rllib.examples.env.two_step_game import TwoStepGame\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import get_trainable_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3af3c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "#args = parser.parse_args()\n",
    "\n",
    "#ray.init(num_cpus=args.num_cpus or None, local_mode=args.local_mode)\n",
    "\n",
    "grouping = {\n",
    "    \"group_1\": [0, 1],\n",
    "}\n",
    "obs_space = Tuple(\n",
    "    [\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "            }\n",
    "        ),\n",
    "        Dict(\n",
    "            {\n",
    "                \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "                ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "act_space = Tuple(\n",
    "    [\n",
    "        Dict({\n",
    "            \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
    "        }\n",
    "        ),\n",
    "        Dict({\n",
    "            \"obs\": MultiDiscrete([2, 2, 2, 3])\n",
    "        })\n",
    "        \n",
    "        #TwoStepGame.action_space,\n",
    "        #TwoStepGame.action_space,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "febad582",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = {\n",
    "    \"group_1\": [0, 1],\n",
    "}\n",
    "register_env(\n",
    "    \"grouped_twostep\",\n",
    "    lambda config: brain(config).with_agent_groups(\n",
    "        grouping, obs_space=obs_space, act_space=act_space\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa863fab",
   "metadata": {},
   "source": [
    "prior precision is important. only the priors at the top will have to be set through various means. Priors for all other neuronal agents will just be inherited as the posteriors of the upper level neuronal agents \n",
    "\n",
    "One of the priors will be the hertz settings. Essentially we want the signals associated with ascending prediction error signals to be gamma frequencies and descending connections with alpha or beta bands. Note that using this for validation may require us to reconstruct the EEG from FMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37f6195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id.startswith(\"low_level_\"):\n",
    "        return \"low_level_policy\"\n",
    "    else:\n",
    "        return \"high_level_policy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c40fd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 21:19:44,794\tINFO tune.py:887 -- Initializing Ray automatically.For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run`.\n",
      "2023-02-14 21:19:47,739\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-14 21:19:56</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:07.27        </td></tr>\n",
       "<tr><td>Memory:      </td><td>31.3/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/20.52 GiB heap, 0.0/10.26 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_5e50c_00000</td><td style=\"text-align: right;\">           1</td><td>C:\\Users\\subar\\ray_results\\QMIX\\QMIX_grouped_twostep_5e50c_00000_0_2023-02-14_21-19-49\\error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_5e50c_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:287: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:280: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _SERIALIZEDDTYPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "\u001b[2m\u001b[36m(pid=19656)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m 2023-02-14 21:19:56,330\tWARNING algorithm_config.py:488 -- Cannot create QMixConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "2023-02-14 21:19:56,544\tERROR trial_runner.py:1088 -- Trial QMIX_grouped_twostep_5e50c_00000: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\execution\\ray_trial_executor.py\", line 1070, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\worker.py\", line 2311, in get\n",
      "    raise value\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1135, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1045, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 782, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 945, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 599, in ray._raylet.store_task_errors\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::QMix.__init__()\u001b[39m (pid=19656, ip=127.0.0.1, repr=QMix)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 255, in check_multiagent_environments\n",
      "    reset_obs = env.reset()\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\group_agents_wrapper.py\", line 89, in reset\n",
      "    obs = self.env.reset()\n",
      "  File \"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_20936\\567957361.py\", line 92, in reset\n",
      "  File \"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_20936\\567957361.py\", line 135, in _obs\n",
      "AttributeError: 'brain' object has no attribute 'with_state'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::QMix.__init__()\u001b[39m (pid=19656, ip=127.0.0.1, repr=QMix)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 823, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 875, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 441, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 169, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 566, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 169, in __init__\n",
      "    self._setup(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 259, in _setup\n",
      "    self._local_worker = self._make_worker(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 941, in _make_worker\n",
      "    worker = cls(\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 592, in __init__\n",
      "    check_env(self.env)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 88, in check_env\n",
      "    raise ValueError(\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 75, in check_env\n",
      "    check_multiagent_environments(env)\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 255, in check_multiagent_environments\n",
      "    reset_obs = env.reset()\n",
      "  File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\group_agents_wrapper.py\", line 89, in reset\n",
      "    obs = self.env.reset()\n",
      "  File \"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_20936\\567957361.py\", line 92, in reset\n",
      "  File \"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_20936\\567957361.py\", line 135, in _obs\n",
      "AttributeError: 'brain' object has no attribute 'with_state'\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior by setting `disable_env_checking=True` in your environment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([env]).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_twostep_5e50c_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m 2023-02-14 21:19:56,528\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m 2023-02-14 21:19:56,534\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::QMix.__init__()\u001b[39m (pid=19656, ip=127.0.0.1, repr=QMix)\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 255, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     reset_obs = env.reset()\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\group_agents_wrapper.py\", line 89, in reset\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     obs = self.env.reset()\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_20936\\567957361.py\", line 92, in reset\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_20936\\567957361.py\", line 135, in _obs\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m AttributeError: 'brain' object has no attribute 'with_state'\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m \n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m \n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m \u001b[36mray::QMix.__init__()\u001b[39m (pid=19656, ip=127.0.0.1, repr=QMix)\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 823, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 875, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 780, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 441, in __init__\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 169, in __init__\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 566, in setup\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 169, in __init__\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     self._setup(\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 259, in _setup\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     self._local_worker = self._make_worker(\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 941, in _make_worker\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     worker = cls(\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 592, in __init__\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     check_env(self.env)\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 88, in check_env\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 75, in check_env\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 255, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     reset_obs = env.reset()\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\group_agents_wrapper.py\", line 89, in reset\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m     obs = self.env.reset()\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_20936\\567957361.py\", line 92, in reset\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m   File \"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_20936\\567957361.py\", line 135, in _obs\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m AttributeError: 'brain' object has no attribute 'with_state'\n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m \n",
      "\u001b[2m\u001b[36m(QMix pid=19656)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior by setting `disable_env_checking=True` in your environment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([env]).\n",
      "2023-02-14 21:19:56,673\tERROR tune.py:758 -- Trials did not complete: [QMIX_grouped_twostep_5e50c_00000]\n",
      "2023-02-14 21:19:56,673\tINFO tune.py:762 -- Total run time: 7.39 seconds (7.26 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    " #args.run == \"QMIX\":\n",
    "\"\"\"    \n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "\"\"\"\n",
    "obs_space = Tuple([Discrete(1), Discrete(4)])\n",
    "act_space = Tuple([Discrete(1), Discrete(4)])\n",
    "#config = QMixConfig() \n",
    "\n",
    "config = (\n",
    "    get_trainable_cls(\"QMIX\")\n",
    "    .get_default_config()\n",
    "    .environment(brain)\n",
    "    .framework(\"torch\")#args.framework)\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    ")\n",
    "(config.framework(\"torch\")\n",
    ".training(mixer=\"qmix\", train_batch_size=32)#args.mixer, train_batch_size=32)\n",
    ".multi_agent(\n",
    "                policies={\n",
    "                    \"high_level_policy\": (\n",
    "                        None,\n",
    "                        obs_space,\n",
    "                        act_space,#Tuple([Discrete(1),Discrete(1)]),#maze.observation_space,\n",
    "                        #Discrete(4),\n",
    "                        config\n",
    "                        #config.overrides(gamma=0.9),\n",
    "                    ),\n",
    "                    \"low_level_policy\": (\n",
    "                        None,\n",
    "                        obs_space,#Tuple([Discrete(1), Discrete(4)]),\n",
    "                        act_space,#maze.action_space,\n",
    "                        config\n",
    "                        #config.overrides(gamma=0.0),\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=policy_mapping_fn,\n",
    "            )\n",
    ".rollouts(num_rollout_workers=0, rollout_fragment_length=4)\n",
    ".exploration(\n",
    "    exploration_config={\n",
    "        \"final_epsilon\": 0.0,\n",
    "    }\n",
    ")\n",
    ".environment(\n",
    "    env=\"grouped_twostep\",\n",
    "    env_config={\n",
    "        \"separate_state_space\": True,\n",
    "        \"one_hot_state_encoding\": True,\n",
    "    },\n",
    ")\n",
    ")\n",
    "stop = {\n",
    "    \"episode_reward_mean\": 8.0,#args.stop_reward,\n",
    "    \"timesteps_total\": 70000,#args.stop_timesteps,\n",
    "    \"training_iteration\": 200 #args.stop_iters,\n",
    "}\n",
    "\n",
    "results = tune.Tuner(\n",
    "    \"QMIX\",#args.run,\n",
    "    run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "    param_space=config,\n",
    ").fit()\n",
    "\n",
    "#if store_true:\n",
    "#check_learning_achieved(results, 8.0)#args.stop_reward)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0433ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tslearn.metrics import dtw\n",
    "x_1d = [1,1,1,1]\n",
    "y_1d = [2,2]\n",
    "dtw(x_1d, y_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e301d65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'EEG_FMRI_DATASETS'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_16932\\1160677477.py\"\u001b[0m, line \u001b[0;32m11\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    import eeg_to_fmri\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\eeg_to_fmri\\__init__.py\"\u001b[0m, line \u001b[0;32m9\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    import eeg_to_fmri.learning\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\eeg_to_fmri\\learning\\__init__.py\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    import eeg_to_fmri.learning.train\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\eeg_to_fmri\\learning\\train.py\"\u001b[0m, line \u001b[0;32m7\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from eeg_to_fmri.utils import print_utils\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\eeg_to_fmri\\utils\\__init__.py\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    import eeg_to_fmri.utils.viz_utils\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\eeg_to_fmri\\utils\\viz_utils.py\"\u001b[0m, line \u001b[0;32m9\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from eeg_to_fmri.data import eeg_utils\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\eeg_to_fmri\\data\\__init__.py\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    import eeg_to_fmri.data.data_utils\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\eeg_to_fmri\\data\\data_utils.py\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from eeg_to_fmri.data import eeg_utils, fmri_utils, outlier_utils\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\eeg_to_fmri\\data\\eeg_utils.py\"\u001b[0m, line \u001b[0;32m50\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    media_directory=os.environ['EEG_FMRI_DATASETS']+\"/\"\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\os.py\"\u001b[1;36m, line \u001b[1;32m679\u001b[1;36m, in \u001b[1;35m__getitem__\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise KeyError(key) from None\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m\u001b[1;31m:\u001b[0m 'EEG_FMRI_DATASETS'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#sys.path.append(\"../src\")\n",
    "\n",
    "import os\n",
    "#os.environ['EEG_FMRI_DATASETS']=\"/home/david/eeg_to_fmri/datasets\"\n",
    "#os.environ['EEG_FMRI']=\"/home/david/eeg_to_fmri\"\n",
    "\n",
    "import eeg_to_fmri\n",
    "from eeg_to_fmri.utils import tf_config\n",
    "\n",
    "#dataset=\"01\"\n",
    "tf_config.set_seed(seed=2)\n",
    "#tf_config.setup_tensorflow(device=\"GPU\", memory_limit=1500, run_eagerly=True)\n",
    "\n",
    "from eeg_to_fmri.models.synthesizers import EEG_to_fMRI\n",
    "from eeg_to_fmri.data import preprocess_data, eeg_utils, data_utils\n",
    "from eeg_to_fmri.learning import train, losses\n",
    "from eeg_to_fmri import metrics\n",
    "from eeg_to_fmri.utils import viz_utils\n",
    "from eeg_to_fmri.models.synthesizers import na_specification_eeg\n",
    "from eeg_to_fmri.models.fmri_ae import na_specification_fmri\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907bac4a",
   "metadata": {},
   "source": [
    "We should use dynamic time warping to determine the difference between fmri and eeg since dnamic time warping is a technique that works even on temporal datasets of different lengths. We should then use ML to alter FMRI until the loss which is the differnce between the two datasets under dtw is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    model = EEG_to_fMRI(latent_dimension, eeg_train.shape[1:], na_specification_eeg, n_channels,\n",
    "                        weight_decay=weight_decay, skip_connections=True, batch_norm=True, fourier_features=True,\n",
    "                        random_fourier=True, topographical_attention=True, conditional_attention_style=True,\n",
    "                        conditional_attention_style_prior=False, local=True, seed=None, \n",
    "                        fmri_args = (latent_dimension, fmri_train.shape[1:], kernel_size, stride_size, n_channels, \n",
    "                        max_pool, batch_norm, weight_decay, skip_connections,\n",
    "                        n_stacks, True, False, outfilter, dropout, None, False, na_specification_fmri))\n",
    "    model.build(eeg_train.shape, fmri_train.shape)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    loss_fn = losses.mae_cosine\n",
    "    train_set = tf.data.Dataset.from_tensor_slices((eeg_train, fmri_train)).batch(batch_size)\n",
    "    test_set= tf.data.Dataset.from_tensor_slices((eeg_test, fmri_test)).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################################\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#simple conv net\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=dtw#tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              #metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import (\n",
    "    WidePreprocessor,\n",
    "    TabPreprocessor,\n",
    "    TextPreprocessor,\n",
    "    ImagePreprocessor,\n",
    ")\n",
    "from pytorch_widedeep.models import (\n",
    "    Wide,\n",
    "    TabMlp,\n",
    "    Vision,\n",
    "    BasicRNN,\n",
    "    WideDeep,\n",
    ")\n",
    "from pytorch_widedeep.losses import RMSELoss\n",
    "from pytorch_widedeep.initializers import *\n",
    "from pytorch_widedeep.callbacks import *\n",
    "\n",
    "image_processor = ImagePreprocessor(img_col=img_col, img_path=img_path)\n",
    "X_images = image_processor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import dtw\n",
    "\n",
    "class dtw(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"dynamictimewarp\"\"\"\n",
    "        super().__init__()\n",
    "        #self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        return dtw(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631feeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Resnet 18\n",
    "resnet = Vision(pretrained_model_name=\"resnet18\", n_trainable=4)\n",
    "wide = Wide(input_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "model = WideDeep(\n",
    "    wide=wide,\n",
    "    #deeptabular=tab_mlp,\n",
    "    #deeptext=basic_rnn,\n",
    "    deepimage=resnet,\n",
    "    #head_hidden_dims=[256, 128],\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, objective=dtw())\n",
    "trainer.fit()\n",
    "\n",
    "trainer.fit(\n",
    "    X_img=X_images,\n",
    "    target=target,\n",
    "    n_epochs=1,\n",
    "    batch_size=32,\n",
    "    val_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e095b5b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The subjects directory has to be specified using the subjects_dir parameter or the SUBJECTS_DIR environment variable.",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\AppData\\Local\\Temp\\ipykernel_11620\\3846482204.py\"\u001b[0m, line \u001b[0;32m7\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    brain = Brain(sub, hemi, surf)\n",
      "  File \u001b[0;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\surfer\\viz.py\"\u001b[0m, line \u001b[0;32m419\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    subjects_dir = _get_subjects_dir(subjects_dir=subjects_dir)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\subar\\anaconda3\\envs\\d\\lib\\site-packages\\surfer\\utils.py\"\u001b[1;36m, line \u001b[1;32m723\u001b[1;36m, in \u001b[1;35m_get_subjects_dir\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise ValueError('The subjects directory has to be specified '\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m\u001b[1;31m:\u001b[0m The subjects directory has to be specified using the subjects_dir parameter or the SUBJECTS_DIR environment variable.\n"
     ]
    }
   ],
   "source": [
    "from surfer import Brain\n",
    "\n",
    "sub = 'fsaverage'\n",
    "hemi = 'lh'\n",
    "surf = 'inflated'\n",
    "\n",
    "brain = Brain(sub, hemi, surf)\n",
    "\n",
    "brain.animate(['l', 'c'])\n",
    "\n",
    "# control number of steps\n",
    "brain.animate(['l', 'm'], n_steps=30)\n",
    "\n",
    "# any path you can think of\n",
    "brain.animate(['l', 'c', 'm', 'r', 'c', 'r', 'l'], n_steps=45)\n",
    "\n",
    "# full turns\n",
    "brain.animate([\"m\"] * 3)\n",
    "\n",
    "# movies\n",
    "brain.animate(['l', 'l'], n_steps=10, fname='simple_animation.avi')\n",
    "\n",
    "# however, rotating out of the axial plane is not allowed\n",
    "try:\n",
    "    brain.animate(['l', 'd'])\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c376fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nitime\n",
    "import nitime.analysis as nta\n",
    "import nitime.timeseries as ts\n",
    "import nitime.utils as tsu\n",
    "from nitime.viz import drawmatrix_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf151c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We then define a few parameters of the data: the TR and the bounds on the frequency band of interest.\n",
    "TR = 1.89\n",
    "f_ub = 0.15\n",
    "f_lb = 0.02\n",
    "#We read in the resting state fMRI data into a recarray from a csv file:\n",
    "data_path = os.path.join(nitime.__path__[0], 'data')\n",
    "\n",
    "fname = os.path.join(data_path, 'fmri_timeseries.csv')\n",
    "\n",
    "data_rec = np.genfromtxt(fname, dtype=float, delimiter=',', names=True)\n",
    "\n",
    "roi_names = np.array(data_rec.dtype.names)\n",
    "nseq = len(roi_names)\n",
    "n_samples = data_rec.shape[0]\n",
    "data = np.zeros((nseq, n_samples))\n",
    "\n",
    "for n_idx, roi in enumerate(roi_names):\n",
    "    data[n_idx] = data_rec[roi]\n",
    "#We normalize the data in each of the ROIs to be in units of % change and initialize the TimeSeries object:\n",
    "pdata = tsu.percent_change(data)\n",
    "time_series = ts.TimeSeries(pdata, sampling_interval=TR)\n",
    "\n",
    "G = nta.GrangerAnalyzer(time_series, order=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
