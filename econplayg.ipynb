{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings\n",
    "    env=dict(\n",
    "        num_taggers=5,\n",
    "        num_runners=20,\n",
    "        episode_length=100,\n",
    "        seed=1234,\n",
    "        use_full_observation=False,\n",
    "        num_other_agents_observed=10,\n",
    "        tagging_distance=0.02,\n",
    "    ),\n",
    "    # Trainer settings\n",
    "    trainer=dict(\n",
    "        num_envs=100,  # number of environment replicas (number of GPU blocks used)\n",
    "        train_batch_size=10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes=5000,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    # Policy network settings\n",
    "    policy=dict(\n",
    "        runner=dict(\n",
    "            to_train=True,  # flag indicating whether the model needs to be trained\n",
    "            algorithm=\"A2C\",  # algorithm used to train the policy\n",
    "            gamma=0.98,  # discount rate\n",
    "            lr=0.005,  # learning rate\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),  # policy model settings\n",
    "        ),\n",
    "        tagger=dict(\n",
    "            to_train=True,\n",
    "            algorithm=\"A2C\",\n",
    "            gamma=0.98,\n",
    "            lr=0.002,\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    # Checkpoint saving setting\n",
    "    saving=dict(\n",
    "        metrics_log_freq=10,  # how often (in iterations) to print the metrics\n",
    "        model_params_save_freq=5000,  # how often (in iterations) to save the model parameters\n",
    "        basedir=\"/tmp\",  # base folder used for saving\n",
    "        name=\"continuous_tag\",  # experiment name\n",
    "        tag=\"example\",  # experiment tag\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class MyDualModeEnvironment(CUDAEnvironmentContext):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    def get_data_dictionary(self):\n",
    "        data_dict = DataFeed()\n",
    "        ...\n",
    "        return data_dict \n",
    "    \n",
    "    def get_tensor_dictionary(self):\n",
    "        tensor_dict = DataFeed()\n",
    "        ...\n",
    "        return tensor_dict\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset for CPU environment\n",
    "        ...\n",
    "    \n",
    "    def step(self, actions=None):\n",
    "        args = [YOUR_CUDA_STEP_ARGUMENTS]\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            self.cuda_step(\n",
    "                *self.cuda_step_function_feed(args),\n",
    "                block=self.cuda_function_manager.block,\n",
    "                grid=self.cuda_function_manager.grid,\n",
    "            )\n",
    "            return None\n",
    "        else:\n",
    "            ...\n",
    "            return obs, rew, done, info\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1716681179\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 1 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     321.26\n",
      "Mean action sample time per iter (ms)   :      43.83\n",
      "Mean env. step time per iter (ms)       :     223.14\n",
      "Mean training time per iter (ms)        :      99.42\n",
      "Mean total time per iter (ms)           :     711.49\n",
      "Mean steps per sec (policy eval)        :   31127.67\n",
      "Mean steps per sec (action sample)      :  228168.81\n",
      "Mean steps per sec (env. step)          :   44815.14\n",
      "Mean steps per sec (training time)      :  100587.37\n",
      "Mean steps per sec (total)              :   14054.99\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.05545\n",
      "Policy loss                             :    0.18206\n",
      "Value function loss                     :    0.20981\n",
      "Mean rewards                            :    0.00114\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.07857\n",
      "Mean advantages                         :    0.03796\n",
      "Mean (norm.) advantages                 :    0.03796\n",
      "Mean (discounted) returns               :    0.11653\n",
      "Mean normalized returns                 :    0.11653\n",
      "Mean entropy                            :    4.79216\n",
      "Variance explained by the value function:    0.00144\n",
      "Std. of action_0 over agents            :    3.14547\n",
      "Std. of action_0 over envs              :    3.16140\n",
      "Std. of action_0 over time              :    3.16108\n",
      "Std. of action_1 over agents            :    3.14621\n",
      "Std. of action_1 over envs              :    3.15789\n",
      "Std. of action_1 over time              :    3.15948\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    2.28000\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.45930\n",
      "Policy loss                             :    2.69197\n",
      "Value function loss                     :    0.69182\n",
      "Mean rewards                            :    0.01752\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   -0.06607\n",
      "Mean advantages                         :    0.56181\n",
      "Mean (norm.) advantages                 :    0.56181\n",
      "Mean (discounted) returns               :    0.49574\n",
      "Mean normalized returns                 :    0.49574\n",
      "Mean entropy                            :    4.79160\n",
      "Variance explained by the value function:    0.00653\n",
      "Std. of action_0 over agents            :    3.02132\n",
      "Std. of action_0 over envs              :    3.13537\n",
      "Std. of action_0 over time              :    3.13511\n",
      "Std. of action_1 over agents            :    3.01994\n",
      "Std. of action_1 over envs              :    3.13413\n",
      "Std. of action_1 over time              :    3.13713\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.76000\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1716680938/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1716680938/runner_10000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1716680938/tagger_10000.state_dict'. \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 11 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     153.52\n",
      "Mean action sample time per iter (ms)   :      35.44\n",
      "Mean env. step time per iter (ms)       :     124.91\n",
      "Mean training time per iter (ms)        :      83.23\n",
      "Mean total time per iter (ms)           :     411.14\n",
      "Mean steps per sec (policy eval)        :   65136.34\n",
      "Mean steps per sec (action sample)      :  282199.75\n",
      "Mean steps per sec (env. step)          :   80055.92\n",
      "Mean steps per sec (training time)      :  120155.74\n",
      "Mean steps per sec (total)              :   24322.44\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.19391\n",
      "Policy loss                             :    0.04385\n",
      "Value function loss                     :    0.19675\n",
      "Mean rewards                            :    0.00067\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.09409\n",
      "Mean advantages                         :    0.00917\n",
      "Mean (norm.) advantages                 :    0.00917\n",
      "Mean (discounted) returns               :    0.10326\n",
      "Mean normalized returns                 :    0.10326\n",
      "Mean entropy                            :    4.79456\n",
      "Variance explained by the value function:    0.05735\n",
      "Std. of action_0 over agents            :    3.13898\n",
      "Std. of action_0 over envs              :    3.15234\n",
      "Std. of action_0 over time              :    3.15171\n",
      "Std. of action_1 over agents            :    3.13285\n",
      "Std. of action_1 over envs              :    3.14966\n",
      "Std. of action_1 over time              :    3.14855\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.00737\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    2.01900\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.41383\n",
      "Policy loss                             :    0.64854\n",
      "Value function loss                     :    0.39998\n",
      "Mean rewards                            :    0.01842\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.37053\n",
      "Mean advantages                         :    0.13597\n",
      "Mean (norm.) advantages                 :    0.13597\n",
      "Mean (discounted) returns               :    0.50650\n",
      "Mean normalized returns                 :    0.50650\n",
      "Mean entropy                            :    4.77417\n",
      "Variance explained by the value function:    0.00852\n",
      "Std. of action_0 over agents            :    3.06976\n",
      "Std. of action_0 over envs              :    3.19122\n",
      "Std. of action_0 over time              :    3.19094\n",
      "Std. of action_1 over agents            :    3.00324\n",
      "Std. of action_1 over envs              :    3.10534\n",
      "Std. of action_1 over time              :    3.10584\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.03400\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.91200\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1716680938/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 21 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     144.34\n",
      "Mean action sample time per iter (ms)   :      34.88\n",
      "Mean env. step time per iter (ms)       :     120.06\n",
      "Mean training time per iter (ms)        :      82.08\n",
      "Mean total time per iter (ms)           :     394.89\n",
      "Mean steps per sec (policy eval)        :   69278.47\n",
      "Mean steps per sec (action sample)      :  286664.20\n",
      "Mean steps per sec (env. step)          :   83290.78\n",
      "Mean steps per sec (training time)      :  121827.31\n",
      "Mean steps per sec (total)              :   25323.64\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.21958\n",
      "Policy loss                             :    0.01829\n",
      "Value function loss                     :    0.17498\n",
      "Mean rewards                            :    0.00110\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.11102\n",
      "Mean advantages                         :    0.00388\n",
      "Mean (norm.) advantages                 :    0.00388\n",
      "Mean (discounted) returns               :    0.11490\n",
      "Mean normalized returns                 :    0.11490\n",
      "Mean entropy                            :    4.79236\n",
      "Variance explained by the value function:    0.16044\n",
      "Std. of action_0 over agents            :    3.13233\n",
      "Std. of action_0 over envs              :    3.14723\n",
      "Std. of action_0 over time              :    3.14772\n",
      "Std. of action_1 over agents            :    3.16744\n",
      "Std. of action_1 over envs              :    3.18236\n",
      "Std. of action_1 over time              :    3.18184\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.00556\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.65900\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.50809\n",
      "Policy loss                             :   -0.27274\n",
      "Value function loss                     :    0.38216\n",
      "Mean rewards                            :    0.01754\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.55415\n",
      "Mean advantages                         :   -0.05675\n",
      "Mean (norm.) advantages                 :   -0.05675\n",
      "Mean (discounted) returns               :    0.49740\n",
      "Mean normalized returns                 :    0.49740\n",
      "Mean entropy                            :    4.78326\n",
      "Variance explained by the value function:    0.03231\n",
      "Std. of action_0 over agents            :    3.06234\n",
      "Std. of action_0 over envs              :    3.19114\n",
      "Std. of action_0 over time              :    3.19193\n",
      "Std. of action_1 over agents            :    3.00040\n",
      "Std. of action_1 over envs              :    3.11837\n",
      "Std. of action_1 over time              :    3.12137\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.02633\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.07800\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1716680938/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 31 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     141.65\n",
      "Mean action sample time per iter (ms)   :      35.50\n",
      "Mean env. step time per iter (ms)       :     118.94\n",
      "Mean training time per iter (ms)        :      81.78\n",
      "Mean total time per iter (ms)           :     391.29\n",
      "Mean steps per sec (policy eval)        :   70596.08\n",
      "Mean steps per sec (action sample)      :  281711.40\n",
      "Mean steps per sec (env. step)          :   84073.57\n",
      "Mean steps per sec (training time)      :  122272.16\n",
      "Mean steps per sec (total)              :   25556.29\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.16808\n",
      "Policy loss                             :    0.07016\n",
      "Value function loss                     :    0.14405\n",
      "Mean rewards                            :    0.00100\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.10002\n",
      "Mean advantages                         :    0.01468\n",
      "Mean (norm.) advantages                 :    0.01468\n",
      "Mean (discounted) returns               :    0.11470\n",
      "Mean normalized returns                 :    0.11470\n",
      "Mean entropy                            :    4.79362\n",
      "Variance explained by the value function:    0.30677\n",
      "Std. of action_0 over agents            :    3.14944\n",
      "Std. of action_0 over envs              :    3.16339\n",
      "Std. of action_0 over time              :    3.16307\n",
      "Std. of action_1 over agents            :    3.15155\n",
      "Std. of action_1 over envs              :    3.16627\n",
      "Std. of action_1 over time              :    3.16556\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.00606\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.78300\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.30826\n",
      "Policy loss                             :   -0.07303\n",
      "Value function loss                     :    0.36586\n",
      "Mean rewards                            :    0.01772\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.50325\n",
      "Mean advantages                         :   -0.01490\n",
      "Mean (norm.) advantages                 :   -0.01490\n",
      "Mean (discounted) returns               :    0.48834\n",
      "Mean normalized returns                 :    0.48834\n",
      "Mean entropy                            :    4.77779\n",
      "Variance explained by the value function:    0.06829\n",
      "Std. of action_0 over agents            :    3.08166\n",
      "Std. of action_0 over envs              :    3.20073\n",
      "Std. of action_0 over time              :    3.20086\n",
      "Std. of action_1 over agents            :    2.98480\n",
      "Std. of action_1 over envs              :    3.09058\n",
      "Std. of action_1 over time              :    3.09078\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.02956\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.02800\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1716680938/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 41 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     138.89\n",
      "Mean action sample time per iter (ms)   :      35.70\n",
      "Mean env. step time per iter (ms)       :     118.29\n",
      "Mean training time per iter (ms)        :      81.59\n",
      "Mean total time per iter (ms)           :     387.81\n",
      "Mean steps per sec (policy eval)        :   72001.24\n",
      "Mean steps per sec (action sample)      :  280076.30\n",
      "Mean steps per sec (env. step)          :   84540.75\n",
      "Mean steps per sec (training time)      :  122569.11\n",
      "Mean steps per sec (total)              :   25785.84\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.41814\n",
      "Policy loss                             :   -0.17997\n",
      "Value function loss                     :    0.13994\n",
      "Mean rewards                            :    0.00055\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.13851\n",
      "Mean advantages                         :   -0.03755\n",
      "Mean (norm.) advantages                 :   -0.03755\n",
      "Mean (discounted) returns               :    0.10096\n",
      "Mean normalized returns                 :    0.10096\n",
      "Mean entropy                            :    4.79141\n",
      "Variance explained by the value function:    0.33172\n",
      "Std. of action_0 over agents            :    3.11670\n",
      "Std. of action_0 over envs              :    3.13046\n",
      "Std. of action_0 over time              :    3.12855\n",
      "Std. of action_1 over agents            :    3.17114\n",
      "Std. of action_1 over envs              :    3.18471\n",
      "Std. of action_1 over time              :    3.18445\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.00516\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.45400\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.03221\n",
      "Policy loss                             :    0.26715\n",
      "Value function loss                     :    0.36801\n",
      "Mean rewards                            :    0.01884\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.45605\n",
      "Mean advantages                         :    0.05643\n",
      "Mean (norm.) advantages                 :    0.05643\n",
      "Mean (discounted) returns               :    0.51249\n",
      "Mean normalized returns                 :    0.51249\n",
      "Mean entropy                            :    4.77230\n",
      "Variance explained by the value function:    0.10406\n",
      "Std. of action_0 over agents            :    3.09175\n",
      "Std. of action_0 over envs              :    3.21236\n",
      "Std. of action_0 over time              :    3.21211\n",
      "Std. of action_1 over agents            :    2.97857\n",
      "Std. of action_1 over envs              :    3.09529\n",
      "Std. of action_1 over time              :    3.09622\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.02557\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.19000\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1716680938/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 50 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     137.34\n",
      "Mean action sample time per iter (ms)   :      35.75\n",
      "Mean env. step time per iter (ms)       :     117.86\n",
      "Mean training time per iter (ms)        :      81.44\n",
      "Mean total time per iter (ms)           :     385.67\n",
      "Mean steps per sec (policy eval)        :   72811.33\n",
      "Mean steps per sec (action sample)      :  279719.08\n",
      "Mean steps per sec (env. step)          :   84849.43\n",
      "Mean steps per sec (training time)      :  122787.55\n",
      "Mean steps per sec (total)              :   25929.18\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.36493\n",
      "Policy loss                             :   -0.12678\n",
      "Value function loss                     :    0.13108\n",
      "Mean rewards                            :    0.00146\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.15828\n",
      "Mean advantages                         :   -0.02641\n",
      "Mean (norm.) advantages                 :   -0.02641\n",
      "Mean (discounted) returns               :    0.13186\n",
      "Mean normalized returns                 :    0.13186\n",
      "Mean entropy                            :    4.78922\n",
      "Variance explained by the value function:    0.36182\n",
      "Std. of action_0 over agents            :    3.12495\n",
      "Std. of action_0 over envs              :    3.14017\n",
      "Std. of action_0 over time              :    3.14026\n",
      "Std. of action_1 over agents            :    3.17608\n",
      "Std. of action_1 over envs              :    3.19138\n",
      "Std. of action_1 over time              :    3.19199\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.00485\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    2.28889\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.14265\n",
      "Policy loss                             :    0.37815\n",
      "Value function loss                     :    0.34764\n",
      "Mean rewards                            :    0.01694\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.38420\n",
      "Mean advantages                         :    0.07898\n",
      "Mean (norm.) advantages                 :    0.07898\n",
      "Mean (discounted) returns               :    0.46318\n",
      "Mean normalized returns                 :    0.46318\n",
      "Mean entropy                            :    4.77950\n",
      "Variance explained by the value function:    0.12629\n",
      "Std. of action_0 over agents            :    3.09596\n",
      "Std. of action_0 over envs              :    3.19899\n",
      "Std. of action_0 over time              :    3.20098\n",
      "Std. of action_1 over agents            :    2.98915\n",
      "Std. of action_1 over envs              :    3.09855\n",
      "Std. of action_1 over time              :    3.09884\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.02400\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.77000\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1716680938/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1716680938/runner_500000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1716680938/tagger_500000.state_dict'. \n"
     ]
    }
   ],
   "source": [
    "# Create a wrapped environment object via the EnvWrapper\n",
    "# Ensure that env_backend is set to be \"pycuda\" or \"numba\" (in order to run on the GPU)\n",
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    env_backend='pycuda',\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    ")\n",
    "\n",
    "# Perform training!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warp_drive.training.pytorch_lightning import (\n",
    "    CUDACallback,\n",
    "    PerfStatsCallback,\n",
    "    WarpDriveModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERPROBLEM\n",
    "\n",
    "rationale for why stricter anti-trust enforcement will not fix \n",
    "\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0167629616303757\n",
    "\n",
    "this article shows that the price elasticity is above -2 and -1 and very close to 0 indicating almost insensitivity to price increases.\n",
    "\n",
    "suppose we have a Hospital Emergency room and we want them to charge the equalibrium market price P for one particular medical procedure. However they presently charge a different price. \n",
    "\n",
    "Solution:\n",
    "\n",
    "We set up an auction. This auction is a sealed bid auction. In the sealed bid auction hospitals provide a bid on how much they would like to charge. \n",
    "\n",
    "hospitals then submit bids. Note: it may be possible to have multiple rounds. The idea is that after the winning bid credits will be given. Hospitals can possibly \n",
    "then choose to participate in further rounds for the oppertunity for more credits or abstain after completing atleast one round. \n",
    "\n",
    "The final price will depend on the final auction rule\n",
    "\n",
    "Unknown to answer: should the winning bid be chosen based on: median bid, second lowest bid or second highest bid? \n",
    "\n",
    "the julia package polynomialradii will be used to find and prove that the equalibrium is the lowest possible price under cournot competition\n",
    "\n",
    "Assume that the competitive regieme that needs to be emulated is cournot competiton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warp_drive.utils.constants import Constants\n",
    "from warp_drive.utils.data_feed import DataFeed\n",
    "from warp_drive.utils.gpu_environment_context import CUDAEnvironmentContext\n",
    "\n",
    "\"\"\"\n",
    "this is what agrs for step should look like\n",
    "\n",
    "            args = [\n",
    "                _LOC_X,\n",
    "                _LOC_Y,\n",
    "                _SP,\n",
    "                _DIR,\n",
    "                _ACC,\n",
    "                \"agent_types\",\n",
    "                \"edge_hit_reward_penalty\",\n",
    "                \"edge_hit_penalty\",\n",
    "                \"grid_length\",\n",
    "                \"acceleration_actions\",\n",
    "                \"turn_actions\",\n",
    "                \"max_speed\",\n",
    "                \"num_other_agents_observed\",\n",
    "                \"skill_levels\",\n",
    "                \"runner_exits_game_after_tagged\",\n",
    "                \"still_in_the_game\",\n",
    "                \"use_full_observation\",\n",
    "                _OBSERVATIONS,\n",
    "                _ACTIONS,\n",
    "                \"neighbor_distances\",\n",
    "                \"neighbor_ids_sorted_by_distance\",\n",
    "                \"nearest_neighbor_ids\",\n",
    "                _REWARDS,\n",
    "                \"step_rewards\",\n",
    "                \"num_runners\",\n",
    "                \"distance_margin_for_reward\",\n",
    "                \"tag_reward_for_tagger\",\n",
    "                \"tag_penalty_for_runner\",\n",
    "                \"end_of_game_reward_for_runner\",\n",
    "                \"_done_\",\n",
    "                \"_timestep_\",\n",
    "                (\"n_agents\", \"meta\"),\n",
    "                (\"episode_length\", \"meta\"),\n",
    "            ]\n",
    "\"\"\"\n",
    "\n",
    "class MyDualModeEnvironment(CUDAEnvironmentContext):\n",
    "    \n",
    "    \n",
    "    def get_data_dictionary(self):\n",
    "        data_dict = DataFeed()\n",
    "        ...\n",
    "        return data_dict \n",
    "    \n",
    "    def get_tensor_dictionary(self):\n",
    "        tensor_dict = DataFeed()\n",
    "        ...\n",
    "        return tensor_dict\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset for CPU environment\n",
    "        ...\n",
    "    \n",
    "    def step(self, actions=None):\n",
    "        args = [YOUR_CUDA_STEP_ARGUMENTS]\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            self.cuda_step(\n",
    "                *self.cuda_step_function_feed(args),\n",
    "                block=self.cuda_function_manager.block,\n",
    "                grid=self.cuda_function_manager.grid,\n",
    "            )\n",
    "            return None\n",
    "        else:\n",
    "            ...\n",
    "            return obs, rew, done, inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings.\n",
    "    env=dict(\n",
    "        num_taggers=5,  # number of taggers in the environment\n",
    "        num_runners=100,  # number of runners in the environment\n",
    "        grid_length=20.0,  # length of the (square) grid on which the game is played\n",
    "        episode_length=200,  # episode length in timesteps\n",
    "        max_acceleration=0.1,  # maximum acceleration\n",
    "        min_acceleration=-0.1,  # minimum acceleration\n",
    "        max_turn=2.35,  # 3*pi/4 radians\n",
    "        min_turn=-2.35,  # -3*pi/4 radians\n",
    "        num_acceleration_levels=10,  # number of discretized accelerate actions\n",
    "        num_turn_levels=10,  # number of discretized turn actions\n",
    "        skill_level_tagger=1.0,  # skill level for the tagger\n",
    "        skill_level_runner=1.0,  # skill level for the runner\n",
    "        use_full_observation=False,  # each agent only sees full or partial information\n",
    "        runner_exits_game_after_tagged=True,  # flag to indicate if a runner stays in the game after getting tagged\n",
    "        num_other_agents_observed=10,  # number of other agents each agent can see\n",
    "        tag_reward_for_tagger=10.0,  # positive reward for the tagger upon tagging a runner\n",
    "        tag_penalty_for_runner=-10.0,  # negative reward for the runner upon getting tagged\n",
    "        end_of_game_reward_for_runner=1.0,  # reward at the end of the game for a runner that isn't tagged\n",
    "        tagging_distance=0.02,  # margin between a tagger and runner to consider the runner as 'tagged'.\n",
    "    ),\n",
    "    # Trainer settings.\n",
    "    trainer=dict(\n",
    "        num_envs=50,  # number of environment replicas (number of GPU blocks used)\n",
    "        train_batch_size=10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes=500,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    # Policy network settings.\n",
    "    policy=dict(\n",
    "        runner=dict(\n",
    "            to_train=True,  # flag indicating whether the model needs to be trained\n",
    "            algorithm=\"A2C\",  # algorithm used to train the policy\n",
    "            gamma=0.98,  # discount rate\n",
    "            lr=0.005,  # learning rate\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),  # policy model settings\n",
    "        ),\n",
    "        tagger=dict(\n",
    "            to_train=True,\n",
    "            algorithm=\"A2C\",\n",
    "            gamma=0.98,\n",
    "            lr=0.002,\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    # Checkpoint saving setting.\n",
    "    saving=dict(\n",
    "        metrics_log_freq=10,  # how often (in iterations) to print the metrics\n",
    "        model_params_save_freq=5000,  # how often (in iterations) to save the model parameters\n",
    "        basedir=\"/tmp\",  # base folder used for saving\n",
    "        name=\"continuous_tag\",  # experiment name\n",
    "        tag=\"example\",  # experiment tag\n",
    "    ),\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Instantiate the WarpDrive Module\n",
    "\n",
    "# %% [markdown]\n",
    "# In order to instantiate the WarpDrive module,\n",
    "# we first use an environment wrapper to specify that the environment needs to\n",
    "# be run on the GPU (via the `env_backend` flag).\n",
    "# Also, agents in the environment can share policy models;\n",
    "# so we specify a dictionary to map each policy network model to the list of agent ids using that model.\n",
    "\n",
    "# %%\n",
    "# Create a wrapped environment object via the EnvWrapper\n",
    "# Ensure that env_backend is set to be \"pycuda\" (in order to run on the GPU)\n",
    "# WarpDrive v2 also supports JIT numba backend,\n",
    "# if you have installed Numba, you can set \"numba\" instead of \"pycuda\" too.\n",
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    env_backend=\"pycuda\",\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\n",
    "wd_module = WarpDriveModule(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "log_freq = run_config[\"saving\"][\"metrics_log_freq\"]\n",
    "# Define callbacks.\n",
    "cuda_callback = CUDACallback(module=wd_module)\n",
    "perf_stats_callback = PerfStatsCallback(\n",
    "    batch_size=wd_module.training_batch_size,\n",
    "    num_iters=wd_module.num_iters,\n",
    "    log_freq=log_freq,\n",
    ")\n",
    "\n",
    "# Instantiate the PytorchLightning trainer with the callbacks.\n",
    "# # Also, set the number of gpus to 1, since this notebook uses just a single GPU.\n",
    "num_gpus = 1\n",
    "num_episodes = run_config[\"trainer\"][\"num_episodes\"]\n",
    "episode_length = run_config[\"env\"][\"episode_length\"]\n",
    "training_batch_size = run_config[\"trainer\"][\"train_batch_size\"]\n",
    "num_epochs = int(num_episodes * episode_length / training_batch_size)\n",
    "\n",
    "# Set reload_dataloaders_every_n_epochs=1 to invoke\n",
    "# train_dataloader() each epoch.\n",
    "trainer = tr(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=num_gpus,\n",
    "    callbacks=[cuda_callback, perf_stats_callback],\n",
    "    max_epochs=num_epochs,\n",
    "    reload_dataloaders_every_n_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(wd_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from juliacall import Main as jl, convert as jlconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Updating registry at `~/.julia/registries/General.toml`\n",
      "   Resolving package versions...\n",
      "   Installed IntervalArithmetic ─ v0.22.17\n",
      "   Installed RadiiPolynomial ──── v0.8.15\n",
      "    Updating `~/miniconda3/julia_env/Project.toml`\n",
      "  [f2081a94] + RadiiPolynomial v0.8.15\n",
      "    Updating `~/miniconda3/julia_env/Manifest.toml`\n",
      "  [d1acc4aa] + IntervalArithmetic v0.22.17\n",
      "  [f2081a94] + RadiiPolynomial v0.8.15\n",
      "  [5eaf0fd0] + RoundingEmulator v0.2.1\n",
      "  [4e9b3aee] + CRlibm_jll v1.0.1+0\n",
      "Precompiling project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIntervalArithmetic\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIntervalArithmetic → IntervalArithmeticLinearAlgebraExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mRadiiPolynomial\n",
      "  3 dependencies successfully precompiled in 5 seconds. 103 already precompiled.\n"
     ]
    }
   ],
   "source": [
    "jl.seval('Pkg.add(\"RadiiPolynomial\")')\n",
    "#jl.seval('Pkg.add(\"GraphPlot\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
