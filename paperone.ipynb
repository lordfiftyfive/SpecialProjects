{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the code for identifying  keystone species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#question 1 for implentation: How is the proposer going to work?\n",
    "\n",
    "explore using mult-arm bandits or hyperparameter optimization\n",
    "\n",
    "bayesian information criteria is the reward. the accuracy aspect of BIC will be defined as how close to the data it is. \n",
    "\n",
    "the data used will be MEG or EEG. \n",
    "\n",
    "the policies will be trained normally. number of groupings of agents will be a hyperparameter. these instances will be trained in parallel fashion\n",
    "\n",
    "A2c will be used. \n",
    "\n",
    "A2C will be used because it is decentralized and only algorithms with decentralized execution support groupings.\n",
    "\n",
    "LAter MAPDDG could also be used since it is centralized learning with decentralized execution \n",
    "\n",
    "the number of groupings will increase until and be compared to a smaller number of groupings until reward stops improving. \n",
    "\n",
    "libraries used:\n",
    "\n",
    "RLLIB \n",
    "rl-warpdrive\n",
    "\n",
    "Primary: \n",
    "\n",
    "rl-warpdrive\n",
    "\n",
    "We will have to switch the architecture to PPO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.connectors.env_to_module import FlattenObservations\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.examples.envs.classes.two_step_game import TwoStepGameWithGroupedAgents\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import register_env, get_trainable_cls\n",
    "\n",
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings\n",
    "    env=dict(\n",
    "        num_taggers=5,\n",
    "        num_runners=20,\n",
    "        episode_length=100,\n",
    "        seed=1234,\n",
    "        use_full_observation=False,\n",
    "        num_other_agents_observed=10,\n",
    "        tagging_distance=0.02,\n",
    "    ),\n",
    "    # Trainer settings\n",
    "    trainer=dict(\n",
    "        num_envs=100,  # number of environment replicas (number of GPU blocks used)\n",
    "        train_batch_size=10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes=5000,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    # Policy network settings\n",
    "    policy=dict(\n",
    "        runner=dict(\n",
    "            to_train=True,  # flag indicating whether the model needs to be trained\n",
    "            algorithm=\"A2C\",  # algorithm used to train the policy\n",
    "            gamma=0.98,  # discount rate\n",
    "            lr=0.005,  # learning rate\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),  # policy model settings\n",
    "        ),\n",
    "        tagger=dict(\n",
    "            to_train=True,\n",
    "            algorithm=\"A2C\",\n",
    "            gamma=0.98,\n",
    "            lr=0.002,\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    # Checkpoint saving setting\n",
    "    saving=dict(\n",
    "        metrics_log_freq=10,  # how often (in iterations) to print the metrics\n",
    "        model_params_save_freq=5000,  # how often (in iterations) to save the model parameters\n",
    "        basedir=\"/tmp\",  # base folder used for saving\n",
    "        name=\"continuous_tag\",  # experiment name\n",
    "        tag=\"example\",  # experiment tag\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tag_continuous', 'env': {'num_taggers': 5, 'num_runners': 20, 'episode_length': 100, 'seed': 1234, 'use_full_observation': False, 'num_other_agents_observed': 10, 'tagging_distance': 0.02}, 'trainer': {'num_envs': 100, 'train_batch_size': 10000, 'num_episodes': 5000}, 'policy': {'runner': {'to_train': True, 'algorithm': 'A2C', 'gamma': 0.98, 'lr': 0.005, 'model': {'type': 'fully_connected', 'fc_dims': [256, 256], 'model_ckpt_filepath': ''}}, 'tagger': {'to_train': True, 'algorithm': 'A2C', 'gamma': 0.98, 'lr': 0.002, 'model': {'type': 'fully_connected', 'fc_dims': [256, 256], 'model_ckpt_filepath': ''}}}, 'saving': {'metrics_log_freq': 10, 'model_params_save_freq': 5000, 'basedir': '/tmp', 'name': 'continuous_tag', 'tag': 'example'}}\n"
     ]
    }
   ],
   "source": [
    "print(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPolicies= 3\n",
    "for i in numPolicies:\n",
    "    run_config['policy'][str(i)] = dict(\n",
    "            num_envs=100,  # number of environment replicas (number of GPU blocks used)\n",
    "            train_batch_size=10000,  # total batch size used for training per iteration (across all the environments)\n",
    "            num_episodes=5000,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "        ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "for i in num_policies:\n",
    "    i = dict(\n",
    "            to_train=True,  # flag indicating whether the model needs to be trained\n",
    "            algorithm=\"A2C\",  # algorithm used to train the policy\n",
    "            gamma=0.98,  # discount rate\n",
    "            lr=0.005,  # learning rate\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),  # policy model settings\n",
    "        ),\n",
    "    policy_dict.append(i)\n",
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings\n",
    "    env=dict(\n",
    "        num_taggers=5,\n",
    "        num_runners=20,\n",
    "        episode_length=100,\n",
    "        seed=1234,\n",
    "        use_full_observation=False,\n",
    "        num_other_agents_observed=10,\n",
    "        tagging_distance=0.02,\n",
    "    ),\n",
    "    # Trainer settings\n",
    "    trainer=dict(\n",
    "        num_envs=100,  # number of environment replicas (number of GPU blocks used)\n",
    "        train_batch_size=10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes=5000,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    # Policy network settings\n",
    "    policy=dict(\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify this enviroment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class CichyEnv(MultiAgentEnv):\n",
    "    def __init__(self, images, expert_rdms):\n",
    "        self.images = images\n",
    "        self.expert_rdms = expert_rdms\n",
    "        self.num_agents = 2\n",
    "        self.agent_ids = [\"IT\", \"EVC\"]\n",
    "        \n",
    "        env_config = {\"env_name\":'cichy'}\n",
    "\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"image\": spaces.Box(low=0, high=255, shape=(175, 175, 3), dtype=np.uint8),#this shape should actually probably be 175, 175,3\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "        # Continuous: 1. expending activity units 2. sending signal\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Dict({ \n",
    "            \"IAU\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
    "            \"other_action\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "            }\n",
    "        )\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.state = {agent: {\"image\": self.images[self.current_step], \"other_action\": np.array([0.0])} for agent in self.agent_ids}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        obs, rewards, dones, infos = {}, {}, {}, {}\n",
    "        self.current_step += 1\n",
    "\n",
    "        for agent_id in self.agent_ids:\n",
    "            other_agent_id = \"EVC\" if agent_id == \"IT\" else \"IT\"\n",
    "            obs[agent_id] = {\"image\": self.images[self.current_step], \"other_action\": action_dict[other_agent_id][1]}\n",
    "            rewards[agent_id] = self._calculate_reward(agent_id, action_dict[agent_id])\n",
    "            dones[agent_id] = self.current_step >= len(self.images) - 1\n",
    "            infos[agent_id] = {}\n",
    "\n",
    "        dones[\"__all__\"] = all(dones.values())\n",
    "        return obs, rewards, dones, infos\n",
    "    def _calculate_rewards(self):\n",
    "        rewards = {}\n",
    "        \"\"\"\n",
    "        this function iterates over agents and withen each agent iterates over images to create a cosine similarity. it then calculates the reward\n",
    "        for each agent \n",
    "        \n",
    "        this is configured to do one pass where it generates simulated RDM for one subject\n",
    "        \n",
    "        \"\"\"\n",
    "        for agent_id in self.agent_ids:\n",
    "            actions = np.array(self.actions[agent_id])\n",
    "            num_images = len(self.images)\n",
    "            simulated_rdm = np.zeros((num_images, num_images))#\n",
    "\n",
    "            for i in range(num_images):\n",
    "                for j in range(num_images):\n",
    "                    if i != j:\n",
    "                        sim = cosine_similarity(actions[i].reshape(1, -1), actions[j].reshape(1, -1))[0][0]\n",
    "                        simulated_rdm[i, j] = 1 - sim\n",
    "\n",
    "            expert_rdm = self.expert_rdms[agent_id]\n",
    "            rewards[agent_id] = -np.sum((simulated_rdm - expert_rdm) ** 2)\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of rl-warpdrive enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import heapq\n",
    "\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "from warp_drive.utils.constants import Constants\n",
    "from warp_drive.utils.data_feed import DataFeed\n",
    "from warp_drive.utils.gpu_environment_context import CUDAEnvironmentContext\n",
    "\n",
    "_OBSERVATIONS = Constants.OBSERVATIONS\n",
    "_ACTIONS = Constants.ACTIONS\n",
    "_REWARDS = Constants.REWARDS\n",
    "_LOC_X = \"loc_x\"\n",
    "_LOC_Y = \"loc_y\"\n",
    "_SP = \"speed\"\n",
    "_DIR = \"direction\"\n",
    "_ACC = \"acceleration\"\n",
    "_SIG = \"still_in_the_game\"\n",
    "\n",
    "\n",
    "class TagContinuous(CUDAEnvironmentContext):\n",
    "    \"\"\"\n",
    "    The game of tag on a continuous circular 2D space.\n",
    "    There are some taggers trying to tag several runners.\n",
    "    The taggers want to get as close as possible to the runner, while the runner\n",
    "    wants to get as far away from them as possible.\n",
    "    Once a runner is tagged, he exits the game if runner_exits_game_after_tagged is True\n",
    "    otherwise he continues to run around (and the tagger can catch him again)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_taggers=1,\n",
    "        num_runners=10,\n",
    "        grid_length=10.0,\n",
    "        episode_length=100,\n",
    "        starting_location_x=None,\n",
    "        starting_location_y=None,\n",
    "        starting_directions=None,\n",
    "        seed=None,\n",
    "        max_speed=1.0,\n",
    "        skill_level_runner=1.0,\n",
    "        skill_level_tagger=1.0,\n",
    "        max_acceleration=1.0,\n",
    "        min_acceleration=-1.0,\n",
    "        max_turn=np.pi / 2,\n",
    "        min_turn=-np.pi / 2,\n",
    "        num_acceleration_levels=10,\n",
    "        num_turn_levels=10,\n",
    "        edge_hit_penalty=-0.0,\n",
    "        use_full_observation=True,\n",
    "        num_other_agents_observed=2,\n",
    "        tagging_distance=0.01,\n",
    "        tag_reward_for_tagger=1.0,\n",
    "        step_penalty_for_tagger=-0.0,\n",
    "        tag_penalty_for_runner=-1.0,\n",
    "        step_reward_for_runner=0.0,\n",
    "        end_of_game_reward_for_runner=1.0,\n",
    "        runner_exits_game_after_tagged=True,\n",
    "        env_backend=\"cpu\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_taggers (int, optional): [number of taggers in the environment].\n",
    "                Defaults to 1.\n",
    "            num_runners (int, optional): [number of taggers in the environment].\n",
    "                Defaults to 10.\n",
    "            grid_length (float, optional): [length of the square grid]. Defaults to 10.0\n",
    "            episode_length (int, optional): [episode length]. Defaults to 100.\n",
    "            starting_location_x ([ndarray], optional): [starting x locations of the\n",
    "                agents]. Defaults to None.\n",
    "            starting_location_y ([ndarray], optional): [starting y locations of the\n",
    "            agents]. Defaults to None.\n",
    "            starting_directions ([ndarray], optional): starting orientations\n",
    "                in [0, 2*pi]. Defaults to None.\n",
    "            seed ([type], optional): [seeding parameter]. Defaults to None.\n",
    "            max_speed (float, optional): [max speed of the agents]. Defaults to 1.0\n",
    "            skill_level_runner (float, optional): [runner skill level;\n",
    "                this essentially is a multiplier to the max_speed].\n",
    "                Defaults to 1.0\n",
    "            skill_level_tagger (float, optional): [tagger skill level]. Defaults to 1.0\n",
    "            max_acceleration (float, optional): [the max acceleration]. Defaults to 1.0.\n",
    "            min_acceleration (float, optional): [description]. Defaults to -1.0\n",
    "            max_turn ([type], optional): [description]. Defaults to np.pi/2.\n",
    "            min_turn ([type], optional): [description]. Defaults to -np.pi/2.\n",
    "            num_acceleration_levels (int, optional): [number of acceleration actions\n",
    "                uniformly spaced between max and min acceleration]. Defaults to 10.\n",
    "            num_turn_levels (int, optional): [number of turn actions uniformly spaced\n",
    "                between max and min turns]. Defaults to 10.\n",
    "            edge_hit_penalty (float, optional): [penalty for hitting the edge (wall)].\n",
    "                Defaults to -0.0.\n",
    "            use_full_observation (bool, optional): [boolean indicating whether to\n",
    "                include all the agents' data in the observation or just the nearest\n",
    "                neighbors]. Defaults to True.\n",
    "            num_other_agents_observed (int, optional): [number of nearest neighbors\n",
    "                in the obs (only takes effect when use_full_observation is False)].\n",
    "                Defaults to 2.\n",
    "            tagging_distance (float, optional): [margin between a\n",
    "                tagger and runner to consider the runner as 'tagged'. This multiplies\n",
    "                on top of the grid length]. Defaults to 0.01.\n",
    "            tag_reward_for_tagger (float, optional): [positive reward for the tagger\n",
    "                upon tagging a runner]. Defaults to 1.0\n",
    "            step_penalty_for_tagger (float, optional): [penalty for every step\n",
    "                the game goes on]. Defaults to -0.0.\n",
    "            tag_penalty_for_runner (float, optional): [negative reward for getting\n",
    "                tagged]. Defaults to -1.0\n",
    "            step_reward_for_runner (float, optional): [reward for every step the\n",
    "                runner isn't tagged]. Defaults to 0.0.\n",
    "            end_of_game_reward_for_runner (float, optional): [reward at the end of\n",
    "                the game for a runner that isn't tagged]. Defaults to 1.0.\n",
    "            runner_exits_game_after_tagged (bool, optional): [boolean indicating\n",
    "                whether runners exit the game after getting tagged or can remain in and\n",
    "                continue to get tagged]. Defaults to True.\n",
    "            env_backend (string, optional): [indicate whether to use the CPU\n",
    "                or the GPU (either pycuda or numba) for stepping through the environment].\n",
    "                Defaults to \"cpu\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.float_dtype = np.float32\n",
    "        self.int_dtype = np.int32\n",
    "        # small number to prevent indeterminate cases\n",
    "        self.eps = self.float_dtype(1e-10)\n",
    "\n",
    "        assert num_taggers > 0\n",
    "        self.num_taggers = num_taggers\n",
    "\n",
    "        assert num_runners > 0\n",
    "        self.num_runners = num_runners\n",
    "\n",
    "        self.num_agents = self.num_taggers + self.num_runners\n",
    "\n",
    "        assert episode_length > 0\n",
    "        self.episode_length = episode_length\n",
    "\n",
    "        # Square 2D grid\n",
    "        assert grid_length > 0\n",
    "        self.grid_length = self.float_dtype(grid_length)\n",
    "        self.grid_diagonal = self.grid_length * np.sqrt(2)\n",
    "\n",
    "        # Penalty for hitting the edges\n",
    "        assert edge_hit_penalty <= 0\n",
    "        self.edge_hit_penalty = self.float_dtype(edge_hit_penalty)\n",
    "\n",
    "        # Seeding\n",
    "        self.np_random = np.random\n",
    "        if seed is not None:\n",
    "            self.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#iterate such that a group is added for num_policies. \n",
    "\n",
    "Fix number of agents depending on estimated total number of neurons. THen keep dividing until BIC stops improving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    env_backend='pycuda',\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    ")\n",
    "\n",
    "# Perform training!\n",
    "trainer.train()\n",
    "\n",
    "# Shut off gracefully\n",
    "trainer.graceful_close()\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:the destination header file /root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/warp_drive/cuda_includes/env_config.h already exists; remove and rebuild.\n",
      "WARNING:root:the destination runner file /root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/warp_drive/cuda_includes/env_runner.cu already exists; remove and rebuild.\n",
      "WARNING:root:PyCUDADataManager casts the data 'agent_types' from type int64 to int32\n",
      "WARNING:root:PyCUDADataManager casts the data 'num_runners' from type int64 to int32\n",
      "WARNING:root:PyCUDADataManager casts the data 'step_rewards' from type float64 to float32\n",
      "WARNING:root:PyCUDADataManager casts the data 'skill_levels' from type float64 to float32\n",
      "WARNING:root:PyCUDADataManager casts the data 'observations' from type float64 to float32\n",
      "WARNING:root:PyCUDADataManager casts the data 'processed_observations_batch_tagger' from type float64 to float32\n",
      "WARNING:root:PyCUDADataManager casts the data 'processed_observations_batch_runner' from type float64 to float32\n",
      "/root/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info.\n",
      "  warnings.warn(\"WARNING: failed to get cudart_version from onnxruntime build info.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 1 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     249.74\n",
      "Mean action sample time per iter (ms)   :      38.92\n",
      "Mean env. step time per iter (ms)       :     147.29\n",
      "Mean training time per iter (ms)        :     338.13\n",
      "Mean total time per iter (ms)           :     786.96\n",
      "Mean steps per sec (policy eval)        :   40041.14\n",
      "Mean steps per sec (action sample)      :  256947.87\n",
      "Mean steps per sec (env. step)          :   67891.39\n",
      "Mean steps per sec (training time)      :   29574.42\n",
      "Mean steps per sec (total)              :   12707.11\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.62760\n",
      "Policy loss                             :    0.86481\n",
      "Value function loss                     :    0.24222\n",
      "Mean rewards                            :    0.00151\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :   -0.04964\n",
      "Mean advantages                         :    0.18048\n",
      "Mean (norm.) advantages                 :    0.18048\n",
      "Mean (discounted) returns               :    0.13083\n",
      "Mean normalized returns                 :    0.13083\n",
      "Mean entropy                            :    4.79267\n",
      "Variance explained by the value function:   -0.01869\n",
      "Std. of action_0 over agents            :    3.11550\n",
      "Std. of action_0 over envs              :    3.13089\n",
      "Std. of action_0 over time              :    3.13190\n",
      "Std. of action_1 over agents            :    3.13973\n",
      "Std. of action_1 over envs              :    3.15248\n",
      "Std. of action_1 over time              :    3.15241\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    3.02000\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.76187\n",
      "Policy loss                             :    1.99592\n",
      "Value function loss                     :    0.55967\n",
      "Mean rewards                            :    0.01686\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.05612\n",
      "Mean advantages                         :    0.41646\n",
      "Mean (norm.) advantages                 :    0.41646\n",
      "Mean (discounted) returns               :    0.47258\n",
      "Mean normalized returns                 :    0.47258\n",
      "Mean entropy                            :    4.79294\n",
      "Variance explained by the value function:   -0.00237\n",
      "Std. of action_0 over agents            :    3.02368\n",
      "Std. of action_0 over envs              :    3.13413\n",
      "Std. of action_0 over time              :    3.13305\n",
      "Std. of action_1 over agents            :    3.10244\n",
      "Std. of action_1 over envs              :    3.20738\n",
      "Std. of action_1 over time              :    3.20611\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.43000\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1726355738/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1726355738/runner_10000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1726355738/tagger_10000.state_dict'. \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 11 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     150.19\n",
      "Mean action sample time per iter (ms)   :      36.48\n",
      "Mean env. step time per iter (ms)       :      92.07\n",
      "Mean training time per iter (ms)        :      81.50\n",
      "Mean total time per iter (ms)           :     372.61\n",
      "Mean steps per sec (policy eval)        :   66582.18\n",
      "Mean steps per sec (action sample)      :  274125.56\n",
      "Mean steps per sec (env. step)          :  108611.22\n",
      "Mean steps per sec (training time)      :  122705.60\n",
      "Mean steps per sec (total)              :   26837.51\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.02729\n",
      "Policy loss                             :    0.26487\n",
      "Value function loss                     :    0.20934\n",
      "Mean rewards                            :    0.00106\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.05604\n",
      "Mean advantages                         :    0.05530\n",
      "Mean (norm.) advantages                 :    0.05530\n",
      "Mean (discounted) returns               :    0.11134\n",
      "Mean normalized returns                 :    0.11134\n",
      "Mean entropy                            :    4.79334\n",
      "Variance explained by the value function:    0.02351\n",
      "Std. of action_0 over agents            :    3.13690\n",
      "Std. of action_0 over envs              :    3.15168\n",
      "Std. of action_0 over time              :    3.15137\n",
      "Std. of action_1 over agents            :    3.14592\n",
      "Std. of action_1 over envs              :    3.16047\n",
      "Std. of action_1 over time              :    3.16074\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.00875\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    2.08400\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.28991\n",
      "Policy loss                             :    0.52454\n",
      "Value function loss                     :    0.41481\n",
      "Mean rewards                            :    0.01762\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.39667\n",
      "Mean advantages                         :    0.11026\n",
      "Mean (norm.) advantages                 :    0.11026\n",
      "Mean (discounted) returns               :    0.50693\n",
      "Mean normalized returns                 :    0.50693\n",
      "Mean entropy                            :    4.77551\n",
      "Variance explained by the value function:    0.00823\n",
      "Std. of action_0 over agents            :    3.06843\n",
      "Std. of action_0 over envs              :    3.18605\n",
      "Std. of action_0 over time              :    3.18509\n",
      "Std. of action_1 over agents            :    2.99673\n",
      "Std. of action_1 over envs              :    3.10291\n",
      "Std. of action_1 over time              :    3.10086\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.02991\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.86400\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1726355738/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 21 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     142.45\n",
      "Mean action sample time per iter (ms)   :      35.61\n",
      "Mean env. step time per iter (ms)       :      86.92\n",
      "Mean training time per iter (ms)        :      68.73\n",
      "Mean total time per iter (ms)           :     345.72\n",
      "Mean steps per sec (policy eval)        :   70201.15\n",
      "Mean steps per sec (action sample)      :  280811.01\n",
      "Mean steps per sec (env. step)          :  115050.07\n",
      "Mean steps per sec (training time)      :  145495.87\n",
      "Mean steps per sec (total)              :   28925.33\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.34303\n",
      "Policy loss                             :   -0.10532\n",
      "Value function loss                     :    0.19737\n",
      "Mean rewards                            :    0.00089\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.13017\n",
      "Mean advantages                         :   -0.02192\n",
      "Mean (norm.) advantages                 :   -0.02192\n",
      "Mean (discounted) returns               :    0.10826\n",
      "Mean normalized returns                 :    0.10826\n",
      "Mean entropy                            :    4.79375\n",
      "Variance explained by the value function:    0.05777\n",
      "Std. of action_0 over agents            :    3.11398\n",
      "Std. of action_0 over envs              :    3.12828\n",
      "Std. of action_0 over time              :    3.12690\n",
      "Std. of action_1 over agents            :    3.16909\n",
      "Std. of action_1 over envs              :    3.18266\n",
      "Std. of action_1 over time              :    3.18241\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.00626\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.74600\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.29461\n",
      "Policy loss                             :   -0.05967\n",
      "Value function loss                     :    0.35673\n",
      "Mean rewards                            :    0.01814\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.52273\n",
      "Mean advantages                         :   -0.01195\n",
      "Mean (norm.) advantages                 :   -0.01195\n",
      "Mean (discounted) returns               :    0.51078\n",
      "Mean normalized returns                 :    0.51078\n",
      "Mean entropy                            :    4.77019\n",
      "Variance explained by the value function:    0.03507\n",
      "Std. of action_0 over agents            :    3.04984\n",
      "Std. of action_0 over envs              :    3.15770\n",
      "Std. of action_0 over time              :    3.15927\n",
      "Std. of action_1 over agents            :    2.94846\n",
      "Std. of action_1 over envs              :    3.06716\n",
      "Std. of action_1 over time              :    3.06862\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.02661\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.05200\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1726355738/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 31 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     139.33\n",
      "Mean action sample time per iter (ms)   :      35.32\n",
      "Mean env. step time per iter (ms)       :      84.94\n",
      "Mean training time per iter (ms)        :      64.01\n",
      "Mean total time per iter (ms)           :     335.51\n",
      "Mean steps per sec (policy eval)        :   71770.49\n",
      "Mean steps per sec (action sample)      :  283119.68\n",
      "Mean steps per sec (env. step)          :  117726.63\n",
      "Mean steps per sec (training time)      :  156221.42\n",
      "Mean steps per sec (total)              :   29804.94\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.21495\n",
      "Policy loss                             :    0.02296\n",
      "Value function loss                     :    0.17745\n",
      "Mean rewards                            :    0.00130\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.12033\n",
      "Mean advantages                         :    0.00491\n",
      "Mean (norm.) advantages                 :    0.00491\n",
      "Mean (discounted) returns               :    0.12524\n",
      "Mean normalized returns                 :    0.12524\n",
      "Mean entropy                            :    4.79352\n",
      "Variance explained by the value function:    0.13747\n",
      "Std. of action_0 over agents            :    3.10917\n",
      "Std. of action_0 over envs              :    3.12613\n",
      "Std. of action_0 over time              :    3.12571\n",
      "Std. of action_1 over agents            :    3.15559\n",
      "Std. of action_1 over envs              :    3.16969\n",
      "Std. of action_1 over time              :    3.17006\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.00589\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.64700\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.19376\n",
      "Policy loss                             :    0.04126\n",
      "Value function loss                     :    0.36059\n",
      "Mean rewards                            :    0.01716\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.46596\n",
      "Mean advantages                         :    0.00911\n",
      "Mean (norm.) advantages                 :    0.00911\n",
      "Mean (discounted) returns               :    0.47508\n",
      "Mean normalized returns                 :    0.47508\n",
      "Mean entropy                            :    4.77248\n",
      "Variance explained by the value function:    0.07231\n",
      "Std. of action_0 over agents            :    3.04905\n",
      "Std. of action_0 over envs              :    3.17317\n",
      "Std. of action_0 over time              :    3.17188\n",
      "Std. of action_1 over agents            :    3.01711\n",
      "Std. of action_1 over envs              :    3.12400\n",
      "Std. of action_1 over time              :    3.12281\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.02959\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.07800\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1726355738/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 41 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     138.94\n",
      "Mean action sample time per iter (ms)   :      35.49\n",
      "Mean env. step time per iter (ms)       :      85.00\n",
      "Mean training time per iter (ms)        :      61.91\n",
      "Mean total time per iter (ms)           :     333.31\n",
      "Mean steps per sec (policy eval)        :   71972.30\n",
      "Mean steps per sec (action sample)      :  281752.41\n",
      "Mean steps per sec (env. step)          :  117646.21\n",
      "Mean steps per sec (training time)      :  161534.78\n",
      "Mean steps per sec (total)              :   30001.79\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.12777\n",
      "Policy loss                             :    0.11022\n",
      "Value function loss                     :    0.14915\n",
      "Mean rewards                            :    0.00163\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.11538\n",
      "Mean advantages                         :    0.02314\n",
      "Mean (norm.) advantages                 :    0.02314\n",
      "Mean (discounted) returns               :    0.13852\n",
      "Mean normalized returns                 :    0.13852\n",
      "Mean entropy                            :    4.78971\n",
      "Variance explained by the value function:    0.26510\n",
      "Std. of action_0 over agents            :    3.12031\n",
      "Std. of action_0 over envs              :    3.13458\n",
      "Std. of action_0 over time              :    3.13424\n",
      "Std. of action_1 over agents            :    3.13362\n",
      "Std. of action_1 over envs              :    3.14584\n",
      "Std. of action_1 over time              :    3.14596\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.00780\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    2.15900\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.27675\n",
      "Policy loss                             :   -0.04121\n",
      "Value function loss                     :    0.34081\n",
      "Mean rewards                            :    0.01658\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.46105\n",
      "Mean advantages                         :   -0.00837\n",
      "Mean (norm.) advantages                 :   -0.00837\n",
      "Mean (discounted) returns               :    0.45268\n",
      "Mean normalized returns                 :    0.45268\n",
      "Mean entropy                            :    4.77890\n",
      "Variance explained by the value function:    0.09664\n",
      "Std. of action_0 over agents            :    3.05328\n",
      "Std. of action_0 over envs              :    3.17085\n",
      "Std. of action_0 over time              :    3.17157\n",
      "Std. of action_1 over agents            :    3.03136\n",
      "Std. of action_1 over envs              :    3.14623\n",
      "Std. of action_1 over time              :    3.14563\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.02531\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.84300\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1726355738/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 50 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     138.11\n",
      "Mean action sample time per iter (ms)   :      35.41\n",
      "Mean env. step time per iter (ms)       :      84.63\n",
      "Mean training time per iter (ms)        :      60.81\n",
      "Mean total time per iter (ms)           :     330.89\n",
      "Mean steps per sec (policy eval)        :   72407.80\n",
      "Mean steps per sec (action sample)      :  282379.95\n",
      "Mean steps per sec (env. step)          :  118162.56\n",
      "Mean steps per sec (training time)      :  164436.88\n",
      "Mean steps per sec (total)              :   30221.11\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.02750\n",
      "Policy loss                             :    0.21073\n",
      "Value function loss                     :    0.14607\n",
      "Mean rewards                            :    0.00098\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.06878\n",
      "Mean advantages                         :    0.04401\n",
      "Mean (norm.) advantages                 :    0.04401\n",
      "Mean (discounted) returns               :    0.11279\n",
      "Mean normalized returns                 :    0.11279\n",
      "Mean entropy                            :    4.79379\n",
      "Variance explained by the value function:    0.30700\n",
      "Std. of action_0 over agents            :    3.13160\n",
      "Std. of action_0 over envs              :    3.14701\n",
      "Std. of action_0 over time              :    3.14663\n",
      "Std. of action_1 over agents            :    3.15771\n",
      "Std. of action_1 over envs              :    3.17291\n",
      "Std. of action_1 over time              :    3.17259\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.00626\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.91667\n",
      "Mean episodic steps                     :  100.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.53002\n",
      "Policy loss                             :   -0.29464\n",
      "Value function loss                     :    0.33814\n",
      "Mean rewards                            :    0.01792\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.56102\n",
      "Mean advantages                         :   -0.06158\n",
      "Mean (norm.) advantages                 :   -0.06158\n",
      "Mean (discounted) returns               :    0.49944\n",
      "Mean normalized returns                 :    0.49944\n",
      "Mean entropy                            :    4.77518\n",
      "Variance explained by the value function:    0.13337\n",
      "Std. of action_0 over agents            :    3.04927\n",
      "Std. of action_0 over envs              :    3.16795\n",
      "Std. of action_0 over time              :    3.16966\n",
      "Std. of action_1 over agents            :    3.01873\n",
      "Std. of action_1 over envs              :    3.13541\n",
      "Std. of action_1 over time              :    3.13547\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.02400\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.97111\n",
      "Mean episodic steps                     :  100.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1726355738/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1726355738/runner_500000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1726355738/tagger_500000.state_dict'. \n",
      "[Device 0]: Trainer exits gracefully \n"
     ]
    }
   ],
   "source": [
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    env_backend='pycuda',\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\"\"\"\n",
    "numPolicies= 3\n",
    "for i in numPolicies:\n",
    "    policy_tag_to_agent_id_map[str(i)] = list(env_wrapper.env.taggers),\n",
    "\n",
    "\"\"\"\n",
    "# Create the trainer object\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    ")\n",
    "\n",
    "# Perform training!\n",
    "trainer.train()\n",
    "\n",
    "# Shut off gracefully\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys, copy, math, time, pdb\n",
    "import os.path\n",
    "import random\n",
    "import pdb\n",
    "import csv\n",
    "import argparse\n",
    "import itertools\n",
    "from itertools import permutations, product\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from tqdm import trange\n",
    "\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.wrappers.pettingzoo_wrappers import PettingZooVectorizationParallelWrapper\n",
    "\n",
    "from agilerl.algorithms.matd3 import MATD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/spxuw/DKI/blob/main/DKI.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium.spaces import Dict, Discrete\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from ray import air, tune\n",
    "from ray.air.constants import TRAINING_ITERATION\n",
    "#from ray import rllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each time the algorithm decides it grouping will be created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
