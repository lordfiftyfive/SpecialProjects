{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 17:58:48.555858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.6.0, llvm 15.0.4, commit f1c6fbbd, linux, python 3.10.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 04/17/24 17:58:51.240 8044] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from jaxmarl import *\n",
    "from jaxmarl.environments.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "import numpyro\n",
    "from numpyro import *\n",
    "import numpyro.distributions as dist\n",
    "from numpyro import handlers\n",
    "from numpyro.infer import Predictive, SVI, Trace_ELBO\n",
    "from pyro.infer.abstract_infer import TracePosterior\n",
    "import pyro.poutine as poutine, queue\n",
    "from pyro.contrib.oed.eig import marginal_eig, posterior_eig\n",
    "#optimization\n",
    "import optax\n",
    "from optax.contrib import prodigy\n",
    "\n",
    "from gymnax import *\n",
    "import mo_gymnasium as mo_gym\n",
    "\n",
    "import dynamax\n",
    "\n",
    "from laplace import Laplace #for model selection \n",
    "from laplace.baselaplace import FullLaplace\n",
    "from laplace.curvature.backpack import BackPackGGN\n",
    "\n",
    "#bm4pml is for bayesian model reduction needed to simulate the way brain does structure learning for causal inference \n",
    "from bmr4pml.models import SVIRegression, BMRRegression\n",
    "from bmr4pml.nn import MLP, LeNet, VisionTransformer, resnet18, MlpMixer\n",
    "#from bmr4pml.datasets import load_data\n",
    "from bmr4pml.inference import fit_and_test\n",
    "from bmr4pml.nn.utils import PatchConvEmbed, PatchLinearEmbed\n",
    "\n",
    "#utilities\n",
    "import omegaconf\n",
    "from hydra import *\n",
    "import distrax\n",
    "from jax import *\n",
    "import functools\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import chex\n",
    "from flax import struct\n",
    "import flax.linen as nn\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from flax.training.train_state import TrainState\n",
    "import flashbax as fbx\n",
    "from flax.core import frozen_dict\n",
    "import wandb\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from safetensors.flax import save_file\n",
    "from flax.traverse_util import flatten_dict\n",
    "\n",
    "from jaxmarl import make\n",
    "from jaxmarl.wrappers.baselines import LogWrapper\n",
    "from jaxmarl.environments.smax import map_name_to_scenario\n",
    "from jaxmarl.environments.overcooked import overcooked_layouts\n",
    "from functools import partial\n",
    "\n",
    "# from gymnax.environments import environment, spaces\n",
    "from gymnax.environments.spaces import Box as BoxGymnax, Discrete as DiscreteGymnax\n",
    "from typing import Optional, List, Tuple, Union\n",
    "from jaxmarl.environments.spaces import Box, Discrete, MultiDiscrete\n",
    "from jaxmarl.environments.multi_agent_env import MultiAgentEnv, State\n",
    "\n",
    "from flowjax.flows import block_neural_autoregressive_flow\n",
    "from flowjax.train import fit_to_data\n",
    "from flowjax.distributions import Normal\n",
    "\n",
    "import zuko#uses normalizing flows that is for ELBO then what we have implemented in \n",
    "from strnn.models.strNN import StrNN\n",
    "from torch2jax import j2t, t2j\n",
    "#hyperparameter optimization\n",
    "import sherpa\n",
    "\n",
    "#visualization\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "from ursina import *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpyro.contrib.oed.eig import posterior_eig\n",
    "from numpyro.optim import Adam\n",
    "#from numpyro.infer.abstract_infer import TracePosterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overall objectives:\n",
    "\n",
    "perception\n",
    "learning\n",
    "action\n",
    "learning with full causal inference\n",
    "consciousness \n",
    "\n",
    "stage 1 objectives\n",
    "\n",
    "perception\n",
    "action\n",
    "\n",
    "achieved through implementation of standard EFE and VFE\n",
    "\n",
    "stage 2 objectives\n",
    "\n",
    "learning \n",
    "swap out all neural networks with either normalizing flows or tsetlin machines \n",
    "enhanced EFE and bayesian model reduction \n",
    "\n",
    "stage 3 objectives\n",
    "\n",
    "consciousness\n",
    "\n",
    "\n",
    "part of qmix is going to be reversed with the mixer networks firing before the Emodels and then bmodels networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "app = Ursina()\n",
    "ground = Entity(\n",
    "    model = 'cube',\n",
    "    color = color.magenta,\n",
    "    z = -.1,\n",
    "    y = -3,\n",
    "    origin = (0, .5),\n",
    "    scale = (50, 1, 10),\n",
    "    collider = 'box',\n",
    "    )\n",
    "\n",
    "app.run()                   # opens a window and starts the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.arange(10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class AgentRNN(nn.Module):\n",
    "    # homogenous agent for parameters sharing, assumes all agents have same obs and action dim\n",
    "    action_dim: int\n",
    "    hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        obs, dones = x\n",
    "        embedding = nn.Dense(self.hidden_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "        \n",
    "        q_vals = nn.Dense(self.action_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(embedding)\n",
    "\n",
    "        return hidden, q_vals\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_dim(space):\n",
    "    # get the proper action/obs space from Discrete-MultiDiscrete-Box spaces\n",
    "    if isinstance(space, (DiscreteGymnax, Discrete)):\n",
    "        return space.n\n",
    "    elif isinstance(space, (BoxGymnax, Box, MultiDiscrete)):\n",
    "        return jnp.prod(space.shape)\n",
    "    else:\n",
    "        print(space)\n",
    "        raise NotImplementedError('Current wrapper works only with Discrete/MultiDiscrete/Box action and obs spaces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JaxMARLWrapper(object):\n",
    "    \"\"\"Base class for all jaxmarl wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env: MultiAgentEnv):\n",
    "        self._env = env\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    # def _batchify(self, x: dict):\n",
    "    #     x = jnp.stack([x[a] for a in self._env.agents])\n",
    "    #     return x.reshape((self._env.num_agents, -1))\n",
    "\n",
    "    def _batchify_floats(self, x: dict):\n",
    "        return jnp.stack([x[a] for a in self._env.agents])\n",
    "class CTRolloutManager(JaxMARLWrapper):\n",
    "    \"\"\"\n",
    "    Rollout Manager for Centralized Training of with Parameters Sharing. Used by JaxMARL Q-Learning Baselines.\n",
    "    - Batchify multiple environments (the number of parallel envs is defined by batch_size in __init__).\n",
    "    - Adds a global state (obs[\"__all__\"]) and a global reward (rewards[\"__all__\"]) in the env.step returns.\n",
    "    - Pads the observations of the agents in order to have all the same length.\n",
    "    - Adds an agent id (one hot encoded) to the observation vectors.\n",
    "\n",
    "    By default:\n",
    "    - global_state is the concatenation of all agents' observations.\n",
    "    - global_reward is the sum of all agents' rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: MultiAgentEnv, batch_size:int, training_agents:List=None, preprocess_obs:bool=True):\n",
    "        \n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # the agents to train could differ from the total trainable agents in the env (f.i. if using pretrained agents)\n",
    "        # it's important to know it in order to compute properly the default global rewards and state\n",
    "        self.training_agents = self.agents if training_agents is None else training_agents  \n",
    "        self.preprocess_obs = preprocess_obs  \n",
    "\n",
    "        # TOREMOVE: this is because overcooked doesn't follow other envs conventions\n",
    "        if len(env.observation_spaces) == 0:\n",
    "            self.observation_spaces = {agent:self.observation_space() for agent in self.agents}\n",
    "        if len(env.action_spaces) == 0:\n",
    "            self.action_spaces = {agent:env.action_space() for agent in self.agents}\n",
    "        \n",
    "        # batched action sampling\n",
    "        self.batch_samplers = {agent: jax.jit(jax.vmap(self.action_space(agent).sample, in_axes=0)) for agent in self.agents}\n",
    "\n",
    "        # assumes the observations are flattened vectors\n",
    "        self.max_obs_length = max(list(map(lambda x: get_space_dim(x), self.observation_spaces.values())))#\n",
    "        self.max_action_space = max(list(map(lambda x: get_space_dim(x), self.action_spaces.values())))#lambda x: get_space_dim(x)\n",
    "        self.obs_size = self.max_obs_length + len(self.agents)\n",
    "\n",
    "        # agents ids\n",
    "        self.agents_one_hot = {a:oh for a, oh in zip(self.agents, jnp.eye(len(self.agents)))}\n",
    "        # valid actions\n",
    "        self.valid_actions = {a:jnp.arange(u.n) for a, u in self.action_spaces.items()}\n",
    "        self.valid_actions_oh ={a:jnp.concatenate((jnp.ones(u.n), jnp.zeros(self.max_action_space - u.n))) for a, u in self.action_spaces.items()}\n",
    "\n",
    "        # custom global state and rewards for specific envs\n",
    "        if 'smax' in env.name.lower():\n",
    "            self.global_state = lambda obs, state: obs['world_state']\n",
    "            self.global_reward = lambda rewards: rewards[self.training_agents[0]]\n",
    "        elif 'hanabi' in env.name.lower():\n",
    "            self.global_state = self.hanabi_world_state\n",
    "        elif 'overcooked' in env.name.lower():\n",
    "            self.global_state = lambda obs, state:  jnp.concatenate([obs[agent].ravel() for agent in self.agents], axis=-1)\n",
    "            self.global_reward = lambda rewards: rewards[self.training_agents[0]]\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def batch_reset(self, key):\n",
    "        keys = jax.random.split(key, self.batch_size)\n",
    "        return jax.vmap(self.wrapped_reset, in_axes=0)(keys)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def batch_step(self, key, states, actions):\n",
    "        keys = jax.random.split(key, self.batch_size)\n",
    "        return jax.vmap(self.wrapped_step, in_axes=(0, 0, 0))(keys, states, actions)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def wrapped_reset(self, key):\n",
    "        obs_, state = self._env.reset(key)\n",
    "        if self.preprocess_obs:\n",
    "            obs = jax.tree_util.tree_map(self._preprocess_obs, {agent:obs_[agent] for agent in self.agents}, self.agents_one_hot)\n",
    "        else:\n",
    "            obs = obs_\n",
    "        obs[\"__all__\"] = self.global_state(obs_, state)\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def wrapped_step(self, key, state, actions):\n",
    "        if 'hanabi' in self._env.name.lower():\n",
    "            actions = jax.tree_util.tree_map(lambda x:jnp.expand_dims(x, 0), actions)\n",
    "        obs_, state, reward, done, infos = self._env.step(key, state, actions)\n",
    "        if self.preprocess_obs:\n",
    "            obs = jax.tree_util.tree_map(self._preprocess_obs, {agent:obs_[agent] for agent in self.agents}, self.agents_one_hot)\n",
    "            obs = jax.tree_util.tree_map(lambda d, o: jnp.where(d, 0., o), {agent:done[agent] for agent in self.agents}, obs) # ensure that the obs are 0s for done agents\n",
    "        else:\n",
    "            obs = obs_\n",
    "        obs[\"__all__\"] = self.global_state(obs_, state)\n",
    "        reward[\"__all__\"] = self.global_reward(reward)\n",
    "        return obs, state, reward, done, infos\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def global_state(self, obs, state):\n",
    "        return jnp.concatenate([obs[agent] for agent in self.agents], axis=-1)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def global_reward(self, reward):\n",
    "        return jnp.stack([reward[agent] for agent in self.training_agents]).sum(axis=0) \n",
    "    \n",
    "    def batch_sample(self, key, agent):\n",
    "        return self.batch_samplers[agent](jax.random.split(key, self.batch_size)).astype(int)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _preprocess_obs(self, arr, extra_features):\n",
    "        # flatten\n",
    "        arr = arr.flatten()\n",
    "        # pad the observation vectors to the maximum length\n",
    "        pad_width = [(0, 0)] * (arr.ndim - 1) + [(0, max(0, self.max_obs_length - arr.shape[-1]))]\n",
    "        arr = jnp.pad(arr, pad_width, mode='constant', constant_values=0)\n",
    "        # concatenate the extra features\n",
    "        arr = jnp.concatenate((arr, extra_features), axis=-1)\n",
    "        return arr\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def hanabi_world_state(self, obs, state):\n",
    "        \"\"\" \n",
    "        For each agent: [agent obs, own hand]\n",
    "        \"\"\"\n",
    "        all_obs = jnp.array([obs[agent] for agent in self._env.agents])\n",
    "        hands = state.env_state.player_hands.reshape((self._env.num_agents, -1))\n",
    "        return jnp.concatenate((all_obs, hands), axis=1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class MLP(nn.Module):                    # create a Flax Module dataclass\n",
    "  out_dims: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = nn.Dense(128)(x)                 # create inline Flax Module submodules\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(self.out_dims)(x)       # shape inference\n",
    "    return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refactor ESM completely in jax and numpyro \n",
    "\n",
    "TODO: replace neural networks with TMU rewritten in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create this like the outcome predictor. This is the expected free energy sub-module ESM\n",
    "this will essentially calculate the q(x|y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class OutcomePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScannedRNN(nn.Module):\n",
    "\n",
    "    @partial(\n",
    "        nn.scan,\n",
    "        variable_broadcast=\"params\",\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        split_rngs={\"params\": False},\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"Applies the module.\"\"\"\n",
    "        rnn_state = carry\n",
    "        ins, resets = x\n",
    "        hidden_size = ins.shape[-1]\n",
    "        rnn_state = jnp.where(\n",
    "            resets[:, jnp.newaxis],\n",
    "            self.initialize_carry(hidden_size, *ins.shape[:-1]),\n",
    "            rnn_state,\n",
    "        )\n",
    "        new_rnn_state, y = nn.LSTMCell(hidden_size)(rnn_state, ins)\n",
    "        return new_rnn_state, y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(hidden_size, *batch_size):\n",
    "        # Use a dummy key since the default state init fn is just zeros.\n",
    "        return nn.LSTMCell(hidden_size, parent=None).initialize_carry(\n",
    "            jax.random.PRNGKey(0), (*batch_size, hidden_size)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#pragmatic value function below. Needs to be modified. \n",
    "\n",
    "#in order to actually be the pragmatic value we need to replace S with C where C is a prespecified prior. We will not start with a \n",
    "pragmatic function \n",
    "\n",
    "for now we will only use epsitemic value \n",
    "\n",
    "#P(O|S) is the first part of the epistemic value\n",
    "\n",
    "#so we have Entropy(RSA speaker P(O|S)) - Entropy(Bayesian optimal experimental design ) for epsistemic value \n",
    "\n",
    "BOED = q(y|policy) \n",
    "\n",
    "#speaker is P(O|S)\n",
    "\n",
    "#teh speaker is optimzing over p(observation given state) we do not need to consider listener right now\n",
    "\n",
    "#although listner could be used for inverse problems later \n",
    "\n",
    "#marginal guide = q(y|l)\n",
    "\n",
    "#y is data theta is hidden state and l is the policy\n",
    "\n",
    "Variational free energy = DKL term - surprise\n",
    "\n",
    "surprise will be part of the reward for low level agents. DKL will be the loss in bmodels\n",
    "\n",
    "we should retask JEPPS for variational free energy in bmodels then use inverse maxentropy RL to create the individual q value in bmodel\n",
    "\n",
    "The reward will be staying upright for most agents and staying upright + surprirse for low level agents/nl. where nl = number of low level agents \n",
    "\n",
    "the reward will eventually be the pragmatic value which is the E(ln(P(O|C))) where C is a prior\n",
    "\n",
    "for the purposes of BOED and RSA components of epistemic value that pertain to us a latent \"state\" would correspond to an action or position where we could expect to receive a reward for keeping cartpole upright\n",
    "\n",
    "So the speaker is attempting to find the probability of an observation given an observed collective reward.Perhaps to be more precise the speaker is identifying the independence between observations and states. If states always generate the same observation this quantity would be 0. \n",
    "\n",
    "Can we do marginal here? We may need to use variatonal BOED here. \n",
    "\n",
    "Please note that P(theta|y,d) is proportional (essentially equivalent) to p(theta)*p(y|d, theta) where p(theta) is the prior over the hidden state\n",
    "\n",
    "furthermore the model likelhood is p(y|d, S)\n",
    "\n",
    "posterior APE = posterior predictive entropy\n",
    "\n",
    "posterior APE expectation subscript is p(y,s|d)\n",
    "\n",
    "\n",
    "final conclusion: \n",
    "q(s|d)-APE =  Epistemic value\n",
    "\n",
    "where q(s|d) = P(y|S)/Q(y|d)*q(s|d,y)\n",
    "\n",
    "q(s|d) = p(O|S)/q(y|d)*q(s|d,y) \n",
    "\n",
    "note: q(s|d,y) is simply APE without the expectation or entropy operation applied so q(s|d,y) is simply the posterior part of APE\n",
    "\n",
    "Enhanced expected free energy is \n",
    "\n",
    "-salience -novelty - pragmatic value\n",
    "\n",
    "we can ignore novelty for now and only focus on\n",
    "\n",
    "-salience-novelty\n",
    "\n",
    "Note: adding the novelty term as part of our enhance expected free energy is crucial for learning. However for now we will just stick with classic EFE\n",
    "\n",
    "novelty optimizes theta which is just a list of parameters for the brains internal model. Novelty allows us to carry out parameter learning. For the full causal experience however\n",
    "\n",
    "Friston introduces his bayesian reduction scheme. The idea is that the brain also occasionally (during periods like sleep) does strucuture learning using bayesian model reduction \n",
    "\n",
    "we already know what both collecive qvalues will be. What we should consider doing is wondering: given first qvalue=APE and second one equals ELBO create a PINN that finds the individual qvalue. \n",
    "\n",
    "one way we can do this is by restoring the agent nn and simply setting the loss (for the mixer network) to be the difference between the APE and the first collective qvalue and ELBO and the second collective qvalue respectively. We will attempt this later. \n",
    "\n",
    "for individual agent networks given q_tot and agent_actions during training and q_tot and subset(agent_actions) during execution generate an individual q-value .\n",
    "\n",
    "This should require nothing but a standard neural network with bottem most and top most neuronal agents doing a step before middle layer neuronal agents. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpyro.handlers import scope, seed, trace, scale\n",
    "import torch#replace all stuff that depends on this torch import jax eventually\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import jax as tfp #this is tensorflow probability on jax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose a person want to go to the cafe they like best but there is uncertainty as to whether the cafe is open. \n",
    "\n",
    "the person would first need to optimize for epestemic value to resolve the uncertainty about the state of the cafes and only then can the person maxmize pragmatic value by choosing the cafe they have the greatest prior preference for that is also open. \n",
    "\n",
    "Optimizing only over pragmatic value is only possible when there is no uncertainty in the enviroment and the person can know beforehand which policy will take them closest to their prior preference \n",
    "\n",
    "note: if we want to get rid of pre specified preference in our model we simply need to cut out pragmatic value and only optimize over \n",
    "\n",
    "epistemic value for expected free energy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "example of the kind of model that needs to go into SVI\n",
    "\n",
    "sensitivity = 1.0\n",
    "prior_mean = torch.tensor(7.0)\n",
    "prior_sd = torch.tensor(2.0)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "We can also run multiple ‘rounds’ or iterations of experiments. \n",
    "When doing this, we take the learned model from step 3 and use it as our prior in step 1 for the next round. \n",
    "This approach can be particularly useful because it allows us to design the next experiment based on what has already been learned: the experiments are adaptive.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def model(l):\n",
    "    # Dimension -1 of `l` represents the number of rounds\n",
    "    # Other dimensions are batch dimensions: we indicate this with a plate_stack\n",
    "    with pyro.plate_stack(\"plate\", l.shape[:-1]):\n",
    "        theta = pyro.sample(\"theta\", dist.Normal(prior_mean, prior_sd))\n",
    "        # Share theta across the number of rounds of the experiment\n",
    "        # This represents repeatedly testing the same participant\n",
    "        theta = theta.unsqueeze(-1)\n",
    "        # This define a *logistic regression* model for y\n",
    "        logit_p = sensitivity * (theta - l)\n",
    "        # The event shape represents responses from the same participant\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n",
    "        return y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal experimental design in this case uses a \"marginal guide\" which is q(y|l)\n",
    "\n",
    "q(y|l) is for approximating p(y|l). This marginal guide is not intended for inference since unlike the standard guide that approximates\n",
    "over p(theta|y,l) y in this case corresponds only to observed sample sites \n",
    "\n",
    "In the working memory tutorial, we used the ‘marginal’ estimator to find the EIG. This involved estimating the marginal density \n",
    ". In this experiment, that would be relatively difficult: \n",
    " is 51-dimensional with some rather tricky constraints that make modelling its density difficult. Furthermore, the marginal estimator requires us to know \n",
    " analytically, which we do not.\n",
    "\n",
    "Fortunately, other variational estimators of EIG exist: see [2] for more details. One such variational estimator is the ‘posterior’ estimator exist\n",
    "\n",
    "\n",
    "\n",
    "Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def posterior_eig(\n",
    "    model,\n",
    "    design,\n",
    "    observation_labels,\n",
    "    target_labels,\n",
    "    num_samples,\n",
    "    num_steps,\n",
    "    guide,\n",
    "    optim,\n",
    "    return_history=False,\n",
    "    final_design=None,\n",
    "    final_num_samples=None,\n",
    "    eig=True,\n",
    "    prior_entropy_kwargs={},\n",
    "    *args,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Posterior estimate of expected information gain (EIG) computed from the average posterior entropy (APE)\n",
    "    using :math:`EIG(d) = H[p(\\\\theta)] - APE(d)`. See [1] for full details.\n",
    "\n",
    "    The posterior representation of APE is\n",
    "\n",
    "        :math:`\\\\sup_{q}\\\\ E_{p(y, \\\\theta | d)}[\\\\log q(\\\\theta | y, d)]`\n",
    "\n",
    "    where :math:`q` is any distribution on :math:`\\\\theta`.\n",
    "\n",
    "    This method optimises the loss over a given `guide` family representing :math:`q`.\n",
    "\n",
    "    [1] Foster, Adam, et al. \"Variational Bayesian Optimal Experimental Design.\" arXiv preprint arXiv:1903.05480 (2019).\n",
    "\n",
    "    :param function model: A pyro model accepting `design` as only argument.\n",
    "    :param torch.Tensor design: Tensor representation of design\n",
    "    :param list observation_labels: A subset of the sample sites\n",
    "        present in `model`. These sites are regarded as future observations\n",
    "        and other sites are regarded as latent variables over which a\n",
    "        posterior is to be inferred.\n",
    "    :param list target_labels: A subset of the sample sites over which the posterior\n",
    "        entropy is to be measured.\n",
    "    :param int num_samples: Number of samples per iteration.\n",
    "    :param int num_steps: Number of optimization steps.\n",
    "    :param function guide: guide family for use in the (implicit) posterior estimation.\n",
    "        The parameters of `guide` are optimised to maximise the posterior\n",
    "        objective.\n",
    "    :param pyro.optim.Optim optim: Optimiser to use.\n",
    "    :param bool return_history: If `True`, also returns a tensor giving the loss function\n",
    "        at each step of the optimization.\n",
    "    :param torch.Tensor final_design: The final design tensor to evaluate at. If `None`, uses\n",
    "        `design`.\n",
    "    :param int final_num_samples: The number of samples to use at the final evaluation, If `None,\n",
    "        uses `num_samples`.\n",
    "    :param bool eig: Whether to compute the EIG or the average posterior entropy (APE). The EIG is given by\n",
    "        `EIG = prior entropy - APE`. If `True`, the prior entropy will be estimated analytically,\n",
    "        or by Monte Carlo as appropriate for the `model`. If `False` the APE is returned.\n",
    "    :param dict prior_entropy_kwargs: parameters for estimating the prior entropy: `num_prior_samples` indicating the\n",
    "        number of samples for a MC estimate of prior entropy, and `mean_field` indicating if an analytic form for\n",
    "        a mean-field prior should be tried.\n",
    "    :return: EIG estimate, optionally includes full optimization history\n",
    "    :rtype: torch.Tensor or tuple\n",
    "    \"\"\"\n",
    "    if isinstance(observation_labels, str):\n",
    "        observation_labels = [observation_labels]\n",
    "    if isinstance(target_labels, str):\n",
    "        target_labels = [target_labels]\n",
    "\n",
    "    ape = _posterior_ape(\n",
    "        model,\n",
    "        design,\n",
    "        observation_labels,\n",
    "        target_labels,\n",
    "        num_samples,\n",
    "        num_steps,\n",
    "        guide,\n",
    "        optim,\n",
    "        return_history=return_history,\n",
    "        final_design=final_design,\n",
    "        final_num_samples=final_num_samples,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = 0\n",
    "prior_covariance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the variational posterior estimator not the marginal one\n",
    "\n",
    "#out come predictor will be the guide \n",
    "\n",
    "#final note: we need to modify this to get rid of alpha. The reason is because we do not need any prior if we are only calculating APE \n",
    "\n",
    "def model(polling_allocation):\n",
    "    # This allows us to run many copies of the model in parallel\n",
    "    with pyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "        # Begin by sampling alpha\n",
    "        alpha = numpyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "            prior_mean, covariance_matrix=prior_covariance))\n",
    "\n",
    "        # Sample y conditional on alpha\n",
    "        poll_results = numpyro.sample(\"y\", dist.Binomial(\n",
    "            polling_allocation, logits=alpha).to_event(1))\n",
    "\n",
    "        # Now compute w according to the (approximate) electoral college formula\n",
    "        dem_win = election_winner(alpha)\n",
    "        numpyro.sample(\"w\", dist.Delta(dem_win))\n",
    "\n",
    "        return poll_results, dem_win, alpha\n",
    "\n",
    "\n",
    "class OutcomePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        numpyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        numpyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))\n",
    "eigs = {}\n",
    "best_strategy, best_eig = None, 0\n",
    "aspace = []\n",
    "allocation = aspace\n",
    "for strategy in aspace:#poll_strategies needs to be replaced with action space\n",
    "    print(strategy, end=\" \")\n",
    "    guide = OutcomePredictor()\n",
    "    numpyro.clear_param_store()\n",
    "    # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "    # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "    # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "    ape = posterior_eig(model, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "        guide = ESM()# marginal_guide = posterior predictive entropy d can also be expressed as pi \n",
    "        pyro.clear_param_store()\n",
    "        # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "        # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "        # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)] where w = x  \n",
    "        with pyro.plate_stack(\"plate\", l.shape[:-1]):\n",
    "            theta = pyro.sample(\"theta\", dist.Dirichlet(1))#Normal(0, 0.8))# in our case this will represent the reward\n",
    "            # Share theta across the number of rounds of the experiment\n",
    "            # This represents repeatedly testing the same participant\n",
    "            theta = theta.unsqueeze(-1)\n",
    "            # This define a *logistic regression* model for y\n",
    "            #logit_p = sensitivity * (theta - l)\n",
    "            # The event shape represents responses from the same participant\n",
    "            y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n",
    "            return y\n",
    "            #ape = average posterior entropy\n",
    "        #we dont need tow wory about sample size or optimizer since here we are optimizing over psuedo observations\n",
    "        ape = posterior_eig(model, allocation, \"y\", \"theta\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "        \n",
    "        eigs[strategy] = prior_entropy - ape\n",
    "        print(eigs[strategy].item())\n",
    "\n",
    "        if eigs[strategy] > best_eig:\n",
    "            best_strategy, best_eig = strategy, eigs[strategy]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outstanding problem: we need to make sure that emodel is actually solving for collective expected free energy\n",
    "\n",
    "we know during training that the states of all the other agents and the outisde will be known to any given agent\n",
    "\n",
    "Possible solutions: \n",
    "\n",
    "If during training the states are all known then for the two networks that govern action selection and evaluation we can calculate the collective Expected free energy, and plug that into a standard neural network to recover the individual Qvalue (corresponding to how well it is believed an action the agent takes will fullfill collective free energy). then with the bmodel we can evaluate the collective variational free energy of our super agent and plug that into a standard neural network to recover the qvalue correpsonding to how the individual agents actions contributed to collective variational free energy \n",
    "\n",
    "alternative solution: \n",
    "\n",
    "We have to be careful not to conflate action selection of an individual agent with the action selection of our collectivized \"super-agent\". Our selection neural network will emit a qvalue that will correspond to a traditional RL selection with exploration for each individual agent which will be sent to the mixer network. \n",
    "\n",
    "our target network will also emit a traditional qvalue from a standard neural network. \n",
    "\n",
    "It is only at the level of the loss and reward that we will integrate expected free energy and variational free energy \n",
    "\n",
    "Note: we should probably make the reward expected free energy and the loss variational free energy for this alternative \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function in the model that us fed into posterior eig needs to be the internal model of the cartpole enviroment. it needs to have a prior gravity parameter where new values for this parameter are continuously chosen by by posterior eig and then evaluated by variational free energy.\n",
    "\n",
    "we will be working backwards. First getting q_tot and then getting the individual q for each agent from q_tot. \n",
    "\n",
    "during training the VAEs will have access to both the q_tot and the actions of all of the other agents. during execution they will only have the q_tot and a small subset of actions during any given time step \n",
    "\n",
    "we should combine VFE and EFE into a single agent class but put them under different functions. \n",
    "\n",
    "emodels and bmodels are going to both be substituted for the mixer networks for selection and evaluation\n",
    "\n",
    "meanwhile we will have a single agent network whose job will be to take the obs and the q_tot and turn it into individual q values.\n",
    "\n",
    "observation space for bmodels and emodels will just be cartpole\n",
    "\n",
    "observation space for individual agents will be: all agents during training, some agents during execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(51, 64)\n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.h3 = nn.Linear(64, 1)\n",
    "\n",
    "    def compute_dem_probability(self, y):\n",
    "        z = nn.functional.relu(self.h1(y))\n",
    "        z = nn.functional.relu(self.h2(z))\n",
    "        return self.h3(z)\n",
    "\n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        numpyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        numpyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def model(polling_allocation):\n",
    "    # This allows us to run many copies of the model in parallel\n",
    "    with numpyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "        # Begin by sampling alpha\n",
    "        alpha = numpyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "            prior_mean, covariance_matrix=prior_covariance))\n",
    "\n",
    "        # Sample y conditional on alpha\n",
    "        poll_results = numpyro.sample(\"y\", dist.Binomial(\n",
    "            polling_allocation, logits=alpha).to_event(1))\n",
    "\n",
    "        # Now compute w according to the (approximate) electoral college formula\n",
    "        dem_win = election_winner(alpha)\n",
    "        numpyro.sample(\"w\", dist.Delta(dem_win))\n",
    "\n",
    "        return poll_results, dem_win, alpha\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the default implementation that we need to modify both target network and the other network are handled by agentRNN. Agentrnn in turn implements scannedRNN in itself\n",
    "\n",
    "It may not matter what we put down for the prior because we are only calculating APE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emodel(nn.Module):    # homogenous agent for parameters sharing, assumes all agents have same obs and action dim\n",
    "    action_dim: int\n",
    "    hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        guide = ESM()# marginal_guide = posterior predictive entropy d can also be expressed as pi \n",
    "        numpyro.clear_param_store()\n",
    "        # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "        # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "        # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)] where w = x\n",
    "        def model(polling_allocation):\n",
    "            # This allows us to run many copies of the model in parallel\n",
    "            with numpyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "                # We use a Normal prior for theta\n",
    "                theta = numpyro.sample(\"theta\", dist.Normal(torch.tensor(0.0), torch.tensor(1.0)))\n",
    "                design = polling_allocation\n",
    "                # We use a simple logistic regression model for the likelihood\n",
    "                #we need to replace this with some kind of function that represents whether the cartpole will fall given action \n",
    "                #the gravity parameter will be one of the unknown parameters whose selection in addition to action we will be optimzing for\n",
    "                logit_p = theta - design\n",
    "                y = numpyro.sample(\"y\", dist.Bernoulli(logits=logit_p))\n",
    "\n",
    "                return y\n",
    "        #ape = average posterior entropy\n",
    "        #allocation here means experiment design\n",
    "        #we dont need tow wory about sample size or optimizer since here we are optimizing over psuedo observations\n",
    "        ape = posterior_eig(model, allocation, \"y\", \"theta\", 10, 12500, guide,\n",
    "                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "        \"\"\"\n",
    "        eigs[strategy] = prior_entropy - ape\n",
    "        print(eigs[strategy].item())\n",
    "\n",
    "        if eigs[strategy] > best_eig:\n",
    "            best_strategy, best_eig = strategy, eigs[strategy]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        EFE + obs -> RNN -> IQValue +h\n",
    "\n",
    "        for now we should have this IQvalue equal ape/n and see what happens. \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        emodels and bmodels are going to both be substituted for the mixer networks for selection and evaluation\n",
    "\n",
    "        meanwhile we will have a single agent network whose job will be to take the obs and the q_tot and turn it into individual q values.\n",
    "\n",
    "        observation space for bmodels and emodels will just be cartpole\n",
    "\n",
    "        observation space for individual agents will be: all agents during training, some agents during execution\n",
    "        \n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(self.obs_size, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, num_outputs)\n",
    "        self.n_agents = model_config[\"n_agents\"]\n",
    "\n",
    "    def get_initial_state(self):\n",
    "\n",
    "        return [\n",
    "            self.fc1.weight.new(self.n_agents, self.rnn_hidden_dim).zero_().squeeze(0)\n",
    "        ]\n",
    "\n",
    "    def forward(self,x,input_dict, hidden_state, seq_lens):\n",
    "\n",
    "        #negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "                # Place hidden states on same device as model.\n",
    "        \n",
    "        aspace=[0,1,2]\n",
    "        for strategy, allocation in aspace:#replace poll strategies with action space \n",
    "            print(strategy, end=\" \")\n",
    "            guide = OutcomePredictor()\n",
    "            numpyro.clear_param_store()\n",
    "            # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n",
    "            # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n",
    "            # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n",
    "            y = input_dict[\"obs_flat\"].float()\n",
    "            ape = posterior_eig(model, allocation, \"y\", \"w\", 10, 12500, guide,\n",
    "                                Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n",
    "            eigs[strategy] = -ape\n",
    "            print(eigs[strategy].item())\n",
    "            if eigs[strategy] > best_eig:\n",
    "                best_strategy, best_eig = strategy, eigs[strategy]\n",
    "        \n",
    "        \n",
    "        #####################################################################################\n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = hidden_state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        \"\"\"\n",
    "        what we are now going to do is take \n",
    "        \"\"\"\n",
    "        \n",
    "        q=0\n",
    "        return q, [h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_moons(n: int, sigma: float = 1e-1):\n",
    "    theta = 2 * torch.pi * torch.rand(n)\n",
    "    label = (theta > torch.pi).float()\n",
    "\n",
    "    x = torch.stack((\n",
    "        torch.cos(theta) + label - 1 / 2,\n",
    "        torch.sin(theta) + label / 2 - 1 / 4,\n",
    "    ), axis=-1)\n",
    "\n",
    "    return torch.normal(x, sigma), label\n",
    "\n",
    "samples, labels = two_moons(16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1236, -0.6747],\n",
      "        [ 0.3960,  0.1330],\n",
      "        [ 0.3516,  0.1287],\n",
      "        ...,\n",
      "        [-0.3374, -0.0342],\n",
      "        [ 1.4945, -0.1835],\n",
      "        [ 1.4973,  0.1248]])\n"
     ]
    }
   ],
   "source": [
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0.,  ..., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data.TensorDataset(*two_moons(16384))\n",
    "trainloader = data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch2jax import j2t, t2j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bmodel(nn.Module):\n",
    "    action_dim: int\n",
    "    hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "\n",
    "        \"\"\"\n",
    "        VFE = ELBO\n",
    "\n",
    "        we may need to call emodels withen bmodels\n",
    "        \n",
    "        \n",
    "        input: obs and q_tots\n",
    "        output: individual q values\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        flow = zuko.flows.NSF(features=2, transforms=3, hidden_features=(64, 64))\n",
    "        print(flow)\n",
    "        j2t(x)\n",
    "        trainset = data.TensorDataset(x)\n",
    "        trainloader = data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(flow.parameters(), lr=1e-3)\n",
    "\n",
    "        for epoch in range(8):\n",
    "            losses = []\n",
    "\n",
    "            for x, label in trainloader:\n",
    "                loss = -flow().log_prob(x).mean()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                losses.append(loss.detach())\n",
    "\n",
    "            losses = torch.stack(losses)\n",
    "\n",
    "        samples = flow().sample((1,))\n",
    "\n",
    "        obs, dones = x\n",
    "        embedding = nn.Dense(self.hidden_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "        q_vals = nn.Dense(self.action_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.0))(embedding)\n",
    "        \n",
    "        return hidden, q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetwork(nn.Module):\n",
    "    \"\"\"HyperNetwork for generating weights of QMix' mixing network.\"\"\"\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.hidden_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.))(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixing network for projecting individual agent Q-values into Q_tot. Follows the original QMix implementation.\n",
    "    \"\"\"\n",
    "    embedding_dim: int\n",
    "    hypernet_hidden_dim: int\n",
    "    init_scale: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, q_vals, states):\n",
    "        \n",
    "        n_agents, time_steps, batch_size = q_vals.shape\n",
    "        q_vals = jnp.transpose(q_vals, (1, 2, 0)) # (time_steps, batch_size, n_agents)\n",
    "        \n",
    "        # hypernetwork\n",
    "        w_1 = HyperNetwork(hidden_dim=self.hypernet_hidden_dim, output_dim=self.embedding_dim*n_agents, init_scale=self.init_scale)(states)\n",
    "        b_1 = nn.Dense(self.embedding_dim, kernel_init=orthogonal(self.init_scale), bias_init=constant(0.))(states)\n",
    "        w_2 = HyperNetwork(hidden_dim=self.hypernet_hidden_dim, output_dim=self.embedding_dim, init_scale=self.init_scale)(states)\n",
    "        b_2 = HyperNetwork(hidden_dim=self.embedding_dim, output_dim=1, init_scale=self.init_scale)(states)\n",
    "        \n",
    "        # monotonicity and reshaping\n",
    "        w_1 = jnp.abs(w_1.reshape(time_steps, batch_size, n_agents, self.embedding_dim))\n",
    "        b_1 = b_1.reshape(time_steps, batch_size, 1, self.embedding_dim)\n",
    "        w_2 = jnp.abs(w_2.reshape(time_steps, batch_size, self.embedding_dim, 1))\n",
    "        b_2 = b_2.reshape(time_steps, batch_size, 1, 1)\n",
    "    \n",
    "        # mix\n",
    "        hidden = nn.elu(jnp.matmul(q_vals[:, :, None, :], w_1) + b_1)\n",
    "        q_tot  = jnp.matmul(hidden, w_2) + b_2\n",
    "        \n",
    "        return q_tot.squeeze() # (time_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    obs: dict\n",
    "    actions: dict\n",
    "    rewards: dict\n",
    "    dones: dict\n",
    "    infos: dict\n",
    "\n",
    "\n",
    "def make_train(config, env):\n",
    "\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    def train(rng):\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        wrapped_env = CTRolloutManager(env, batch_size=config[\"NUM_ENVS\"])\n",
    "        test_env = CTRolloutManager(env, batch_size=config[\"NUM_TEST_EPISODES\"]) # batched env for testing (has different batch size)\n",
    "        init_obs, env_state = wrapped_env.batch_reset(_rng)\n",
    "        init_dones = {agent:jnp.zeros((config[\"NUM_ENVS\"]), dtype=bool) for agent in env.agents+['__all__']}\n",
    "\n",
    "        # INIT BUFFER\n",
    "        # to initalize the buffer is necessary to sample a trajectory to know its strucutre\n",
    "        def _env_sample_step(env_state, unused):\n",
    "            rng, key_a, key_s = jax.random.split(jax.random.PRNGKey(0), 3) # use a dummy rng here\n",
    "            key_a = jax.random.split(key_a, env.num_agents)\n",
    "            actions = {agent: wrapped_env.batch_sample(key_a[i], agent) for i, agent in enumerate(env.agents)}\n",
    "            obs, env_state, rewards, dones, infos = wrapped_env.batch_step(key_s, env_state, actions)\n",
    "            transition = Transition(obs, actions, rewards, dones, infos)\n",
    "            return env_state, transition\n",
    "        _, sample_traj = jax.lax.scan(\n",
    "            _env_sample_step, env_state, None, config[\"NUM_STEPS\"]\n",
    "        )\n",
    "        sample_traj_unbatched = jax.tree_map(lambda x: x[:, 0], sample_traj) # remove the NUM_ENV dim\n",
    "        buffer = fbx.make_trajectory_buffer(\n",
    "            max_length_time_axis=config['BUFFER_SIZE']//config['NUM_ENVS'],\n",
    "            min_length_time_axis=config['BUFFER_BATCH_SIZE'],\n",
    "            sample_batch_size=config['BUFFER_BATCH_SIZE'],\n",
    "            add_batch_size=config['NUM_ENVS'],\n",
    "            sample_sequence_length=1,\n",
    "            period=1,\n",
    "        )\n",
    "        buffer_state = buffer.init(sample_traj_unbatched) \n",
    "\n",
    "        # INIT NETWORK\n",
    "        # init agent\n",
    "        agent = AgentRNN(action_dim=wrapped_env.max_action_space, hidden_dim=config[\"AGENT_HIDDEN_DIM\"], init_scale=config['AGENT_INIT_SCALE'])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        if config[\"PARAMETERS_SHARING\"]:\n",
    "            init_x = (\n",
    "                jnp.zeros((1, 1, wrapped_env.obs_size)), # (time_step, batch_size, obs_size)\n",
    "                jnp.zeros((1, 1)) # (time_step, batch size)\n",
    "            )\n",
    "            init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], 1) # (batch_size, hidden_dim)\n",
    "            agent_params = agent.init(_rng, init_hs, init_x)\n",
    "        else:\n",
    "            init_x = (\n",
    "                jnp.zeros((len(env.agents), 1, 1, wrapped_env.obs_size)), # (time_step, batch_size, obs_size)\n",
    "                jnp.zeros((len(env.agents), 1, 1)) # (time_step, batch size)\n",
    "            )\n",
    "            init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents),  1) # (n_agents, batch_size, hidden_dim)\n",
    "            rngs = jax.random.split(_rng, len(env.agents)) # a random init for each agent\n",
    "            agent_params = jax.vmap(agent.init, in_axes=(0, 0, 0))(rngs, init_hs, init_x)\n",
    "\n",
    "        # init mixer\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros((len(env.agents), 1, 1))\n",
    "        state_size = sample_traj.obs['__all__'].shape[-1]  # get the state shape from the buffer\n",
    "        init_state = jnp.zeros((1, 1, state_size))\n",
    "        mixer = MixingNetwork(config['MIXER_EMBEDDING_DIM'], config[\"MIXER_HYPERNET_HIDDEN_DIM\"], config['MIXER_INIT_SCALE'])\n",
    "        mixer_params = mixer.init(_rng, init_x, init_state)\n",
    "\n",
    "        # init optimizer\n",
    "        network_params = frozen_dict.freeze({'agent':agent_params, 'mixer':mixer_params})\n",
    "        def linear_schedule(count):\n",
    "            frac = 1.0 - (count / config[\"NUM_UPDATES\"])\n",
    "            return config[\"LR\"] * frac\n",
    "        lr = linear_schedule if config.get('LR_LINEAR_DECAY', False) else config['LR']\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adamw(learning_rate=lr, eps=config['EPS_ADAM'], weight_decay=config['WEIGHT_DECAY_ADAM']),\n",
    "        )\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=None,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "        # target network params\n",
    "        target_network_params = jax.tree_map(lambda x: jnp.copy(x), train_state.params)\n",
    "        \"\"\"\n",
    "        # INIT EXPLORATION STRATEGY\n",
    "        explorer = EpsilonGreedy(\n",
    "            start_e=config[\"EPSILON_START\"],\n",
    "            end_e=config[\"EPSILON_FINISH\"],\n",
    "            duration=config[\"EPSILON_ANNEAL_TIME\"]\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # depending if using parameters sharing or not, q-values are computed using one or multiple parameters\n",
    "        if config[\"PARAMETERS_SHARING\"]:\n",
    "            def homogeneous_pass(params, hidden_state, obs, dones):\n",
    "                # concatenate agents and parallel envs to process them in one batch\n",
    "                agents, flatten_agents_obs = zip(*obs.items())\n",
    "                original_shape = flatten_agents_obs[0].shape # assumes obs shape is the same for all agents\n",
    "                batched_input = (\n",
    "                    jnp.concatenate(flatten_agents_obs, axis=1), # (time_step, n_agents*n_envs, obs_size)\n",
    "                    jnp.concatenate([dones[agent] for agent in agents], axis=1), # ensure to not pass other keys (like __all__)\n",
    "                )\n",
    "                hidden_state, q_vals = agent.apply(params, hidden_state, batched_input)\n",
    "                q_vals = q_vals.reshape(original_shape[0], len(agents), *original_shape[1:-1], -1) # (time_steps, n_agents, n_envs, action_dim)\n",
    "                q_vals = {a:q_vals[:,i] for i,a in enumerate(agents)}\n",
    "                return hidden_state, q_vals\n",
    "        else:\n",
    "            def homogeneous_pass(params, hidden_state, obs, dones):\n",
    "                # homogeneous pass vmapped in respect to the agents parameters (i.e., no parameter sharing)\n",
    "                agents, flatten_agents_obs = zip(*obs.items())\n",
    "                batched_input = (\n",
    "                    jnp.stack(flatten_agents_obs, axis=0), # (n_agents, time_step, n_envs, obs_size)\n",
    "                    jnp.stack([dones[agent] for agent in agents], axis=0), # ensure to not pass other keys (like __all__)\n",
    "                )\n",
    "                # computes the q_vals with the params of each agent separately by vmapping\n",
    "                hidden_state, q_vals = jax.vmap(agent.apply, in_axes=0)(params, hidden_state, batched_input)\n",
    "                q_vals = {a:q_vals[i] for i,a in enumerate(agents)}\n",
    "                return hidden_state, q_vals\n",
    "\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "\n",
    "            train_state, target_network_params, env_state, buffer_state, time_state, init_obs, init_dones, test_metrics, rng = runner_state\n",
    "\n",
    "            # EPISODE STEP\n",
    "            def _env_step(step_state, unused):\n",
    "\n",
    "                params, env_state, last_obs, last_dones, hstate, rng, t = step_state\n",
    "\n",
    "                # prepare rngs for actions and step\n",
    "                rng, key_a, key_s = jax.random.split(rng, 3)\n",
    "\n",
    "                # SELECT ACTION\n",
    "                # add a dummy time_step dimension to the agent input\n",
    "                obs_   = {a:last_obs[a] for a in env.agents} # ensure to not pass the global state (obs[\"__all__\"]) to the network\n",
    "                obs_   = jax.tree_map(lambda x: x[jnp.newaxis, :], obs_)\n",
    "                dones_ = jax.tree_map(lambda x: x[jnp.newaxis, :], last_dones)\n",
    "                # get the q_values from the agent netwoek\n",
    "                hstate, q_vals = homogeneous_pass(params, hstate, obs_, dones_)\n",
    "                # remove the dummy time_step dimension and index qs by the valid actions of each agent \n",
    "                valid_q_vals = jax.tree_util.tree_map(lambda q, valid_idx: q.squeeze(0)[..., valid_idx], q_vals, wrapped_env.valid_actions)\n",
    "                # explore with epsilon greedy_exploration\n",
    "                actions = explorer.choose_actions(valid_q_vals, t, key_a)\n",
    "\n",
    "                # STEP ENV\n",
    "                obs, env_state, rewards, dones, infos = wrapped_env.batch_step(key_s, env_state, actions)\n",
    "                transition = Transition(last_obs, actions, rewards, dones, infos)\n",
    "\n",
    "                step_state = (params, env_state, obs, dones, hstate, rng, t+1)\n",
    "                return step_state, transition\n",
    "\n",
    "\n",
    "            # prepare the step state and collect the episode trajectory\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            if config[\"PARAMETERS_SHARING\"]:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents)*config[\"NUM_ENVS\"]) # (n_agents*n_envs, hs_size)\n",
    "            else:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents), config[\"NUM_ENVS\"]) # (n_agents, n_envs, hs_size)\n",
    "\n",
    "            step_state = (\n",
    "                train_state.params['agent'],\n",
    "                env_state,\n",
    "                init_obs,\n",
    "                init_dones,\n",
    "                hstate, \n",
    "                _rng,\n",
    "                time_state['timesteps'] # t is needed to compute epsilon\n",
    "            )\n",
    "\n",
    "            step_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, step_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # BUFFER UPDATE: save the collected trajectory in the buffer\n",
    "            buffer_traj_batch = jax.tree_util.tree_map(\n",
    "                lambda x:jnp.swapaxes(x, 0, 1)[:, np.newaxis], # put the batch dim first and add a dummy sequence dim\n",
    "                traj_batch\n",
    "            ) # (num_envs, 1, time_steps, ...)\n",
    "            buffer_state = buffer.add(buffer_state, buffer_traj_batch)\n",
    "\n",
    "            # LEARN PHASE\n",
    "            def q_of_action(q, u):\n",
    "                \"\"\"index the q_values with action indices\"\"\"\n",
    "                q_u = jnp.take_along_axis(q, jnp.expand_dims(u, axis=-1), axis=-1)\n",
    "                return jnp.squeeze(q_u, axis=-1)\n",
    "\n",
    "            def _loss_fn(params, target_network_params, init_hstate, learn_traj):\n",
    "\n",
    "                obs_ = {a:learn_traj.obs[a] for a in env.agents} # ensure to not pass the global state (obs[\"__all__\"]) to the network\n",
    "                _, q_vals = homogeneous_pass(params['agent'], init_hstate, obs_, learn_traj.dones)\n",
    "                _, target_q_vals = homogeneous_pass(target_network_params['agent'], init_hstate, obs_, learn_traj.dones)\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                right now it is computing individual q values followed by mixer network qvalues\n",
    "\n",
    "                we will have to reverse this order \n",
    "                \"\"\"\n",
    "\n",
    "                # get the q_vals of the taken actions (with exploration) for each agent\n",
    "                chosen_action_qvals = jax.tree_map(\n",
    "                    lambda q, u: q_of_action(q, u)[:-1], # avoid last timestep\n",
    "                    q_vals,\n",
    "                    learn_traj.actions\n",
    "                )\n",
    "\n",
    "                # get the target q value of the greedy actions for each agent\n",
    "                valid_q_vals = jax.tree_util.tree_map(lambda q, valid_idx: q[..., valid_idx], q_vals, wrapped_env.valid_actions)\n",
    "                target_max_qvals = jax.tree_map(\n",
    "                    lambda t_q, q: q_of_action(t_q, jnp.argmax(q, axis=-1))[1:], # avoid first timestep\n",
    "                    target_q_vals,\n",
    "                    jax.lax.stop_gradient(valid_q_vals)\n",
    "                )\n",
    "\n",
    "                # compute q_tot with the mixer network\n",
    "                chosen_action_qvals_mix = mixer.apply(\n",
    "                    params['mixer'], \n",
    "                    jnp.stack(list(chosen_action_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][:-1] # avoid last timestep\n",
    "                )\n",
    "                target_max_qvals_mix = mixer.apply(\n",
    "                    target_network_params['mixer'], \n",
    "                    jnp.stack(list(target_max_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][1:] # avoid first timestep\n",
    "                )\n",
    "\n",
    "                # compute target\n",
    "                if config.get('TD_LAMBDA_LOSS', True):\n",
    "                    # time difference loss\n",
    "                    def _td_lambda_target(ret, values):\n",
    "                        reward, done, target_qs = values\n",
    "                        ret = jnp.where(\n",
    "                            done,\n",
    "                            target_qs,\n",
    "                            ret*config['TD_LAMBDA']*config['GAMMA']\n",
    "                            + reward\n",
    "                            + (1-config['TD_LAMBDA'])*config['GAMMA']*(1-done)*target_qs\n",
    "                        )\n",
    "                        return ret, ret\n",
    "\n",
    "                    ret = target_max_qvals_mix[-1] * (1-learn_traj.dones['__all__'][-1])\n",
    "                    ret, td_targets = jax.lax.scan(\n",
    "                        _td_lambda_target,\n",
    "                        ret,\n",
    "                        (learn_traj.rewards['__all__'][-2::-1], learn_traj.dones['__all__'][-2::-1], target_max_qvals_mix[-1::-1])\n",
    "                    )\n",
    "                    targets = td_targets[::-1]\n",
    "                    loss = jnp.mean(0.5*((chosen_action_qvals_mix - jax.lax.stop_gradient(targets))**2))\n",
    "                else:\n",
    "                    # standard DQN loss\n",
    "                    targets = (\n",
    "                        learn_traj.rewards['__all__'][:-1]\n",
    "                        + config['GAMMA']*(1-learn_traj.dones['__all__'][:-1])*target_max_qvals_mix\n",
    "                    )\n",
    "                    loss = jnp.mean((chosen_action_qvals_mix - jax.lax.stop_gradient(targets))**2)\n",
    "                \n",
    "                return loss\n",
    "\n",
    "\n",
    "            # sample a batched trajectory from the buffer and set the time step dim in first axis\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            learn_traj = buffer.sample(buffer_state, _rng).experience # (batch_size, 1, max_time_steps, ...)\n",
    "            learn_traj = jax.tree_map(\n",
    "                lambda x: jnp.swapaxes(x[:, 0], 0, 1), # remove the dummy sequence dim (1) and swap batch and temporal dims\n",
    "                learn_traj\n",
    "            ) # (max_time_steps, batch_size, ...)\n",
    "            if config[\"PARAMETERS_SHARING\"]:\n",
    "                init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents)*config[\"BUFFER_BATCH_SIZE\"]) # (n_agents*batch_size, hs_size)\n",
    "            else:\n",
    "                init_hs = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents), config[\"BUFFER_BATCH_SIZE\"]) # (n_agents, batch_size, hs_size)\n",
    "\n",
    "            # compute loss and optimize grad\n",
    "            grad_fn = jax.value_and_grad(_loss_fn, has_aux=False)\n",
    "            loss, grads = grad_fn(train_state.params, target_network_params, init_hs, learn_traj)\n",
    "            train_state = train_state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "            # UPDATE THE VARIABLES AND RETURN\n",
    "            # reset the environment\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            init_obs, env_state = wrapped_env.batch_reset(_rng)\n",
    "            init_dones = {agent:jnp.zeros((config[\"NUM_ENVS\"]), dtype=bool) for agent in env.agents+['__all__']}\n",
    "\n",
    "            # update the states\n",
    "            time_state['timesteps'] = step_state[-1]\n",
    "            time_state['updates']   = time_state['updates'] + 1\n",
    "\n",
    "            # update the target network if necessary\n",
    "            target_network_params = jax.lax.cond(\n",
    "                time_state['updates'] % config['TARGET_UPDATE_INTERVAL'] == 0,\n",
    "                lambda _: jax.tree_map(lambda x: jnp.copy(x), train_state.params),\n",
    "                lambda _: target_network_params,\n",
    "                operand=None\n",
    "            )\n",
    "\n",
    "            # update the greedy rewards\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            test_metrics = jax.lax.cond(\n",
    "                time_state['updates'] % (config[\"TEST_INTERVAL\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]) == 0,\n",
    "                lambda _: get_greedy_metrics(_rng, train_state.params['agent'], time_state),\n",
    "                lambda _: test_metrics,\n",
    "                operand=None\n",
    "            )\n",
    "\n",
    "            # update the returning metrics\n",
    "            metrics = {\n",
    "                'timesteps': time_state['timesteps']*config['NUM_ENVS'],\n",
    "                'updates' : time_state['updates'],\n",
    "                'loss': loss,\n",
    "                'rewards': jax.tree_util.tree_map(lambda x: jnp.sum(x, axis=0).mean(), traj_batch.rewards),\n",
    "                'eps': explorer.get_epsilon(time_state['timesteps'])\n",
    "            }\n",
    "            metrics['test_metrics'] = test_metrics # add the test metrics dictionary\n",
    "\n",
    "            if config.get('WANDB_ONLINE_REPORT', False):\n",
    "                def callback(metrics, infos):\n",
    "                    info_metrics = {\n",
    "                        k:v[...,0][infos[\"returned_episode\"][..., 0]].mean()\n",
    "                        for k,v in infos.items() if k!=\"returned_episode\"\n",
    "                    }\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"returns\": metrics['rewards']['__all__'].mean(),\n",
    "                            \"timestep\": metrics['timesteps'],\n",
    "                            \"updates\": metrics['updates'],\n",
    "                            \"loss\": metrics['loss'],\n",
    "                            'epsilon': metrics['eps'],\n",
    "                            **info_metrics,\n",
    "                            **{k:v.mean() for k, v in metrics['test_metrics'].items()}\n",
    "                        }\n",
    "                    )\n",
    "                jax.debug.callback(callback, metrics, traj_batch.infos)\n",
    "\n",
    "            runner_state = (\n",
    "                train_state,\n",
    "                target_network_params,\n",
    "                env_state,\n",
    "                buffer_state,\n",
    "                time_state,\n",
    "                init_obs,\n",
    "                init_dones,\n",
    "                test_metrics,\n",
    "                rng\n",
    "            )\n",
    "\n",
    "            return runner_state, metrics\n",
    "\n",
    "        def get_greedy_metrics(rng, params, time_state):\n",
    "            \"\"\"Help function to test greedy policy during training\"\"\"\n",
    "            def _greedy_env_step(step_state, unused):\n",
    "                params, env_state, last_obs, last_dones, hstate, rng = step_state\n",
    "                rng, key_s = jax.random.split(rng)\n",
    "                obs_   = {a:last_obs[a] for a in env.agents}\n",
    "                obs_   = jax.tree_map(lambda x: x[np.newaxis, :], obs_)\n",
    "                dones_ = jax.tree_map(lambda x: x[np.newaxis, :], last_dones)\n",
    "                hstate, q_vals = homogeneous_pass(params, hstate, obs_, dones_)\n",
    "                actions = jax.tree_util.tree_map(lambda q, valid_idx: jnp.argmax(q.squeeze(0)[..., valid_idx], axis=-1), q_vals, test_env.valid_actions)\n",
    "                obs, env_state, rewards, dones, infos = test_env.batch_step(key_s, env_state, actions)\n",
    "                step_state = (params, env_state, obs, dones, hstate, rng)\n",
    "                return step_state, (rewards, dones, infos)\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            init_obs, env_state = test_env.batch_reset(_rng)\n",
    "            init_dones = {agent:jnp.zeros((config[\"NUM_TEST_EPISODES\"]), dtype=bool) for agent in env.agents+['__all__']}\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            if config[\"PARAMETERS_SHARING\"]:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents)*config[\"NUM_TEST_EPISODES\"]) # (n_agents*n_envs, hs_size)\n",
    "            else:\n",
    "                hstate = ScannedRNN.initialize_carry(config['AGENT_HIDDEN_DIM'], len(env.agents), config[\"NUM_TEST_EPISODES\"]) # (n_agents, n_envs, hs_size)\n",
    "            step_state = (\n",
    "                params,\n",
    "                env_state,\n",
    "                init_obs,\n",
    "                init_dones,\n",
    "                hstate, \n",
    "                _rng,\n",
    "            )\n",
    "            step_state, (rewards, dones, infos) = jax.lax.scan(\n",
    "                _greedy_env_step, step_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            # compute the metrics of the first episode that is done for each parallel env\n",
    "            def first_episode_returns(rewards, dones):\n",
    "                first_done = jax.lax.select(jnp.argmax(dones)==0., dones.size, jnp.argmax(dones))\n",
    "                first_episode_mask = jnp.where(jnp.arange(dones.size) <= first_done, True, False)\n",
    "                return jnp.where(first_episode_mask, rewards, 0.).sum()\n",
    "            all_dones = dones['__all__']\n",
    "            first_returns = jax.tree_map(lambda r: jax.vmap(first_episode_returns, in_axes=1)(r, all_dones), rewards)\n",
    "            first_infos   = jax.tree_map(lambda i: jax.vmap(first_episode_returns, in_axes=1)(i[..., 0], all_dones), infos)\n",
    "            metrics = {\n",
    "                'test_returns': first_returns['__all__'],# episode returns\n",
    "                **{'test_'+k:v for k,v in first_infos.items()}\n",
    "            }\n",
    "            if config.get('VERBOSE', False):\n",
    "                def callback(timestep, val):\n",
    "                    print(f\"Timestep: {timestep}, return: {val}\")\n",
    "                jax.debug.callback(callback, time_state['timesteps']*config['NUM_ENVS'], first_returns['__all__'].mean())\n",
    "            return metrics\n",
    "        \n",
    "        time_state = {\n",
    "            'timesteps':jnp.array(0),\n",
    "            'updates':  jnp.array(0)\n",
    "        }\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        test_metrics = get_greedy_metrics(_rng, train_state.params['agent'],time_state) # initial greedy metrics\n",
    "\n",
    "        # train\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (\n",
    "            train_state,\n",
    "            target_network_params,\n",
    "            env_state,\n",
    "            buffer_state,\n",
    "            time_state,\n",
    "            init_obs,\n",
    "            init_dones,\n",
    "            test_metrics,\n",
    "            _rng\n",
    "        )\n",
    "        runner_state, metrics = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {'runner_state':runner_state, 'metrics':metrics}\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the def _loss_fn from the jaxmarl qmix from the class make_train. We have to fogure out how to get the losses of swap out agent and target networks for bmodels and emodels\n",
    "\n",
    "also in pegasus we do not use epsilon greedy or any other explorer because it is automatically handled by bmodels and optimization of expected free energy. we will have to figure out how to modify\n",
    "the jaxmarl implementation and remove the epsiolon greedy explorer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "            def _loss_fn(params, target_network_params, init_hstate, learn_traj):\n",
    "\n",
    "                obs_ = {a:learn_traj.obs[a] for a in env.agents} # ensure to not pass the global state (obs[\"__all__\"]) to the network\n",
    "                _, q_vals = homogeneous_pass(params['agent'], init_hstate, obs_, learn_traj.dones)\n",
    "                _, target_q_vals = homogeneous_pass(target_network_params['agent'], init_hstate, obs_, learn_traj.dones)\n",
    "\n",
    "                # get the q_vals of the taken actions (with exploration) for each agent\n",
    "                chosen_action_qvals = jax.tree_map(\n",
    "                    lambda q, u: q_of_action(q, u)[:-1], # avoid last timestep\n",
    "                    q_vals,\n",
    "                    learn_traj.actions\n",
    "                )\n",
    "\n",
    "                # get the target q value of the greedy actions for each agent\n",
    "                valid_q_vals = jax.tree_util.tree_map(lambda q, valid_idx: q[..., valid_idx], q_vals, wrapped_env.valid_actions)\n",
    "                target_max_qvals = jax.tree_map(\n",
    "                    lambda t_q, q: q_of_action(t_q, jnp.argmax(q, axis=-1))[1:], # avoid first timestep\n",
    "                    target_q_vals,\n",
    "                    jax.lax.stop_gradient(valid_q_vals)\n",
    "                )\n",
    "\n",
    "                # compute q_tot with the mixer network\n",
    "                chosen_action_qvals_mix = mixer.apply(\n",
    "                    params['mixer'], \n",
    "                    jnp.stack(list(chosen_action_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][:-1] # avoid last timestep\n",
    "                )\n",
    "                target_max_qvals_mix = mixer.apply(\n",
    "                    target_network_params['mixer'], \n",
    "                    jnp.stack(list(target_max_qvals.values())),\n",
    "                    learn_traj.obs['__all__'][1:] # avoid first timestep\n",
    "                )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from juliacall import Main as jl, convert as jlconvert\n",
    "#julia_module1 = pyjulia.Pyjulia(\"C://Users//subar//.julia//packages//CUDA//35NC6//lib//cutensor//test//base.jl\")\n",
    "#jul2 = pyjulia.Pyjulia(\"C://Users//subar//.julia//packages//NeuralPDE//OPjbo//src//NeuralPDE.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl.seval(\"using Pkg\")\n",
    "jl.seval('Pkg.add(\"Starlight\")')\n",
    "#jl.seval(\"using Starlight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jl.seval(\"using Starlight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl.seval(\"using Flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Info: Precompiling TMLE [8afdd2fb-6e73-43df-8b62-b1650cd9c8cf]\n",
      "[ Info: Precompiling CategoricalArraysStructTypesExt [e0b1e8f7-f9d4-55ed-aa4c-3cb4a66b5cfe]\n"
     ]
    }
   ],
   "source": [
    "jl.seval(\"using TMLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = jl.Chain(\n",
    "    jl.Dense(1, 10, jl.relu),\n",
    "    jl.Dense(10, 10, jl.relu),\n",
    "    jl.Dense(10, 10, jl.relu),\n",
    "    jl.Dense(10, 1),\n",
    ")\n",
    "loss = jl.seval(\"m -> (x, y) -> Flux.Losses.mse(m(x), y)\")(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
